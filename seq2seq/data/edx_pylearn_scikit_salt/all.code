self.eof_index = len(string)
return self.eof_index
has_previous_line = self.start_line > 1 if has_previous_line: line_to_check = string_lines.line_number_to_line(self.start_line - 1) self._mark_disabled(line_to_check, scope_start_string=True) if self.is_disabled: return
print("", file=out) print("{} violations total".format(self.total_violations), file=out)
return False
return None
start_delim_index = template.find(start_delim, start_index, close_char_index) if 0 <= start_delim_index < open_char_index: return None
match = uncommented_line_start_index_regex.search(template, line_start_index) if match is None: return None elif match.start() < start_index: return start_index else: return match.start()
return start_index
elif "+" not in argument: if argument.endswith('.el') or argument.endswith('.$el'): return True return False
if last_expression is not None: results.violations.append(ExpressionRuleViolation( rule, last_expression ))
start_index = end_triple_quote_match.start()
if node.attr == 'format': self.contains_format_call = True else: self.generic_visit(node)
self.format_caller_node = node.func.value
self.interpolates_text_or_html = True
else: self.generic_visit(node)
if is_caller_html_or_text is False: self.results.violations.append(ExpressionRuleViolation( Rules.python_requires_html_or_text, self.node_to_expression(node.func) ))
if file_name == os.path.basename(__file__): return results
if root_node is not None: visitor = OuterFormatVisitor(file_contents, results) visitor.visit(root_node) results.prepare_results(file_contents, line_comment_delim=self.LINE_COMMENT_DELIM)
visitor = AllNodeVisitor(python_code, results) visitor.visit(root_node)
python_block_regex = re.compile(r'<%\s(?P<code>.*?)%>', re.DOTALL)
filters_regex = re.compile(r'\|([.,\w\s]*)\}') filters_match = filters_regex.search(expression.expression)
match_type = match_type.group()[6:-1].lower() if match_type in html_types: context_type = 'html' elif match_type not in javascript_types: context_type = 'unknown'
uncommented_start_index = self._uncommented_start_index(mako_template, start_index) if uncommented_start_index != start_index: start_index = uncommented_start_index continue
start_index = start_index + len(start_delim)
start_index = expression.end_index
epilog += " http://edx.readthedocs.org/projects/edx-developer-guide/en/latest/conventions/safe_templates.html#safe-template-linter\n"
from django.core import management
message = "<script>alert('XSS');</script>" x = "<string>{}</strong>".format(message)
if len(results.violations) != len(rules): for violation in results.violations: print("Found violation: {}".format(violation.rule))
<%block name="requirejs"> {expression} </%block>
<script type="{}"> ${{x | n, dump_js_escaped_json}} </script>
course_id=course_overview.id
return self.func(*args)
msg = "No merge commit for {commit} in {branch}!".format( commit=commit, branch=branch, ) raise DoesNotExist(msg, commit, branch)
username = email.split("@")[0] try: email = people[username]['email'] except KeyError: pass
args.date = parse_datestring(args.date).date()
startup.enable_microsites() directories = LOOKUP['main'].directories self.assertEqual(len([directory for directory in directories if 'external_module' in directory]), 1)
STATICFILES_STORAGE = 'openedx.core.lib.django_require.staticstorage.OptimizedCachedRequireJsStorage'
TEST_ROOT = REPO_ROOT / "test_root" LOG_DIR = (TEST_ROOT / "log").abspath()
STATIC_ROOT = (TEST_ROOT / "staticfiles" / "lms").abspath()
os.environ['REQUIRE_BUILD_PROFILE_OPTIMIZE'] = 'none'
SERVICE_VARIANT = os.environ.get('SERVICE_VARIANT', None)
CONFIG_ROOT = path(os.environ.get('CONFIG_ROOT', ENV_ROOT))
CONFIG_PREFIX = SERVICE_VARIANT + "." if SERVICE_VARIANT else ""
BROKER_POOL_LIMIT = 0 BROKER_CONNECTION_TIMEOUT = 1
CELERY_RESULT_BACKEND = 'djcelery.backends.cache:CacheBackend'
BROKER_HEARTBEAT = 10.0 BROKER_HEARTBEAT_CHECKRATE = 2
CELERYD_PREFETCH_MULTIPLIER = 1
STATIC_ROOT_BASE = ENV_TOKENS.get('STATIC_ROOT_BASE', None) if STATIC_ROOT_BASE: STATIC_ROOT = path(STATIC_ROOT_BASE)
DEFAULT_COURSE_ABOUT_IMAGE_URL = ENV_TOKENS.get('DEFAULT_COURSE_ABOUT_IMAGE_URL', DEFAULT_COURSE_ABOUT_IMAGE_URL)
MEDIA_ROOT = ENV_TOKENS.get('MEDIA_ROOT', MEDIA_ROOT) MEDIA_URL = ENV_TOKENS.get('MEDIA_URL', MEDIA_URL)
PLATFORM_TWITTER_ACCOUNT = ENV_TOKENS.get('PLATFORM_TWITTER_ACCOUNT', PLATFORM_TWITTER_ACCOUNT) PLATFORM_FACEBOOK_ACCOUNT = ENV_TOKENS.get('PLATFORM_FACEBOOK_ACCOUNT', PLATFORM_FACEBOOK_ACCOUNT)
SOCIAL_MEDIA_FOOTER_URLS = ENV_TOKENS.get('SOCIAL_MEDIA_FOOTER_URLS', SOCIAL_MEDIA_FOOTER_URLS)
EDXMKTG_LOGGED_IN_COOKIE_NAME = ENV_TOKENS.get('EDXMKTG_LOGGED_IN_COOKIE_NAME', EDXMKTG_LOGGED_IN_COOKIE_NAME) EDXMKTG_USER_INFO_COOKIE_NAME = ENV_TOKENS.get('EDXMKTG_USER_INFO_COOKIE_NAME', EDXMKTG_USER_INFO_COOKIE_NAME)
PAID_COURSE_REGISTRATION_CURRENCY = ENV_TOKENS.get('PAID_COURSE_REGISTRATION_CURRENCY', PAID_COURSE_REGISTRATION_CURRENCY)
PAYMENT_REPORT_GENERATOR_GROUP = ENV_TOKENS.get('PAYMENT_REPORT_GENERATOR_GROUP', PAYMENT_REPORT_GENERATOR_GROUP)
BULK_EMAIL_ROUTING_KEY_SMALL_JOBS = LOW_PRIORITY_QUEUE
THEME_NAME = ENV_TOKENS.get('THEME_NAME', None) COMPREHENSIVE_THEME_DIR = path(ENV_TOKENS.get('COMPREHENSIVE_THEME_DIR', COMPREHENSIVE_THEME_DIR))
MKTG_URL_LINK_MAP.update(ENV_TOKENS.get('MKTG_URL_LINK_MAP', {}))
MOBILE_STORE_URLS = ENV_TOKENS.get('MOBILE_STORE_URLS', MOBILE_STORE_URLS)
TIME_ZONE = ENV_TOKENS.get('TIME_ZONE', TIME_ZONE)
for app in ENV_TOKENS.get('ADDL_INSTALLED_APPS', []): INSTALLED_APPS += (app,)
GIT_REPO_DIR = ENV_TOKENS.get('GIT_REPO_DIR', '/edx/var/edxapp/course_repos') GIT_IMPORT_STATIC = ENV_TOKENS.get('GIT_IMPORT_STATIC', True)
if "TRACKING_IGNORE_URL_PATTERNS" in ENV_TOKENS: TRACKING_IGNORE_URL_PATTERNS = ENV_TOKENS.get("TRACKING_IGNORE_URL_PATTERNS")
SSL_AUTH_EMAIL_DOMAIN = ENV_TOKENS.get("SSL_AUTH_EMAIL_DOMAIN", "MIT.EDU") SSL_AUTH_DN_FORMAT_STRING = ENV_TOKENS.get("SSL_AUTH_DN_FORMAT_STRING", "/C=US/ST=Massachusetts/O=Massachusetts Institute of Technology/OU=Client CA v1/CN={0}/emailAddress={1}")
VIDEO_CDN_URL = ENV_TOKENS.get('VIDEO_CDN_URL', {})
NOTIFICATION_EMAIL_CSS = ENV_TOKENS.get('NOTIFICATION_EMAIL_CSS', NOTIFICATION_EMAIL_CSS) NOTIFICATION_EMAIL_EDX_LOGO = ENV_TOKENS.get('NOTIFICATION_EMAIL_EDX_LOGO', NOTIFICATION_EMAIL_EDX_LOGO)
CSRF_COOKIE_SECURE = ENV_TOKENS.get('CSRF_COOKIE_SECURE', False)
FIELD_OVERRIDE_PROVIDERS = tuple(ENV_TOKENS.get('FIELD_OVERRIDE_PROVIDERS', []))
AWS_QUERYSTRING_AUTH = AUTH_TOKENS.get('AWS_QUERYSTRING_AUTH', True) AWS_S3_CUSTOM_DOMAIN = AUTH_TOKENS.get('AWS_S3_CUSTOM_DOMAIN', 'edxuploads.s3.amazonaws.com')
FILE_UPLOAD_STORAGE_BUCKET_NAME = ENV_TOKENS.get('FILE_UPLOAD_STORAGE_BUCKET_NAME', FILE_UPLOAD_STORAGE_BUCKET_NAME) FILE_UPLOAD_STORAGE_PREFIX = ENV_TOKENS.get('FILE_UPLOAD_STORAGE_PREFIX', FILE_UPLOAD_STORAGE_PREFIX)
DATABASES = AUTH_TOKENS['DATABASES']
DATADOG = AUTH_TOKENS.get("DATADOG", {}) DATADOG.update(ENV_TOKENS.get("DATADOG", {}))
ANALYTICS_SERVER_URL = ENV_TOKENS.get("ANALYTICS_SERVER_URL") ANALYTICS_API_KEY = AUTH_TOKENS.get("ANALYTICS_API_KEY", "")
ANALYTICS_DATA_URL = ENV_TOKENS.get("ANALYTICS_DATA_URL", ANALYTICS_DATA_URL) ANALYTICS_DATA_TOKEN = AUTH_TOKENS.get("ANALYTICS_DATA_TOKEN", ANALYTICS_DATA_TOKEN)
ANALYTICS_DASHBOARD_URL = ENV_TOKENS.get("ANALYTICS_DASHBOARD_URL", ANALYTICS_DASHBOARD_URL) ANALYTICS_DASHBOARD_NAME = ENV_TOKENS.get("ANALYTICS_DASHBOARD_NAME", PLATFORM_NAME + " Insights")
MAILCHIMP_NEW_USER_LIST_ID = ENV_TOKENS.get("MAILCHIMP_NEW_USER_LIST_ID")
ZENDESK_USER = AUTH_TOKENS.get("ZENDESK_USER") ZENDESK_API_KEY = AUTH_TOKENS.get("ZENDESK_API_KEY")
EDX_API_KEY = AUTH_TOKENS.get("EDX_API_KEY")
STUDENT_FILEUPLOAD_MAX_SIZE = ENV_TOKENS.get("STUDENT_FILEUPLOAD_MAX_SIZE", STUDENT_FILEUPLOAD_MAX_SIZE)
VERIFY_STUDENT = AUTH_TOKENS.get("VERIFY_STUDENT", VERIFY_STUDENT)
GRADES_DOWNLOAD_ROUTING_KEY = HIGH_MEM_QUEUE
FINANCIAL_REPORTS = ENV_TOKENS.get("FINANCIAL_REPORTS", FINANCIAL_REPORTS)
ORA2_FILE_PREFIX = ENV_TOKENS.get("ORA2_FILE_PREFIX", ORA2_FILE_PREFIX)
SOCIAL_AUTH_PIPELINE_TIMEOUT = ENV_TOKENS.get('SOCIAL_AUTH_PIPELINE_TIMEOUT', 600)
THIRD_PARTY_AUTH_OLD_CONFIG = AUTH_TOKENS.get('THIRD_PARTY_AUTH', None)
THIRD_PARTY_AUTH_CUSTOM_AUTH_FORMS = AUTH_TOKENS.get('THIRD_PARTY_AUTH_CUSTOM_AUTH_FORMS', {})
INVOICE_CORP_ADDRESS = ENV_TOKENS.get('INVOICE_CORP_ADDRESS', INVOICE_CORP_ADDRESS) INVOICE_PAYMENT_INSTRUCTIONS = ENV_TOKENS.get('INVOICE_PAYMENT_INSTRUCTIONS', INVOICE_PAYMENT_INSTRUCTIONS)
COURSE_CATALOG_VISIBILITY_PERMISSION = ENV_TOKENS.get( 'COURSE_CATALOG_VISIBILITY_PERMISSION', COURSE_CATALOG_VISIBILITY_PERMISSION ) COURSE_ABOUT_VISIBILITY_PERMISSION = ENV_TOKENS.get( 'COURSE_ABOUT_VISIBILITY_PERMISSION', COURSE_ABOUT_VISIBILITY_PERMISSION )
ENROLLMENT_COURSE_DETAILS_CACHE_TIMEOUT = ENV_TOKENS.get('ENROLLMENT_COURSE_DETAILS_CACHE_TIMEOUT', 60)
SEARCH_ENGINE = "search.elastic.ElasticSearchEngine"
FACEBOOK_API_VERSION = AUTH_TOKENS.get("FACEBOOK_API_VERSION") FACEBOOK_APP_SECRET = AUTH_TOKENS.get("FACEBOOK_APP_SECRET") FACEBOOK_APP_ID = AUTH_TOKENS.get("FACEBOOK_APP_ID")
LTI_AGGREGATE_SCORE_PASSBACK_DELAY = ENV_TOKENS.get( 'LTI_AGGREGATE_SCORE_PASSBACK_DELAY', LTI_AGGREGATE_SCORE_PASSBACK_DELAY )
MICROSITE_BACKEND = ENV_TOKENS.get("MICROSITE_BACKEND", MICROSITE_BACKEND) MICROSITE_TEMPLATE_BACKEND = ENV_TOKENS.get("MICROSITE_TEMPLATE_BACKEND", MICROSITE_TEMPLATE_BACKEND) MICROSITE_DATABASE_TEMPLATE_CACHE_TTL = ENV_TOKENS.get( "MICROSITE_DATABASE_TEMPLATE_CACHE_TTL", MICROSITE_DATABASE_TEMPLATE_CACHE_TTL )
MAX_BOOKMARKS_PER_COURSE = ENV_TOKENS.get('MAX_BOOKMARKS_PER_COURSE', MAX_BOOKMARKS_PER_COURSE)
STUDENTMODULEHISTORYEXTENDED_OFFSET = ENV_TOKENS.get( 'STUDENTMODULEHISTORYEXTENDED_OFFSET', STUDENTMODULEHISTORYEXTENDED_OFFSET )
if ENV_TOKENS.get('AUDIT_CERT_CUTOFF_DATE', None): AUDIT_CERT_CUTOFF_DATE = dateutil.parser.parse(ENV_TOKENS.get('AUDIT_CERT_CUTOFF_DATE'))
if FEATURES.get('ENABLE_CSMH_EXTENDED'): INSTALLED_APPS += ('coursewarehistoryextended',)
APP_UPGRADE_CACHE_TIMEOUT = ENV_TOKENS.get('APP_UPGRADE_CACHE_TIMEOUT', APP_UPGRADE_CACHE_TIMEOUT)
PLATFORM_NAME = "Your Platform Name Here" CC_MERCHANT_NAME = PLATFORM_NAME COPYRIGHT_YEAR = "2015"
FEATURES = { 'DISPLAY_DEBUG_INFO_TO_STAFF': True,
'ENABLE_DISCUSSION_SERVICE': True, 'ENABLE_TEXTBOOK': True,
'ENABLE_DISCUSSION_HOME_PANEL': False,
'AUTH_USE_OPENID': False, 'AUTH_USE_CERTIFICATES': False, 'AUTH_USE_OPENID_PROVIDER': False, 'AUTH_USE_SHIB': False, 'AUTH_USE_CAS': False,
'SHIB_DISABLE_TOS': False,
'ENABLE_OAUTH2_PROVIDER': False,
'ENABLE_XBLOCK_VIEW_ENDPOINT': False,
'ENABLE_CORS_HEADERS': False,
'COURSES_ARE_BROWSABLE': True,
'RESTRICT_ENROLL_BY_REG_METHOD': False,
'RUN_AS_ANALYTICS_SERVER_ENABLED': False,
'USE_YOUTUBE_OBJECT_API': False,
'ENABLE_STUDENT_HISTORY_VIEW': True,
'ENABLE_FEEDBACK_SUBMISSION': False,
'ENABLE_DEBUG_RUN_PYTHON': False,
'ENABLE_SERVICE_STATUS': False,
'USE_CUSTOM_THEME': False,
'AUTOPLAY_VIDEOS': False,
'ENABLE_INSTRUCTOR_BACKGROUND_TASKS': True,
'INDIVIDUAL_DUE_DATES': False,
'CUSTOM_COURSES_EDX': False,
'ENABLE_VERIFIED_CERTIFICATES': False,
'AUTOMATIC_AUTH_FOR_TESTING': False,
'ENABLE_SHOPPING_CART': False,
'STORE_BILLING_INFO': False,
'ENABLE_PAID_COURSE_REGISTRATION': False,
'ENABLE_COSMETIC_DISPLAY_PRICE': False,
'AUTOMATIC_VERIFY_STUDENT_IDENTITY_FOR_TESTING': False,
'MAX_ENROLLMENT_INSTR_BUTTONS': 200,
'ENABLE_S3_GRADE_DOWNLOADS': False,
'ENFORCE_PASSWORD_POLICY': True,
'ALLOW_COURSE_STAFF_GRADE_DOWNLOADS': False,
'ENABLE_MAX_FAILED_LOGIN_ATTEMPTS': True,
'SQUELCH_PII_IN_LOGS': True,
'EMBARGO': False,
'ALLOW_WIKI_ROOT_ACCESS': True,
'USE_MICROSITES': False,
'ENABLE_THIRD_PARTY_AUTH': False,
'ENABLE_MKTG_SITE': False,
'PREVENT_CONCURRENT_LOGINS': True,
'ADVANCED_SECURITY': True,
'ALWAYS_REDIRECT_HOMEPAGE_TO_DASHBOARD_FOR_AUTHENTICATED_USER': True,
'ENABLE_COURSE_SORTING_BY_START_DATE': True,
'ENABLE_MOBILE_REST_API': False,
'ENABLE_COMBINED_LOGIN_REGISTRATION': False,
'ENABLE_MKTG_EMAIL_OPT_IN': False,
'ALLOW_AUTOMATED_SIGNUPS': False,
'DISPLAY_ANALYTICS_ENROLLMENTS': True,
'ENABLE_FOOTER_MOBILE_APP_LINKS': False,
'ENABLE_EDXNOTES': False,
'MILESTONES_APP': False,
'ORGANIZATIONS_APP': False,
'ENABLE_PREREQUISITE_COURSES': False,
'MODE_CREATION_FOR_TESTING': False,
'ENABLE_COURSEWARE_SEARCH': False,
'ENABLE_DASHBOARD_SEARCH': False,
'LOG_POSTPAY_CALLBACKS': True,
'ENABLE_VIDEO_BEACON': False,
'ENABLE_ONLOAD_BEACON': False,
'LICENSING': False,
'CERTIFICATES_HTML_VIEW': False,
'CERTIFICATES_INSTRUCTOR_GENERATION': False,
'ENABLE_COURSE_DISCOVERY': False,
'ENABLE_SOFTWARE_SECURE_FAKE': False,
'ENABLE_TEAMS': True,
'ENABLE_VIDEO_BUMPER': False,
'SHOW_BUMPER_PERIODICITY': 7 * 24 * 3600,
'ENABLE_SPECIAL_EXAMS': False,
'ENABLE_OPENBADGES': False,
'ENABLE_DISABLING_XBLOCK_TYPES': True,
'ENABLE_MAX_SCORE_CACHE': True,
'ENABLE_LTI_PROVIDER': False,
'SHOW_LANGUAGE_SELECTOR': False,
'ENABLE_CSMH_EXTENDED': False,
'ENABLE_READING_FROM_MULTIPLE_HISTORY_TABLES': True,
ASSET_IGNORE_REGEX = r"(^\._.*$)|(^\.DS_Store$)|(^.*~$)"
DEFAULT_GROUPS = []
GENERATE_PROFILE_SCORES = False
COMPREHENSIVE_THEME_DIR = ""
GEOIP_PATH = REPO_ROOT / "common/static/data/geoip/GeoIP.dat" GEOIPV6_PATH = REPO_ROOT / "common/static/data/geoip/GeoIPv6.dat"
STATUS_MESSAGE_PATH = ENV_ROOT / "status_message.json"
'edxmako.shortcuts.marketing_link_context_processor',
'shoppingcart.context_processor.user_has_cart_context_processor',
'edxmako.shortcuts.microsite_footer_context_processor',
'context_processors.doc_url',
'debug': False
AUTHENTICATION_BACKENDS = ( 'ratelimitbackend.backends.RateLimitModelBackend', )
STATIC_GRAB = False DEV_CONTENT = True
TRACK_MAX_EVENT = 50000
TRACKING_IGNORE_URL_PATTERNS = [r'^/event', r'^/login', r'^/heartbeat', r'^/segmentio/event', r'^/performance']
from xmodule.modulestore.inheritance import InheritanceMixin from xmodule.modulestore import prefer_xmodules from xmodule.x_module import XModuleMixin
XBLOCK_MIXINS = (LmsBlockMixin, InheritanceMixin, XModuleMixin, EditInfoMixin)
XBLOCK_SELECT_FUNCTION = prefer_xmodules
XBLOCK_FIELD_DATA_WRAPPERS = ()
'python_bin': None, 'user': 'sandbox',
'limits': { 'CPU': 1, },
COURSES_WITH_UNSAFE_CODE = []
DEBUG = False USE_TZ = True SESSION_COOKIE_SECURE = False SESSION_SAVE_EVERY_REQUEST = False SESSION_SERIALIZER = 'django.contrib.sessions.serializers.PickleSerializer'
CMS_BASE = 'localhost:8001'
SITE_ID = 1 SITE_NAME = "example.com" HTTPS = 'on' ROOT_URLCONF = 'lms.urls'
CONTACT_MAILING_ADDRESS = ''
EDX_PLATFORM_REVISION = dealer.git.Backend(path=REPO_ROOT).revision
EDX_PLATFORM_REVISION = 'unknown'
STATIC_URL = '/static/' STATIC_ROOT = ENV_ROOT / "staticfiles"
MEDIA_ROOT = '/edx/var/edxapp/media/' MEDIA_URL = '/media/'
LANGUAGES_BIDI = ("he", "ar", "fa", "ur", "fa-ir", "rtl")
LANGUAGES = ( ('en', u'English'), ('rtl', u'Right-to-Left Test Language'),
MESSAGE_STORAGE = 'django.contrib.messages.storage.session.SessionStorage'
PAID_COURSE_REGISTRATION_CURRENCY = ['usd', '$']
PAYMENT_REPORT_GENERATOR_GROUP = 'shoppingcart_report_access'
EDXNOTES_PUBLIC_API = 'http://localhost:8120/api/v1' EDXNOTES_INTERNAL_API = 'http://localhost:8120/api/v1'
PARENTAL_CONSENT_AGE_LIMIT = 13
FOOTER_OPENEDX_URL = "http://open.edx.org"
FOOTER_ORGANIZATION_IMAGE = "images/logo.png"
FOOTER_CACHE_TIMEOUT = 30 * 60
FOOTER_BROWSER_CACHE_MAX_AGE = 5 * 60
CREDIT_NOTIFICATION_CACHE_TIMEOUT = 5 * 60 * 60
simplefilter('ignore')
'openedx.core.djangoapps.safe_sessions.middleware.SafeSessionMiddleware',
#'django.contrib.auth.middleware.AuthenticationMiddleware', 'cache_toolbox.middleware.CacheBackedAuthenticationMiddleware', 'django.contrib.auth.middleware.SessionAuthenticationMiddleware',
'openedx.core.djangoapps.user_api.middleware.UserTagsEventContextMiddleware',
'corsheaders.middleware.CorsMiddleware', 'cors_csrf.middleware.CorsCSRFMiddleware', 'cors_csrf.middleware.CsrfCrossDomainCookieMiddleware', 'django.middleware.csrf.CsrfViewMiddleware',
'lang_pref.middleware.LanguagePreferenceMiddleware',
'dark_lang.middleware.DarkLangMiddleware',
'django.middleware.locale.LocaleMiddleware',
'ratelimitbackend.middleware.RateLimitMiddleware', 'edxmako.middleware.MakoMiddleware',
'session_inactivity_timeout.middleware.SessionInactivityTimeout',
'django.middleware.clickjacking.XFrameOptionsMiddleware',
'courseware.middleware.RedirectUnenrolledMiddleware',
'microsite_configuration.middleware.MicrositeSessionCookieDomainMiddleware',
X_FRAME_OPTIONS = 'ALLOW'
P3P_HEADER = 'CP="Open EdX does not have a P3P policy."'
PIPELINE_COMPILE_INPLACE = True
PIPELINE_DISABLE_WRAPPER = True
PIPELINE_UGLIFYJS_BINARY = 'node_modules/.bin/uglifyjs'
'edx-ui-toolkit/js/utils/global-loader.js', 'edx-ui-toolkit/js/utils/string-utils.js', 'edx-ui-toolkit/js/utils/html-utils.js',
'js/vendor/requirejs/require.js', 'js/RequireJS-namespace-undefine.js', 'js/vendor/URI.min.js', 'common/js/vendor/backbone.js', 'edx-pattern-library/js/modernizr-custom.js',
"spec", "spec_helpers",
"xmodule_js",
REQUIRE_BASE_URL = "./"
REQUIRE_JS = "js/vendor/requirejs/require.js"
REQUIRE_STANDALONE_MODULES = {}
REQUIRE_DEBUG = False
REQUIRE_EXCLUDE = ("build.txt",)
CELERY_IMPORTS = ( 'openedx.core.djangoapps.programs.tasks.v1.tasks', )
CELERYD_HIJACK_ROOT_LOGGER = False
BULK_EMAIL_DEFAULT_FROM_EMAIL = 'no-reply@example.com'
BULK_EMAIL_EMAILS_PER_TASK = 100
BULK_EMAIL_DEFAULT_RETRY_DELAY = 30
BULK_EMAIL_MAX_RETRIES = 5
BULK_EMAIL_INFINITE_RETRY_CAP = 1000
BULK_EMAIL_ROUTING_KEY = HIGH_PRIORITY_QUEUE
BULK_EMAIL_ROUTING_KEY_SMALL_JOBS = LOW_PRIORITY_QUEUE
BULK_EMAIL_JOB_SIZE_THRESHOLD = 100
BULK_EMAIL_LOG_SENT_EMAILS = False
BULK_EMAIL_RETRY_DELAY_BETWEEN_SENDS = 0.02
EMAIL_OPTIN_MINIMUM_AGE = PARENTAL_CONSENT_AGE_LIMIT
'API': 'https://www.youtube.com/iframe_api',
'METADATA_URL': 'https://www.googleapis.com/youtube/v3/videos/',
'openedx.core.djangoapps.common_views',
'simple_history',
'config_models',
'service_status',
'status',
'edxmako', 'pipeline', 'static_replace',
'contentserver',
'openedx.core.djangoapps.theming',
'openedx.core.djangoapps.site_configuration',
'courseware', 'student',
'support',
'external_auth', 'django_openid_auth',
'provider', 'provider.oauth2', 'edx_oauth2_provider',
'oauth2_provider',
#'wiki.plugins.notifications', 'course_wiki.plugins.markdownedx',
'django_comment_client', 'django_comment_common', 'discussion_api', 'notes',
'splash',
'datadog',
'rest_framework', 'openedx.core.djangoapps.user_api',
'shoppingcart',
'notification_prefs',
'course_modes',
'enrollment',
'lms.djangoapps.verify_student',
'dark_lang',
'microsite_configuration',
'rss_proxy',
'reverification',
'monitoring',
'course_action_state',
'django_countries',
'mobile_api', 'social.apps.django_app.default',
'survey',
'openedx.core.djangoapps.content.course_overviews', 'openedx.core.djangoapps.content.course_structures', 'lms.djangoapps.course_blocks',
'course_structure_api',
'mailing',
'corsheaders', 'cors_csrf',
'openedx.core.djangoapps.credit',
'lms.djangoapps.teams',
'openedx.core.djangoapps.bookmarks',
'openedx.core.djangoapps.programs',
'openedx.core.djangoapps.self_paced',
'openedx.core.djangoapps.credentials',
'milestones',
'gating.apps.GatingConfig',
'statici18n',
'openedx.core.djangoapps.coursetalk',
'openedx.core.djangoapps.api_admin',
'verified_track_content',
'learner_dashboard',
'badges',
MIGRATION_MODULES = { 'social.apps.django_app.default': 'social.apps.django_app.default.south_migrations' }
CSRF_COOKIE_AGE = 60 * 60 * 24 * 7 * 52 CSRF_COOKIE_SECURE = False
'WHAT_IS_VERIFIED_CERT': 'verified-certificate',
SOCIAL_MEDIA_FOOTER_NAMES = [ "facebook", "twitter", "youtube", "linkedin", "google_plus", "reddit", ]
SOCIAL_MEDIA_FOOTER_URLS = {}
MOBILE_STORE_URLS = { 'apple': '#', 'google': '#' }
XDOMAIN_PROXY_CACHE_TIMEOUT = 60 * 15
REGISTRATION_EMAIL_PATTERNS_ALLOWED = None
BADGR_API_TOKEN = None BADGR_BASE_URL = "http://localhost:8005" BADGR_ISSUER_SLUG = "example-issuer" BADGR_TIMEOUT = 10
GRADES_DOWNLOAD_ROUTING_KEY = HIGH_MEM_QUEUE
ORA2_FILE_PREFIX = None
FILE_UPLOAD_STORAGE_BUCKET_NAME = 'edxuploads' FILE_UPLOAD_STORAGE_PREFIX = 'submissions_attachments'
'submissions', 'openassessment', 'openassessment.assessment', 'openassessment.fileupload', 'openassessment.workflow', 'openassessment.xblock',
'edxval',
'edx_proctoring',
'organizations',
try: imp.find_module(app_name) except ImportError: try: __import__(app_name) except ImportError: continue INSTALLED_APPS += (app_name,)
ADVANCED_SECURITY_CONFIG = {}
INVOICE_CORP_ADDRESS = "Please place your corporate address\nin this configuration" INVOICE_PAYMENT_INSTRUCTIONS = "This is where you can\nput directions on how people\nbuying registration codes"
COUNTRIES_OVERRIDE = { "TW": "Taiwan", 'XK': _('Kosovo'), }
COURSE_CATALOG_VISIBILITY_PERMISSION = 'see_exists'
COURSE_ABOUT_VISIBILITY_PERMISSION = 'see_exists'
ENROLLMENT_COURSE_DETAILS_CACHE_TIMEOUT = 60
NOTES_DISABLED_TABS = ['course_structure', 'tags']
CDN_VIDEO_URLS = {}
ONLOAD_BEACON_SAMPLE_RATE = 0.0
ACCOUNT_VISIBILITY_CONFIGURATION = { "default_visibility": "all_users",
"shareable_fields": [ 'username', 'profile_image', 'country', 'time_zone', 'language_proficiencies', 'bio', 'account_privacy', 'accomplishments_shared', ],
"public_fields": [ 'username', 'profile_image', 'account_privacy', ],
ECOMMERCE_PUBLIC_URL_ROOT = None ECOMMERCE_API_URL = None ECOMMERCE_API_SIGNING_KEY = None ECOMMERCE_API_TIMEOUT = 5 ECOMMERCE_SERVICE_WORKER_USERNAME = 'ecommerce_worker'
CHECKPOINT_PATTERN = r'(?P<checkpoint_name>[^/]+)'
FIELD_OVERRIDE_PROVIDERS = ()
MODULESTORE_FIELD_OVERRIDE_PROVIDERS = ()
HOMEPAGE_COURSE_MAX = None
CREDIT_TASK_DEFAULT_RETRY_DELAY = 30
CREDIT_TASK_MAX_RETRIES = 5
SECRET_KEY = 'dev key'
CREDIT_PROVIDER_SECRET_KEYS = {}
CREDIT_PROVIDER_TIMESTAMP_EXPIRATION = 15 * 60
CREDIT_HELP_LINK_URL = "#"
LTI_USER_EMAIL_DOMAIN = 'lti.example.com'
PUBLIC_RSA_KEY = None PRIVATE_RSA_KEY = None
NOTIFICATION_EMAIL_CSS = "templates/credit_notifications/credit_notification.css" NOTIFICATION_EMAIL_EDX_LOGO = "templates/credit_notifications/edx-logo-header.png"
CCX_MAX_STUDENTS_ALLOWED = 200
FINANCIAL_ASSISTANCE_MIN_LENGTH = 800 FINANCIAL_ASSISTANCE_MAX_LENGTH = 2500
MAX_BOOKMARKS_PER_COURSE = 100
MOBILE_APP_USER_AGENT_REGEXES = [ r'edX/org.edx.mobile', ]
APP_UPGRADE_CACHE_TIMEOUT = 3600
DEPRECATED_ADVANCED_COMPONENT_TYPES = []
DEFAULT_SITE_ID = 1
AFFILIATE_COOKIE_NAME = 'affiliate_id'
from yaml import Loader, SafeLoader
SERVICE_VARIANT = os.environ.get('SERVICE_VARIANT', None)
CONFIG_ROOT = path(os.environ.get('CONFIG_ROOT', ENV_ROOT))
CONFIG_PREFIX = SERVICE_VARIANT + "." if SERVICE_VARIANT else ""
SSL_AUTH_EMAIL_DOMAIN = "MIT.EDU" SSL_AUTH_DN_FORMAT_STRING = "/C=US/ST=Massachusetts/O=Massachusetts Institute of Technology/OU=Client CA v1/CN={0}/emailAddress={1}"
BROKER_POOL_LIMIT = 0 BROKER_CONNECTION_TIMEOUT = 1
CELERY_RESULT_BACKEND = 'djcelery.backends.cache:CacheBackend'
BROKER_HEARTBEAT = 10.0 BROKER_HEARTBEAT_CHECKRATE = 2
CELERYD_PREFETCH_MULTIPLIER = 1
ENV_TOKENS = convert_tokens(ENV_TOKENS)
if 'FEATURES' in ENV_TOKENS: del ENV_TOKENS['FEATURES']
vars().update(ENV_TOKENS)
SESSION_COOKIE_NAME = str(SESSION_COOKIE_NAME)
BULK_EMAIL_ROUTING_KEY_SMALL_JOBS = LOW_PRIORITY_QUEUE
for app in ADDL_INSTALLED_APPS: INSTALLED_APPS += (app,)
AUTH_TOKENS = convert_tokens(AUTH_TOKENS)
GRADES_DOWNLOAD_ROUTING_KEY = HIGH_MEM_QUEUE
FEATURES['ENABLE_MKTG_SITE'] = True FEATURES['USE_MICROSITES'] = True
'debug_toolbar_mongo',
HOSTNAME_MODULESTORE_DEFAULT_MAPPINGS = { 'preview\.': 'draft-preferred' }
SECRET_KEY = 'dev key'
for database_name in DATABASES: DATABASES[database_name]['ATOMIC_REQUESTS'] = False
from util.testing import patch_testcase, patch_sessions patch_testcase() patch_sessions()
MONGO_PORT_NUM = int(os.environ.get('EDXAPP_TEST_MONGO_PORT', '27017')) MONGO_HOST = os.environ.get('EDXAPP_TEST_MONGO_HOST', 'localhost')
FEATURES['DISABLE_START_DATES'] = True
FEATURES['ENABLE_S3_GRADE_DOWNLOADS'] = True FEATURES['ALLOW_COURSE_STAFF_GRADE_DOWNLOADS'] = True
FEATURES['EMBARGO'] = True
PARENTAL_CONSENT_AGE_LIMIT = 13
TEST_RUNNER = 'openedx.core.djangolib.nose.NoseTestSuiteRunner'
TEST_ROOT = path("test_root") STATIC_ROOT = TEST_ROOT / "staticfiles"
GITHUB_REPO_ROOT = ENV_ROOT / "data"
MOCK_STAFF_GRADING = True MOCK_PEER_GRADING = True
STATICFILES_STORAGE = 'pipeline.storage.NonPackagingPipelineStorage'
PIPELINE_JS_COMPRESSOR = None
MIGRATION_MODULES = NoOpMigrationModules()
FEATURES['ENABLE_CSMH_EXTENDED'] = True INSTALLED_APPS += ('coursewarehistoryextended',)
'default': { 'BACKEND': 'django.core.cache.backends.dummy.DummyCache', },
SECRET_KEY = '85920908f28904ed733fe576320db18cabd7b6cd'
filterwarnings('ignore', message='No request passed to the backend, unable to rate-limit')
simplefilter('ignore')
FEATURES['ENFORCE_PASSWORD_POLICY'] = False FEATURES['ENABLE_MAX_FAILED_LOGIN_ATTEMPTS'] = False FEATURES['SQUELCH_PII_IN_LOGS'] = False FEATURES['PREVENT_CONCURRENT_LOGINS'] = False FEATURES['ADVANCED_SECURITY'] = False PASSWORD_MIN_LENGTH = None PASSWORD_COMPLEXITY = {}
OIDC_COURSE_HANDLER_CACHE_TIMEOUT = 0
FEATURES['ENABLE_PAYMENT_FAKE'] = True
from random import choice from string import letters, digits, punctuation RANDOM_SHARED_SECRET = ''.join( choice(letters + digits + punctuation) for x in range(250) )
'WHAT_IS_VERIFIED_CERT': 'verified-certificate',
for static_dir in STATICFILES_DIRS: try: _, data_dir = static_dir except ValueError: data_dir = static_dir
LETTUCE_SERVER_PORT = 8003 XQUEUE_PORT = 8040 YOUTUBE_PORT = 8031 LTI_PORT = 8765 VIDEO_SOURCE_PORT = 8777
'django.contrib.auth.hashers.SHA1PasswordHasher', 'django.contrib.auth.hashers.MD5PasswordHasher',
VERIFY_STUDENT["SOFTWARE_SECURE"] = { "API_ACCESS_KEY": "BBBBBBBBBBBBBBBBBBBB", "API_SECRET_KEY": "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC", }
FEATURES['ENABLE_EDXNOTES'] = True
FEATURES['ENABLE_TEAMS'] = True
FEATURES['ENABLE_COURSEWARE_SEARCH'] = True
FEATURES['ENABLE_DASHBOARD_SEARCH'] = True
SEARCH_ENGINE = "search.tests.mock_search_engine.MockSearchEngine"
FEATURES['ENABLE_LTI_PROVIDER'] = True INSTALLED_APPS += ('lti_provider',) AUTHENTICATION_BACKENDS += ('lti_provider.users.LtiBackend',)
FEATURES['ORGANIZATIONS_APP'] = True
FEATURES['ENABLE_FINANCIAL_ASSISTANCE_FORM'] = True
from openedx.core.lib.block_structure.transformer_registry import TransformerRegistry TransformerRegistry.USE_PLUGIN_MANAGER = False
OAUTH2_PROVIDER_APPLICATION_MODEL = 'oauth2_provider.Application'
DEBUG = True
REQUIRE_DEBUG = False
STATICFILES_STORAGE = 'pipeline.storage.PipelineCachedStorage'
INSTALLED_APPS += ('django_extensions',)
GITHUB_REPO_ROOT = (TEST_ROOT / "data").abspath() LOG_DIR = (TEST_ROOT / "log").abspath()
DEBUG = True
PIPELINE_JS_COMPRESSOR = None
XQUEUE_INTERFACE['url'] = 'http://localhost:8040'
EDXNOTES_PUBLIC_API = 'http://localhost:8042/api/v1' EDXNOTES_INTERNAL_API = 'http://localhost:8042/api/v1'
FEATURES['MILESTONES_APP'] = True
FEATURES['ENABLE_OAUTH2_PROVIDER'] = True
FEATURES['ENABLE_PREREQUISITE_COURSES'] = True
FEATURES['ENABLE_COURSE_DISCOVERY'] = True
FEATURES['ENABLE_EDXNOTES'] = True
FEATURES['ENABLE_TEAMS'] = True
FEATURES['LICENSING'] = True
FEATURES['AUTOMATIC_AUTH_FOR_TESTING'] = True
FEATURES['ENFORCE_PASSWORD_POLICY'] = False FEATURES['ENABLE_MAX_FAILED_LOGIN_ATTEMPTS'] = False FEATURES['SQUELCH_PII_IN_LOGS'] = False FEATURES['PREVENT_CONCURRENT_LOGINS'] = False FEATURES['ADVANCED_SECURITY'] = False
FEATURES['ENABLE_COURSEWARE_SEARCH'] = True
FEATURES['ENABLE_DASHBOARD_SEARCH'] = True
FEATURES['ENABLE_OPENBADGES'] = True
SEARCH_ENGINE = "search.tests.mock_search_engine.MockSearchEngine" MOCK_SEARCH_BACKING_FILE = ( TEST_ROOT / "index_file.dat" ).abspath()
SECRET_KEY = "very_secret_bok_choy_key"
FEATURES['ENABLE_CSMH_EXTENDED'] = True INSTALLED_APPS += ('coursewarehistoryextended',)
try:
del DEFAULT_FILE_STORAGE MEDIA_ROOT = "/edx/var/edxapp/uploads"
CELERY_ALWAYS_EAGER = True HTTPS = 'off'
ANALYTICS_DASHBOARD_URL = None
)
PIPELINE_JS_COMPRESSOR = None
REQUIRE_DEBUG = DEBUG
FEATURES['COURSES_ARE_BROWSEABLE'] = True HOMEPAGE_COURSE_MAX = 9
FEATURES['ENABLE_SOFTWARE_SECURE_FAKE'] = True
VERIFY_STUDENT["SOFTWARE_SECURE"] = { "API_ACCESS_KEY": "BBBBBBBBBBBBBBBBBBBB", "API_SECRET_KEY": "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC", }
SEARCH_SKIP_ENROLLMENT_START_DATE_FILTERING = True
if os.path.isfile(join(dirname(abspath(__file__)), 'private.py')):
MODULESTORE = convert_module_store_setting_if_needed(MODULESTORE)
DEBUG = True SITE_NAME = 'localhost:{}'.format(LETTUCE_SERVER_PORT)
import logging logging.basicConfig(filename=TEST_ROOT / "log" / "lms_acceptance.log", level=logging.ERROR)
logging.getLogger().setLevel(logging.ERROR)
FEATURES['AUTOMATIC_AUTH_FOR_TESTING'] = True
FEATURES['ENABLE_PAYMENT_FAKE'] = True
FEATURES['ENABLE_SPECIAL_EXAMS'] = True
FEATURES['AUTOMATIC_VERIFY_STUDENT_IDENTITY_FOR_TESTING'] = True
USE_I18N = True
INSTALLED_APPS += ('lettuce.django',) LETTUCE_APPS = ('courseware', 'instructor')
LETTUCE_SELENIUM_CLIENT = os.environ.get('LETTUCE_SELENIUM_CLIENT', 'local')
try:
SEARCH_ENGINE = "search.tests.mock_search_engine.MockSearchEngine"
import uuid SECRET_KEY = uuid.uuid4().hex
MIGRATION_MODULES = {}
CACHE_TIMEOUT = 0
SECRET_KEY = '85920908f28904ed733fe576320db18cabd7b6cd'
VIRTUAL_UNIVERSITIES = []
META_UNIVERSITIES = {'UTx': ['UTAustinX']}
CELERY_ALWAYS_EAGER = True
LMS_SEGMENT_KEY = os.environ.get('SEGMENT_KEY')
try:
SECRET_KEY = '85920908f28904ed733fe576320db18cabd7b6cd'
from .aws import * import os from django.core.exceptions import ImproperlyConfigured
if db != 'read_replica': DATABASES[db].update(get_db_overrides(db))
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'proj.settings')
APP.config_from_object('django.conf:settings') APP.autodiscover_tasks(lambda: settings.INSTALLED_APPS)
if settings.DEBUG or settings.FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'): admin.autodiscover()
urlpatterns = ( '',
url(r'^user_api/', include('openedx.core.djangoapps.user_api.legacy_urls')),
url(r'^submit_feedback$', 'util.views.submit_feedback'),
url(r'^api/enrollment/v1/', include('enrollment.urls')),
url(r'^search/', include('search.urls')),
url(r'^api/course_structure/', include('course_structure_api.urls', namespace='course_structure_api')),
url(r'^api/courses/', include('course_api.urls')),
url(r'^api/user/', include('openedx.core.djangoapps.user_api.urls')),
url(r'^api/bookmarks/', include('openedx.core.djangoapps.bookmarks.urls')),
url(r'^api/profile_images/', include('openedx.core.djangoapps.profile_images.urls')),
url(r'^api/val/v0/', include('edxval.urls')),
url(r'^lang_pref/session_language', 'lang_pref.views.update_session_language', name='session_language'),
url(r'^api-admin/', include('openedx.core.djangoapps.api_admin.urls', namespace='api_admin')),
'packages': ('openassessment',),
if settings.FEATURES["ENABLE_SYSADMIN_DASHBOARD"]: urlpatterns += ( url(r'^sysadmin/', include('dashboard.sysadmin_urls')), )
urlpatterns += ( url(r'^404$', 'static_template_view.views.render', {'template': '404.html'}, name="404"), )
url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),
for key, value in settings.MKTG_URL_LINK_MAP.items(): if value is None: continue
if key == "ROOT" or key == "COURSES": continue
template = key.lower() if '.' not in template: template = "%s.%s" % (template, settings.STATIC_TEMPLATE_VIEW_DEFAULT_FILE_EXTENSION)
if settings.FEATURES["USE_CUSTOM_THEME"]: template = "theme-" + template
urlpatterns += (url(r'^%s$' % key.lower(), 'static_template_view.views.render', {'template': template}, name=value),)
if settings.WIKI_ENABLED: from wiki.urls import get_pattern as wiki_pattern from django_notify.urls import get_pattern as notify_pattern
url( r'^courses/{course_key}/xblock/{usage_key}/view/(?P<view_name>[^/]*)$'.format( course_key=settings.COURSE_ID_PATTERN, usage_key=settings.USAGE_ID_PATTERN, ), 'courseware.module_render.xblock_view', name='xblock_view', ),
url( r'^courses/{}/survey$'.format( settings.COURSE_ID_PATTERN, ), 'courseware.views.views.course_survey', name='course_survey', ),
url( r'^courses/{}/progress/(?P<student_id>[^/]*)/$'.format( settings.COURSE_ID_PATTERN, ), 'courseware.views.views.progress', name='student_progress', ),
url( r'^courses/{}/instructor$'.format( settings.COURSE_ID_PATTERN, ), 'instructor.views.instructor_dashboard.instructor_dashboard_2', name='instructor_dashboard', ),
url( r'^courses/{}/lti_rest_endpoints/'.format( settings.COURSE_ID_PATTERN, ), 'courseware.views.views.get_course_lti_endpoints', name='lti_rest_endpoints', ),
url( r'^account/', include('student_account.urls') ),
url( r'^u/(?P<username>[\w.@+-]+)$', 'student_profile.views.learner_profile', name='learner_profile', ),
url( r'^courses/{}/edxnotes'.format( settings.COURSE_ID_PATTERN, ), include('edxnotes.urls'), name='edxnotes_endpoints', ),
if settings.FEATURES.get('EMBARGO'): urlpatterns += ( url(r'^embargo/', include('embargo.urls')), )
urlpatterns += ( url(r'^survey/', include('survey.urls')), )
if settings.FEATURES.get('AUTOMATIC_AUTH_FOR_TESTING'): urlpatterns += ( url(r'^auto_auth$', 'student.views.auto_auth'), )
if settings.FEATURES.get('ENABLE_OAUTH2_PROVIDER'): urlpatterns += ( url( r'^oauth2/login/$', auth_exchange.views.LoginWithAccessTokenView.as_view(), name="login_with_access_token" ), )
urlpatterns += ( url(r'^certificates/', include('certificates.urls', app_name="certificates", namespace="certificates")),
url(r'^api/certificates/', include('lms.djangoapps.certificates.apis.urls', namespace='certificates_api')),
urlpatterns += ( url(r'^xdomain_proxy.html$', 'cors_csrf.views.xdomain_proxy', name='xdomain_proxy'), )
if settings.FEATURES.get("ENABLE_LTI_PROVIDER"): urlpatterns += ( url(r'^lti_provider/', include('lti_provider.urls')), )
urlpatterns += url(r'^template/(?P<template>.+)$', 'openedx.core.djangoapps.debug.views.show_reference_template'),
handler404 = 'static_template_view.views.render_404' handler500 = 'static_template_view.views.render_500'
urlpatterns += ( url(r'^404$', handler404), url(r'^500$', handler500), )
urlpatterns += ( url(r'^api/', include('edx_proctoring.urls')), )
from .celery import APP as CELERY_APP
from safe_lxml import defuse_xml_libs defuse_xml_libs()
from django.core.wsgi import get_wsgi_application
if settings.FEATURES.get('ENABLE_THIRD_PARTY_AUTH', False): enable_third_party_auth()
if settings.COMPREHENSIVE_THEME_DIR: enable_comprehensive_theme(settings.COMPREHENSIVE_THEME_DIR)
microsite.enable_microsites_pre_startup(log)
microsite.enable_microsites(log)
if settings.LMS_SEGMENT_KEY: analytics.write_key = settings.LMS_SEGMENT_KEY
set_runtime_service('instructor', InstructorService())
if getattr(settings, "THEME_NAME", "") == "": settings.THEME_NAME = None return
theme_root = settings.ENV_ROOT / "themes" / settings.THEME_NAME
settings.DEFAULT_TEMPLATE_ENGINE['DIRS'].insert(0, theme_root / 'templates') edxmako.paths.add_lookup('main', theme_root / 'templates', prepend=True)
settings.STATICFILES_DIRS.append( (u'themes/{}'.format(settings.THEME_NAME), theme_root / 'static') )
settings.LOCALE_PATHS = (theme_root / 'conf/locale',) + settings.LOCALE_PATHS
super(LmsSearchResultProcessorTestCase, self).setUp() self.build_course()
course_org_filter = microsite.get_value('course_org_filter') if course_org_filter: field_dictionary['org'] = course_org_filter
if not course_org_filter: org_filter_out_set = microsite.get_all_orgs() if org_filter_out_set: exclude_dictionary['org'] = list(org_filter_out_set)
video = self.store.get_item(child_to_move_location)
video = self.store.get_item(child_to_move_location) self.assertEqual( old_parent_location, video.get_parent().location.for_branch(None) )
metric_tag_fields = [ 'course_id', 'group_id', 'pinned', 'closed', 'anonymous', 'anonymous_to_peers', 'endorsed', 'read' ]
initializable_fields = updatable_fields + ['thread_type', 'context']
from .comment import Comment from .thread import Thread from .user import User from .commentable import Commentable
self.save() response = perform_request( 'get', url, retrieve_params, metric_action='model.retrieve', metric_tags=self._metric_tags, )
if text_message is None: text_message = html_to_text(html_message)
course_email = cls( course_id=course_id, sender=sender, subject=subject, html_message=html_message, text_message=text_message, template_name=template_name, from_addr=from_addr, )
user = models.ForeignKey(User, db_index=True, null=True) course_id = CourseKeyField(max_length=255, db_index=True)
COURSE_EMAIL_MESSAGE_BODY_TAG = '{{message_body}}'
if 'user_id' in context and 'course_id' in context: message_body = substitute_keywords_with_data(message_body, context)
return wrap_message(result)
course_id = CourseKeyField(max_length=255, db_index=True, unique=True)
email_enabled = models.BooleanField(default=False)
return u"Course '{}': Instructor Email {}Enabled".format(self.course_id.to_deprecated_string(), not_en)
require_course_email_auth = models.BooleanField(default=True)
actions = None
name = self.cleaned_data.get("name").strip() or None
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
pass
email.to_option = next( ( t_type for t_type in ( target.target_type for target in email.targets.all() ) if t_type in EMAIL_TARGETS ), SEND_TO_MYSELF ) email.save()
pass
from __future__ import unicode_literals
self.assertTrue(BulkEmailFlag.feature_enabled(self.course.id))
bad_id = SlashSeparatedCourseKey(u'Broken{}'.format(self.course.id.org), 'hello', self.course.id.run + '_typo')
self.assertFalse(form.is_valid())
self.assertFalse(form.is_valid())
form_data = {'course_id': self.course.id.run, 'email_enabled': True} form = CourseAuthorizationAdminForm(data=form_data) self.assertFalse(form.is_valid())
cet = CourseEmailTemplate.objects.get(name=None) self.assertIsNotNone(cet)
cet = CourseEmailTemplate.objects.get(name=None) self.assertIsNotNone(cet)
cet = CourseEmailTemplate.objects.get(name='foo') self.assertIsNotNone(cet)
cet = CourseEmailTemplate.objects.get(name='foo') self.assertIsNotNone(cet)
form = CourseEmailTemplateForm(form_data) self.assertFalse(form.is_valid())
call_command("loaddata", "course_email_template.json")
self.assertEqual(len(mail.outbox), 1) self.assertEqual(mail.outbox[0].to[0], self.instructor.email)
response = self.client.get(url) email_section = '<div class="vert-left send-email" id="section-send-email">' self.assertIn(email_section, response.content)
call_command("loaddata", "course_email_template.json")
self.assertContains(response, "Email is not enabled for this course.", status_code=403)
response = self.client.post(self.send_mail_url, test_email) self.assertEquals(json.loads(response.content), self.success_content)
response = self.client.post(self.send_mail_url, test_email) self.assertEquals(json.loads(response.content), self.success_content)
unicode_user = UserFactory(first_name=u'Ⓡⓞⓑⓞⓣ', last_name=u'ՇﻉรՇ') CourseEnrollmentFactory.create(user=unicode_user, course_id=self.course.id) self.students.append(unicode_user)
response = self.client.post(self.send_mail_url, test_email) self.assertEquals(json.loads(response.content), self.success_content)
long_name = u"Финансовое программирование и политика, часть 1: макроэкономические счета и анализ"
self.assertEqual(len(encoded_unexpected_from_addr), 318) self.assertEqual(len(escaped_encoded_unexpected_from_addr), 324) self.assertEqual(len(unexpected_from_addr), 137)
response = self.client.post(self.send_mail_url, test_email) self.assertEquals(json.loads(response.content), self.success_content)
response = self.client.post(self.send_mail_url, test_email) self.assertEquals(json.loads(response.content), self.success_content)
call_command("loaddata", "course_email_template.json")
template = CourseEmailTemplate.get_template() self.assertIsNotNone(template.html_template) self.assertIsNotNone(template.plain_template)
self.assertFalse(BulkEmailFlag.feature_enabled(course_id))
self.assertTrue(BulkEmailFlag.feature_enabled(course_id))
cauth = CourseAuthorization(course_id=course_id, email_enabled=False) cauth.save()
self.assertTrue(BulkEmailFlag.feature_enabled(course_id))
call_command("loaddata", "course_email_template.json")
self.assertEquals(parent_status.get('total'), total) self.assertEquals(parent_status.get('action_name'), action_name)
self._test_email_address_failures(SMTPDataError(554, "Email address is blacklisted"))
self._test_email_address_failures(SESAddressBlacklistedError(554, "Email address is blacklisted"))
self._test_email_address_failures(SESIllegalAddressError(554, "Email address is illegal"))
self._test_email_address_failures(SESLocalAddressCharacterError(554, "Email address contains a bad character"))
self._test_email_address_failures(SESDomainEndsWithDotError(554, "Email address ends with a dot"))
course_image = u'在淡水測試.jpg' self.course = CourseFactory.create(course_image=course_image)
self._create_students(num_emails - 1)
self.assertTrue(retry.called) (__, kwargs) = retry.call_args exc = kwargs['exc'] self.assertIsInstance(exc, SMTPDataError)
with self.assertRaisesRegexp(ValueError, r"(?i)course not found"): perform_delegate_email_batches(entry.id, course_id, task_input, "action_name")
SINGLE_EMAIL_FAILURE_ERRORS = (
LIMITED_RETRY_ERRORS = ( SMTPConnectError, SMTPServerDisconnected, AWSConnectionError, )
BULK_EMAIL_FAILURE_ERRORS = (
user_id = entry.requester.id task_id = entry.task_id
course = get_course(course_id)
targets = email_obj.targets.all() global_email_context = _get_course_email_context(course)
if total_recipients <= settings.BULK_EMAIL_JOB_SIZE_THRESHOLD: routing_key = settings.BULK_EMAIL_ROUTING_KEY_SMALL_JOBS
log.info("Send-email task %s for email %s: succeeded", current_task_id, email_id) update_subtask_status(entry_id, current_task_id, new_subtask_status)
log.warning("Send-email task %s for email %s: being retried", current_task_id, email_id)
log.info("Send-email task %s for email %s: returning status %s", current_task_id, email_id, new_subtask_status) return new_subtask_status.to_dict()
num_optout = len(optouts) to_list = [recipient for recipient in to_list if recipient['email'] not in optouts] return to_list, num_optout
course_name = re.sub(r"[^\w.-]", '_', course_id.course)
__, encoded_from_addr = forbid_multi_line_headers('from', from_addr, 'utf-8')
escaped_encoded_from_addr = escape(encoded_from_addr) if len(escaped_encoded_from_addr) >= 320 and truncate: from_addr = format_address(course_name)
parent_task_id = InstructorTask.objects.get(pk=entry_id).task_id task_id = subtask_status.task_id total_recipients = len(to_list) recipient_num = 0 total_recipients_successful = 0 total_recipients_failed = 0 recipients_info = Counter()
from_addr = course_email.from_addr if course_email.from_addr else \ _get_source_address(course_email.course_id, course_title)
course_email_template = course_email.get_template() try: connection = get_connection() connection.open()
email_context = {'name': '', 'email': ''} email_context.update(global_email_context)
plaintext_msg = course_email_template.render_plaintext(course_email.text_message, email_context) html_msg = course_email_template.render_htmltext(course_email.html_message, email_context)
email_msg = EmailMultiAlternatives( course_email.subject, plaintext_msg, from_addr, [email], connection=connection ) email_msg.attach_alternative(html_msg, 'text/html')
recipients_info[email] += 1 to_list.pop()
subtask_status.increment(retried_nomax=1, state=RETRY) return _submit_for_retry( entry_id, email_id, to_list, global_email_context, exc, subtask_status, skip_retry_max=True )
subtask_status.increment(failed=num_pending, state=FAILURE) return subtask_status, exc
subtask_status.increment(state=SUCCESS) return subtask_status, None
connection.close()
countdown = ((2 ** retry_index) * base_delay) * random.uniform(.75, 1.25)
update_subtask_status(entry_id, task_id, subtask_status)
try: return self.matches[state] except IndexError: return None
current_commit_id = get_commit_id(def_ms.courses[reload_dir]) log.debug('commit_id="%s"', commit_id) log.debug('current_commit_id="%s"', current_commit_id)
pass
if not consumer: consumer = LtiConsumer.objects.get( consumer_key=consumer_key, )
if instance_guid and not consumer.instance_guid: consumer.instance_guid = instance_guid consumer.save() return consumer
lti_user = create_lti_user(lti_user_id, lti_consumer)
switch_user(request, lti_user, lti_consumer)
pass
raise PermissionDenied()
REQUIRED_PARAMETERS = [ 'roles', 'context_id', 'oauth_version', 'oauth_consumer_key', 'oauth_signature', 'oauth_signature_method', 'oauth_timestamp', 'oauth_nonce', 'user_id' ]
params = get_required_parameters(request.POST) if not params: return HttpResponseBadRequest() params.update(get_optional_parameters(request.POST))
try: lti_consumer = LtiConsumer.get_or_supplement( params.get('tool_consumer_instance_guid', None), params['oauth_consumer_key'] ) except LtiConsumer.DoesNotExist: return HttpResponseForbidden()
if not SignatureValidator(lti_consumer).verify(request): return HttpResponseForbidden()
authenticate_lti_user(request, params['user_id'], lti_consumer)
store_outcome_parameters(params, request.user, lti_consumer)
from courseware.views.views import render_xblock return render_xblock(request, unicode(usage_key), check_if_enrolled=False)
headers = {"Content-Type": request.META['CONTENT_TYPE']} result, __ = self.endpoint.validate_request(url, method, body, headers) return result
usage_key = request_params['usage_key'] course_key = request_params['course_key']
outcomes, __ = OutcomeService.objects.get_or_create( lis_outcome_service_url=result_service, lti_consumer=lti_consumer )
response = None log.exception("Outcome Service: Error when sending result.")
if response.status_code != 200: log.error( "Outcome service response: Unexpected status code %s", response.status_code ) return False
from __future__ import unicode_literals
from __future__ import unicode_literals
SignatureValidator.verify = MagicMock(return_value=True) self.consumer = models.LtiConsumer( consumer_name='consumer', consumer_key=LTI_DEFAULT_PARAMS['oauth_consumer_key'], consumer_secret='secret' ) self.consumer.save()
self.assertIn( 'oauth_body_hash="00hq6RNueFa8QiEjhep5cJRHWAI%3D"', prepped_req.headers['Authorization'] )
assignments = outcomes.get_assignments_for_problem( problem_descriptor, user_id, course_key ) for assignment in assignments: assignment.version_number += 1 assignment.save() return assignments
context = { "request": get_request_or_stub() }
serialized_course_team['pk'] = self.course_team.pk serialized_course_team.pop('membership', None)
serialized_course_team['content'] = { 'text': self.content_text() }
FIELD_BLACKLIST = ['last_activity_at', 'team_size']
sort_order = 'name' topics = get_alphabetical_topics(course)
topics_data = self._serialize_and_paginate( TopicsPagination, topics, request, BulkTeamCountTopicSerializer, {'course_id': course.id}, ) topics_data["sort_order"] = sort_order
serializer_ctx["request"] = request
paginator = pagination_cls() page = paginator.paginate_queryset(queryset, request)
serializer = serializer_cls(page, context=serializer_ctx, many=True)
authentication_classes = (OAuth2Authentication, SessionAuthentication) permission_classes = (permissions.IsAuthenticated,) serializer_class = CourseTeamSerializer
course_module = modulestore().get_course(course_key) if course_module is None: return Response(status=status.HTTP_404_NOT_FOUND) result_filter.update({'course_id': course_key})
queryset = queryset.order_by('name')
'user_message': _(u"The ordering {ordering} is not supported").format(ordering=order_by_input),
if not modulestore().has_course(course_key): return Response(status=status.HTTP_404_NOT_FOUND)
page_kwarg = self.kwargs.get(self.paginator.page_query_param) page_query_param = self.request.query_params.get(self.paginator.page_query_param) return page_kwarg or page_query_param or 1
memberships = list(CourseTeamMembership.get_memberships(team_id=team_id))
course_module = modulestore().get_course(course_id)
'user_message': _(u"The ordering {ordering} is not supported").format(ordering=ordering),
course_module = modulestore().get_course(course_id) if course_module is None: return Response(status=status.HTTP_404_NOT_FOUND)
from ...search_indexes import CourseTeamIndexer
from __future__ import unicode_literals
self.user = UserFactory.create(password=self.test_password) self.teams_url = reverse('teams_dashboard', args=[self.course.id])
CourseEnrollmentFactory.create(user=self.user, course_id=self.course.id) self.client.login(username=self.user.username, password=self.test_password)
with self.assertNumQueries(18): self.client.get(self.teams_url)
for topic_id in range(self.NUM_TOPICS): team = CourseTeamFactory.create( name=u"Team for topic {}".format(topic_id), course_id=self.course.id, topic_id=topic_id, )
team.add_user(self.user)
with self.assertNumQueries(24): self.client.get(self.teams_url)
course_one_team = CourseTeamFactory.create(name="Course one team", course_id=self.course.id, topic_id=1)
course_one_team.add_user(self.user)
response = self.client.get(course_one_teams_url) self.assertIn('"teams": {"count": 1', response.content)
cls.create_and_enroll_student( courses=[cls.test_course_1, cls.test_course_2], username='student_enrolled_both_courses_other_team' )
cls.create_and_enroll_student( courses=[cls.test_course_2], username='student_enrolled_public_profile' ) profile = cls.users['student_enrolled_public_profile'].profile profile.year_of_birth = 1970 profile.save()
cls.create_and_enroll_student( courses=[cls.test_course_2], username='student_enrolled_other_course_not_on_team' )
team_list = self.get_teams_list(user=user, expected_status=200, data=course_one_data) self.assertEqual(team_list['count'], 0)
self.solar_team.add_user(self.users[user])
team_list = self.get_teams_list(user=user, expected_status=200, data=course_one_data) self.assertEqual(team_list['count'], 1)
team_list = self.get_teams_list(user=user, expected_status=200, data=course_two_data) self.assertEqual(team_list['count'], 0)
self.verify_expected_team_id(new_teams[0], 'the-best-team') self.verify_expected_team_id(new_teams[1], 'the-best-team') self.assertNotEqual(new_teams[0]['id'], new_teams[1]['id'])
self.verify_expected_team_id(new_teams[2], 'a-really-long-team-n')
self.post_create_membership( 200, self.build_membership_data(user, self.solar_team), user=user )
self.verify_expected_team_id(team, 'fully-specified-team') del team['id']
del team['date_created'] del team['discussion_topic_id']
team_membership = team['membership'] del team['membership']
self.assertEqual(len(team_membership), 1) member = team_membership[0]['user'] self.assertEqual(member['username'], creator)
team = self.post_create_team(data=self.build_team_data( name="New team", course=self.test_course_1, description="Another fantastic team", ), user=user)
result = self.get_team_detail(self.solar_team.team_id, 200, {'expand': 'user'}) self.verify_expanded_private_user(result['membership'][0]['user'])
serializer = None
self.assertIsNotNone(current_last_activity)
self.assertEqual(self.team_membership11.last_activity_at, current_last_activity)
PLATFORM_CLASSES = {IOS.NAME: IOS, Android.NAME: Android}
import datetime
courses = [ course_with_prereq, CourseFactory.create(start=self.NEXT_WEEK, mobile_available=True), CourseFactory.create(visible_to_staff_only=True, mobile_available=True), CourseFactory.create(start=self.LAST_WEEK, mobile_available=True, visible_to_staff_only=False), ]
for course in courses: self.enroll(course.id)
return self.client.patch(url, data=kwargs.get('data', None))
self.api_response(data={"last_visited_module_id": unicode(initial_unit.location)})
return self._get_course_info(request, course)
return self._get_course_info(request, course)
'id': course_id, 'name': course_overview.display_name, 'number': course_overview.display_number_with_default, 'org': course_overview.display_org_with_default,
'start': course_overview.start, 'start_display': course_overview.start_display, 'start_type': course_overview.start_type, 'end': course_overview.end,
'subscription_id': course_overview.clean_id(padding_char='_'),
'courseware_access': has_access( request.user, 'load_mobile', course_overview ).to_json(),
MobileApiConfig(video_profiles="mobile_low,mobile_high,youtube").save()
course_outline = self.api_response().data course_outline[0]['summary'].pop("id") self.assertEqual(course_outline[0]['summary'], expected_output)
MobileApiConfig(video_profiles="mobile_low,youtube").save()
MobileApiConfig(video_profiles="youtube,mobile_high").save()
add_user_to_cohort(cohorts[cohort_index], self.user.username)
remove_user_from_cohort(cohorts[cohort_index], self.user.username)
video_outline = self.api_response().data self.assertEqual(len(video_outline), 0)
self.user.is_staff = True self.user.save() video_outline = self.api_response().data self.assertEqual(len(video_outline), 2)
'name': block.display_name_with_default_escaped, 'category': block.category, 'id': unicode(block.location)
video_data = local_cache['course_videos'].get(video_descriptor.edx_video_id, {})
default_encoded_video = {}
elif video_descriptor.html5_sources: video_url = video_descriptor.html5_sources[0] else: video_url = video_descriptor.source
duration = video_data.get('duration', None) size = default_encoded_video.get('file_size', 0)
transcripts_info = video_descriptor.get_transcripts_info() transcript_langs = video_descriptor.available_translations(transcripts_info, verify_assets=False)
from __future__ import unicode_literals
from __future__ import unicode_literals
from datetime import timedelta
self.login_and_enroll() self.logout()
other = UserFactory.create() self.client.login(username=other.username, password='test') self.enroll() self.logout()
self.login() self.api_response(expected_response_code=404, username=other.username)
if role: role(self.course.id).add_users(self.user)
response = self.api_response()
self.assertNotIn("\"/static/", response.content)
underlying_updates = modulestore().get_item(updates_usage_key) underlying_content = underlying_updates.items[0]['content'] if new_format else underlying_updates.data self.assertIn("\"/static/", underlying_content)
response = self.api_response() self.assertNotIn('\'/static/', response.data['handouts_html'])
response = self.api_response() self.assertIn("/courses/{}/jump_to_id/".format(self.course.id), response.data['handouts_html'])
response = self.api_response() self.assertIn("/courses/{}/".format(self.course.id), response.data['handouts_html'])
return Response({'handouts_html': None})
msg += _('Failed in authenticating {username}, error {error}\n').format( username=euser, error=err ) continue
msg = _('All ok!')
msg += _('Email address must end in {domain}').format(domain="@{0}".format(email_domain)) return msg
output = StringIO.StringIO() import_log_handler = logging.StreamHandler(output) import_log_handler.setLevel(logging.DEBUG)
for logger in loggers: logger.setLevel(logging.NOTSET) logger.removeHandler(import_log_handler)
_('Git Commit'), _('Last Change'), _('Last Editor')],
mongo_db = { 'host': 'localhost', 'user': '', 'password': '', 'db': 'xlog', }
if not request.user.is_staff: raise Http404 cilset = CourseImportLog.objects.order_by('-created')
parser.add_argument('repository_url') parser.add_argument('--directory_path', action='store') parser.add_argument('--repository_branch', action='store')
if not os.path.isdir(self.git_repo_dir / 'edx4edx'): os.mkdir(self.git_repo_dir / 'edx4edx')
call_command('git_add_course', self.TEST_REPO, directory_path=self.git_repo_dir / 'edx4edx_lite', repository_branch=self.TEST_BRANCH)
if not os.path.isdir(repo_dir): os.mkdir(repo_dir) self.addCleanup(shutil.rmtree, repo_dir)
with self.assertRaises(GitImportErrorRemoteBranchMissing): git_import.add_repo(self.TEST_REPO, repo_dir / 'edx4edx_lite', 'asdfasdfasdf')
git_import.add_repo(self.TEST_REPO, repo_dir / 'edx4edx_lite', self.TEST_BRANCH) def_ms = modulestore() self.assertIsNotNone(def_ms.get_course(self.TEST_BRANCH_COURSE))
git_import.add_repo(self.TEST_REPO, repo_dir / 'edx4edx_lite', self.TEST_BRANCH)
repo_dir = self.git_repo_dir if not os.path.isdir(repo_dir): os.mkdir(repo_dir) self.addCleanup(shutil.rmtree, repo_dir)
output = StringIO.StringIO() test_log_handler = logging.StreamHandler(output) test_log_handler.setLevel(logging.DEBUG) glog = git_import.log glog.addHandler(test_log_handler)
MESSAGE = _('The specified remote branch is not available.')
MESSAGE = _('Unable to switch to specified branch. Please check your branch name.')
output = StringIO.StringIO() import_log_handler = logging.StreamHandler(output) import_log_handler.setLevel(logging.DEBUG)
for logger in loggers: logger.setLevel(logging.NOTSET) logger.removeHandler(import_log_handler)
mongouri = 'mongodb://{user}:{password}@{host}:{port}/{db}'.format(**mongo_db)
course = def_ms.courses.get(course_path, None)
course = def_ms.get_course(SlashSeparatedCourseKey('MITx', 'edx4edx', 'edx4edx'))
response = self._add_edx4edx() self.assertIn(GitImportErrorNoDir(settings.GIT_REPO_DIR).message, response.content.decode('UTF-8'))
response = self.client.get(reverse('sysadmin_courses')) self.assertNotRegexpMatches(response.content, table_re)
response = self._add_edx4edx() self.assertRegexpMatches(response.content, table_re)
self.assertIn('/gitlogs/MITx/edx4edx/edx4edx', response.content)
import_logs = CourseImportLog.objects.all() import_logs.delete()
def_ms = modulestore() course = def_ms.get_course(SlashSeparatedCourseKey('MITx', 'edx4edx', 'edx4edx')) CourseStaffRole(course.id).add_users(self.user)
from django.core.cache import cache from django.test.utils import override_settings from lang_pref import LANGUAGE_KEY
from edx_oauth2_provider.tests import IDTokenTestCase, UserInfoTestCase
cache.clear()
CourseFactory.create(emit_signals=True)
value = anonymous_id_for_user(data['user'], None) return value
language = UserPreference.get_value(data['user'], LANGUAGE_KEY)
if not language: language = settings.LANGUAGE_CODE
if values is not None: course_ids = list(set(course_ids) & set(values))
def _get_courses_with_access_type(self, user, access_type):
if not GlobalStaff().has_user(user): course_keys = [course_key for course_key in course_keys if has_access(user, access_type, course_key)]
if data.get('essential'): return super(IDTokenHandler, self).claim_instructor_courses(data) else: return None
if data.get('essential'): return super(IDTokenHandler, self).claim_staff_courses(data) else: return None
content_type, __ = mimetypes.guess_type(template)
resp = self.client.get(url, HTTP_HOST=settings.MICROSITE_TEST_HOSTNAME) self.assertContains(resp, settings.MICROSITE_CONFIGURATION['test_microsite']['email_from_address'])
data = dict(self.data.items()) self.cleaned_data['confirmed'] = data['confirmed'] = 'true' self.data = data is_valid = False
from .index import * from .certificate import * from .enrollments import * from .refund import * from .programs import IssueProgramCertificatesView
import logging
return login_required(inner)
SupportStaffRole().remove_users(self.admin) response = self.client.get('/support/') self.assertTrue(response.status_code, 302)
self.client.logout() response = self.client.get(url)
redirect_url = "{login_url}?next={original_url}".format( login_url=reverse("signin_user"), original_url=url, ) self.assertRedirects(response, redirect_url)
for url_name in self.EXPECTED_URL_NAMES: self.assertContains(response, reverse(url_name))
response = self.client.get(reverse("support:certificates")) self.assertContains(response, "userFilter: ''")
if 'course_id' in data and data['course_id'] is None:
from django.core.urlresolvers import reverse from django.test import TestCase import mock from edx_oauth2_provider.tests.factories import AccessTokenFactory, ClientFactory
url(r'^{}/all_sequential_open_distrib$'.format(settings.COURSE_ID_PATTERN), 'class_dashboard.views.all_sequential_open_distrib', name="all_sequential_open_distrib"),
url(r'^{}/problem_grade_distribution/(?P<section>\d+)$'.format(settings.COURSE_ID_PATTERN), 'class_dashboard.views.section_problem_grade_distrib', name="section_problem_grade_distrib"),
url(r'^get_students_opened_subsection$', 'class_dashboard.dashboard_data.get_students_opened_subsection', name="get_students_opened_subsection"),
url(r'^get_students_problem_grades$', 'class_dashboard.dashboard_data.get_students_problem_grades', name="get_students_problem_grades"),
url(r'^post_metrics_data_csv_url', 'class_dashboard.dashboard_data.post_metrics_data_csv', name="post_metrics_data_csv"),
course_key = SlashSeparatedCourseKey.from_deprecated_string(course_id) if has_instructor_access_for_class(request.user, course_key): try: data = dashboard_data.get_d3_sequential_open_distrib(course_key)
course_key = SlashSeparatedCourseKey.from_deprecated_string(course_id) if has_instructor_access_for_class(request.user, course_key): try: data = dashboard_data.get_d3_problem_grade_distrib(course_key)
course_key = SlashSeparatedCourseKey.from_deprecated_string(course_id) if has_instructor_access_for_class(request.user, course_key): try: data = dashboard_data.get_d3_section_grade_distrib(course_key, section)
MAX_SCREEN_LIST_LENGTH = 250
for row in db_query: curr_problem = course_id.make_usage_key_from_deprecated_string(row['module_state_key'])
if curr_problem in prob_grade_distrib: prob_grade_distrib[curr_problem]['grade_distrib'].append((row['grade'], row['count_grade']))
total_student_count[curr_problem] = total_student_count.get(curr_problem, 0) + row['count_grade']
db_query = models.StudentModule.objects.filter( course_id__exact=course_id, module_type__exact="sequential", ).values('module_state_key').annotate(count_sequential=Count('module_state_key'))
sequential_open_distrib = {} for row in db_query: row_loc = course_id.make_usage_key_from_deprecated_string(row['module_state_key']) sequential_open_distrib[row_loc] = row['count_sequential']
for row in db_query: row_loc = course_id.make_usage_key_from_deprecated_string(row['module_state_key']) if row_loc not in prob_grade_distrib: prob_grade_distrib[row_loc] = { 'max_grade': 0, 'grade_distrib': [], }
course = modulestore().get_course(course_id, depth=4)
if child.location.category == 'problem': c_problem += 1 stack_data = []
label = "P{0}.{1}.{2}".format(c_subsection, c_unit, c_problem)
if child.location in prob_grade_distrib:
problem_info = prob_grade_distrib[child.location]
problem_name = own_metadata(child).get('display_name', '')
student_count_percent = 0 if total_student_count.get(child.location, 0) > 0: student_count_percent = count_grade * 100 / total_student_count[child.location]
stack_data.append({ 'color': percent, 'value': count_grade, 'tooltip': tooltip, 'module_url': child.location.to_deprecated_string(), })
course = modulestore().get_course(course_id, depth=2)
for section in course.get_children(): curr_section = {} curr_section['display_name'] = own_metadata(section).get('display_name', '') data = [] c_subsection = 0
for subsection in section.get_children(): c_subsection += 1 subsection_name = own_metadata(subsection).get('display_name', '')
tooltip = { 'type': 'subsection', 'num_students': num_students, 'subsection_num': c_subsection, 'subsection_name': subsection_name }
course = modulestore().get_course(course_id, depth=4)
grade_distrib = get_problem_set_grade_distrib(course_id, problem_set)
for problem in problem_set: stack_data = []
for student in students[0:MAX_SCREEN_LIST_LENGTH + 1]: results.append({ 'name': student['student__profile__name'], 'username': student['student__username'], })
del results[-1] max_exceeded = True
filename = sanitize_filename(' '.join(tooltip.split(' ')[3:]))
del results[-1] max_exceeded = True
if data_type == 'subsection': for tooltip_dict in tooltips[index]: num_students = tooltip_dict['num_students'] subsection = tooltip_dict['subsection_name'] results.append(['', subsection, num_students])
results.append(['', label, problem_name, count_grade, student_count_percent, percent])
self.assertEquals(2, len(response_results)) self.assertEquals(True, response_max_exceeded)
self.assertEquals(2, len(response_results)) self.assertEquals(True, response_max_exceeded)
self.assertEquals(USER_COUNT + 1, len(response.content.splitlines()))
self.assertEquals(3, len(response.content.splitlines()))
self.assertEquals(4, len(response.content.splitlines()))
SUBSCRIBE_BATCH_SIZE = 1000
for batch in chunk(formated_data, SUBSCRIBE_BATCH_SIZE): result = mailchimp.listBatchSubscribe(id=list_id, batch=batch, double_optin=False, update_existing=True)
segments = mailchimp.listStaticSegments(id=list_id) for seg in segments: if seg['name'].startswith('random'): mailchimp.listStaticSegmentDel(id=list_id, seg_id=seg['id'])
emails = list(emails)
if DEBUG_ACCESS: log.debug(*args, **kwargs)
return start
def get_context(self): context = super(TodaysDate, self).get_context() context['date'] = '' return context
if enrollment_mode is None and is_active is None: return True
return is_active and enrollment_mode in CourseMode.UPSELL_TO_VERIFIED_MODES
inheritable = InheritanceMixin.fields.keys() if name in inheritable: for ancestor in _lineage(block): if self.get_override(ancestor, name) is not NOTSET: return False
module_state_key = LocationKeyField(max_length=255, db_index=True, db_column='module_id') student = models.ForeignKey(User, db_index=True)
state = models.TextField(null=True, blank=True)
'student_id': self.student_id, 'module_state_key': self.module_state_key, 'state': str(self.state)[:20],
created = models.DateTimeField(db_index=True) state = models.TextField(null=True, blank=True) grade = models.FloatField(null=True, blank=True) max_grade = models.FloatField(null=True, blank=True)
student_module__in=[module.id for module in student_modules]
if settings.FEATURES.get('ENABLE_READING_FROM_MULTIPLE_HISTORY_TABLES'): history_entries += StudentModuleHistory.objects.prefetch_related('student_module').filter( student_module__in=student_modules ).order_by('-id')
if not settings.FEATURES.get('ENABLE_CSMH_EXTENDED'): post_save.connect(save_history, sender=StudentModule)
field_name = models.CharField(max_length=64, db_index=True)
value = models.TextField(default='null')
usage_id = LocationKeyField(max_length=255, db_index=True)
module_type = BlockTypeKeyField(max_length=64, db_index=True)
required_content = milestones_helpers.get_required_content(course, user)
gated_content = gating_api.get_gated_content(course, user)
if not user_must_complete_entrance_exam(request, user, course): required_content = [content for content in required_content if not content == course.entrance_exam_id]
display_id = slugify(chapter.display_name_with_default_escaped) local_hide_from_toc = False if required_content: if unicode(chapter.location) not in required_content: local_hide_from_toc = True
if chapter.hide_from_toc or local_hide_from_toc: continue
if gated_content and unicode(section.location) in gated_content: continue if section.hide_from_toc: continue
timed_exam_attempt_context = None try: timed_exam_attempt_context = get_attempt_status_summary( user.id, unicode(course.id), unicode(section.location) )
log.exception(ex)
section_context.update({ 'proctoring': timed_exam_attempt_context, })
log.exception("Error in get_module") return None
score_bucket = get_score_bucket(grade, max_grade)
_fulfill_content_milestones( user, course_id, descriptor.location, )
SCORE_CHANGED.send( sender=None, points_possible=event['max_value'], points_earned=event['value'], user_id=user_id, course_id=unicode(course_id), usage_id=unicode(descriptor.location) )
module.runtime = inner_system inner_system.xmodule_instance = module
block_wrappers = []
block_wrappers.append(partial( replace_static_urls, getattr(descriptor, 'data_dir', None), course_id=course_id, static_asset_path=static_asset_path or descriptor.static_asset_path ))
block_wrappers.append(partial(replace_course_urls, course_id))
if position is not None: try: position = int(position) except (ValueError, TypeError): log.exception('Non-integer %r passed as position.', position) position = None
if has_access(user, u'staff', descriptor.location, course_id): system.error_descriptor_class = ErrorDescriptor else: system.error_descriptor_class = NonStaffErrorDescriptor
for key in ['xqueue_header', 'xqueue_body']: if key not in data: raise Http404
data.update({'queuekey': header['lms_key']})
try: instance.handle_ajax(dispatch, data) instance.save() except: log.exception("error processing ajax call") raise
if descriptor_orig_usage_key is not None: tracking_context['module']['original_usage_key'] = unicode(descriptor_orig_usage_key) tracking_context['module']['original_usage_version'] = unicode(descriptor_orig_version)
log.debug("No module %s for user %s -- access denied?", usage_key, user) raise Http404
files = request.FILES or {} error_msg = _check_files_limits(files) if error_msg: return JsonResponse({'success': error_msg}, status=413)
try: course_key = CourseKey.from_string(course_id) except InvalidKeyError: raise Http404
newrelic.agent.add_custom_parameter('course_id', unicode(course_key)) newrelic.agent.add_custom_parameter('org', unicode(course_key.org))
except NotFoundError: log.exception("Module indicating to user that request doesn't exist") raise Http404
except ProcessingError as err: log.warning("Module encountered an error while processing AJAX call", exc_info=True) return JsonResponse({'success': err.args[0]}, status=200)
except Exception: log.exception("error executing xblock handler") raise
if len(inputfiles) > settings.MAX_FILEUPLOADS_PER_INPUT: msg = 'Submission aborted! Maximum %d files may be submitted at once' % \ settings.MAX_FILEUPLOADS_PER_INPUT return msg
for inputfile in inputfiles:
from __future__ import division
cache_key = u"{}".format(course.id)
max_scores_cache.fetch_from_remote(field_data_cache.scorable_locations)
for section_format, sections in grading_context['graded_sections'].iteritems(): format_scores = [] for section in sections: section_descriptor = section['section_descriptor'] section_name = section_descriptor.display_name_with_default_escaped
if not should_grade_section: should_grade_section = any( descriptor.location.to_deprecated_string() in submissions_scores for descriptor in section['xmoduledescriptors'] )
if should_grade_section: scores = []
graded = False
course.set_grading_policy(course.grading_policy) grade_summary = course.grader.grade(totaled_scores, generate_random_scores=settings.GENERATE_PROFILE_SCORES)
grade_summary['percent'] = round(grade_summary['percent'] * 100 + 0.05) / 100
grade_summary['raw_scores'] = raw_scores
descending_grades = sorted(grade_cutoffs, key=lambda x: grade_cutoffs[x], reverse=True) for possible_grade in descending_grades: if percentage >= grade_cutoffs[possible_grade]: letter_grade = possible_grade break
max_scores_cache.fetch_from_remote(field_data_cache.scorable_locations)
gated_content = gating_api.get_gated_content(course, student)
for chapter_module in course_module.get_display_items(): if chapter_module.hide_from_toc: continue
with outer_atomic(): if section_module.hide_from_toc or unicode(section_module.location) in gated_content: continue
if weight is None or raw_total == 0: return (raw_correct, raw_total) return (float(raw_correct) * weight / raw_total, float(weight))
return (None, None)
if total is None: return (None, None) else: max_scores_cache.set(problem_descriptor.location, total)
request.session = {} gradeset = grade(student, request, course, keep_raw_scores) yield student, gradeset, ""
raise CoursewareAccessException(access_response)
if not ((user.id and CourseEnrollment.is_enrolled(user, course.id)) or has_access(user, 'staff', course)): raise UserNotEnrolled(course.id)
field_data_cache = FieldDataCache([], course.id, request.user) about_module = get_module( request.user, request, loc, field_data_cache, log_if_not_found=False, wrap_xmodule_display=False, static_asset_path=course.static_asset_path, course=course )
field_data_cache = FieldDataCache([], course.id, user)
key = lambda course: course.sorting_score courses = sorted(courses, key=key)
return u"//{}/{}/{}".format(settings.CMS_BASE, page, unicode(course.id))
return u"//{}/{}/{}".format(settings.CMS_BASE, page, block.location)
section_descriptor = modulestore().get_item(section_key, depth=3)
if name == 'due': return None if name == 'start' and block.category != 'course': return None
self._raise_unless_scope_is_allowed(key)
assert key.user_id == self.user.id
assert key.user_id == self.user.id
saved_fields.extend(key.field_name for key in set_many_data)
assert key.user_id == self.user.id
assert key.user_id == self.user.id
assert key.user_id == self.user.id
title = ugettext_noop("Textbooks") is_collection = True is_default = False
title = ugettext_noop('Discussion') priority = None is_default = False
course_tab_list += _get_dynamic_tabs(course, user) return course_tab_list
modulestore = XMLModuleStore( data_dir, default_class=None, source_dirs=source_dirs )
validators = ( traverse_tree, )
print "======== Roundtrip diff: ========="
correct_map = CorrectMap() if 'correct_map' in state_dict: correct_map.set_dict(state_dict['correct_map'])
correct = 0 for key in correct_map: correct += correct_map.get_npoints(key)
if cls.test_course_key not in [c.id for c in courses]: import_course_from_xml( store, ModuleStoreEnum.UserID.mgmt_command, DATA_DIR, XML_COURSE_DIRS, create_if_not_present=True )
if isinstance(module, DiscussionDescriptor) and 'discussion_id' not in items: items['discussion_id'] = module.discussion_id
inherited_metadata_filter_list = list(filtered_metadata.keys()) inherited_metadata_filter_list.extend(INHERITED_FILTER_LIST)
pipe_results = False if filename == '-': filename = mktemp() pipe_results = True
from __future__ import unicode_literals
if not user: user = AnonymousUser()
if isinstance(obj, CourseDescriptor): return _has_access_course(user, action, obj)
if isinstance(obj, XBlock): return _has_access_descriptor(user, action, obj, course_key)
raise TypeError("Unknown object type in has_access(): '{0}'" .format(type(obj)))
course_key = courselike.id
if user is not None and user.is_authenticated(): if CourseEnrollmentAllowed.objects.filter(email=user.email, course_id=course_key): return ACCESS_GRANTED
return ACCESS_GRANTED
merged_access = descriptor.merged_group_access if False in merged_access.values(): log.warning("Group access check excludes all students, access will be denied.", exc_info=True) return ACCESS_DENIED
user_groups = {} for partition, groups in partition_groups: user_groups[partition.id] = partition.scheme.get_group_for_user( course_key, user, partition, )
if not all(user_groups.get(partition.id) in groups for partition, groups in partition_groups): return ACCESS_DENIED
return ACCESS_GRANTED
return has_access(user, action, xmodule.descriptor, course_key)
API_DATADOG_SAMPLE_RATE = 0.1
if state == {}: continue
finish_time = time() self._ddog_histogram(evt_time, 'get_many.blks_out', block_count) self._ddog_histogram(evt_time, 'get_many.response_time', (finish_time - evt_time) * 1000)
if self.user is not None and self.user.username == username: user = self.user else: user = User.objects.get(username=username)
return
student_module.save(force_update=True)
if created: self._ddog_increment(evt_time, 'set_many.state_created') else: self._ddog_increment(evt_time, 'set_many.state_updated')
self._ddog_histogram(evt_time, 'set_many.fields_in', len(state))
num_new_fields_set = num_fields_after - num_fields_before self._ddog_histogram(evt_time, 'set_many.fields_set', num_new_fields_set)
num_fields_updated = max(0, len(state) - num_new_fields_set) self._ddog_histogram(evt_time, 'set_many.fields_updated', num_fields_updated)
student_module.save(force_update=True)
finish_time = time() self._ddog_histogram(evt_time, 'delete_many.response_time', (finish_time - evt_time) * 1000)
if not history_entries: raise self.DoesNotExist()
if state is not None: state = json.loads(state)
if state == {}: state = None
REQUIREMENTS_DISPLAY_MODES = CourseMode.CREDIT_MODES + [CourseMode.VERIFIED]
if user_must_complete_entrance_exam(request, user, course): return redirect(reverse('courseware', args=[unicode(course.id)]))
if request.user.is_authenticated() and survey.utils.must_answer_survey(course, user): return redirect(reverse('course_survey', args=[unicode(course.id)]))
url_to_enroll = reverse(course_about, args=[course_id]) if settings.FEATURES.get('ENABLE_MKTG_SITE'): url_to_enroll = marketing_link('COURSES')
context['last_accessed_courseware_url'] = None if SelfPacedConfiguration.current().enable_course_home_improvements: context['last_accessed_courseware_url'] = get_last_accessed_courseware(course, request, user)
context['disable_student_access'] = True
return _("{currency_symbol}{price}").format(currency_symbol=currency_symbol, price=price)
return _('Free')
return redirect(reverse('about_course', args=[unicode(course_key)]))
in_cart = False reg_then_add_to_cart_link = ""
registration_price = CourseMode.min_course_price_for_currency( course_key, settings.PAID_COURSE_REGISTRATION_CURRENCY[0] ) course_price = get_cosmetic_display_price(course, registration_price) can_add_course_to_cart = _is_shopping_cart_enabled and registration_price
can_enroll = bool(has_access(request.user, 'enroll', course)) invitation_only = course.invitation_only is_course_full = CourseEnrollment.objects.is_course_full(course)
active_reg_button = not(registered or is_course_full or not can_enroll)
pre_requisite_courses = get_prerequisite_courses_display(course)
overview = CourseOverview.get_from_id(course.id)
if survey.utils.must_answer_survey(course, request.user): return redirect(reverse('course_survey', args=[unicode(course.id)]))
student = request.user
if not has_access_on_students_profiles: raise Http404 try: student = User.objects.get(id=student_id) except (ValueError, User.DoesNotExist): raise Http404
student = User.objects.prefetch_related("groups").get(id=student.id)
enrollment_mode, is_active = CourseEnrollment.enrollment_mode_for_user(student, course_key) show_generate_cert_btn = ( is_active and CourseMode.is_eligible_for_certificate(enrollment_mode) and certs_api.cert_generation_enabled(course_key) )
if not (settings.FEATURES.get("ENABLE_CREDIT_ELIGIBILITY", False) and is_credit_course(course_key)): return None
enrollment = CourseEnrollment.get_enrollment(student, course_key) if enrollment and enrollment.mode not in REQUIREMENTS_DISPLAY_MODES: return None
non_eligible_statuses = ['failed', 'declined']
elif any(requirement['status'] in non_eligible_statuses for requirement in requirement_statuses): eligibility_status = "not_eligible"
else: eligibility_status = "partial_eligible"
if (student_username != request.user.username) and (not staff_access): raise PermissionDenied
if not course.course_survey_name: return redirect(redirect_url)
try: course = get_course_with_access(request.user, 'load', course_key, check_if_enrolled=check_if_enrolled) except UserNotEnrolled: raise Http404("Course not found.")
block, _ = get_module_by_usage_id( request, unicode(course_key), unicode(usage_key), disable_staff_debug_info=True, course=course )
username = data['username'] if request.user.username != username: return HttpResponseForbidden()
return HttpResponseBadRequest(u'Could not parse request JSON.')
return HttpResponseBadRequest(u'Could not parse request course key.')
return HttpResponseBadRequest(u'The field {} is required.'.format(err.message))
return HttpResponse(status=status.HTTP_500_INTERNAL_SERVER_ERROR)
raise
self.request.user = self.effective_user
if not self._is_masquerading_as_student(): raise Http404('No {block_type} found with name {url_name}'.format( block_type=block_type, url_name=url_name, ))
self.section = modulestore().get_item(self.section.location, depth=None) self.field_data_cache.add_descriptor_descendents(self.section, depth=None)
self.section = get_module_for_descriptor( self.effective_user, self.request, self.section, self.field_data_cache, self.course_key, self.position, course=self.course, )
if self.section.default_tab: courseware_context['default_tab'] = self.section.default_tab
if position != seq_module.position: seq_module.position = position
if is_staff: user = User.objects.get(email=email) user.is_staff = True user.save()
self._login(staff_email, staff_password, should_succeed=False, err_msg_check="Your password has expired due to password policy on this account")
self._update_password(staff_email, "updated") self._login(staff_email, "updated")
resp = self.client.post('/password_reset_confirm/{0}-{1}/'.format(uidb36, token), { 'new_password1': 'bar', 'new_password2': 'bar' }, follow=True)
user = User.objects.get(email=staff_email) token = default_token_generator.make_token(user) uidb36 = int_to_base36(user.id)
user = User.objects.get(email=staff_email) token = default_token_generator.make_token(user) uidb36 = int_to_base36(user.id)
self.assertIn( err_msg, resp.content )
user = User.objects.get(email=staff_email) token = default_token_generator.make_token(user) uidb36 = int_to_base36(user.id)
user = User.objects.get(email=staff_email) token = default_token_generator.make_token(user) uidb36 = int_to_base36(user.id)
user = User.objects.get(email=staff_email) token = default_token_generator.make_token(user) uidb36 = int_to_base36(user.id)
user = User.objects.get(email=staff_email) token = default_token_generator.make_token(user) uidb36 = int_to_base36(user.id)
user = User.objects.get(email=staff_email) token = default_token_generator.make_token(user) uidb36 = int_to_base36(user.id)
course = store.get_course(course_key) self.enroll(course, True)
items = store.get_items(course_key)
for descriptor in items:
store.get_items(SlashSeparatedCourseKey('abc', 'def', 'ghi'), qualifiers={'category': 'vertical'})
cls.course_hidden_visibility = CourseFactory.create( display_name='Hidden_course', org='TestMicrositeX', catalog_visibility=CATALOG_VISIBILITY_NONE, emit_signals=True, )
cls.course_with_visibility = CourseFactory.create( display_name='visible_course', org='TestMicrositeX', course="foo", catalog_visibility=CATALOG_VISIBILITY_CATALOG_AND_ABOUT, emit_signals=True, )
self.assertContains(resp, 'Robot_Super_Course')
self.assertContains(resp, 'visible_course')
self.assertNotContains(resp, 'Robot_Course_Outside_Microsite')
self.assertNotContains(resp, 'Hidden_course')
self.assertContains(resp, 'This is a Test Microsite footer')
self.assertNotContains(resp, '<section class="university-partners university-partners2x6">')
self.assertNotContains(resp, 'Explore free courses from')
self.assertNotContains(resp, 'Robot_Super_Course')
self.assertContains(resp, 'Robot_Course_Outside_Microsite')
self.assertNotContains(resp, 'This is a Test Microsite footer')
resp = self.client.get(reverse('dashboard'), HTTP_HOST=settings.MICROSITE_TEST_HOSTNAME) self.assertContains(resp, 'Robot_Super_Course') self.assertNotContains(resp, 'Robot_Course_Outside_Microsite')
resp = self.client.get(reverse('dashboard')) self.assertNotContains(resp, 'Robot_Super_Course') self.assertContains(resp, 'Robot_Course_Outside_Microsite')
self.coach = AdminFactory.create(password="test") self.client.login(username=self.coach.username, password="test")
role = CourseCcxCoachRole(self.course.id) role.add_users(self.coach) self.request_factory = RequestFactory()
self.assertTrue(access.has_ccx_coach_role(self.coach, ccx_locator))
self.setup_user() self.assertFalse(access.has_ccx_coach_role(self.user, ccx_locator))
CourseEnrollment.enroll(student, ccx_locator)
request = self.request_factory.get(reverse('about_course', args=[unicode(ccx_locator)])) request.user = student mako_middleware_process_request(request)
CourseEnrollmentFactory(user=self.student, course_id=self.course.id)
self.assertTrue( bool(access.has_staff_access_to_preview_mode(self.global_staff, obj=self.course, course_key=course_key)) )
CourseEnrollmentFactory(user=self.student, course_id=self.course.id)
user = StaffFactory.create(course_key=course.id) self.assertTrue(access._has_access_course(user, 'enroll', course))
fulfill_course_milestone(pre_requisite_course.id, user) self.assertTrue(access._has_access_course(user, 'view_courseware_with_prerequisites', course))
self._install_masquerade(self.course_staff) self.assertEqual( 'student', access.get_user_role(self.course_staff, self.course_key) )
self._install_masquerade(self.course_instructor) self.assertEqual( 'student', access.get_user_role(self.course_instructor, self.course_key) )
if user_attr_name == 'user_anonymous': user = AnonymousUserFactory() else: user = getattr(self, user_attr_name) user = User.objects.get(id=user.id)
num_queries = 1
num_queries = 2
UserCourseTagFactory( user=self.student, course_id=self.course.id, key='xblock.partition_service.partition_{0}'.format(self.partition.id), value=str(user_tag) )
self.assertIn('<button class="{} inactive progress-0 nav-item"'.format(self.ICON_CLASSES[user_tag]), content) for tooltip in self.TOOLTIPS[user_tag]: self.assertIn(tooltip, content)
for visible in self.VISIBLE_CONTENT[user_tag]: self.assertIn(visible, content)
VISIBLE_CONTENT = [ ['class=&#34;problems-wrapper'], ['Some HTML for group 1'] ]
super(TestSplitTestVert, self).setUp()
VISIBLE_CONTENT = [ ['class=&#34;problems-wrapper'], ['Some HTML for group 1'] ]
super(TestVertSplitTestVert, self).setUp()
course.position = 2 course.save()
import json from functools import partial import factory from factory.django import DjangoModelFactory
with super(TestNavigation, cls).setUpClassAndTestData(): cls.test_course = CourseFactory.create() cls.test_course_proctored = CourseFactory.create() cls.course = CourseFactory.create()
resp = self.client.get(reverse('dashboard')) self.assertEquals(resp.status_code, 200)
time.sleep(2)
self.assertRedirects(resp, settings.LOGIN_REDIRECT_URL + '?next=' + reverse('dashboard'))
self.assertIn(REG_STR, resp.content)
self.xml_data = "about page 463139"
resp = self.client.get(url) self.assertEqual(resp.status_code, 200) self.assertIn("Course is full", resp.content)
result = self.enroll(self.course) self.assertFalse(result)
self.assertNotIn(REG_STR, resp.content)
self.assertNotIn(REG_STR, resp.content)
self.assertIn(REG_STR, resp.content)
now = datetime.datetime.now(pytz.UTC) tomorrow = now + datetime.timedelta(days=1) nextday = tomorrow + datetime.timedelta(days=1)
self.assertNotIn(REG_STR, resp.content)
self.assertNotIn('<span class="important-dates-item-text">$10</span>', resp.content)
CourseEnrollment.enroll(self.user, self.course.id)
self.assertIn('<span class="important-dates-item-text">$10</span>', resp.content)
CourseEnrollment.enroll(self.user, course.id)
self.coach = coach = AdminFactory.create(password="test") self.client.login(username=coach.username, password="test")
ccx = CcxFactory(course_id=self.course.id, coach=self.coach) ccx_locator = CCXLocator.from_course_locator(self.course.id, unicode(ccx.id))
self.assertGreater(exam_score * 100, 50)
chaos_user = UserFactory() locked_toc = self._return_table_of_contents() for toc_section in self.expected_locked_toc: self.assertIn(toc_section, locked_toc)
locked_toc = self._return_table_of_contents() for toc_section in self.expected_locked_toc: self.assertIn(toc_section, locked_toc)
self.client.logout() staff_user = StaffFactory(course_key=self.course.id) staff_user.is_staff = True self.client.login(username=staff_user.username, password='test')
self.request.user = staff_user unlocked_toc = self._return_table_of_contents() for toc_section in self.expected_unlocked_toc: self.assertIn(toc_section, unlocked_toc)
self._assert_chapter_loaded(self.course, self.chapter)
module = get_module( user, request, problem.scope_ids.usage_id, field_data_cache, )._xmodule module.system.publish(problem, 'grade', grade_dict)
self.assertEqual(orphan_sequential.location.block_type, root.location.block_type) self.assertEqual(orphan_sequential.location.block_id, root.location.block_id)
from django.test import TestCase from nose.plugins.attrib import attr
store.get_items(SlashSeparatedCourseKey('a', 'b', 'c'), qualifiers={'category': 'vertical'})
tab = tab_class(tab_dict=dict_tab)
self.assertEqual(tab.name, expected_name)
self.assertEqual(tab.link_func(self.course, self.reverse), expected_link)
self.assertEqual(tab.tab_id, expected_tab_id)
self.assertTrue(tab.validate(dict_tab)) if invalid_dict_tab: with self.assertRaises(xmodule_tabs.InvalidTabsException): tab.validate(invalid_dict_tab)
self.check_get_and_set_methods(tab)
self.check_tab_json_methods(tab)
self.check_tab_equality(tab, dict_tab)
return tab
tab_content = get_static_tab_contents(request, course, tab) self.assertIn(self.course.id.to_deprecated_string(), tab_content) self.assertIn('static_tab', tab_content)
self.xml_data = "static 463139" self.xml_url = "8e4cce2b4aaf4ba28b1220804619e41f"
instructor = InstructorFactory(course_key=self.course.id) self.client.logout() self.client.login(username=instructor.username, password='test')
unique_tab_types = [ CoursewareTab.type, CourseInfoTab.type, 'textbooks', 'pdf_textbooks', 'html_textbooks', ]
{'type': unique_tab_type}, {'type': unique_tab_type},
self.set_up_books(1)
self.course.tabs = self.all_valid_tab_list
for i, tab in enumerate(xmodule_tabs.CourseTabList.iterate_displayable( self.course, inline_collections=False )): self.assertEquals(tab.type, self.course.tabs[i].type)
self.assertIn( {'type': 'html_textbooks'}, list(xmodule_tabs.CourseTabList.iterate_displayable(self.course, inline_collections=False)), )
self.course.html_textbooks = [] self.assertNotIn( {'type': 'html_textbooks'}, list(xmodule_tabs.CourseTabList.iterate_displayable(self.course, inline_collections=False)), )
self.assertEquals(xmodule_tabs.CourseTabList.get_tab_by_type(self.course.tabs, tab.type), tab)
self.assertEquals(xmodule_tabs.CourseTabList.get_tab_by_id(self.course.tabs, tab.tab_id), tab)
CATEGORY = "vertical" DATA = '' METADATA = {} MODEL_DATA = {'data': '<some_module></some_module>'}
modulestore().request_cache = None modulestore().metadata_inheritance_cache_subsystem = None
self.users = [ UserFactory.create() for dummy0 in range(self.USER_COUNT) ]
self.release_languages('fa')
response = self.client.get('/?preview-lang=fa-ir') self.assert_tag_has_attr(response.content, "html", "lang", "fa-ir")
response = self.client.get('/?clear-lang') self.assert_tag_has_attr(response.content, "html", "lang", site_lang)
RegistrationFactory(user=self.user)
UserProfileFactory(user=self.user)
self.client = Client()
self.url = reverse('dashboard') self.site_lang = settings.LANGUAGE_CODE
self.release_languages('ar, es-419')
response = self.client.get(self.url) self.assert_tag_has_attr(response.content, "html", "lang", self.site_lang)
self.release_languages('ar, es-419')
_upload_sjson_file(good_sjson, self.item_descriptor.location)
_upload_file(self.srt_file, self.item_descriptor.location, os.path.split(self.srt_file.name)[1])
_upload_file(en_translation, self.item_descriptor.location, en_translation_filename)
_upload_file(self.srt_file, self.item_descriptor.location, uk_translation_filename)
request = Request.blank('/translation/uk') response = self.item.transcript(request=request, dispatch='translation/uk') self.assertEqual(response.status, '404 Not Found')
request = Request.blank('') response = self.item_descriptor.studio_transcript(request=request, dispatch='translation') self.assertEqual(response.status, '400 Bad Request')
request = Request.blank('') response = self.item_descriptor.studio_transcript(request=request, dispatch='translation/uk') self.assertEqual(response.status, '400 Bad Request')
with self.assertRaises(NotFoundError): self.item.get_transcript(transcripts)
self.item.youtube_id_1_0 = None with self.assertRaises(ValueError): self.item.get_transcript(transcripts)
self.course_key = SlashSeparatedCourseKey('edX', 'toy', '2012_Fall')
self.course = modulestore().get_course(self.course.id)
self.chapter = self.store.get_item(self.chapter.location)
for section in self.chapter.get_children(): section.visible_to_staff_only = True self.store.update_item(section, ModuleStoreEnum.UserID.test)
self.assertTrue(CourseEnrollment.is_enrolled(self.global_staff, self.course.id))
mako_middleware_process_request(request) response = views.course_about(request, unicode(course.id)) self.assertEqual(response.status_code, 200) self.assertNotIn(in_cart_span, response.content)
mako_middleware_process_request(request)
mock_user = MagicMock() mock_user.is_authenticated.return_value = False self.assertEqual(views.user_groups(mock_user), [])
self.assertEqual(views.get_cosmetic_display_price(self.course, registration_price), "$99")
self.assertEqual(views.get_cosmetic_display_price(self.course, registration_price), "$10")
self.assertEqual(views.get_cosmetic_display_price(self.course, registration_price), "Free")
self.verify_end_date('edX/toy/TT_2012_Fall')
self.verify_end_date("edX/test_end/2012_Fall", "Sep 17, 2015")
self.verify_end_date("edX/test_about_blob_end_date/2012_Fall", "Learning never ends")
admin = AdminFactory()
self.assertFalse('Invalid' in response.content)
admin = AdminFactory()
admin = AdminFactory.create()
state_client.set( username=admin.username, block_key=usage_key, state={'field_a': 'x', 'field_b': 'y'} )
self.assertContains(response, checkbox_html, html=True) self.assertContains(response, org_name_string)
self.assertNotContains(response, checkbox_html, html=True)
self.assertEqual(response.status_code, 200) self.assertIn('Financial Assistance Application', response.content)
self.assertIn(str(verified_course_audit_track), response.content) for course in ( non_verified_course, verified_course_verified_track, verified_course_deadline_passed, unenrolled_course ): self.assertNotIn(str(course), response.content)
request.user = self.user
mako_middleware_process_request(request)
self.assertEqual(response.status_code, 302) self.assertEqual( response.url, reverse('courseware', args=[course_id]) )
course = self.set_up_course(due_date_display_format=None) text = self.get_text(course) self.assertIn(self.time_with_tz, text)
course = self.set_up_course(due_date_display_format=u"") text = self.get_text(course) self.assertNotIn("due ", text)
mako_middleware_process_request(self.request) return views.progress(self.request, course_id=unicode(course.id), student_id=self.user.id).content
mako_middleware_process_request(self.request) self.request.user = self.user
self.assertIn("2013-SEPTEMBER-16", text)
self.assertIn("2015-JULY-17", text)
mako_middleware_process_request(self.request)
self.assertNotIn(malicious_code, resp.content)
self.course = CourseFactory.create(default_store=default_store)
course = CourseFactory.create(default_store=default_store) not_enrolled_user = UserFactory.create() self.request.user = AdminFactory.create()
CreditCourse.objects.create(course_key=course.id, enabled=True)
CreditProvider.objects.create( provider_id="ASU", enable_integration=True, provider_url="https://credit.example.com/request" )
set_credit_requirements(course.id, requirements)
CertificateGenerationConfiguration(enabled=True).save() resp = views.progress(self.request, course_id=unicode(self.course.id)) self.assertNotContains(resp, 'Request Certificate')
CertificateGenerationConfiguration(enabled=True).save()
certs_api.set_cert_generation_enabled(self.course.id, True)
certificates[0]['is_active'] = False self.store.update_item(self.course, self.user.id)
CertificateGenerationConfiguration(enabled=True).save()
certs_api.set_cert_generation_enabled(self.course.id, True)
self.assertFalse(views.is_course_passed(self.course, None, self.student, self.request))
self.assertTrue(views.is_course_passed(self.course, None, self.student, self.request))
self.assertFalse(views.is_course_passed(self.course, None, self.student, self.request))
self.assertTrue(views.is_course_passed(self.course, None, self.student, self.request))
resp = self.client.post('/courses/def/abc/in_valid/generate_user_cert') self.assertEqual(resp.status_code, HttpResponseBadRequest.status_code) self.assertIn("Course is not valid", resp.content)
resp = self.client.post('/courses/def/generate_user_cert') self.assertEqual(resp.status_code, 404)
has_children = False
mako_middleware_process_request(request)
mako_middleware_process_request(request)
mako_middleware_process_request(request)
self.assertIn("example_source.mp4", self.item_descriptor.render(STUDENT_VIEW).content)
'sources': [u'example.mp4', u'example.webm', u'http://www.meowmix.com'],
'sources': [u'example.mp4', u'example.webm'] + [video['url'] for video in encoded_videos],
cases = [ dict(case_data, edx_video_id=""), dict(case_data, edx_video_id="vid-v1:12345"), ]
result = self.get_result(allow_cache_miss) self.verify_result_with_val_profile(result)
result = self.get_result(allow_cache_miss) self.verify_result_with_fallback_and_youtube(result)
COURSE_SLUG = "100" COURSE_NAME = "test_course"
enrollment_exists = CourseEnrollment.objects.filter( user=self.user, course_id=self.course.id ).exists() self.assertFalse(enrollment_exists)
self.coach = coach = AdminFactory.create(password="test") self.client.login(username=coach.username, password="test")
ccx = CcxFactory(course_id=self.course.id, coach=self.coach) ccx_locator = CCXLocator.from_course_locator(self.course.id, unicode(ccx.id))
self.xml_data = "course info 463139"
self.assert_request_status_code(302, reverse('logout'))
user = User.objects.get(email=email) self.assertFalse(user.is_active) return user
url = reverse('activate', kwargs={'key': activation_key}) self.assert_request_status_code(200, url) self.assertTrue(User.objects.get(email=email).is_active)
response_dict = {(answer_key_prefix + k): v for k, v in responses.items()} resp = self.client.post(modx_url, response_dict)
multi_db = True COURSE_SLUG = "100" COURSE_NAME = "test_course"
self.refresh_course() return problem
if not hasattr(self, 'chapter'): self.chapter = ItemFactory.create( parent_location=self.course.location, category='chapter' )
self.refresh_course() return section
sections_list = [] for chapter in self.get_progress_summary(): sections_list.extend(chapter['sections'])
hw_section = next(section for section in sections_list if section.get('url_name') == hw_url_name) return [s.earned for s in hw_section['scores']]
multi_db = True
self.hw1_names = ['h1p1', 'h1p2'] self.hw2_names = ['h2p1', 'h2p2'] self.hw3_names = ['h3p1', 'h3p2']
student_module = StudentModule.objects.filter( course_id=self.course.id, student=self.student_user ) baseline = BaseStudentModuleHistory.get_history(student_module) self.assertEqual(len(baseline), 3)
self.show_question_answer('p1')
csmh = BaseStudentModuleHistory.get_history(student_module) self.assertEqual(len(csmh), 3)
max_scores_cache.fetch_from_remote([location_to_cache]) self.assertIsNone(max_scores_cache.get(location_to_cache)) self.check_grade_percent(0.33)
max_scores_cache.fetch_from_remote([location_to_cache]) self.assertIsNotNone(max_scores_cache.get(location_to_cache)) self.check_grade_percent(0.33)
mock_get_scores.assert_called_with( self.course.id.to_deprecated_string(), anonymous_id_for_user(self.student_user, self.course.id) )
self.submit_question_answer('H1P1', {'2_1': 'Correct', '2_2': 'Correct'}) self.check_grade_percent(0.25)
credit_course = CreditCourse.objects.create( course_key=self.course.id, enabled=True, )
CreditProvider.objects.create( provider_id="ASU", enable_integration=True, provider_url="https://credit.example.com/request", )
set_credit_requirements(self.course.id, requirements)
multi_db = True
self.refresh_course()
multi_db = True
self.correct_responses[name] = self.SCHEMATIC_CORRECT self.incorrect_responses[name] = self.SCHEMATIC_INCORRECT
self.refresh_course()
self.correct_responses[name] = expect self.incorrect_responses[name] = self.CUSTOM_RESPONSE_INCORRECT
self.refresh_course()
self.correct_responses[name] = self.COMPUTED_ANSWER_CORRECT self.incorrect_responses[name] = self.COMPUTED_ANSWER_INCORRECT
self.refresh_course()
empty_distribution = grades.answer_distributions(self.course.id)
self.submit_question_answer('p1', {'2_1': u'ⓤⓝⓘⓒⓞⓓⓔ'}) self.submit_question_answer('p2', {'2_1': 'Correct'})
self.submit_question_answer('p1', {'2_1': u'Correct'}) self.submit_question_answer('p2', {'2_1': u'Correct'})
self.submit_question_answer('p1', {'2_1': u'Correct'})
self.submit_question_answer('p1', {'2_1': 'Incorrect'})
empty_distribution = grades.answer_distributions(self.course.id)
self.submit_question_answer('p1', {'2_1': u'Correct'})
prb1 = StudentModule.objects.get( course_id=self.course.id, student=self.student_user )
self.submit_question_answer('p2', {'2_1': u'Incorrect'})
UserCourseTagFactory( user=self.student_user, course_id=self.course.id, key='xblock.partition_service.partition_{0}'.format(self.partition.id), value=str(user_partition_group) )
self.add_dropdown_to_section(vertical_1.location, 'H2P1_GROUP1', 1).location.html_id()
self.submit_question_answer('H1P1', {'2_1': 'Correct', '2_2': 'Incorrect'})
homework_1_score = 1.0 / 2 homework_2_score = (1.0 + 2.0) / 4 self.check_grade_percent(round((homework_1_score + homework_2_score) / 2, 2))
homework_1_score = 1.0 / 2 homework_2_score = 1.0 / 1 self.check_grade_percent(round((homework_1_score + homework_2_score) / 2, 2))
self.add_dropdown_to_section(vertical_1.location, 'H2P1_GROUP1', 1).location.html_id()
homework_1_score = 1.0 / 2 homework_2_score = 0.0 self.check_grade_percent(round((homework_1_score + homework_2_score) / 2, 2))
homework_1_score = 1.0 / 2 homework_2_score = 1.0 / 1 self.check_grade_percent(round((homework_1_score + homework_2_score) / 2, 2))
users_state = {}
users_state = self._get_users_state()
users_state_after_post = self._post_words(['word1', 'word2'])
users_state_before_fail = self._get_users_state()
users_state_after_post = self._post_words( ['word1', 'word2', 'word3'])
current_users_state = self._get_users_state() self._check_response(users_state_before_fail, current_users_state)
url = self._reverse_urls(['courseware'], course)[0] self.assert_request_status_code(302, url)
for url in urls: self.assert_request_status_code(404, url)
url = reverse('instructor_dashboard', kwargs={'course_id': self.course.id.to_deprecated_string()}) self.assert_request_status_code(200, url)
url = reverse('instructor_dashboard', kwargs={'course_id': self.course.id.to_deprecated_string()}) self.assert_request_status_code(200, url)
self.login(self.enrolled_user)
self._check_non_staff_light(self.course) self._check_non_staff_dark(self.course) self._check_non_staff_light(self.test_course) self._check_non_staff_dark(self.test_course)
self.enroll(self.course, True) self.enroll(self.test_course, True)
self._check_non_staff_light(self.test_course) self._check_non_staff_dark(self.test_course) self._check_staff(self.course)
self._check_staff(self.course) self._check_staff(self.test_course)
self.login(self.unenrolled_user) self.assertFalse(self.enroll(self.course)) self.assertTrue(self.enroll(self.test_course))
self.logout() self.login(self.instructor_user) self.assertTrue(self.enroll(self.course))
self.logout() self.login(self.global_staff_user) self.assertTrue(self.enroll(self.course))
self.assertFalse(has_access(self.normal_student, 'load', self.content, self.course.id))
self.assertTrue(has_access(self.beta_tester, 'load', self.content, self.course.id))
answer_objs = SurveyAnswer.objects.filter( user=self.user, form=self.survey )
primary_course = CourseFactory.create(org=primary, emit_signals=True) alternate_course = CourseFactory.create(org=alternate, emit_signals=True)
no_courses = get_courses(user, org=primary) self.assertEqual(no_courses, [])
microsite_courses = get_courses(user, org=alternate) self.assertTrue( all(course.org == alternate_course.org for course in microsite_courses) )
course_about = get_course_about_section(self.request, self.course, 'short_description') self.assertEqual(course_about, "A course about toys.")
self.verify_staff_debug_present(True)
self.update_masquerade(role='student') self.verify_staff_debug_present(False)
self.update_masquerade(role='staff') self.verify_staff_debug_present(True)
self.verify_show_answer_present(True)
self.update_masquerade(role='student') self.verify_show_answer_present(False)
self.update_masquerade(role='staff') self.verify_show_answer_present(True)
self.login_staff() response = self.get_course_info_page() self.assertEqual(response.status_code, 200) content = response.content self.assertIn("OOGIE BLOOGIE", content)
self.login_student() self.submit_answer('Correct', 'Correct') self.assertEqual(self.get_progress_detail(), u'2/2')
self.login_staff() self.assertEqual(self.get_progress_detail(), u'0/2')
self.update_masquerade(role='student', user_name=self.student_user.username) self.assertEqual(self.get_progress_detail(), u'2/2')
self.submit_answer('Correct', 'Incorrect') self.assertEqual(self.get_progress_detail(), u'1/2')
self.get_courseware_page() self.assertEqual(self.get_progress_detail(), u'2/2')
self.update_masquerade(role='staff') self.assertEqual(self.get_progress_detail(), u'0/2')
self.login_student() self.assertEqual(self.get_progress_detail(), u'2/2')
self.login_staff() content = self.get_course_info_page().content self.assertIn("OOGIE BLOOGIE", content)
self.update_masquerade(role='student', user_name=self.student_user.username) content = self.get_course_info_page().content self.assertIn("OOGIE BLOOGIE", content)
group_id, user_partition_id = get_masquerading_group_info(self.test_user, self.course.id) self.assertIsNone(group_id) self.assertIsNone(user_partition_id)
group_id, user_partition_id = get_masquerading_group_info(self.test_user, self.course.id) self.assertEqual(group_id, 1) self.assertEqual(user_partition_id, 0)
CreditCourse.objects.create(course_key=self.course.id, enabled=True)
self.user = UserFactory.create(username=self.USERNAME, password=self.PASSWORD) self.user.profile.name = self.USER_FULL_NAME self.user.profile.save()
self.enrollment = CourseEnrollmentFactory( user=self.user, course_id=self.course.id, mode="verified" )
response = self._get_progress_page()
credit_api.set_credit_requirement_status( self.user.username, self.course.id, "grade", "grade", status="satisfied", reason={"final_grade": 0.95} )
credit_api.set_credit_requirement_status( self.user.username, self.course.id, "reverification", "midterm", status="failed", reason={} )
classes = ('credit-eligibility', 'eligibility-heading') method = self.assertContains if is_requirement_displayed else self.assertNotContains
staticfiles.finders.get_finder.cache_clear()
self.assertContains(resp, "super-ugly") self.assertContains(resp, "This file is only for demonstration, and is horrendous!")
before_finders = list(settings.STATICFILES_FINDERS) before_dirs = list(settings.STATICFILES_DIRS)
import datetime import pytz
ItemFactory.create( parent=parent, category='discussion', display_name='released', start=self.now, )
ItemFactory.create( parent=parent, category='discussion', display_name='scheduled', start=self.future, )
self_paced_course, self_paced_section = self.setup_course(**course_options) beta_tester = BetaTesterFactory(course_key=self_paced_course.id)
self.assertTrue(self_paced_course.self_paced) self.assertEqual(self_paced_course.start, one_month_from_now) self.assertIsNone(self_paced_section.start)
self.assertFalse(has_access(self.non_staff_user, 'load', self_paced_course))
self.assertTrue(has_access(beta_tester, 'load', self_paced_course)) self.assertTrue(has_access(beta_tester, 'load', self_paced_section, self_paced_course.id))
modules = get_accessible_discussion_modules(course, self.non_staff_user) self.assertTrue( all(module.display_name == 'released' for module in modules) )
import unittest from nose.plugins.attrib import attr
with self.assertRaises(KeyError): data.get('block', 'foo')
multi_db = True
return 'problem'
@skip("Not supported by DjangoXBlockUserStateClient") def test_iter_blocks_deleted_block(self): pass
self.animal_partition.groups.pop() self.color_partition.groups.pop()
self.staff = StaffFactory.create(course_key=self.course.id)
return
self.set_group_access(self.chapter_location, {self.animal_partition.id: [self.dog_group.id]}) self.check_access(self.red_cat, self.vertical_location, False)
self.set_user_partitions(self.vertical_location, []) self.check_access(self.red_cat, self.vertical_location, True)
self.set_user_partitions(self.vertical_location, [split_test_partition, self.animal_partition]) self.check_access(self.red_cat, self.vertical_location, False)
expected_url = reverse( "about_course", args=[self.course.id.to_deprecated_string()] )
multi_db = True
with self.assertNumQueries(1): self.field_data_cache = FieldDataCache( [mock_descriptor([mock_field(Scope.user_state, 'a_field')])], course_id, self.user )
with self.assertNumQueries(0): self.assertEquals('a_value', self.kvs.get(user_state_key('a_field')))
with self.assertNumQueries(0): self.assertRaises(KeyError, self.kvs.get, user_state_key('not_a_field'))
for key in kv_dict: self.kvs.set(key, 'test_value')
multi_db = True
with self.assertNumQueries(0): self.field_data_cache = FieldDataCache([mock_descriptor()], course_id, self.user) self.kvs = DjangoKeyValueStore(self.field_data_cache)
with self.assertNumQueries(1): self.field_data_cache = FieldDataCache([self.mock_descriptor], course_id, self.user) self.kvs = DjangoKeyValueStore(self.field_data_cache)
with self.assertNumQueries(len(kv_dict)): self.kvs.set_many(kv_dict) for key in kv_dict: self.assertEquals(self.kvs.get(key), kv_dict[key])
self.mock_module = MagicMock() self.mock_module.id = 1 self.dispatch = 'score_update'
html = module.render(STUDENT_VIEW).content
self.assertIn('/courses/' + self.course_key.to_deprecated_string() + '/jump_to_id/vertical_test', html)
request.POST['queuekey'] = fake_key self.mock_module.handle_ajax.assert_called_once_with(self.dispatch, request.POST)
self.assertEquals(render.get_score_bucket(11, 10), 'incorrect') self.assertEquals(render.get_score_bucket(-1, 10), 'incorrect')
self.assertIs(descriptor._unwrapped_field_data, original_field_data) self.assertIsNot(descriptor._unwrapped_field_data, descriptor._field_data)
for user in [UserFactory(), UserFactory(), UserFactory()]: render.get_module_for_descriptor( user, request, descriptor, field_data_cache, course.id, course=course )
self.assertIsInstance(descriptor._field_data, LmsFieldData)
self.assertIsInstance( descriptor._field_data._authored_data._source, OverrideFieldData )
self.assertIs( descriptor._field_data._authored_data._source.fallback, descriptor._unwrapped_field_data )
self.mock_module = MagicMock() self.mock_module.id = 1 self.dispatch = 'score_update'
mock_file.name = name return mock_file
self.assertNotIn('proctoring', section_actual)
self.field_data_cache = FieldDataCache.cache_for_descriptor_descendents( self.course_key, self.request.user, self.toy_course, depth=2 )
self.assertEqual(html.count("</script>"), 1)
self.child_module = self._get_module(course.id, child_descriptor, child_descriptor.location)
PER_STUDENT_ANONYMIZED_DESCRIPTORS = set( class_ for (name, class_) in XModuleDescriptor.load_classes() if not issubclass(class_, PER_COURSE_ANONYMIZED_DESCRIPTORS) )
descriptor.bind_for_student = partial(xblock_class.bind_for_student, descriptor)
'5afe5d9bb03796557ee2614f5c9611fb', self._get_anonymous_id(CourseKey.from_string(course_id), descriptor_class)
'e3b0b940318df9c14be59acb08e78af5', self._get_anonymous_id(SlashSeparatedCourseKey('MITx', '6.00x', '2012_Fall'), descriptor_class)
'f82b5416c9f54b5ce33989511bb5ef2e', self._get_anonymous_id(SlashSeparatedCourseKey('MITx', '6.00x', '2013_Spring'), descriptor_class)
user2 = UserFactory.create() module.descriptor.bind_for_student(module.system, user2.id)
self.assertFalse(runtime.user_is_beta_tester) self.assertEqual(runtime.days_early_for_beta, 5)
def setUp(self): super(TestFilteredChildren, self).setUp() self.users = {number: UserFactory() for number in USER_NUMBERS}
if isinstance(block, XModuleDescriptor):
if isinstance(block, XModuleDescriptor):
self.children_for_user = { user: [ ItemFactory(category=child_type, parent=self.parent).scope_ids.usage_id for child_type in BLOCK_TYPES ] for user in self.users.itervalues() }
def setUp(self): super(TestDisabledXBlockTypes, self).setUp()
self.assertEqual(len(all_gradesets), 5)
self.assertFalse(all_gradesets[student3]) self.assertFalse(all_gradesets[student4])
self.assertTrue(all_gradesets[student1]) self.assertTrue(all_gradesets[student2]) self.assertTrue(all_gradesets[student5])
max_scores_cache.set(self.locations[0], 1) self.assertEqual(max_scores_cache.num_cached_updates(), 1)
max_scores_cache.push_to_remote()
max_scores_cache = MaxScoresCache("test_max_scores_cache") max_scores_cache.fetch_from_remote(self.locations)
self.assertEqual(max_scores_cache.num_cached_from_remote(), 1)
multi_db = True
module = get_module( user, request, problem.scope_ids.usage_id, field_data_cache, )._xmodule module.system.publish(problem, 'grade', grade_dict)
XBLOCK_REMOVED_HTML_ELEMENTS = [ '<div class="wrap-instructor-info"', ]
MASQUERADE_SETTINGS_KEY = 'masquerade_settings'
MASQUERADE_DATA_KEY = 'masquerade_data'
self.course_key = course_key self.role = role self.user_partition_id = user_partition_id self.group_id = group_id self.user_name = user_name
_DELETED_SENTINEL = object()
self.set(key, _DELETED_SENTINEL)
world.disable_jquery_animations()
world.css_click(css_selector='.chapter', index=1) subsection_css = 'a[href*="Test_Subsection_2/"]'
world.css_click(subsection_css)
subsection_css = 'a[href*="Test_Subsection_2/"]' world.css_click(subsection_css)
factory_dict = PROBLEM_DICT['multiple choice'] problem_xml = factory_dict['factory'].build_xml(**factory_dict['kwargs'])
world.ItemFactory.create( parent_location=parent_location, category='problem', display_name=display_name, data=problem_xml )
world.wait_for_ajax_complete()
assert world.is_css_present('.error_message', wait_time=0)
assert not world.is_css_present('iframe', wait_time=0)
assert not world.is_css_present('.link_lti_new_window', wait_time=0)
assert world.css_visible('iframe') check_lti_iframe_content("This is LTI tool. Success.")
assert len(world.browser.windows) == 1 alert = world.browser.get_alert() alert.accept() check_no_alert()
world.wait_for( lambda _: len(world.browser.windows) == 2, timeout=5, timeout_msg="Timed out waiting for the LTI window to appear." )
check_lti_popup(parent_window)
check_lti_iframe_content("Wrong LTI signature")
world.clear_courses()
world.scenario_dict['COURSE'] = world.CourseFactory.create( org='edx', number=course, display_name='Test Course', metadata=metadata, grading_policy=grading_policy, )
user = BetaTesterFactory(course_key=course_descriptor.id) normal_student = UserFactory() instructor = InstructorFactory(course_key=course_descriptor.id)
if has_access(user, 'load', course_descriptor): world.enroll_user(user, course_descriptor.id)
windows = world.browser.windows assert_equal(len(windows), 2)
tabs = [] expected_tabs = [u'LTI | Test Section | {0} Courseware | edX'.format(TEST_COURSE_NAME), u'TEST TITLE']
world.clear_courses()
world.scenario_dict['COURSE'] = world.CourseFactory.create( org='edx', number=course, display_name='Test Course' )
world.scenario_dict['CHAPTER'] = world.ItemFactory.create( parent_location=world.scenario_dict['COURSE'].location, category='chapter', display_name='Test Chapter',
create_course(step, course)
world.create_user('robot', 'test') user = User.objects.get(username='robot')
def course_id(course_num): return world.scenario_dict['COURSE'].id.replace(course=course_num)
world.wait_for_ajax_complete()
first_addend = random.randint(-100, 100) second_addend = 10 - first_addend
if correctness == 'incorrect': second_addend += random.randint(1, 10)
pass
category_name = "problem" return world.ItemFactory.create( parent_location=section_location(course), category=category_name, display_name=str(problem_type), data=problem_xml, metadata=metadata )
if problem_type in ("radio_text", "checkbox_text"): selector_template = "input#{}_2_{input}" else: selector_template = "input#input_{}_2_{input}"
assert world.is_css_present(sel)
return sel
assert element.value.strip() == expected
add_problem_to_course(world.scenario_dict['COURSE'].number, problem_type, problem_settings)
visit_scenario_item('SECTION')
world.xqueue.config['default'] = response_dict
input_problem_answer(step, problem_type, correctness)
check_problem(step)
world.wait_for_ajax_complete()
world.browser.execute_script("window.scrollTo(0,1024)") assert world.is_css_present("button.check.is-disabled")
world.wait_for_ajax_complete()
label_css = 'button.show span.show-label' world.wait_for(lambda _: world.css_has_text(label_css, label_name))
score_css = 'div.problem-progress' expected_text = '({})'.format(score) world.wait_for(lambda _: world.css_has_text(score_css, expected_text))
assert correctness in ['correct', 'incorrect', 'unanswered'] assert problem_type in PROBLEM_DICT
for sel in PROBLEM_DICT[problem_type][correctness]: if bool(isnt_marked):
if has_expected: break
assert has_expected
world.mongo_client.fsync()
from . import signals
self.course_key = course_key
self.user = user
self._has_staff_access = None
update_course_in_cache.apply_async([unicode(course_key)], countdown=0)
COURSE_BLOCK_ACCESS_TRANSFORMERS = [ library_content.ContentLibraryTransformer(), start_date.StartDateTransformer(), user_partitions.UserPartitionTransformer(), visibility.VisibilityTransformer(), ]
parents = block_structure.get_parents(block_key)
block_structure.set_transformer_block_field( block_key, cls, cls.MERGED_VISIBLE_TO_STAFF_ONLY, ( all_parents_visible_to_staff_only or block_structure.get_xblock(block_key).visible_to_staff_only ) )
if usage_info.has_staff_access: return
parents = block_structure.get_parents(block_key) min_all_parents_start_date = min( cls.get_merged_start_date(block_structure, parent_key) for parent_key in parents ) if parents else None
block_start = get_field_on_block(block_structure.get_xblock(block_key), 'start') if min_all_parents_start_date is None: merged_start_value = block_start or DEFAULT_START_DATE
merged_start_value = min_all_parents_start_date
merged_start_value = max(min_all_parents_start_date, block_start)
if usage_info.has_staff_access: return
previous_count = len(selected) block_keys = LibraryContentModule.make_selection(selected, library_children, max_count, mode) selected = block_keys['selected']
self._publish_events(block_structure, block_key, previous_count, max_count, block_keys) all_selected_children.update(usage_info.course_key.make_usage_key(s[0], s[1]) for s in selected)
block_structure.remove_block_if( check_child_removal )
child_to_group = { xblock.group_id_to_child.get(unicode(group.id), None): group.id for group in partition_for_this_block.groups }
for child_location in xblock.children: child = block_structure.get_xblock(child_location) group = child_to_group.get(child_location, None) child.group_access[partition_for_this_block.id] = [group] if group else []
block_structure.remove_block_if( lambda block_key: block_key.block_type == 'split_test', keep_descendants=True, )
SplitTestTransformer.collect(block_structure)
root_block = block_structure.get_xblock(block_structure.root_block_usage_key) user_partitions = getattr(root_block, 'user_partitions', []) or [] block_structure.set_transformer_data(cls, 'user_partitions', user_partitions)
if not user_partitions: return
self._access = {}
xblock_group_access = get_field_on_block(xblock, 'group_access', default_value={})
merged_parent_group_ids = None
merged_parent_group_ids = set()
xblock_partition_access = set(xblock_group_access.get(partition.id) or []) or None
merged_group_ids = _MergedGroupAccess._intersection(xblock_partition_access, merged_parent_group_ids)
if merged_group_ids is not None: self._access[partition.id] = merged_group_ids
if partition_id not in user_groups: return False
elif user_groups[partition_id].id in allowed_group_ids: continue
else: return False
return True
CourseEnrollmentFactory.create(user=self.user, course_id=self.course.id, is_active=True)
self.password = 'test' self.user = UserFactory.create(password=self.password) self.staff = UserFactory.create(password=self.password, is_staff=True)
course = modulestore().get_item(block_map['course'].location) course.children.remove(block_key) block_map['course'] = update_block(course)
for parent_ref in parents: parent_block = modulestore().get_item(block_map[parent_ref].location) parent_block.children.append(block_key) block_map[parent_ref] = update_block(parent_block)
for child_hierarchy in block_hierarchy.get('#children', []): self.add_parents(child_hierarchy, block_map)
for block_hierarchy in course_hierarchy: self.build_xblock(block_hierarchy, block_map, parent=None)
for block_hierarchy in course_hierarchy: self.add_parents(block_hierarchy, block_map)
parents_map = [[], [0], [0], [1], [1], [2], [2, 4]]
self.course = CourseFactory.create()
self.xblock_keys = [self.course.location]
for i, parents_index in enumerate(self.parents_map): if i == 0:
self._check_results( test_user, expected_user_accessible_blocks, blocks_with_differing_access, transformers, )
self._check_results(self.staff, set(range(len(self.parents_map))), {}, transformers)
block_structure_result = xblock_key in block_structure has_access_result = bool(has_access(user, 'load', self.get_block(i), course_key=self.course.id))
self.assertEquals( block_structure_result, i in expected_accessible_blocks, "block_structure return value {0} not equal to expected value for block {1} for user {2}".format( block_structure_result, i, user.username ) )
self.groups = [] for group_num in range(1, num_groups + 1): self.groups.append(Group(group_num, 'Group ' + unicode(group_num)))
self.setup_groups_partitions() self.user_partition = self.user_partitions[0]
self.course_hierarchy = self.get_course_hierarchy() self.blocks = self.build_course(self.course_hierarchy) self.course = self.blocks['course']
CourseEnrollmentFactory.create(user=self.user, course_id=self.course.id, is_active=True)
self.setup_cohorts(self.course)
self.setup_groups_partitions(num_user_partitions=3)
self.setup_cohorts(self.course)
AccessTestData(expected_access=True), AccessTestData(xblock_access={1: None}, expected_access=True), AccessTestData(xblock_access={1: []}, expected_access=True),
AccessTestData(partition_groups={1: 3, 2: 3}, xblock_access={1: [1, 2], 2: [1, 2]}),
AccessTestData(partition_groups={1: 1, 2: 2}, merged_parents_list=[{1: {1}}], expected_access=True),
AccessTestData(partition_groups={1: 1, 2: 2}, merged_parents_list=[{1: {}}]), AccessTestData(partition_groups={1: 1, 2: 2}, merged_parents_list=[{1: {3}}]),
AccessTestData(partition_groups={1: 1, 2: 2}, merged_parents_list=[{1: {3}}, {1: {1}}], expected_access=True),
AccessTestData(partition_groups={1: 1, 2: 2}, xblock_access={1: [3]}, merged_parents_list=[{1: {1}}]), AccessTestData(partition_groups={1: 1, 2: 2}, xblock_access={1: [2]}, merged_parents_list=[{1: {1}}]),
AccessTestData( partition_groups={1: 1, 2: 2}, xblock_access={1: [1]}, merged_parents_list=[{1: {3}}, {1: {1}}], expected_access=True, ),
block = self.course
if xblock_access is not None: block.group_access = xblock_access update_block(self.course)
for ind, merged_parent in enumerate(merged_parents_list): converted_object = _MergedGroupAccess([], block, []) converted_object._access = merged_parent merged_parents_list[ind] = converted_object
for partition_id, group_id in user_partition_groups.iteritems(): user_partition_groups[partition_id] = self.groups[group_id - 1]
self.course_hierarchy = self.get_course_hierarchy() self.blocks = self.build_course(self.course_hierarchy) self.course = self.blocks['course']
CourseEnrollmentFactory.create(user=self.user, course_id=self.course.id, is_active=True)
user_groups = _get_user_partition_groups( self.course.id, [self.split_test_user_partition], self.user ) self.assertEquals(len(user_groups), 1)
self.assertFalse(self.course.visible_to_staff_only) orig_block_structure = get_course_blocks(self.user, self.course_usage_key) self.assertFalse( VisibilityTransformer.get_visible_to_staff_only(orig_block_structure, self.course_usage_key) )
self.course.visible_to_staff_only = True self.store.update_item(self.course, self.user.id)
mode = models.CharField(max_length=100, default='', blank=True) image = models.ImageField(upload_to='badge_classes', validators=[validate_badge_image])
help_text=_( u"Badge images must be square PNG files. The file size should be under 250KB." ), upload_to='course_complete_badges', validators=[validate_badge_image]
if self.default and CourseCompleteImageConfiguration.objects.filter(default=True).exclude(id=self.id): raise ValidationError(_(u"There can be only one default image."))
return cls.objects.get(default=True).icon
admin.site.register(CourseEventBadgesConfiguration, ConfigurationModelAdmin)
slug = hashlib.sha256(slug + unicode(badge_class.course_id)).hexdigest()
slug = hashlib.sha256(slug).hexdigest()
EXAMPLE_SLUG = '15bb687e0c59ef2f0a49f6838f511bf4ca6c566dd45da6293cabbd9369390e1a'
from __future__ import unicode_literals
from __future__ import unicode_literals
continue
from __future__ import unicode_literals
return
return
CourseEnrollment.enroll(user, course_key=course.location.course_key) self.assertFalse(user.badgeassertion_set.all())
user=user, course_id=course.location.course_key, status=CertificateStatuses.downloadable
self.assertFalse(user.badgeassertion_set.all())
user=user, course_id=course.location.course_key, status=CertificateStatuses.downloadable
self.courses.append([CourseFactory().location.course_key for _i in range(3)])
user=user, course_id=course.location.course_key, status=CertificateStatuses.downloadable
self.assertFalse(user.badgeassertion_set.all())
if i + 1 == len(course_keys): self.assertTrue(badge_class.get_for_user(user)) else: self.assertFalse(badge_class.get_for_user(user))
self.client.login(username=self.user.username, password='test')
self.check_assertion_structure(assertion, response['results'][0])
self.check_assertion_structure(assertion, response['results'][0])
alt_class = BadgeClassFactory.create( slug=badge_class.slug, issuing_component=badge_class.issuing_component, course_id=CourseFactory.create().location.course_key ) BadgeAssertionFactory.create(user=self.user, badge_class=alt_class)
for dummy in range(6): BadgeAssertionFactory.create()
self.assertEqual(len(response['results']), expected_length) unused_class = self.create_badge_class(check_course, slug='unused_slug', issuing_component='unused_component')
self.assertEqual(len(response['results']), 0)
self.check_assertion_structure(assertion, response['results'][0])
course_id = None
course_id = None
course_id = CourseKeyField.Empty
badge_class = BadgeClass.get_badge_class( slug='new_slug', issuing_component='new_component', description=None, criteria=None, display_name=None, image_file_handle=None, create=False ) self.assertIsNone(badge_class)
from course_wiki.plugins.markdownedx.wiki_plugin import ExtendMarkdownPlugin
from markdown.util import etree, AtomicString
md.inlinePatterns.add('mathjax', MathJaxPattern(), '<escape')
from markdown.util import etree
for key, value in configs: self.setConfig(key, value)
width = self.ext.config['bliptv_width'][0] height = self.ext.config['bliptv_height'][0] return flash_object(url, width, height)
width = self.ext.config['dailymotion_width'][0] height = self.ext.config['dailymotion_height'][0] return flash_object(url, width, height)
width = self.ext.config['gametrailers_width'][0] height = self.ext.config['gametrailers_height'][0] return flash_object(url, width, height)
width = self.ext.config['metacafe_width'][0] height = self.ext.config['metacafe_height'][0] return flash_object(url, width, height)
width = self.ext.config['veoh_width'][0] height = self.ext.config['veoh_height'][0] return flash_object(url, width, height)
width = self.ext.config['vimeo_width'][0] height = self.ext.config['vimeo_height'][0] return flash_object(url, width, height)
width = self.ext.config['youtube_width'][0] height = self.ext.config['youtube_height'][0] return flash_object(url, width, height)
urlpath = None article = None
root = get_or_create_root()
urlpath.delete()
_("This is the wiki for **{organization}**'s _{course_name}_.").format( organization=course.display_org_with_default, course_name=course.display_name_with_default_escaped, )
resp = self.client.get(course_wiki_page, follow=False, HTTP_REFERER=referer) self.assertEqual(resp.status_code, 302)
resp = self.client.get(course_wiki_page, follow=False) self.assertEqual(resp.status_code, 302)
resp = self.client.get(course_wiki_page, follow=True) target_url, __ = resp.redirect_chain[-1] self.assertTrue(reverse('signin_user') in target_url)
self.assertContains(response, "super-ugly")
if slug_is_numerical(slug): slug = slug + "_"
ancestors = urlpath.cached_ancestors
try: get_course_overview_with_access(request.user, 'load', course_id) return redirect("/courses/{course_id}/wiki/{path}".format(course_id=course_id.to_deprecated_string(), path=wiki_path)) except Http404: pass
if not view_func.__module__.startswith('wiki.'): return
if not request.user.is_authenticated(): return redirect(reverse('signin_user'), next=request.path)
course_path = "/courses/{}".format(course_id.to_deprecated_string())
return redirect('about_course', course_id.to_deprecated_string())
if not settings.FEATURES.get('ALLOW_WIKI_ROOT_ACCESS', False): raise PermissionDenied()
request.grant_type = None
request.user = request.client.user
request.grant_type = grant_type request.user = user
sale_order_dict = dict((feature, getattr(purchased_course.order, feature)) for feature in sale_order_features)
order_item_dict = dict((feature, getattr(purchased_course, feature, None)) for feature in order_item_features)
if coupon_redemption.exists(): coupon_codes = [redemption.coupon.code for redemption in coupon_redemption] order_item_dict.update({'coupon_code': ", ".join(coupon_codes)})
sale_dict = dict((feature, getattr(invoice, feature)) for feature in sale_features)
for data in generated_certificates: data['report_run_date'] = report_run_date
meta_features = [] for feature in features: if 'meta.' in feature: meta_key = feature.split('.')[1] meta_features.append((feature, meta_key))
meta_dict = json.loads(profile.meta) if profile.meta else {} for meta_feature, meta_key in meta_features: student_dict[meta_feature] = meta_dict.get(meta_key)
student_dict['cohort'] = next( (cohort.name for cohort in student.course_groups.all() if cohort.course_id == course_key), "[unassigned]" )
run = problem_key.run if not run: problem_key = course_key.make_usage_key_from_deprecated_string(problem_location) if problem_key.course_key != course_key: return []
if csv_type is not None: try: redemption_set = registration_code.registrationcoderedemption_set redeemed_by = redemption_set.get(registration_code=registration_code).redeemed_by course_registration_dict['redeemed_by'] = redeemed_by.email except ObjectDoesNotExist: pass
mock_problem_key = Mock(return_value=u'') mock_problem_key.course_key = self.course_key with patch.object(UsageKey, 'from_string') as patched_from_string: patched_from_string.return_value = mock_problem_key
mock_results = MagicMock(return_value=[result_factory(n) for n in range(5)]) with patch.object(StudentModule, 'objects') as patched_manager: patched_manager.filter.return_value = mock_results
patched_from_string.assert_called_once_with(mock_problem_location) patched_manager.filter.assert_called_once_with( course_id=self.course_key, module_state_key=mock_problem_key )
self.assertEqual(userreport['city'], "None") self.assertEqual(userreport['country'], "")
item = order.orderitem_set.all().select_subclasses()[0] coupon_redemption = CouponRedemption.objects.select_related('coupon').filter(order=order)
item = order.orderitem_set.all().select_subclasses()[0]
mode = CourseModeFactory.create() mode.course_id = self.course.id mode.min_price = 1 mode.save()
_EASY_CHOICE_FEATURES = ('gender', 'level_of_education') _OPEN_CHOICE_FEATURES = ('year_of_birth',)
self.type = None self.data = None self.choices_display_names = None
choices = [(short, full) for (short, full) in raw_choices] + [('no_data', 'No Data')]
if None in distribution:
distribution['no_data'] = profiles.filter( **{feature: None} ).count()
super(SurveyForm, self).save(*args, **kwargs)
self.clear_user_answers(user) SurveyAnswer.save_answers(self, user, answers, course_key)
tree = etree.fromstring(u'<div>{}</div>'.format(html))
course_key = CourseKeyField(max_length=255, db_index=True, null=True)
value = answers[name] defaults = {"field_value": value} if course_key: defaults['course_key'] = course_key
answer.field_value = value answer.course_key = course_key answer.save()
existing_answers = survey.get_answers(user=user).get(user.id, {})
array_val = request.POST.getlist(key) answers[key] = request.POST[key] if len(array_val) == 0 else ','.join(array_val)
redirect_url = answers['_redirect_url'] if '_redirect_url' in answers else reverse('dashboard')
filtered_answers = {} for answer_key in answers.keys(): if answer_key in allowed_field_names: filtered_answers[answer_key] = escape(answers[answer_key])
"redirect_url": redirect_url,
from __future__ import unicode_literals
self.password = 'abc' self.student = UserFactory.create(username='student', email='student@test.com', password=self.password)
self.assertIn(self.test_form, resp.content)
answer_objs = SurveyAnswer.objects.filter( user=self.student, form=self.survey )
survey.save_user_answers(self.student, self.student_answers_update, self.course_id)
survey.save_user_answers(self.student, self.student_answers_update2, self.course_id)
all_answers = survey.get_answers(limit_num_users=1) self.assertEquals(len(all_answers.keys()), 1)
survey = SurveyForm.get(course_descriptor.course_survey_name)
answered_survey = SurveyAnswer.do_survey_answers_exist(survey, user) return not answered_survey and not has_staff_access
from commerce import signals
error_summary = _("An error occurred while creating your receipt.")
log.exception( "Unexpected exception while attempting to initiate refund for user [%s], course [%s]", course_enrollment.user.id, course_enrollment.course_id, )
log.warning("User [%s] was not authorized to initiate a refund for user [%s] " "upon unenrollment from course [%s]", request_user.id, unenrolled_user.id, course_key_str) return []
raise exc
log.info( "Refund successfully opened for user [%s], course [%s]: %r", unenrolled_user.id, course_key_str, refund_ids, )
log.warning("Could not send email notification for refund.", exc_info=True)
log.debug("No refund opened for user [%s], course [%s]", unenrolled_user.id, course_key_str)
tags = list(tags or []) tags.append('LMS')
tags = list(set(tags))
payload = json.dumps(data)
raise NotImplementedError("Unable to send refund processing emails to microsite teams.")
from django.conf import settings from django.contrib.auth.models import User from django.db import models, migrations
from __future__ import unicode_literals
from __future__ import unicode_literals
authentication_classes = (EnrollmentCrossDomainSessionAuth, OAuth2AuthenticationAllowInactiveUser) permission_classes = (IsAuthenticated,)
log.exception( 'Failed to handle marketing opt-in flag: user="%s", course="%s"', user.username, course_key )
honor_mode = CourseMode.mode_for_course(course_key, CourseMode.HONOR) audit_mode = CourseMode.mode_for_course(course_key, CourseMode.AUDIT)
default_enrollment_mode = audit_mode or honor_mode
try: response_data = api.baskets.post({ 'products': [{'sku': default_enrollment_mode.sku}], 'checkout': True, })
response = JsonResponse(payment_data)
msg = Messages.ORDER_COMPLETED.format(order_number=response_data['order']['number']) log.debug(msg) response = DetailResponse(msg)
self.reset_tracker()
self.assertTrue(mock_audit_log.called)
if is_completed: msg = Messages.ORDER_COMPLETED.format(order_number=TEST_ORDER_NUMBER) self.assertResponseMessage(response, msg) else: self.assertResponsePaymentData(response)
self.user.is_active = user_is_active
self.user.is_active = user_is_active
with mock_create_basket(expect_called=False): response = self._post_to_view()
for course_mode in CourseMode.objects.filter(course_id=self.course.id): course_mode.sku = None course_mode.save()
self.assertEqual(response.status_code, 200) msg = Messages.NO_ECOM_API.format(username=self.user.username, course_id=self.course.id) self.assertResponseMessage(response, msg)
self.assertTrue(CourseEnrollment.is_enrolled(self.user, self.course.id))
self.assertEqual(response.status_code, 406) msg = Messages.NO_DEFAULT_ENROLLMENT_MODE.format(course_id=self.course.id) self.assertResponseMessage(response, msg)
for course_mode in CourseMode.objects.filter(course_id=self.course.id): course_mode.sku = '' course_mode.save()
CourseEnrollment.enroll(self.user, self.course.id) self.assertTrue(CourseEnrollment.is_enrolled(self.user, self.course.id))
log.warning('Failed to retrieve CourseOverview for [%s]. Using empty course name.', course_id) return None
VerificationDeadline.set_deadline(self.id, self.verification_deadline, is_explicit=True)
queryset = CourseMode.objects.all()
pass
if upgrade_deadline is not None and verification_deadline < upgrade_deadline: raise serializers.ValidationError( 'Verification deadline must be after the course mode upgrade deadlines.')
response = self.client.put(self.path, json.dumps(expected), content_type=JSON_CONTENT_TYPE)
self.assertIsNone(VerificationDeadline.deadline_for_course(self.course.id))
verification_deadline = datetime(year=2020, month=12, day=31, tzinfo=pytz.utc) expiration_datetime = datetime.now(pytz.utc) response, expected = self._get_update_response_and_expected_data(expiration_datetime, verification_deadline)
self.assertEqual(response.status_code, 200)
actual = json.loads(response.content) self.assertEqual(actual, expected)
self.assertEqual(VerificationDeadline.deadline_for_course(self.course.id), verification_deadline)
response, __ = self._get_update_response_and_expected_data(None, None) self.assertEqual(response.status_code, 200)
self.assertFalse(CourseMode.objects.filter(id=self.course_mode.id).exists())
course_modes = CourseMode.objects.filter(course_id=course.id) actual = [course_mode.mode_display_name for course_mode in course_modes] self.assertListEqual(actual, ['Verified Certificate', 'Honor Certificate'])
httpretty.register_uri( httpretty.POST, '{}/baskets/1/'.format(TEST_API_URL), status=200, body='{}', adding_headers={'Content-Type': JSON} )
del post_data[post_key] expected_pattern = r"<title>(\s+)Receipt"
self.assertNotContains(response, "How it Works") self.assertNotContains(response, "Find courses") self.assertNotContains(response, "Schools & Partners")
message = 'foo: bar="baz", qux="quux"' self.assertTrue(mock_log.info.called_with(message))
default_response = None
method = None
mock_refund_seat.reset_mock() self.send_signal(skip_refund=True) self.assertFalse(mock_refund_seat.called)
mock_refund_seat.reset_mock() self.course_enrollment.refundable = mock.Mock(return_value=False) self.send_signal() self.assertFalse(mock_refund_seat.called)
self.send_signal() self.assertTrue(mock_refund_seat.called) self.assertEqual(mock_refund_seat.call_args[0], (self.course_enrollment, self.student))
mock_get_request_user.return_value = AnonymousUser() mock_refund_seat.reset_mock() self.send_signal() self.assertFalse(mock_refund_seat.called)
STATUS = Choices('created', 'ready', 'submitted', 'must_retry', 'approved', 'denied') user = models.ForeignKey(User, db_index=True)
face_image_url = models.URLField(blank=True, max_length=255) photo_id_image_url = models.URLField(blank=True, max_length=255)
receipt_id = models.CharField( db_index=True, default=generateUUID, max_length=255, )
reviewing_user = models.ForeignKey( User, db_index=True, default=None, null=True, related_name="photo_verifications_reviewed" )
reviewing_service = models.CharField(blank=True, max_length=255)
error_msg = models.TextField(blank=True)
error_code = models.CharField(blank=True, max_length=50)
active_attempts = cls.objects.filter(user=user, status='ready').order_by('-created_at') if active_attempts: return active_attempts[0] else: return None
status = 'pending'
if attempt.status == 'denied': status = 'must_reverify'
if deadline is None: return candidates[0]
for verification in candidates: if verification.active_at_datetime(deadline): return verification
self.name = self.user.profile.name self.status = "ready" self.save()
if self.status == "approved": return
s3_key = self._generate_s3_key("photo_id") s3_key.set_contents_from_string(encrypt_and_encode(img_data, aes_key))
self.photo_id_key = rsa_encrypted_aes_key.encode('base64') self.save()
category_msgs = msg_dict[category] for category_msg in category_msgs: msg.append(message_dict[(category, category_msg)])
log.error('PhotoVerification: Error parsing this error message: %s', self.error_msg) return _("There was an error verifying your ID photos.")
receipt_id = self.receipt_id if override_receipt_id is None else override_receipt_id
photo_id_url = ( self.image_url("photo_id") if copy_id_photo_from is None else self.image_url("photo_id", override_receipt_id=copy_id_photo_from.receipt_id) )
deadline_is_explicit = models.BooleanField(default=False)
history = HistoricalRecords()
url( r'^reverify/{course_id}/{usage_id}/$'.format( course_id=settings.COURSE_ID_PATTERN, usage_id=settings.USAGE_ID_PATTERN ), views.InCourseReverifyView.as_view(), name="verify_student_incourse_reverify" ),
SKIP_STEPS = [ INTRO_STEP, ]
FIRST_TIME_VERIFY_MSG = 'first-time-verify' VERIFY_NOW_MSG = 'verify-now' VERIFY_LATER_MSG = 'verify-later' UPGRADE_MSG = 'upgrade' PAYMENT_CONFIRMATION_MSG = 'payment-confirmation'
VERIFICATION_DEADLINE = "verification" UPGRADE_DEADLINE = "upgrade"
course_key = CourseKey.from_string(course_id) course = modulestore().get_course(course_key)
if course is None: log.warn(u"Could not find course with ID %s.", course_id) raise Http404
redirect_url = embargo_api.redirect_if_blocked( course_key, user=request.user, ip_address=get_ip(request), url=request.path ) if redirect_url: return redirect(redirect_url)
redirect_response = self._redirect_if_necessary( message, already_verified, already_paid, is_enrolled, course_key, user_is_trying_to_pay, request.user, relevant_course_mode.sku ) if redirect_response is not None: return redirect_response
contribution_amount = request.session.get( 'donation_for_course', {} ).get(unicode(course_key), '')
request.session['attempting_upgrade'] = (message == self.UPGRADE_MSG)
verification_good_until = self._verification_valid_until(request.user)
if relevant_course_mode.sku: processors = ecommerce_api_client(request.user).payment.processors.get() else: processors = [settings.CC_PROCESSOR_NAME]
if not already_paid: url = reverse('verify_student_upgrade_and_verify', kwargs=course_kwargs)
url = reverse('verify_student_start_flow', kwargs=course_kwargs)
url = reverse('verify_student_verify_now', kwargs=course_kwargs)
ecommerce_service = EcommerceService() if ecommerce_service.is_enabled(user): url = ecommerce_service.checkout_page_url(sku)
if url is not None: return redirect(url)
all_modes, unexpired_modes = CourseMode.all_and_unexpired_modes_for_courses([course_key])
for mode in unexpired_modes[course_key]: if mode.min_price > 0 and not CourseMode.is_credit_mode(mode): return mode
for mode in all_modes[course_key]: if mode.min_price > 0 and not CourseMode.is_credit_mode(mode): return mode
return None
remove_steps |= set([self.INTRO_STEP])
if photo_verifications: return photo_verifications[0].expiration_datetime.strftime(date_format)
result = api.baskets.post({ 'products': [{'sku': course_mode.sku}], 'checkout': True, 'payment_processor_name': processor })
return result.get('payment_data')
payment_data = checkout_with_ecommerce_service( request.user, course_id, current_mode, request.POST.get('processor') )
payment_data = payment_data['payment_form_data']
params, response = self._validate_parameters(request, bool(initial_verification)) if response is not None: return response
if "full_name" in params: response = self._update_full_name(request.user, params["full_name"]) if response is not None: return response
face_image, photo_id_image, response = self._decode_image_data( params["face_image"], params.get("photo_id_image") )
if photo_id_image is not None: initial_verification = None
attempt = self._submit_attempt(request.user, face_image, photo_id_image, initial_verification)
redirect_url = get_redirect_url(params["course_key"], params["checkpoint"]) return JsonResponse({"url": redirect_url})
params = { param_name: request.POST[param_name] for param_name in [ "face_image", "photo_id_image", "course_key", "checkpoint", "full_name" ] if param_name in request.POST }
if "face_image" not in params: msg = _("Missing required parameter face_image") return None, HttpResponseBadRequest(msg)
face_image = decode_image_data(face_data)
photo_id_image = ( decode_image_data(photo_id_data) if photo_id_data is not None else None )
attempt.upload_face_image(face_image)
attempt.mark_ready() attempt.submit(copy_id_photo_from=initial_verification)
log.exception("Could not send notification email for initial verification for user %s", user.id)
log.error("Unable to add Credit requirement status for user with id %d", attempt.user.id)
#if not sig_valid:
if access_key != settings.VERIFY_STUDENT["SOFTWARE_SECURE"]["API_ACCESS_KEY"]: return HttpResponseBadRequest("Access key invalid")
icrv_status_emails = IcrvStatusEmailsConfiguration.current() if icrv_status_emails.enabled and checkpoints: user_id = attempt.user.id course_key = checkpoints[0].course_id related_assessment_location = checkpoints[0].checkpoint_location
self._track_reverification_events('edx.bi.reverify.started', user.id, course_id, checkpoint.checkpoint_name)
if force_must_retry: attempt.status = 'must_retry'
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
VerificationCheckpoint.get_or_create_verification_checkpoint(course_key, related_assessment_location)
from openedx.core.djangoapps.credit.api import set_credit_requirement_status
set_credit_requirement_status( user.username, course_key, 'reverification', checkpoint.checkpoint_location, status='declined' )
MIN_PRICE = 1438
resp = self.client.get(self.urls['course_modes_choose'], follow=True) self.assertRedirects(resp, self.urls['verify_student_start_flow'])
self.assertFalse(CourseEnrollment.is_enrolled(self.user, self.course_key))
self.assertContains(resp, 'payment-button')
assert_roundtrip("12345678901234561234567890123456123456789012345601") assert_roundtrip("")
assert_equals(len(base64.urlsafe_b64encode(encrypted_aes_key)), 344)
response = self._get_page(payment_flow, course.id) self._assert_steps_displayed( response, PayAndVerifyView.PAYMENT_STEPS, PayAndVerifyView.MAKE_PAYMENT_STEP ) self._assert_requirements_displayed(response, [])
response = self._get_page(payment_flow, course.id) self._assert_steps_displayed( response, PayAndVerifyView.PAYMENT_STEPS, PayAndVerifyView.MAKE_PAYMENT_STEP ) self._assert_requirements_displayed(response, [])
self.assertNotContains(response, "How it Works") self.assertNotContains(response, "Find courses") self.assertNotContains(response, "Schools & Partners")
course = self._create_course("verified") self._enroll(course.id, "verified") response = self._get_page('verify_student_verify_now', course.id)
self._assert_steps_displayed( response, PayAndVerifyView.PAYMENT_STEPS + PayAndVerifyView.VERIFICATION_STEPS, PayAndVerifyView.FACE_PHOTO_STEP )
self._assert_requirements_displayed(response, [ PayAndVerifyView.PHOTO_ID_REQ, PayAndVerifyView.WEBCAM_REQ, ])
response = self._get_page( 'verify_student_verify_now', course.id, expected_status_code=302 ) self._assert_redirects_to_dashboard(response)
self._assert_steps_displayed( response, PayAndVerifyView.PAYMENT_STEPS + PayAndVerifyView.VERIFICATION_STEPS, PayAndVerifyView.PAYMENT_CONFIRMATION_STEP, )
self._assert_requirements_displayed(response, [ PayAndVerifyView.PHOTO_ID_REQ, PayAndVerifyView.WEBCAM_REQ, ])
self._assert_steps_displayed( response, PayAndVerifyView.PAYMENT_STEPS + PayAndVerifyView.VERIFICATION_STEPS, PayAndVerifyView.MAKE_PAYMENT_STEP, )
self._assert_steps_displayed( response, PayAndVerifyView.PAYMENT_STEPS, PayAndVerifyView.PAYMENT_CONFIRMATION_STEP, )
response = self._get_page( 'verify_student_upgrade_and_verify', course.id, expected_status_code=302 ) self._assert_redirects_to_verify_start(response, course.id)
response = self._get_page( 'verify_student_upgrade_and_verify', course.id, expected_status_code=302 ) self._assert_redirects_to_dashboard(response)
course = self._create_course("verified") response = self._get_page(payment_flow, course.id) self._assert_contribution_amount(response, "")
response = self._get_page(payment_flow, course.id) self._assert_contribution_amount(response, "")
course = self._create_course("verified") self._set_contribution("12.34", course.id)
response = self._get_page(payment_flow, course.id) self._assert_contribution_amount(response, "12.34")
self._set_deadlines(course.id, upgrade_deadline=deadline, verification_deadline=deadline)
self._enroll(course.id, "verified")
self._enroll(course.id, "verified")
response = self._get_page("verify_student_verify_now", course.id) self.assertNotContains(response, "Verification is no longer available")
self.assertEqual(data['course_mode_slug'], "verified")
self._enroll(course.id, "verified")
response = self._get_page(payment_flow, course.id, expected_status_code=302) self.assertRedirects(response, redirect_url)
mode = CourseMode.objects.get(course_id=course_key, mode_slug=mode_slug) mode.expiration_datetime = upgrade_deadline mode.save()
VerificationDeadline.set_deadline(course_key, verification_deadline)
course = self._create_course("verified", sku='nonempty-sku') self._enroll(course.id)
self.assertNotEqual(httpretty.last_request().headers, {})
httpretty.register_uri( httpretty.POST, "{}/baskets/".format(TEST_API_URL), body=json.dumps({'payment_data': expected_payment_data}), content_type="application/json", )
actual_payment_data = checkout_with_ecommerce_service( user, 'dummy-course-key', course_mode, 'test-processor' )
self.assertTrue(mock_audit_log.called)
response = self.client.get(reverse('verify_student_create_order'), create_order_post_data) self.assertEqual(response.status_code, 405)
self._submit_photos( face_image=self.IMAGE_DATA, photo_id_image=self.IMAGE_DATA )
attempt = SoftwareSecurePhotoVerification.objects.get(user=self.user) self.assertEqual(attempt.status, "submitted")
self._assert_user_name(self.user.profile.name)
self._submit_photos( face_image=self.IMAGE_DATA, photo_id_image=self.IMAGE_DATA, full_name=self.FULL_NAME )
self._assert_user_name(self.FULL_NAME)
self._submit_photos(expected_status_code=400) self._assert_confirmation_email(False)
httpretty.register_uri(httpretty.POST, "https://verify.example.com/submit/")
self._submit_photos( face_image=self.IMAGE_DATA + "4567", photo_id_image=self.IMAGE_DATA + "8910", ) initial_data = self._get_post_data()
self._submit_photos(face_image=self.IMAGE_DATA + "1112") reverification_data = self._get_post_data()
self.assertEqual(initial_data["PhotoIDKey"], reverification_data["PhotoIDKey"])
initial_photo_response = requests.get(initial_data["UserPhoto"]) self.assertEqual(initial_photo_response.status_code, 200)
self._submit_photos( face_image=self.IMAGE_DATA + "9999", photo_id_image=self.IMAGE_DATA + "1111", ) two_photo_reverification_data = self._get_post_data()
self.assertNotEqual(initial_data["PhotoIDKey"], two_photo_reverification_data["PhotoIDKey"])
params = { 'photo_id_image': self.IMAGE_DATA } response = self._submit_photos(expected_status_code=400, **params) self.assertEqual(response.content, "Missing required parameter face_image")
response = self._submit_photos(expected_status_code=400, face_image=self.IMAGE_DATA) self.assertEqual( response.content, "Photo ID image is required if the user does not have an initial verification attempt." )
self._submit_photos( face_image=self.IMAGE_DATA, photo_id_image=self.IMAGE_DATA, ) attempt = SoftwareSecurePhotoVerification.objects.get(user=self.user) attempt.photo_id_key = "dummy_photo_id_key" attempt.save()
self._submit_photos(face_image=self.IMAGE_DATA)
self.assertEqual(len(mail.outbox), 1) self.assertEqual("Verification photos received", mail.outbox[0].subject)
self.assertEqual(len(mail.outbox), 0)
self.assertEqual(len(mail.outbox), 0) user_status = VerificationStatus.objects.filter(user=self.user).count() self.assertEqual(user_status, 0)
IcrvStatusEmailsConfiguration.objects.create(enabled=True) self.create_reverification_xblock()
self.create_reverification_xblock()
subject = "Re-verification Status" mock_send_email.assert_called_once_with(self.user.id, subject, ANY)
checkpoint = VerificationCheckpoint(course_id=self.course_id, checkpoint_location=reverification.location) checkpoint.save()
checkpoint.add_verification_attempt(self.attempt)
VerificationStatus.add_verification_status(checkpoint, self.user, "submitted")
attempt = SoftwareSecurePhotoVerification.objects.create(user=self.user) attempt.mark_ready() attempt.submit() attempt.deny("error") self._assert_can_reverify()
attempt = SoftwareSecurePhotoVerification.objects.create(user=self.user) attempt.mark_ready() attempt.submit() attempt.approve()
self._assert_can_reverify()
attempt = SoftwareSecurePhotoVerification.objects.create(user=self.user) attempt.mark_ready() attempt.submit()
self._assert_can_reverify()
attempt = SoftwareSecurePhotoVerification.objects.create(user=self.user) attempt.mark_ready() attempt.submit() attempt.approve()
self._assert_cannot_reverify()
CourseEnrollment.enroll(self.user, self.course_key, mode="verified")
analytics_patcher = patch('lms.djangoapps.verify_student.views.analytics') self.mock_tracker = analytics_patcher.start() self.addCleanup(analytics_patcher.stop)
response = self.client.get(self._get_url(self.course_key, "invalid_checkpoint")) self.assertEqual(response.status_code, 404)
response = self._submit_photos(self.course_key, self.reverification_location, self.IMAGE_DATA) self.assertEqual(response.status_code, 400)
status = VerificationStatus.get_user_status_at_checkpoint( self.user, self.course_key, self.reverification_location ) self.assertEqual(status, "submitted")
with check_mongo_calls(1): ver_block = modulestore().get_item(self.reverification.location)
self.assertIsNotNone(ver_block)
from datetime import timedelta, datetime import json
photo_id_key = data_dict["PhotoIDKey"].decode("base64") user_photo_key = data_dict["UserPhotoKey"].decode("base64")
assert_raises(VerificationException, attempt.submit) assert_raises(VerificationException, attempt.approve) assert_raises(VerificationException, attempt.deny)
attempt.mark_ready() assert_equals(attempt.status, "ready")
assert_raises(VerificationException, attempt.approve) assert_raises(VerificationException, attempt.deny)
attempt.status = "must_retry" attempt.system_error("System error") attempt.approve() attempt.status = "must_retry" attempt.deny(DENY_ERROR_MSG)
attempt.status = "submitted" attempt.deny(DENY_ERROR_MSG) attempt.status = "submitted" attempt.approve()
assert_raises(VerificationException, attempt.submit)
assert_raises(VerificationException, attempt.submit)
attempt = self.create_and_submit() assert_equals(attempt.status, "submitted")
with patch('lms.djangoapps.verify_student.models.requests.post', new=mock_software_secure_post_error): attempt = self.create_and_submit() assert_equals(attempt.status, "must_retry")
with patch('lms.djangoapps.verify_student.models.requests.post', new=mock_software_secure_post_unavailable): attempt = self.create_and_submit() assert_equals(attempt.status, "must_retry")
assert_is_none(SoftwareSecurePhotoVerification.active_for_user(user))
attempt = SoftwareSecurePhotoVerification(user=user) attempt.mark_ready() assert_equals(attempt, SoftwareSecurePhotoVerification.active_for_user(user))
user2 = UserFactory.create() user2.save() assert_is_none(SoftwareSecurePhotoVerification.active_for_user(user2))
attempt_2 = SoftwareSecurePhotoVerification(user=user) attempt_2.mark_ready() assert_equals(attempt_2, SoftwareSecurePhotoVerification.active_for_user(user))
attempt_3 = SoftwareSecurePhotoVerification( user=user, created_at=attempt_2.created_at + timedelta(days=1) ) attempt_3.save()
assert_equals(attempt_2, SoftwareSecurePhotoVerification.active_for_user(user))
attempt_3.mark_ready() assert_equals(attempt_3, SoftwareSecurePhotoVerification.active_for_user(user))
for status in ["created", "ready", "denied"]: attempt.status = status attempt.save() assert_false(SoftwareSecurePhotoVerification.user_has_valid_or_pending(user), status)
for status in ["submitted", "must_retry", "approved"]: attempt.status = status attempt.save() assert_true(SoftwareSecurePhotoVerification.user_has_valid_or_pending(user), status)
user = UserFactory.create() status = SoftwareSecurePhotoVerification.user_status(user) self.assertEquals(status, ('none', ''))
attempt = SoftwareSecurePhotoVerification(user=user) attempt.status = 'approved' attempt.save()
attempt2 = SoftwareSecurePhotoVerification(user=user) attempt2.status = 'denied' attempt2.error_msg = '[{"photoIdReasons": ["Not provided"]}]' attempt2.save()
attempt.delete() status = SoftwareSecurePhotoVerification.user_status(user) self.assertEquals(status, ('must_reverify', "No photo ID was provided."))
before = attempt.created_at - timedelta(seconds=1) self.assertFalse(attempt.active_at_datetime(before))
after_created = attempt.created_at + timedelta(seconds=1) self.assertTrue(attempt.active_at_datetime(after_created))
expiration = attempt.created_at + timedelta(days=settings.VERIFY_STUDENT["DAYS_GOOD_FOR"]) before_expiration = expiration - timedelta(seconds=1) self.assertTrue(attempt.active_at_datetime(before_expiration))
after = expiration + timedelta(seconds=1) self.assertFalse(attempt.active_at_datetime(after))
query = SoftwareSecurePhotoVerification.objects.filter(user=user) result = SoftwareSecurePhotoVerification.verification_for_datetime(now, query) self.assertIs(result, None)
query = SoftwareSecurePhotoVerification.objects.filter(user=user) result = SoftwareSecurePhotoVerification.verification_for_datetime(None, query) self.assertIs(result, None)
attempt = SoftwareSecurePhotoVerification.objects.create(user=user)
before = attempt.created_at - timedelta(seconds=1) query = SoftwareSecurePhotoVerification.objects.filter(user=user) result = SoftwareSecurePhotoVerification.verification_for_datetime(before, query) self.assertIs(result, None)
after_created = attempt.created_at + timedelta(seconds=1) query = SoftwareSecurePhotoVerification.objects.filter(user=user) result = SoftwareSecurePhotoVerification.verification_for_datetime(after_created, query) self.assertEqual(result, attempt)
query = SoftwareSecurePhotoVerification.objects.filter(user=user) result = SoftwareSecurePhotoVerification.verification_for_datetime(None, query) self.assertEqual(result, attempt)
after = expiration + timedelta(seconds=1) query = SoftwareSecurePhotoVerification.objects.filter(user=user) result = SoftwareSecurePhotoVerification.verification_for_datetime(after, query) self.assertIs(result, None)
second_attempt = SoftwareSecurePhotoVerification.objects.create(user=user)
deadline = second_attempt.created_at + timedelta(days=1) query = SoftwareSecurePhotoVerification.objects.filter(user=user) result = SoftwareSecurePhotoVerification.verification_for_datetime(deadline, query) self.assertEqual(result, second_attempt)
result = SoftwareSecurePhotoVerification.get_initial_verification(user=user) self.assertIs(result, None)
attempt = SoftwareSecurePhotoVerification(user=user, photo_id_key="dummy_photo_id_key") attempt.status = 'approved' attempt.save()
first_result = SoftwareSecurePhotoVerification.get_initial_verification(user=user) self.assertIsNotNone(first_result)
attempt = SoftwareSecurePhotoVerification(user=user) attempt.status = 'submitted' attempt.save()
second_result = SoftwareSecurePhotoVerification.get_initial_verification(user=user) self.assertIsNotNone(second_result) self.assertEqual(second_result, first_result)
verification_checkpoint = VerificationCheckpoint.objects.create( course_id=self.course.id, checkpoint_location=checkpoint_location ) self.assertEqual( VerificationCheckpoint.get_or_create_verification_checkpoint(self.course.id, checkpoint_location), verification_checkpoint )
location = u'i4x://edX/DemoX/edx-reverification-block/invalid_location' checkpoint = VerificationCheckpoint.get_or_create_verification_checkpoint(self.course.id, location)
VerificationCheckpoint.objects.create( course_id=self.course.id, checkpoint_location=self.checkpoint_midterm, )
VerificationCheckpoint.objects.create(course_id=self.course.id, checkpoint_location=self.checkpoint_midterm)
with self.assertRaises(IntegrityError): VerificationCheckpoint.objects.create(course_id=self.course.id, checkpoint_location=self.checkpoint_midterm)
first_checkpoint = VerificationCheckpoint.objects.create( course_id=self.course.id, checkpoint_location=self.checkpoint_midterm ) second_checkpoint = VerificationCheckpoint.objects.create( course_id=self.course.id, checkpoint_location=self.checkpoint_final )
first_checkpoint.add_verification_attempt(SoftwareSecurePhotoVerification.objects.create(user=self.user)) self.assertEqual(first_checkpoint.photo_verification.count(), 1)
first_checkpoint.add_verification_attempt(SoftwareSecurePhotoVerification.objects.create(user=self.user)) self.assertEqual(first_checkpoint.photo_verification.count(), 2)
attempt = SoftwareSecurePhotoVerification.objects.create(user=self.user) second_checkpoint.add_verification_attempt(attempt) self.assertEqual(second_checkpoint.photo_verification.count(), 1)
second_checkpoint.photo_verification.remove(attempt) self.assertEqual(second_checkpoint.photo_verification.count(), 0)
VerificationStatus.add_verification_status( checkpoint=self.first_checkpoint, user=self.user, status=status )
result = VerificationStatus.objects.filter(checkpoint=self.first_checkpoint)[0] self.assertEqual(result.status, status) self.assertEqual(result.user, self.user)
initial_status = "submitted" VerificationStatus.add_verification_status( checkpoint=self.first_checkpoint, user=self.user, status=initial_status ) VerificationStatus.add_verification_status( checkpoint=self.second_checkpoint, user=self.user, status=initial_status )
VerificationStatus.add_status_from_checkpoints( checkpoints=[self.first_checkpoint, self.second_checkpoint], user=self.user, status=status )
self.first_checkpoint.add_verification_attempt(SoftwareSecurePhotoVerification.objects.create(user=self.user))
VerificationStatus.add_verification_status( checkpoint=self.first_checkpoint, user=self.user, status='submitted', ) attempt = SoftwareSecurePhotoVerification.objects.filter(user=self.user)
SkippedReverification.add_skipped_reverification_attempt( checkpoint=self.checkpoint, user_id=self.user.id, course_id=unicode(self.course.id) )
user2 = UserFactory.create() SkippedReverification.add_skipped_reverification_attempt( checkpoint=self.checkpoint, user_id=user2.id, course_id=unicode(self.course.id) )
with self.assertNumQueries(1): all_deadlines = VerificationDeadline.deadlines_for_courses(course_keys) self.assertEqual(all_deadlines, {})
for course_key, deadline in deadlines.iteritems(): VerificationDeadline.objects.create( course_key=course_key, deadline=deadline, )
with self.assertNumQueries(1): VerificationDeadline.deadlines_for_courses(course_keys)
with self.assertNumQueries(0): all_deadlines = VerificationDeadline.deadlines_for_courses(course_keys) self.assertEqual(all_deadlines, deadlines)
VerificationDeadline.objects.all().delete()
with self.assertNumQueries(1): all_deadlines = VerificationDeadline.deadlines_for_courses(course_keys) self.assertEqual(all_deadlines, {})
self.enrollment = CourseEnrollment.enroll(self.user, self.course_id, mode=CourseMode.VERIFIED)
self.assertEqual( reverification_service.get_status(self.user.id, unicode(self.course_id), self.final_checkpoint_location), 'skipped' )
self.enrollment.update_enrollment(mode=CourseMode.HONOR)
service = ReverificationService() status = service.get_status(self.user.id, unicode(self.course_id), self.final_checkpoint_location) self.assertEqual(status, service.NON_VERIFIED_TRACK)
self.make_course(textbooks=[IMAGE_BOOK]) with self.assertRaises(NoReverseMatch): self.make_url('book', book_index='fooey')
self.make_course(textbooks=[IMAGE_BOOK]) with self.assertRaises(NoReverseMatch): self.make_url('book', book_index=0, page='xyzzy')
self.make_course(pdf_textbooks=[PDF_BOOK]) with self.assertRaises(NoReverseMatch): self.make_url('pdf_book', book_index='fooey', chapter=1)
self.make_course() url = self.make_url('pdf_book', book_index=0, chapter=1) response = self.client.get(url) self.assertEqual(response.status_code, 404)
self.make_course(pdf_textbooks=[PDF_BOOK]) with self.assertRaises(NoReverseMatch): self.make_url('pdf_book', book_index=0, chapter='xyzzy')
self.make_course(pdf_textbooks=[PDF_BOOK]) with self.assertRaises(NoReverseMatch): self.make_url('pdf_book', book_index=0, page='xyzzy')
self.make_course(pdf_textbooks=[PDF_BOOK]) with self.assertRaises(NoReverseMatch): self.make_url('pdf_book', book_index=0, chapter='fooey', page='xyzzy')
self.make_course() url = self.make_url('html_book', book_index=0, chapter=1) response = self.client.get(url) self.assertEqual(response.status_code, 404)
self.make_course(pdf_textbooks=[HTML_BOOK]) with self.assertRaises(NoReverseMatch): self.make_url('html_book', book_index=0, chapter='xyzzy')
if 'chapters' in textbook: for entry in textbook['chapters']: entry['url'] = remap_static_url(entry['url'], course)
QUEUING = 'QUEUING' PROGRESS = 'PROGRESS'
task_id = str(uuid4())
instructor_task = cls( course_id=course_id, task_type=task_type, task_id=task_id, task_key=task_key, task_input=json_task_input, task_state=QUEUING, requester=requester ) instructor_task.save_now()
json_output = json.dumps(returned_result) if len(json_output) > 1023: raise ValueError("Length of task output is too long: {0}".format(json_output)) return json_output
task_progress['traceback'] = traceback_string
key.set_contents_from_string( data, headers={ "Content-Encoding": content_encoding, "Content-Length": len(data), "Content-Type": content_type, } )
for state in READY_STATES: running_tasks = running_tasks.exclude(task_state=state) return len(running_tasks) > 0
return InstructorTask.create(course_id, task_type, task_key, task_input, requester)
entry_needs_updating = True entry_needs_saving = False task_output = None
log.info("background task (%s), state %s: result: %s", task_id, result_state, returned_result) task_output = InstructorTask.create_output_for_success(returned_result)
entry_needs_saving = True log.warning("background task (%s) revoked.", task_id) task_output = InstructorTask.create_output_for_revoked()
if entry_needs_updating: instructor_task.task_state = result_state if task_output is not None: instructor_task.task_output = task_output
if instructor_task.task_state not in READY_STATES: result = AsyncResult(task_id) _update_instructor_task(instructor_task, result)
task_key = hashlib.md5(task_key_stub).hexdigest()
task_key = hashlib.md5(task_key_stub).hexdigest()
instructor_task = _reserve_task(course_key, task_type, task_key, task_input, request.user)
STATES_WITH_STATUS = [state for state in READY_STATES] + [PROGRESS]
msg_format = _("Progress: {action} {succeeded} of {attempted} so far")
msg_format = _("Problem {action} for {succeeded} of {attempted} students")
msg_format = _("Message {action} for {succeeded} of {attempted} recipients")
msg_format = _("Status: {action} {succeeded} of {attempted}")
msg_format += _(" (skipping {skipped})")
msg_format += _(" (out of {total})")
message = msg_format.format( action=action_name, succeeded=num_succeeded, attempted=num_attempted, total=num_total, skipped=num_skipped, student=student ) return (succeeded, message)
from __future__ import unicode_literals
for state in READY_STATES: instructor_tasks = instructor_tasks.exclude(task_state=state) return instructor_tasks.order_by('-id')
check_arguments_for_rescoring(usage_key)
check_arguments_for_rescoring(usage_key)
task_type = 'rescore_problem' task_class = rescore_problem task_input, task_key = encode_problem_and_student_input(usage_key) return submit_task(request, task_type, task_class, usage_key.course_key, task_input, task_key)
check_entrance_exam_problems_for_rescoring(usage_key)
task_type = 'rescore_problem' task_class = rescore_problem task_input, task_key = encode_entrance_exam_and_student_input(usage_key, student) return submit_task(request, task_type, task_class, usage_key.course_key, task_input, task_key)
modulestore().get_item(usage_key)
modulestore().get_item(usage_key)
modulestore().get_item(usage_key)
modulestore().get_item(usage_key)
milestones_helpers.remove_course_content_user_milestones( course_key=usage_key.course_key, content_key=usage_key, user=student, relationship='fulfills' )
email_obj = CourseEmail.objects.get(id=email_id) targets = [target.target_type for target in email_obj.targets.all()]
task_key = hashlib.md5(task_key_stub).hexdigest() return submit_task(request, task_type, task_class, course_key, task_input, task_key)
TASK_LOG = logging.getLogger('edx.celery.task')
UNKNOWN_TASK_ID = 'unknown-task_id' FILTERED_OUT_ROLES = ['staff', 'instructor', 'finance_admin', 'sales_admin'] UPDATE_STATUS_SUCCEEDED = 'succeeded' UPDATE_STATUS_FAILED = 'failed' UPDATE_STATUS_SKIPPED = 'skipped'
REPORT_REQUESTED_EVENT_NAME = u'edx.instructor.report.requested'
TASK_LOG.error(u"Task (%s) has no InstructorTask object for id %s", task_id, entry_id)
with outer_atomic(): entry = InstructorTask.objects.get(pk=entry_id) entry.task_state = PROGRESS entry.save_now()
task_id = entry.task_id course_id = entry.course_id task_input = json.loads(entry.task_input)
with dog_stats_api.timer('instructor_tasks.time.overall', tags=[u'action:{name}'.format(name=action_name)]): task_progress = task_fcn(entry_id, course_id, task_input, action_name)
reset_queries()
TASK_LOG.info(u'%s, Task type: %s, Finishing task: %s', task_info_string, action_name, task_progress) return task_progress
if problem_url: usage_key = course_id.make_usage_key_from_deprecated_string(problem_url) usage_keys.append(usage_key)
problem_descriptor = modulestore().get_item(usage_key) problems[unicode(usage_key)] = problem_descriptor
if entrance_exam_url: problems = get_problems_in_section(entrance_exam_url) usage_keys = [UsageKey.from_string(location) for location in problems.keys()]
modules_to_update = StudentModule.objects.filter(course_id=course_id, module_state_key__in=usage_keys)
request_info = xmodule_instance_args.get('request_info', {}) if xmodule_instance_args is not None else {} task_info = {'student': student.username, 'task_id': _get_task_id_from_xmodule_args(xmodule_instance_args)}
field_data_cache = FieldDataCache.cache_for_descriptor_descendents(course_id, student, module_descriptor) student_data = KvsFieldData(DjangoKeyValueStore(field_data_cache))
request_info = xmodule_instance_args.get('request_info', {}) if xmodule_instance_args is not None else {} task_info = {"student": student.username, "task_id": _get_task_id_from_xmodule_args(xmodule_instance_args)}
request_token=None, course=course
course_id = student_module.course_id student = student_module.student usage_key = student_module.module_state_key
msg = "No module {loc} for student {student}--access denied?".format( loc=usage_key, student=student ) TASK_LOG.debug(msg) raise UpdateProblemModuleStateError(msg)
msg = "Specified problem does not support rescoring." raise UpdateProblemModuleStateError(msg)
track_function = _get_track_function_for_task(student_module.student, xmodule_instance_args) track_function('problem_delete_state', {}) return UPDATE_STATUS_SUCCEEDED
output_buffer = StringIO(render_to_string("instructor/instructor_dashboard_2/executive_summary.html", data_dict))
header = None rows = [] err_rows = [["id", "username", "error_msg"]] current_step = {'step': 'Calculating Grades'}
if task_progress.attempted % status_interval == 0: task_progress.update_task_state(extra_meta=current_step) task_progress.attempted += 1
task_progress.failed += 1 err_rows.append([student.id, student.username, err_msg])
upload_csv_to_report_store(rows, 'grade_report', course_id, start_date)
if len(err_rows) > 1: upload_csv_to_report_store(err_rows, 'grade_report_err', course_id, start_date)
TASK_LOG.info(u'%s, Task type: %s, Finalizing grade task', task_info_string, action_name) return task_progress.update_task_state(extra_meta=current_step)
for block in blocks: if blocks[block]['block_type'] == 'sequential': block_format = blocks[block]['format'] if block_format not in assignments: assignments[block_format] = OrderedDict() assignments[block_format][block] = list()
problem_location = task_input.get('problem_location') student_data = list_problem_responses(course_id, problem_location) features = ['username', 'state'] header, rows = format_dictlist(student_data, features)
problem_location = re.sub(r'[:/]', '_', problem_location) csv_name = 'student_state_from_{}'.format(problem_location) upload_csv_to_report_store(rows, csv_name, course_id, start_date)
header_row = OrderedDict([('id', 'Student ID'), ('email', 'Email'), ('username', 'Username')])
if not err_msg: err_msg = u"Unknown error" error_rows.append(student_fields + [err_msg]) task_progress.failed += 1 continue
query_features = task_input.get('features') student_data = enrolled_students_features(course_id, query_features) header, rows = format_dictlist(student_data, query_features)
upload_csv_to_report_store(rows, 'student_profile_info', course_id, start_date)
if task_progress.attempted % status_interval == 0: task_progress.update_task_state(extra_meta=current_step) task_progress.attempted += 1
display_headers.append(enrollment_report_headers.get(header_element, header_element))
upload_csv_to_report_store(rows, 'enrollment_report', course_id, start_date, config_name='FINANCIAL_REPORTS')
TASK_LOG.info(u'%s, Task type: %s, Finalizing detailed enrollment task', task_info_string, action_name) return task_progress.update_task_state(extra_meta=current_step)
query_features = task_input.get('features') student_data = list_may_enroll(course_id, query_features) header, rows = format_dictlist(student_data, query_features)
upload_csv_to_report_store(rows, 'may_enroll_info', course_id, start_date)
data_dict = get_executive_report(course_id) data_dict.update( { 'total_enrollments': true_enrollment_count, 'report_generation_date': report_generation_date.strftime("%Y-%m-%d"), } )
upload_csv_to_report_store(csv_rows, 'course_survey_results', course_id, start_date)
query_features = _task_input.get('features') student_data = get_proctored_exam_results(course_id, query_features) header, rows = format_dictlist(student_data, query_features)
upload_csv_to_report_store(rows, 'proctored_exam_results_report', course_id, start_date)
students_to_generate_certs_for = students_to_generate_certs_for.filter( certificatewhitelist__course_id=course_id, certificatewhitelist__whitelist=True )
students_to_generate_certs_for = students_to_generate_certs_for.filter( certificatewhitelist__course_id=course_id, certificatewhitelist__whitelist=True ).exclude( generatedcertificate__course_id=course_id, generatedcertificate__status__in=CertificateStatuses.PASSED_STATUSES )
students_require_certs = students_to_generate_certs_for
invalidate_generated_certificates(course_id, students_to_generate_certs_for, statuses_to_regenerate)
for student in students_require_certs: task_progress.attempted += 1 status = generate_user_certificates( student, course_id, course=course )
with DefaultStorage().open(task_input['file_name']) as f: total_assignments = 0 for _line in unicodecsv.DictReader(UniversalNewlineIterator(f)): total_assignments += 1
cohorts_status = {}
username_or_email = row.get('email') or row.get('username') cohort_name = row.get('cohort') or '' task_progress.attempted += 1
task_progress.skipped += 1
students_require_certificates = enrolled_students.filter( generatedcertificate__course_id=course_id, generatedcertificate__status__in=statuses_to_regenerate ) return list(students_require_certificates)
students_already_have_certs = User.objects.filter( ~Q(generatedcertificate__status=CertificateStatuses.unavailable), generatedcertificate__course_id=course_id)
return list(set(enrolled_students) - set(students_already_have_certs))
certificates.update( status=CertificateStatuses.unavailable, verify_uuid='', download_uuid='', download_url='', grade='', )
status = InstructorTaskModuleTestCase.get_task_status(instructor_task.task_id) self.assertEqual(status['message'], expected_message)
self.setup_user()
problem_url_name = 'H1P1' self.define_option_problem(problem_url_name) location = InstructorTaskModuleTestCase.problem_location(problem_url_name) descriptor = self.module_store.get_item(location)
self.redefine_option_problem(problem_url_name) self.render_problem('u1', problem_url_name) self.check_state('u1', descriptor, 2, 2, 1)
self.define_randomized_custom_response_problem(problem_url_name, redefine=True) self.render_problem('u1', problem_url_name) self.check_state('u1', descriptor, 1, 1, 2)
self.submit_rescore_all_student_answers('instructor', problem_url_name)
for username in userlist: self.check_state(username, descriptor, 0, 1, 2)
self.submit_student_answer(self.student_a.username, problem_a_url, [OPTION_1, OPTION_1]) self.submit_student_answer(self.student_b.username, problem_b_url, [OPTION_1, OPTION_2])
instructor_task = InstructorTask.objects.get(id=instructor_task.id) instructor_task.task_state = PROGRESS instructor_task.save()
self.assertTrue(certificate_generation_history.exists())
self.assertTrue(certificate_generation_history.exists())
progress = {'message': TEST_FAILURE_MESSAGE, 'exception': TEST_FAILURE_EXCEPTION, } return self._create_entry(task_state=FAILURE, task_output=progress)
chapter = ItemFactory.create(parent_location=self.course.location, display_name=TEST_SECTION_NAME)
self.problem_section = ItemFactory.create(parent_location=chapter.location, category='sequential', metadata={'graded': True, 'format': 'Homework'}, display_name=TEST_SECTION_NAME)
csv_rows = [row for row in unicodecsv.DictReader(csv_file)]
instructor_task = self._create_entry() succeeded, message = get_task_completion_info(instructor_task) self.assertFalse(succeeded) self.assertEquals(message, "No status information available")
instructor_task = self._create_success_entry() instructor_task.task_output = None succeeded, message = get_task_completion_info(instructor_task) self.assertFalse(succeeded) self.assertEquals(message, "No status information available")
mock_create_subtask_fcn_args = mock_create_subtask_fcn.call_args_list self.assertEqual(len(mock_create_subtask_fcn_args[0][0][0]), 3) self.assertEqual(len(mock_create_subtask_fcn_args[1][0][0]), 3) self.assertEqual(len(mock_create_subtask_fcn_args[2][0][0]), 2)
mock_create_subtask_fcn_args = mock_create_subtask_fcn.call_args_list self.assertEqual(len(mock_create_subtask_fcn_args[0][0][0]), 3) self.assertEqual(len(mock_create_subtask_fcn_args[1][0][0]), 3) self.assertEqual(len(mock_create_subtask_fcn_args[2][0][0]), 5)
self._assert_num_attempts(students, initial_attempts) self._test_run_with_task(reset_problem_attempts, 'reset', num_students) self._assert_num_attempts(students, 0)
self._assert_num_attempts(students, initial_attempts) self._test_run_with_task(reset_problem_attempts, 'reset', 0, expected_num_skipped=num_students) self._assert_num_attempts(students, 0)
self._verify_cell_data_for_user(user_1, course.id, 'Cohort Name', '') self._verify_cell_data_for_user(user_2, course.id, 'Cohort Name', '')
self.initialize_course( course_factory_kwargs={ 'user_partitions': [user_partition] } )
cohort_a = CohortFactory.create(course_id=course.id, name=u'Cohørt A', users=[user_a]) CourseUserGroupPartitionGroup( course_user_group=cohort_a, partition_id=cohort_scheme_partition.id, group_id=cohort_scheme_partition.groups[0].id ).save()
self.assertTrue('Activate Course Enrollment' in response.content)
self.assertTrue('Activate Course Enrollment' in response.content)
self.assertTrue('Activate Course Enrollment' in response.content)
for row in unicodecsv.DictReader(csv_file): if row.get('Username') == username: self.assertEqual(row[column_header], expected_cell_content)
problem_section = ItemFactory.create(parent_location=chapter.location, category='sequential', metadata={'graded': True, 'format': problem_section_format}, display_name=problem_section_name)
problem_vertical = ItemFactory.create( parent_location=problem_section.location, category='vertical', display_name=problem_vertical_name ) problem_vertical_list.append(problem_vertical)
expected_grades = [self._format_user_grade(header_row, **user_grade) for user_grade in user_grades] self.verify_rows_in_csv(expected_grades)
self.assertTrue(cohorts.is_course_cohorted(self.course.id))
for user in [self.alpha_user, self.beta_user, self.non_cohorted_user]: self.assertTrue(CourseEnrollment.is_enrolled(user, self.course.id))
expected_grades = [self._format_user_grade(header_row, **grade) for grade in user_grades] self.verify_rows_in_csv(expected_grades)
self.assertTrue('Activate Course Enrollment' in response.content)
resp = self.client.post(reverse('shoppingcart.views.use_code'), {'code': 'coupon1'}) self.assertEqual(resp.status_code, 200)
num_students = len(students) self.assertDictContainsSubset({'attempted': num_students, 'succeeded': num_students, 'failed': 0}, result)
num_enrollments = len(enrollments) self.assertDictContainsSubset({'attempted': num_enrollments, 'succeeded': num_enrollments, 'failed': 0}, result)
students = self._create_students(10)
for student in students[:2]: GeneratedCertificateFactory.create( user=student, course_id=self.course.id, status=CertificateStatuses.downloadable, mode='honor' )
for student in students[2:7]: CertificateWhitelistFactory.create(user=student, course_id=self.course.id, whitelist=True)
for student in students[:3]: CertificateWhitelistFactory.create( user=student, course_id=self.course.id, whitelist=True )
for student in students[:2]: GeneratedCertificateFactory.create( user=student, course_id=self.course.id, status=status, )
for student in students[:3]: self.assertIn( GeneratedCertificate.certificate_for_student(student, self.course.id).status, CertificateStatuses.PASSED_STATUSES )
for student in students[3:]: self.assertIsNone( GeneratedCertificate.certificate_for_student(student, self.course.id) )
students = self._create_students(5)
for student in students[:2]: GeneratedCertificateFactory.create( user=student, course_id=self.course.id, status=status, )
for student in students[:4]: CertificateWhitelistFactory.create( user=student, course_id=self.course.id, whitelist=True )
for student in students[:4]: self.assertIn( GeneratedCertificate.certificate_for_student(student, self.course.id).status, CertificateStatuses.PASSED_STATUSES )
self.assertIsNone( GeneratedCertificate.certificate_for_student(students[4], self.course.id) )
students = self._create_students(10)
for student in students[:2]: GeneratedCertificateFactory.create( user=student, course_id=self.course.id, status=CertificateStatuses.downloadable, mode='honor' )
for student in students[2:5]: GeneratedCertificateFactory.create( user=student, course_id=self.course.id, status=CertificateStatuses.error, mode='honor' )
for student in students[5:6]: GeneratedCertificateFactory.create( user=student, course_id=self.course.id, status=CertificateStatuses.deleted, mode='honor' )
for student in students[:7]: CertificateWhitelistFactory.create(user=student, course_id=self.course.id, whitelist=True)
task_input = {'statuses_to_regenerate': [CertificateStatuses.downloadable, CertificateStatuses.error]}
default_grade = '-1'
students = self._create_students(10)
for student in students[:2]: GeneratedCertificateFactory.create( user=student, course_id=self.course.id, status=CertificateStatuses.downloadable, mode='honor', grade=default_grade )
for student in students[2:5]: GeneratedCertificateFactory.create( user=student, course_id=self.course.id, status=CertificateStatuses.error, mode='honor', grade=default_grade )
for student in students[5:6]: GeneratedCertificateFactory.create( user=student, course_id=self.course.id, status=CertificateStatuses.deleted, mode='honor', grade=default_grade )
for student in students[:7]: CertificateWhitelistFactory.create(user=student, course_id=self.course.id, whitelist=True)
task_input = {'statuses_to_regenerate': [CertificateStatuses.deleted, CertificateStatuses.generating]}
self.assertEqual(certificate_grades.count('0.0'), 5) self.assertEqual(certificate_grades.count(default_grade), 5)
default_grade = '-1'
students = self._create_students(10)
for student in students[:2]: GeneratedCertificateFactory.create( user=student, course_id=self.course.id, status=CertificateStatuses.downloadable, mode='honor', grade=default_grade )
for student in students[2:5]: GeneratedCertificateFactory.create( user=student, course_id=self.course.id, status=CertificateStatuses.error, mode='honor', grade=default_grade )
for student in students[5:7]: GeneratedCertificateFactory.create( user=student, course_id=self.course.id, status=CertificateStatuses.unavailable, mode='honor', grade=default_grade )
for student in students[7:]: GeneratedCertificateFactory.create( user=student, course_id=self.course.id, status=CertificateStatuses.generating, mode='honor', grade=default_grade )
for student in students[:]: CertificateWhitelistFactory.create(user=student, course_id=self.course.id, whitelist=True)
task_input = { 'statuses_to_regenerate': [ CertificateStatuses.downloadable, CertificateStatuses.error, CertificateStatuses.generating ] }
self.assertEqual(certificate_statuses.count(CertificateStatuses.generating), 8) self.assertEqual(certificate_statuses.count(CertificateStatuses.unavailable), 2)
self.assertEqual(certificate_grades.count('0.0'), 8) self.assertEqual(certificate_grades.count(default_grade), 2)
unavailable_certificates = \ [cert for cert in generated_certificates if cert.status == CertificateStatuses.unavailable and cert.grade == default_grade]
students = self._create_students(10)
for student in students[:2]: GeneratedCertificateFactory.create( user=student, course_id=self.course.id, status=CertificateStatuses.downloadable, mode='honor' )
for student in students[2:5]: GeneratedCertificateFactory.create( user=student, course_id=self.course.id, status=CertificateStatuses.error, mode='honor' )
for student in students[5:6]: GeneratedCertificateFactory.create( user=student, course_id=self.course.id, status=CertificateStatuses.deleted, mode='honor' )
for student in students[6:7]: GeneratedCertificateFactory.create( user=student, course_id=self.course.id, status=CertificateStatuses.notpassing, mode='honor' )
for student in students[:7]: CertificateWhitelistFactory.create(user=student, course_id=self.course.id, whitelist=True)
task_input = {'student_set': "all_whitelisted"}
MAX_DATABASE_LOCK_RETRIES = 5
if items_for_task: yield items_for_task num_items_queued += len(items_for_task)
entry.save_now() return task_progress
total_num_subtasks = _get_number_of_subtasks(total_num_items, items_per_task) subtask_id_list = [str(uuid4()) for _ in range(total_num_subtasks)]
item_list_generator = _generate_items_for_subtask( item_querysets, item_fields, total_num_items, items_per_task, total_num_subtasks, entry.course_id, )
return progress
key = "subtask-{}".format(task_id) cache.delete(key)
subtask_status_info[current_task_id] = new_subtask_status.to_dict()
action_name = ugettext_noop('rescored') update_fcn = partial(rescore_problem_module_state, xmodule_instance_args)
action_name = ugettext_noop('reset') update_fcn = partial(reset_attempts_module_state, xmodule_instance_args) visit_fcn = partial(perform_module_state_update, update_fcn, None) return run_main_task(entry_id, visit_fcn, action_name)
action_name = ugettext_noop('deleted') update_fcn = partial(delete_problem_module_state, xmodule_instance_args) visit_fcn = partial(perform_module_state_update, update_fcn, None) return run_main_task(entry_id, visit_fcn, action_name)
action_name = ugettext_noop('emailed') visit_fcn = perform_delegate_email_batches return run_main_task(entry_id, visit_fcn, action_name)
action_name = ugettext_noop('generated') task_fn = partial(upload_problem_responses_csv, xmodule_instance_args) return run_main_task(entry_id, task_fn, action_name)
action_name = ugettext_noop('generated') task_fn = partial(upload_students_csv, xmodule_instance_args) return run_main_task(entry_id, task_fn, action_name)
action_name = ugettext_noop('generating_enrollment_report') task_fn = partial(upload_enrollment_report, xmodule_instance_args) return run_main_task(entry_id, task_fn, action_name)
action_name = 'generating_exec_summary_report' task_fn = partial(upload_exec_summary_report, xmodule_instance_args) return run_main_task(entry_id, task_fn, action_name)
action_name = ugettext_noop('generated') task_fn = partial(upload_course_survey_report, xmodule_instance_args) return run_main_task(entry_id, task_fn, action_name)
action_name = ugettext_noop('generated') task_fn = partial(upload_may_enroll_csv, xmodule_instance_args) return run_main_task(entry_id, task_fn, action_name)
action_name = ugettext_noop('cohorted') task_fn = partial(cohort_students_and_upload, xmodule_instance_args) return run_main_task(entry_id, task_fn, action_name)
super(ViewsExceptionTestCase, self).setUp()
self.course = CourseFactory.create(org='MITx', course='999', display_name='Robot Super Course')
with patch('student.models.cc.User.save'): uname = 'student' email = 'student@edx.org' password = 'test'
self.student = UserFactory(username=uname, password=password, email=email)
CourseEnrollmentFactory(user=self.student, course_id=self.course.id)
self.client = Client() assert_true(self.client.login(username=uname, password=password))
mock_threads.return_value = [], 1, 1
mock_from_django_user.return_value = Mock()
mock_threads.return_value = CommentClientPaginatedResult(collection=[], page=1, num_pages=1)
mock_from_django_user.return_value = Mock()
if kwargs.get('params', {}).get('course_id'): data.update({ "threads_count": 1, "comments_count": 2 })
self.assertEquals( response_data["content"], strip_none(make_mock_thread_data(course=self.course, text=text, thread_id=thread_id, num_children=1)) ) mock_request.assert_called_with( "get",
self.assertEquals( response_data["content"], strip_none(make_mock_thread_data(course=self.course, text=text, thread_id=thread_id, num_children=1)) ) mock_request.assert_called_with( "get",
cached_calls = [ [num_uncached_mongo_calls, num_uncached_sql_queries], [num_cached_mongo_calls, num_cached_sql_queries], ] for expected_mongo_calls, expected_sql_queries in cached_calls: with self.assertNumQueries(expected_sql_queries): with check_mongo_calls(expected_mongo_calls): call_single_thread()
self.assertRegexpMatches(html, r'&#34;group_name&#34;: &#34;student_cohort&#34;')
self.assert_can_access(self.beta_user, self.alpha_module.discussion_id, thread_id, False)
self.assert_can_access(self.beta_user, self.alpha_module.discussion_id, thread_id, True)
try: CourseUserGroup.objects.get(id=group_id) kwargs['group_id'] = group_id except CourseUserGroup.DoesNotExist: pass
params_without_course_id = get_params_from_user_info_call(False) self.assertNotIn("group_id", params_without_course_id)
verify_group_id_always_present(profiled_user=self.student, pass_group_id=True) verify_group_id_always_present(profiled_user=self.student, pass_group_id=False) verify_group_id_always_present(profiled_user=self.moderator, pass_group_id=True) verify_group_id_always_present(profiled_user=self.moderator, pass_group_id=False)
verify_group_id_present(profiled_user=self.student, pass_group_id=True) verify_group_id_present(profiled_user=self.moderator, pass_group_id=True) verify_group_id_present( profiled_user=self.student, pass_group_id=True, requested_cohort=self.student_cohort )
verify_group_id_not_present(profiled_user=self.student, pass_group_id=False) verify_group_id_not_present(profiled_user=self.moderator, pass_group_id=False)
with super(InlineDiscussionUnicodeTestCase, cls).setUpClassAndTestData(): cls.course = CourseFactory.create()
with super(ForumFormDiscussionUnicodeTestCase, cls).setUpClassAndTestData(): cls.course = CourseFactory.create()
with super(ForumDiscussionSearchUnicodeTestCase, cls).setUpClassAndTestData(): cls.course = CourseFactory.create()
with super(SingleThreadUnicodeTestCase, cls).setUpClassAndTestData(): cls.course = CourseFactory.create(discussion_topics={'dummy_discussion_id': {'id': 'dummy_discussion_id'}})
with super(UserProfileUnicodeTestCase, cls).setUpClassAndTestData(): cls.course = CourseFactory.create()
with super(FollowedThreadsUnicodeTestCase, cls).setUpClassAndTestData(): cls.course = CourseFactory.create()
if discussion_id is not None: default_query_params['commentable_id'] = discussion_id if get_team(discussion_id) is not None: default_query_params['context'] = ThreadContext.STANDALONE
cc_user = cc.User.from_django_user(request.user) cc_user.default_sort_key = request.GET.get('sort_key') cc_user.save()
if 'pinned' not in thread: thread['pinned'] = False
return render_to_response('discussion/index.html', context)
thread_context = getattr(thread, "context", "course") if thread_context == "course" and not utils.discussion_category_id_access(course, request.user, discussion_id): raise Http404
if "pinned" not in thread: thread["pinned"] = False
}
call_command('seed_permissions_roles', unicode(self.course_id))
with patch('student.models.cc.User.save'): uname = 'student' email = 'student@edx.org'
CourseEnrollmentFactory(user=self.student, course_id=self.course_id)
CourseEnrollmentFactory(user=self.moderator, course_id=self.course.id) self.moderator.roles.add(Role.objects.get(name="Moderator", course_id=self.course.id))
call_command('seed_permissions_roles', unicode(cls.course_id))
super(ViewsTestCase, self).setUp()
with patch('student.models.cc.User.save'): uname = 'student' email = 'student@edx.org'
CourseEnrollmentFactory(user=self.student, course_id=self.course_id)
CourseEnrollmentFactory(user=self.moderator, course_id=self.course.id) self.moderator.roles.add(Role.objects.get(name="Moderator", course_id=self.course.id))
team.add_user(self.student)
self.create_thread_helper(mock_request, extra_response_data={'context': ThreadContext.STANDALONE})
with super(ViewPermissionsTestCase, cls).setUpClassAndTestData(): cls.course = CourseFactory.create()
with super(CreateThreadUnicodeTestCase, cls).setUpClassAndTestData(): cls.course = CourseFactory.create()
with super(UpdateThreadUnicodeTestCase, cls).setUpClassAndTestData(): cls.course = CourseFactory.create()
with super(CreateCommentUnicodeTestCase, cls).setUpClassAndTestData(): cls.course = CourseFactory.create()
with super(UpdateCommentUnicodeTestCase, cls).setUpClassAndTestData(): cls.course = CourseFactory.create()
with super(CreateSubCommentUnicodeTestCase, cls).setUpClassAndTestData(): cls.course = CourseFactory.create()
cls.team_commentable_id = "team_discussion_id" cls.team = CourseTeamFactory.create( name=u'The Only Team', course_id=cls.course.id, topic_id='topic_id', discussion_topic_id=cls.team_commentable_id )
cls.course_commentable_id = "course_level_commentable"
thread_author = getattr(self, thread_author) self._setup_mock(
with super(ForumEventTestCase, cls).setUpClassAndTestData(): cls.course = CourseFactory.create()
with super(UsersEndpointTestCase, cls).setUpClassAndTestData(): cls.course = CourseFactory.create()
self.enrollment.delete()
} track_created_event(request, event_name, course, thread, event_data)
if get_team(commentable_id) is not None: params['context'] = ThreadContext.STANDALONE else: params['context'] = ThreadContext.COURSE
try: group_id = get_group_id_for_comments_service(request, course_key, commentable_id) except ValueError: return HttpResponseBadRequest("Invalid cohort id") if group_id is not None: thread.group_id = group_id
if 'pinned' not in thread.attributes: thread['pinned'] = False
return JsonError(status=404)
return JsonError(["username parameter is required"])
return check_question_author(user, Thread(id=content["thread_id"]).to_dict())
logging.warning("Did not find key commentable_id in content.") passes_condition = False
import datetime import json import ddt import mock from nose.plugins.attrib import attr from pytz import UTC from django.utils.timezone import UTC as django_utc
test_discussion = self.store.create_child(self.user.id, course.location, 'discussion', 'test_discussion')
self.assertNotIn(test_discussion.location, self.store.get_orphans(course.id))
self.assertEqual(len(utils.get_accessible_discussion_modules(course, self.user)), 1)
self.assertIn(orphan, self.store.get_orphans(course.id))
start=datetime.datetime(2012, 2, 3, tzinfo=UTC)
self.course.discussion_topics = {} self.course.save() self.discussion_num = 0 self.instructor = InstructorFactory(course_key=self.course.id)
set_course_cohort_settings( course_key=self.course.id, is_cohorted=False, cohorted_discussions=["Topic_A"], always_cohort_inline_discussions=False, ) check_cohorted_topics([])
check_cohorted(False)
set_course_cohort_settings(course_key=self.course.id, is_cohorted=False) check_cohorted(False)
set_course_cohort_settings(course_key=self.course.id, is_cohorted=True) check_cohorted(True)
self.assertFalse( utils.is_commentable_cohorted(course.id, to_id("General")), "Course doesn't even have a 'General' topic" )
config_course_cohorts(course, is_cohorted=False, discussion_topics=["General", "Feedback"])
config_course_cohorts(course, is_cohorted=True, discussion_topics=["General", "Feedback"])
config_course_cohorts( course, is_cohorted=True, discussion_topics=["General", "Feedback"], cohorted_discussions=["Feedback"] )
self.assertFalse(utils.is_commentable_cohorted(course.id, team.discussion_topic_id)) self.assertTrue(utils.is_commentable_cohorted(course.id, "random"))
raise SkipTest
server_port = 4567 self.server_url = 'http://127.0.0.1:%d' % server_port
server_thread = threading.Thread(target=self.server.serve_forever) server_thread.daemon = True server_thread.start()
response = urllib2.urlopen(req)
response_dict = json.loads(response.read())
self.assertEqual(response_dict, self.expected_response)
length = int(self.headers.getheader('content-length')) data_string = self.rfile.read(length) post_dict = json.loads(data_string)
logger.debug( "Comment Service received POST request {0} to path {1}" .format(json.dumps(post_dict), self.path) )
if 'X-Edx-Api-Key' in self.headers: response = self.server._response_str logger.debug("Comment Service: sending response %s", json.dumps(response))
self.send_response(200) self.send_header('Content-type', 'application/json') self.end_headers() self.wfile.write(response)
self.send_response(500, 'Bad Request: does not contain API key') self.send_header('Content-type', 'text/plain') self.end_headers() return False
length = int(self.headers.getheader('content-length')) data_string = self.rfile.read(length) post_dict = json.loads(data_string)
logger.debug( "Comment Service received PUT request {0} to path {1}" .format(json.dumps(post_dict), self.path) )
if 'X-Edx-Api-Key' in self.headers: response = self.server._response_str logger.debug("Comment Service: sending response %s", json.dumps(response))
self.send_response(200) self.send_header('Content-type', 'application/json') self.end_headers() self.wfile.write(response)
self.send_response(500, 'Bad Request: does not contain API key') self.send_header('Content-type', 'text/plain') self.end_headers() return False
HTTPServer.shutdown(self)
self.socket.close()
self.TA_role_2.inherit_permissions(self.TA_role)
roles = get_role_ids(course_id) for role in roles: if user.id in roles[role]: return True return False
category_start_date = None for entry in entries: if category_start_date is None or entry["start_date"] < category_start_date: category_start_date = entry["start_date"]
dupe_counters[title] += 1 title = u"{title} ({counter})".format(title=title, counter=dupe_counters[title])
query_time = query.get('duration', 0) / 1000
if content.get('group_id') is not None: content['group_name'] = get_cohort_by_id(course_key, content.get('group_id')).name
content.pop('group_id', None)
group_id = get_cohort_id(request.user, course_key)
return None
ans = False
ans = commentable_id in course_cohort_settings.cohorted_discussions
ans = True
multi_db = True
from __future__ import unicode_literals
from __future__ import unicode_literals
StudentModuleHistory( id=initial_id, course_key=None, usage_key=None, username="", version="", created=datetime.datetime.now(), ).save()
return "BIGSERIAL"
from course_modes.models import CourseMode
eligible_certificates = EligibleCertificateManager()
objects = models.Manager()
return _("regenerated") if self.is_regeneration else _("generated")
return _("All learners")
from course_modes.models import CourseMode
if 'honor' not in course_mode_slugs: cert_status['status'] = CertificateStatuses.auditing return cert_status
from course_modes.models import CourseMode cert_set = cls.objects.create(course_key=course_key)
STATUS_STARTED = 'started' STATUS_SUCCESS = 'success' STATUS_ERROR = 'error'
EXAMPLE_FULL_NAME = u'John Doë'
url( r'^user/(?P<user_id>[^/]*)/course/{course_id}'.format(course_id=settings.COURSE_ID_PATTERN), views.render_html_view, name='html_view' ),
url( r'^(?P<certificate_uuid>[0-9a-f]{32})$', views.render_cert_by_uuid, name='render_cert_by_uuid' ),
from . import signals
resp = self.client.get(self.get_url(self.student.username)) self.assertEqual(resp.status_code, status.HTTP_401_UNAUTHORIZED)
self.dot_access_token.expires = datetime.utcnow() - timedelta(weeks=1) self.dot_access_token.save() self.assert_oauth_status(self.dot_access_token, status.HTTP_401_UNAUTHORIZED)
course = modulestore().get_course(course_key, depth=2)
ret = generate_user_certificates( student, course_key, course=course, insecure=options['insecure'] )
ret = regenerate_user_certificates( student, course_id, course=course, forced_grade=options['grade_value'], template_file=options['template_file'], insecure=options['insecure'] )
status_headings = sorted( set([status for course in cert_data for status in cert_data[course]]) )
print "{:>26}".format("course ID"), print ' '.join(["{:>16}".format(heading) for heading in status_headings])
$ ./manage.py lms resubmit_error_certificates
$ ./manage.py lms resubmit_error_certificates -c edX/DemoX/Fall_2015 -c edX/DemoX/Spring_2016
queryset = (
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
"download_url": ( cert.download_url or get_certificate_url(cert.user.id, cert.course_id) if cert.status == CertificateStatuses.downloadable else None ),
if not settings.FEATURES.get('CERTIFICATES_HTML_VIEW', False): return False
return course.cert_html_view_enabled if course else False
course_organization = get_course_organizations(course_key) if course_organization: org_id = course_organization[0]['id']
terms_of_service_and_honor_code = branding_api.get_tos_and_honor_code_url() if terms_of_service_and_honor_code != branding_api.EMPTY_URL: data.update({'company_tos_url': terms_of_service_and_honor_code})
privacy_policy = branding_api.get_privacy_url() if privacy_policy != branding_api.EMPTY_URL: data.update({'company_privacy_url': privacy_policy})
about = branding_api.get_about_url() if about != branding_api.EMPTY_URL: data.update({'company_about_url': about})
if course is None: course = modulestore().get_course(course_id, depth=0)
self.request.user = student self.request.session = {}
if cert_mode == CourseMode.CREDIT_MODE: cert_mode = CourseMode.VERIFIED
template_pdf = "certificate-template-{id.org}-{id.course}.pdf".format(id=course_id)
if is_whitelisted: LOGGER.info( u"Student %s is whitelisted in '%s'", student.id, unicode(course_id) ) passing = True else: passing = False
if self.restricted.filter(user=student).exists(): cert.status = status.restricted cert.save()
return self._generate_cert(cert, course, student, grade_contents, template_pdf, generate_pdf)
'example_certificate': True,
callback_url_path = reverse('certificates.views.update_example_certificate')
params, response = _validate_post_params(request.POST) if response is not None: return response
try: api.regenerate_user_certificates(params["user"], params["course_key"], course=course)
params, response = _validate_post_params(request.POST) if response is not None: return response
CourseOverview.get_from_id(params["course_key"])
generate_certificates_for_students( request, params["course_key"], student_set="specific_student", specific_student_id=params["user"].id ) return HttpResponse(200)
from .xqueue import * from .support import * from .webview import *
certificate_type = context.get('certificate_type')
context['document_title'] = _("{partner_short_name} {course_number} Certificate | {platform_name}").format( partner_short_name=context['organization_short_name'], course_number=context['course_number'], platform_name=platform_name )
context.update(configuration.get('default', {}))
reserved = _("All rights reserved") context['copyright_text'] = u'&copy; {year} {platform_name}. {reserved}.'.format( year=settings.COPYRIGHT_YEAR, platform_name=platform_name, reserved=reserved )
context['document_title'] = _("Invalid Certificate")
context['company_tos_urltext'] = _("Terms of Service &amp; Honor Code")
context['company_privacy_urltext'] = _("Privacy Policy")
context['logo_subtitle'] = _("Certificate Validation")
context['accomplishment_copy_about'] = _('About {platform_name} Accomplishments').format( platform_name=platform_name )
context['certificate_date_issued_title'] = _("Issued On:")
context['certificate_id_number_title'] = _('Certificate ID Number')
context['company_about_description'] = _("{platform_name} offers interactive online classes and MOOCs.").format( platform_name=platform_name)
context['document_banner'] = _("{platform_name} acknowledges the following student accomplishment").format( platform_name=platform_name )
context['accomplishment_copy_course_description'] = _('a course of study offered by ' '{partner_short_name}.').format( partner_short_name=context['organization_short_name'], platform_name=platform_name)
context['accomplishment_banner_opening'] = _("{fullname}, you earned a certificate!").format( fullname=user_fullname )
context['accomplishment_copy_more_about'] = _("More about {fullname}'s accomplishment").format( fullname=user_fullname )
try: user_certificate = GeneratedCertificate.eligible_certificates.get( user=user, course_id=course_key, status=CertificateStatuses.downloadable ) except GeneratedCertificate.DoesNotExist: pass
course_key = course.location.course_key
context = {} _update_context_with_basic_info(context, course_id, platform_name, configuration) invalid_template_path = 'certificates/invalid.html'
if not has_html_certificates_enabled(course_id): log.info( "Invalid cert: HTML certificates disabled for %s. User id: %d", course_id, user_id, ) return render_to_response(invalid_template_path, context)
try: course_key = CourseKey.from_string(course_id) user = User.objects.get(id=user_id) course = modulestore().get_course(course_key)
context.update(configuration.get(user_certificate.mode, {}))
_update_organization_context(context, course)
_update_course_context(request, context, course, platform_name)
_update_context_with_user_info(context, user, user_certificate)
_update_social_context(request, context, course, user, user_certificate, platform_name)
_update_certificate_context(context, user_certificate, platform_name)
_update_badge_context(context, course, user)
_update_microsite_context(context, configuration)
context.update(get_certificate_header_context(is_secure=request.is_secure())) context.update(get_certificate_footer_context())
context.update(course.cert_html_view_overrides)
_track_certificate_events(request, context, course, user, user_certificate)
return _render_certificate_template(request, context, course, user_certificate)
if rate_limiter.is_rate_limit_exceeded(request): log.info(u"Bad request rate limit exceeded for update example certificate end-point.") return HttpResponseForbidden("Rate limit exceeded")
return JsonResponse({'return_code': 0})
cert = GeneratedCertificate.eligible_certificates.get(user=self.student, course_id=self.course.id) self.assertEqual(cert.status, CertificateStatuses.downloadable)
cache.clear()
CertificateGenerationConfiguration.objects.create(enabled=True)
certs_api.set_cert_generation_enabled(self.COURSE_KEY, True) self._assert_enabled_for_course(self.COURSE_KEY, True)
certs_api.set_cert_generation_enabled(self.COURSE_KEY, False) self._assert_enabled_for_course(self.COURSE_KEY, False)
CertificateGenerationConfiguration.objects.create(enabled=True)
certs_api.set_cert_generation_enabled(self.COURSE_KEY, True) self._assert_enabled_for_course(self.COURSE_KEY, True)
other_course = CourseLocator(org='other', course='other', run='other') self._assert_enabled_for_course(other_course, False)
CourseModeFactory.create(course_id=self.COURSE_KEY, mode_slug=CourseMode.HONOR) with self._mock_xqueue() as mock_queue: certs_api.generate_example_certificates(self.COURSE_KEY)
self._assert_certs_in_queue(mock_queue, 1)
self._assert_cert_status({ 'description': 'honor', 'status': 'started' })
CourseModeFactory.create(course_id=self.COURSE_KEY, mode_slug='honor') CourseModeFactory.create(course_id=self.COURSE_KEY, mode_slug='verified')
with self._mock_xqueue() as mock_queue: certs_api.generate_example_certificates(self.COURSE_KEY)
self._assert_certs_in_queue(mock_queue, 2)
CourseModeFactory.create(course_id=self.COURSE_KEY, mode_slug=CourseMode.HONOR) data = certs_api.get_certificate_header_context(is_secure=True)
CourseModeFactory.create(course_id=self.COURSE_KEY, mode_slug=CourseMode.HONOR) data = certs_api.get_certificate_footer_context()
self.assertItemsEqual( data.keys(), ['company_about_url', 'company_privacy_url', 'company_tos_url'] )
self.assertIn( settings.MICROSITE_CONFIGURATION['test_microsite']["urls"]['ABOUT'], data['company_about_url'] )
self.assertIn( settings.MICROSITE_CONFIGURATION['test_microsite']["urls"]['PRIVACY'], data['company_privacy_url'] )
self.assertIn( settings.MICROSITE_CONFIGURATION['test_microsite']["urls"]['TOS_AND_HONOR'], data['company_tos_url'] )
set_prerequisite_courses(course.id, [unicode(pre_requisite_course.id)]) completed_milestones = milestones_achieved_by_user(student, unicode(pre_requisite_course.id)) self.assertEqual(len(completed_milestones), 0)
CourseEnrollmentFactory( user=self.user_2, course_id=self.course.id, is_active=True, mode='audit' ) CertificateWhitelistFactory(course_id=self.course.id, user=self.user_2)
self.assertTrue(mock_send.called) __, kwargs = mock_send.call_args_list[0]
self.assertFalse(mock_send.called)
self._assert_queue_task(mock_send, cert)
self.assertEqual(cert.status, ExampleCertificate.STATUS_STARTED)
self.assertEqual(cert.status, ExampleCertificate.STATUS_ERROR) self.assertIn(self.ERROR_MSG, cert.error_reason)
from uuid import uuid4
cache.clear()
for _ in range(100): response = self.client.post(self.url, data=payload) if response.status_code == 403: break
self.assertEqual(response.status_code, 403)
command = resubmit_error_certificates
CourseEnrollmentFactory.create( user=user, course_id=course_key, mode=mode )
GeneratedCertificate.eligible_certificates.create( user=user, course_id=course_key, status=status )
self._create_cert(self.courses[0].id, self.user, CertificateStatuses.error, mode)
with check_mongo_calls(1): self._run_command()
self._assert_cert_status(self.courses[0].id, self.user, CertificateStatuses.notpassing)
for idx in range(3): self._create_cert(self.courses[idx].id, self.user, CertificateStatuses.error)
self._run_command(course_key_list=[ unicode(self.courses[0].id), unicode(self.courses[1].id) ])
self._create_cert(self.courses[0].id, self.user, CertificateStatuses.error) self._create_cert(self.courses[1].id, self.user, other_status)
self._run_command()
self._assert_cert_status(self.courses[0].id, self.user, CertificateStatuses.notpassing) self._assert_cert_status(self.courses[1].id, self.user, other_status)
with check_mongo_calls(1): self._run_command()
self._assert_cert_status(phantom_course, self.user, CertificateStatuses.error)
self.support = UserFactory( username=self.SUPPORT_USERNAME, email=self.SUPPORT_EMAIL, password=self.SUPPORT_PASSWORD, ) SupportStaffRole().add_users(self.support)
self.student = UserFactory( username=self.STUDENT_USERNAME, email=self.STUDENT_EMAIL, password=self.STUDENT_PASSWORD, )
self.cert = GeneratedCertificate.eligible_certificates.create( user=self.student, course_id=self.CERT_COURSE_KEY, grade=self.CERT_GRADE, status=self.CERT_STATUS, mode=self.CERT_MODE, download_url=self.CERT_DOWNLOAD_URL, )
success = self.client.login(username=self.SUPPORT_USERNAME, password=self.SUPPORT_PASSWORD) self.assertTrue(success, msg="Couldn't log in as support staff")
if role is not None: role().add_users(user)
response = self._search("foo")
if role is not None: role().add_users(user)
response = self._regenerate()
cert = GeneratedCertificate.eligible_certificates.get(user=self.student) self.assertEqual(cert.status, CertificateStatuses.notpassing)
response = self._regenerate(course_key=self.CERT_COURSE_KEY) self.assertEqual(response.status_code, 400)
response = self._regenerate(username=self.STUDENT_USERNAME) self.assertEqual(response.status_code, 400)
CourseEnrollment.unenroll(self.student, self.CERT_COURSE_KEY)
response = self._regenerate( course_key=self.CERT_COURSE_KEY, username=self.STUDENT_USERNAME ) self.assertEqual(response.status_code, 400)
GeneratedCertificate.eligible_certificates.all().delete()
response = self._regenerate( course_key=self.CERT_COURSE_KEY, username=self.STUDENT_USERNAME ) self.assertEqual(response.status_code, 200)
num_certs = GeneratedCertificate.eligible_certificates.filter(user=self.student).count() self.assertEqual(num_certs, 1)
if role is not None: role().add_users(user)
response = self._generate()
response = self._generate(course_key=self.EXISTED_COURSE_KEY_2) self.assertEqual(response.status_code, 400)
response = self._generate(username=self.STUDENT_USERNAME) self.assertEqual(response.status_code, 400)
CourseEnrollment.unenroll(self.student, self.EXISTED_COURSE_KEY_2)
response = self._generate( course_key=self.EXISTED_COURSE_KEY_2, username=self.STUDENT_USERNAME ) self.assertEqual(response.status_code, 400)
GeneratedCertificate.eligible_certificates.all().delete()
response = self._generate( course_key=self.EXISTED_COURSE_KEY_2, username=self.STUDENT_USERNAME ) self.assertEqual(response.status_code, 200)
num_certs = GeneratedCertificate.eligible_certificates.filter(user=self.student).count() self.assertEqual(num_certs, 1)
TEST_DIR = path(__file__).dirname() TEST_DATA_DIR = 'common/test/data/' PLATFORM_ROOT = TEST_DIR.parent.parent.parent.parent TEST_DATA_ROOT = PLATFORM_ROOT / TEST_DATA_DIR
ExampleCertificateSet.objects.all().delete()
result = ExampleCertificateSet.latest_status(self.COURSE_KEY) self.assertIs(result, None)
certificate_template_asset.asset = SimpleUploadedFile('picture1.jpg', 'file contents') certificate_template_asset.save() self.assertEqual(certificate_template_asset.asset, 'certificate_template_assets/1/picture1.jpg')
certificate_template_asset.asset = SimpleUploadedFile('picture2.jpg', 'file contents') certificate_template_asset.save()
response = self.client.get(test_url) self.assertIn(str(self.cert.verify_uuid), response.content)
self.cert.mode = 'audit' self.cert.status = status self.cert.save()
response = self.client.get(test_url) self.assertIn(str(self.cert.verify_uuid), response.content)
self.assertContains(response, "<li class=\"wrapper-organization\">", 1)
self.assertIn('Cannot Find Certificate', response.content)
with self.assertRaises(Exception): self.client.get(test_url)
CertificateGenerationConfiguration.objects.create(enabled=True)
from datetime import datetime from mock import patch, Mock
default_store=ModuleStoreEnum.Type.mongo
return response
response = self.http_get_for_course(HTTP_AUTHORIZATION=None) self.assertEqual(response.status_code, 401)
response = self.http_get_for_course(HTTP_AUTHORIZATION=auth_header) self.assertEqual(response.status_code, 200)
response = self.http_get_for_course(course_id=unicode(self.empty_course.id), HTTP_AUTHORIZATION=auth_header) self.assertEqual(response.status_code, 404)
response = self.http_get(reverse(self.view), HTTP_AUTHORIZATION=auth_header) self.assertEqual(response.status_code, 200)
update_course_structure(unicode(self.course.id))
self.assertTrue(CourseStructure.objects.filter(course_id=self.course.id).exists()) response = self.http_get_for_course() self.assertEqual(response.status_code, 200)
results = (course for course in results if course.scope_ids.block_type == 'course')
results = (course for course in results if self.user_can_access_course(self.request.user, course))
return sorted(results, key=lambda course: unicode(course.id))
return Response(status=503, headers={'Retry-After': '120'})
redirect_to = get_next_url_for_login_page(request)
if request.user.is_authenticated(): return redirect(redirect_to)
form_descriptions = _get_form_descriptions(request)
ext_auth_response = _external_auth_intercept(request, initial_mode) if ext_auth_response is not None: return ext_auth_response
email = user.email if user.is_authenticated() else request.POST.get('email')
limiter.tick_bad_request_counter(request)
context["autoSubmitRegForm"] = True
for msg in messages.get_messages(request): if msg.extra_tags.split()[0] == "social-auth":
request = HttpRequest() request.method = "GET" request.session = session
view, args, kwargs = resolve(url) response = view(request, *args, **kwargs)
return response.content
context['duplicate_provider'] = pipeline.get_duplicate_provider(messages.get_messages(request))
u"{user}@example.com".format( user=(u'e' * (EMAIL_MAX_LENGTH - 11)) )
activation_key = create_account(self.USERNAME, self.OLD_PASSWORD, self.OLD_EMAIL) activate_account(activation_key)
result = self.client.login(username=self.USERNAME, password=self.OLD_PASSWORD) self.assertTrue(result)
response = self._change_password() self.assertEqual(response.status_code, 200)
self.assertEqual(len(mail.outbox), 1)
response = self.client.get(activation_link) self.assertEqual(response.status_code, 200)
self.client.logout()
result = self.client.login(username=self.USERNAME, password=self.NEW_PASSWORD) self.assertTrue(result)
result = self.client.login(username=self.USERNAME, password=self.OLD_PASSWORD) self.assertFalse(result)
result = self.client.login(username=self.USERNAME, password=self.NEW_PASSWORD) self.assertTrue(result)
self.client.logout()
self.client.logout()
create_account(self.ALTERNATE_USERNAME, self.OLD_PASSWORD, self.NEW_EMAIL)
response = self._change_password(email=self.NEW_EMAIL)
self.assertEqual(response.status_code, 200) self.assertEqual(len(mail.outbox), 1)
self.client.logout()
response = self._change_password(email=self.NEW_EMAIL) self.assertEqual(response.status_code, 400)
self.client.logout()
for attempt in xrange(self.INVALID_ATTEMPTS): self._change_password(email=self.NEW_EMAIL)
response = self.client.get(reverse(url_name)) self.assertRedirects(response, reverse("dashboard"))
with with_edx_domain_context(is_edx_domain): response = self.client.get(reverse(url_name), params)
with with_edx_domain_context(is_edx_domain): response = self.client.get(reverse(url_name), params)
else: response = self.client.get(reverse(url_name), params)
self.configure_google_provider(enabled=True) self.configure_facebook_provider(enabled=True)
self.settings_patcher = patch.dict('django.conf.settings.FEATURES', {'MILESTONES_APP': True}) self.settings_patcher.start()
self.chapter1 = ItemFactory.create( parent_location=self.course.location, category='chapter', display_name='untitled chapter 1' )
self.vert1 = ItemFactory.create( parent_location=self.seq1.location, category='vertical', display_name='untitled vertical 1' )
self.prob1 = ItemFactory.create( parent_location=self.vert1.location, category='problem', display_name='untitled problem 1' )
self.prob2 = ItemFactory.create( parent_location=self.course.location, category='problem', display_name='untitled problem 2' )
gating_api.add_prerequisite(self.course.id, self.seq1.location)
sync_cohort_with_mode.apply_async(kwargs=args, countdown=300)
from __future__ import unicode_literals
from __future__ import unicode_literals
self.assertFalse(VerifiedTrackCohortedCourse.is_verified_track_cohort_enabled(course_key))
config = VerifiedTrackCohortedCourse.objects.create(course_key=course_key, enabled=True) config.save() self.assertTrue(VerifiedTrackCohortedCourse.is_verified_track_cohort_enabled(course_key))
config.enabled = False config.save() self.assertFalse(VerifiedTrackCohortedCourse.is_verified_track_cohort_enabled(course_key))
celery_task_patcher = patch.object( sync_cohort_with_mode, 'apply_async', mock.Mock(wraps=sync_cohort_with_mode.apply_async) ) self.mocked_celery_task = celery_task_patcher.start() self.addCleanup(celery_task_patcher.stop)
self._enable_cohorting() self._create_verified_cohort() self.assertFalse(VerifiedTrackCohortedCourse.is_verified_track_cohort_enabled(self.course.id)) self._verify_no_automatic_cohorting() self.assertFalse(error_logger.called)
self._enable_cohorting() self._create_verified_cohort() self._enable_verified_track_cohorting() self.assertTrue(VerifiedTrackCohortedCourse.is_verified_track_cohort_enabled(self.course.id)) self._enroll_in_course()
self._enable_cohorting() self._create_verified_cohort() self._create_named_random_cohort("Random 1") self._create_named_random_cohort("Random 2") self._enable_verified_track_cohorting()
self._unenroll() self.assertEqual(DEFAULT_VERIFIED_COHORT_NAME, get_cohort(self.user, self.course.id, assign=False).name)
modified_cohort_name = "renamed random cohort" default_cohort.name = modified_cohort_name default_cohort.save()
current_cohort = get_cohort(user, course_key) verified_cohort = get_cohort_by_name(course_key, verified_cohort_name)
structure_json = models.TextField(verbose_name='Structure JSON', blank=True, null=True)
from .overrides import get_override_for_ccx return get_override_for_ccx(self, self.course, 'start')
from .overrides import get_override_for_ccx return get_override_for_ccx(self, self.course, 'due')
from .overrides import get_override_for_ccx return get_override_for_ccx(self, self.course, 'max_student_enrollments_allowed')
return restore( self._modulestore._clean_locator_for_mapping(locator) )
return self._modulestore._get_modulestore_for_courselike(locator)
return view(request, course, ccx)
assign_coach_role_to_ccx(ccx_locator, request.user, course.id)
context = get_ccx_creation_dict(course) messages.error(request, context['use_ccx_con_error_message']) return render_to_response('ccx/coach_dashboard.html', context)
start = TODAY().replace(tzinfo=pytz.UTC) override_field_for_ccx(ccx, course, 'start', start) override_field_for_ccx(ccx, course, 'due', None)
override_field_for_ccx(ccx, course, 'max_student_enrollments_allowed', settings.CCX_MAX_STUDENTS_ALLOWED)
email_params = get_email_params(course, auto_enroll=True, course_key=ccx_id, display_name=ccx.display_name) enroll_email( course_id=ccx_id, student_email=request.user.email, auto_enroll=True, email_students=True, email_params=email_params, )
ccx_ids_to_delete.append(get_override_for_ccx(ccx, block, 'due_id')) clear_ccx_field_info_from_ccx_map(ccx, block, 'due')
if child.visible_to_staff_only: continue
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
continue
continue
error_code = 'course_id_not_provided' if not is_ccx: log.info('Master course ID not provided') error_code = 'master_course_id_not_provided'
if not ignore_missing: for field in mandatory_fields: if field not in request_data: field_errors[field] = {'error_code': 'missing_field_{0}'.format(field)} if field_errors: return valid_input, field_errors
valid_input['course_modules'] = None
valid_input, field_errors = get_valid_input(request.data) if field_errors: return Response( status=status.HTTP_400_BAD_REQUEST, data={ 'field_errors': field_errors } )
course_modules_json = json.dumps(valid_input.get('course_modules'))
start = TODAY().replace(tzinfo=pytz.UTC) override_field_for_ccx(ccx_course_object, master_course_object, 'start', start) override_field_for_ccx(ccx_course_object, master_course_object, 'due', None)
override_field_for_ccx( ccx_course_object, master_course_object, 'max_student_enrollments_allowed', valid_input['max_students_allowed'] )
make_user_coach(coach, master_course_key)
master_course_object, master_course_key, _, _ = get_valid_course(unicode(ccx_course_object.course_id))
response.data["current_page"] = self.page.number
response.data["start"] = (self.page.number - 1) * self.get_page_size(self.request)
instructor = UserFactory() allow_access(self.course, instructor, 'instructor')
staff_user = UserFactory(username='test_staff_user', email='test_staff_user@openedx.org', password='test') CourseStaffRole(self.master_course_key).add_users(staff_user)
instructor_user = UserFactory( username='test_instructor_user', email='test_instructor_user@openedx.org', password='test' ) CourseInstructorRole(self.master_course_key).add_users(instructor_user)
coach_user = UserFactory( username='test_coach_user', email='test_coach_user@openedx.org', password='test' ) CourseCcxCoachRole(self.master_course_key).add_users(coach_user)
resp = self.client.get(self.list_url_master_course, {}, HTTP_AUTHORIZATION=self.auth)
url = '{0}&order_by=display_name&sort_order=desc'.format(self.list_url_master_course) resp = self.client.get(url, {}, HTTP_AUTHORIZATION=self.auth)
for key, val in data.iteritems():
self.assertEqual(len(outbox), 1)
staff_user = User.objects.create_user('test_staff_user', 'test_staff_user@openedx.org', 'test') CourseStaffRole(self.master_course_key).add_users(staff_user)
instructor_user = User.objects.create_user('test_instructor_user', 'test_instructor_user@openedx.org', 'test') CourseInstructorRole(self.master_course_key).add_users(instructor_user)
coach_user = User.objects.create_user('test_coach_user', 'test_coach_user@openedx.org', 'test') CourseCcxCoachRole(self.master_course_key).add_users(coach_user)
return False
return True
cls.coach = AdminFactory.create()
with self.assertNumQueries(6): override_field_for_ccx(self.ccx, chapter, 'start', ccx_start)
with self.assertNumQueries(6): override_field_for_ccx(self.ccx, chapter, 'start', ccx_start)
response = render_to_response(path, context) response.mako_context = context response.mako_template = path return response
self.client.login(username=self.coach.username, password="test")
staff = UserFactory() allow_access(self.course, staff, 'staff') self.assertTrue(CourseStaffRole(self.course.id).has_user(staff))
instructor = UserFactory() allow_access(self.course, instructor, 'instructor') self.assertTrue(CourseInstructorRole(self.course.id).has_user(instructor))
path = urlparse.urlparse(url).path resolver = resolve(path) ccx_key = resolver.kwargs['course_id']
ccx = CustomCourseForEdX.objects.get() course_enrollments = get_override_for_ccx(ccx, self.course, 'max_student_enrollments_allowed') self.assertEqual(course_enrollments, settings.CCX_MAX_STUDENTS_ALLOWED)
role = CourseCcxCoachRole(course_key) self.assertTrue(role.has_user(self.coach))
list_staff_master_course = list_with_level(self.course, 'staff') list_instructor_master_course = list_with_level(self.course, 'instructor')
self.assertEqual(len(response.redirect_chain), 1) self.assertIn(302, response.redirect_chain[0]) self.assertEqual(len(outbox), outbox_count) if send_email:
self.assertTrue( CourseEnrollment.objects.filter(course_id=self.course.id, user=student).exists() )
ccx = self.make_ccx(max_students_allowed=2) ccx_course_key = CCXLocator.from_course_locator(self.course.id, ccx.id) staff = self.make_staff() instructor = self.make_instructor()
students = [instructor, staff, self.coach] + [ UserFactory.create(is_staff=False) for _ in range(3) ]
self.assertEqual(len(response.redirect_chain), 1) self.assertIn(302, response.redirect_chain[0]) self.assertEqual(len(outbox), outbox_count) if send_email:
self.assertFalse( CourseEnrollment.objects.filter(course_id=self.course.id, user=student).exists() )
self.assertEqual(len(response.redirect_chain), 1) self.assertIn(302, response.redirect_chain[0]) self.assertEqual(len(outbox), outbox_count)
if view_name == 'ccx_manage_student' and not is_email(identifier): self.assertContains(response, 'Could not find a user with name or email ', status_code=200)
with self.store.bulk_operations(course.id, emit_signals=False):
self.coach = UserFactory.create() self.mstore = modulestore()
self.client.login(username=self.coach.username, password="test")
staff = UserFactory() allow_access(self.course, staff, 'staff') self.assertTrue(CourseStaffRole(self.course.id).has_user(staff))
instructor = UserFactory() allow_access(self.course, instructor, 'instructor') self.assertTrue(CourseInstructorRole(self.course.id).has_user(instructor))
self.coach = coach = AdminFactory.create() self.client.login(username=coach.username, password="test")
role = CourseCcxCoachRole(self._course.id) role.add_users(coach) ccx = CcxFactory(course_id=self._course.id, coach=self.coach)
self.assertEqual( response['content-disposition'], 'attachment' ) rows = response.content.strip().split('\r') headers = rows[0]
self.coach = AdminFactory.create() role = CourseCcxCoachRole(self.split_course.id) role.add_users(self.coach)
self.ccx = CcxFactory(course_id=self.split_course.id, coach=self.coach) last_week = datetime.datetime.now(UTC()) - datetime.timedelta(days=7)
fake_course_key = CourseKey.from_string('course-v1:FakeOrg+CN1+CR-FALLNEVER1') self.assertEqual(utils.get_course_chapters(fake_course_key), None)
self.client.login(username=self.coach.username, password="test")
self.mstore = modulestore()
staff = self.make_staff() self.assertTrue(CourseStaffRole(self.course.id).has_user(staff))
instructor = self.make_instructor() self.assertTrue(CourseInstructorRole(self.course.id).has_user(instructor))
list_staff_master_course = list_with_level(self.course, 'staff') list_instructor_master_course = list_with_level(self.course, 'instructor')
instructor = self.make_instructor() self.assertTrue(CourseInstructorRole(self.course.id).has_user(instructor))
instructor = self.make_instructor() self.assertTrue(CourseInstructorRole(self.course.id).has_user(instructor))
instructor = self.make_instructor() self.assertTrue(CourseInstructorRole(self.course.id).has_user(instructor))
remove_master_course_staff_from_ccx( self.course, self.ccx_locator, self.ccx.display_name, send_email=True ) self.assertEqual(len(outbox), len(list_staff_master_course) + len(list_instructor_master_course))
remove_master_course_staff_from_ccx(self.course, self.ccx_locator, self.ccx.display_name) self.assertEqual(len(outbox), len(list_staff_master_course) + len(list_instructor_master_course))
multi_db = True
TEST_DATA = None
with self.settings(MODULESTORE_BRANCH='published-only'): for cache in settings.CACHES: caches[cache].clear()
modulestore().get_course(self.course.id, depth=None)
RequestCache.clear_request_cache()
OverrideFieldData.provider_classes = None
with cls.store.bulk_operations(course.id, emit_signals=False):
self.coach = UserFactory.create() self.mstore = modulestore()
date = date.strftime('%Y-%m-%d %H:%M')
date = master_date.strftime('%Y-%m-%d %H:%M')
date = get_date(ccx, node=parent_node, date_type=date_type)
if ccxs.exists(): return ccxs[0] return None
if staff not in list_staff_ccx: try: enroll_email( course_id=ccx_key, student_email=staff.email, auto_enroll=True, email_students=send_email, email_params=email_params, )
allow_access(course_ccx, staff, 'staff')
if instructor not in list_instructor_ccx: try: enroll_email( course_id=ccx_key, student_email=instructor.email, auto_enroll=True, email_students=send_email, email_params=email_params, )
allow_access(course_ccx, instructor, 'instructor')
revoke_access(course_ccx, staff, 'staff')
unenroll_email( course_id=ccx_key, student_email=staff.email, email_students=send_email, email_params=email_params, )
revoke_access(course_ccx, instructor, 'instructor')
unenroll_email( course_id=ccx_key, student_email=instructor.email, email_students=send_email, email_params=email_params, )
from __future__ import unicode_literals
exists_ce = is_active is not None and is_active full_name = user.profile.name
return UserPreference.get_value(user, LANGUAGE_KEY)
if CourseMode.is_white_label(course_id): course_mode = CourseMode.DEFAULT_SHOPPINGCART_MODE_SLUG else: course_mode = None
send_mail_to_student(student_email, email_params, language=language)
problem_state = json.loads(studentmodule.state) problem_state["attempts"] = 0
studentmodule.state = json.dumps(problem_state) studentmodule.save()
if 'display_name' in param_dict: param_dict['course_name'] = param_dict['display_name']
message_type = param_dict['message']
message = message.strip()
subject = ''.join(subject.splitlines()) from_address = theming_helpers.get_value( 'email_from_address', settings.DEFAULT_FROM_EMAIL )
meta = {} if user_info.profile.meta: meta = json.loads(user_info.profile.meta)
registration_code_redemption = RegistrationCodeRedemption.registration_code_used_for_enrollment( course_enrollment) paid_course_reg_item = PaidCourseRegistration.get_course_item_for_user_enrollment( user=user, course_id=course_id, course_enrollment=course_enrollment )
list_price = 'N/A' payment_amount = 'N/A' coupon_codes_used = 'N/A' registration_code_used = 'N/A' payment_status = _('Data Integrity Error') transaction_reference_number = 'N/A'
if invoice_transaction.amount > 0: payment_status = 'Invoice Paid' else: payment_status = 'Refunded'
url(r'^get_proctored_exam_results$', 'instructor.views.api.get_proctored_exam_results', name="get_proctored_exam_results"),
url(r'^list_financial_report_downloads$', 'instructor.views.api.list_financial_report_downloads', name="list_financial_report_downloads"),
url(r'get_coupon_codes', 'instructor.views.api.get_coupon_codes', name="get_coupon_codes"),
url(r'^gradebook$', 'instructor.views.gradebook_api.spoc_gradebook', name='spoc_gradebook'),
url(r'add_users_to_cohorts$', 'instructor.views.api.add_users_to_cohorts', name="add_users_to_cohorts"),
url(r'^generate_example_certificates$', 'instructor.views.api.generate_example_certificates', name='generate_example_certificates'),
MAX_STUDENTS_PER_PAGE_GRADE_BOOK = 20
next_offset = offset + MAX_STUDENTS_PER_PAGE_GRADE_BOOK previous_offset = offset - MAX_STUDENTS_PER_PAGE_GRADE_BOOK
page_num = ((offset / MAX_STUDENTS_PER_PAGE_GRADE_BOOK) + 1)
total_pages = int(math.ceil(float(total_students) / MAX_STUDENTS_PER_PAGE_GRADE_BOOK)) or 1
previous_offset = None
next_offset = None
enrolled_students = enrolled_students[offset: offset + MAX_STUDENTS_PER_PAGE_GRADE_BOOK]
'staff_access': True, 'ordered_grades': sorted(course.grade_cutoffs.items(), key=lambda i: i[1], reverse=True),
email_feature_dict['email'] = email_info
success, task_message = get_task_completion_info(task) status = _("Complete") if success else _("Incomplete") task_feature_dict['status'] = status task_feature_dict['task_message'] = task_message
if CourseMode.is_white_label(course_id): course_mode = CourseMode.DEFAULT_SHOPPINGCART_MODE_SLUG else: course_mode = None
email = student[EMAIL_INDEX] username = student[USERNAME_INDEX] name = student[NAME_INDEX] country = student[COUNTRY_INDEX][:2]
user = User.objects.get(email=email)
password = generate_unique_password(generated_passwords) errors = create_and_enroll_user( email, username, name, country, password, course_id, course_mode, request.user, email_params ) row_errors.extend(errors)
user = create_user_and_user_profile(email, username, name, country, password)
create_manual_course_enrollment( user=user, course_id=course_id, mode=course_mode, enrolled_by=enrolled_by, reason='Enrolling via csv upload', state_transition=UNENROLLED_TO_ENROLLED, )
user = None email = None language = None try: user = get_student_from_identifier(identifier) except User.DoesNotExist: email = identifier else: email = user.email language = get_user_email_language(user)
results.append({ 'identifier': identifier, 'invalidIdentifier': True, })
log.exception(u"Error while #{}ing student") log.exception(exc) results.append({ 'identifier': identifier, 'error': True, })
if email_students: send_beta_role_email(action, user, email_params) if auto_enroll: if not CourseEnrollment.is_enrolled(user, course_id): CourseEnrollment.enroll(user, course_id)
results.append({ 'identifier': identifier, 'error': error, 'userDoesNotExist': user_does_not_exist })
if not user.is_active: response_payload = { 'unique_student_identifier': user.username, 'inactiveUser': True, } return JsonResponse(response_payload)
run = problem_key.run if not run: problem_key = course_key.make_usage_key_from_deprecated_string(problem_location) if problem_key.course_key != course_key: raise InvalidKeyError(type(problem_key), problem_key)
query_features = microsite.get_value('student_profile_download_fields')
query_features.append('cohort') query_features_names['cohort'] = _('Cohort')
instructor_task.api.submit_cohort_students(request, course_key, filename)
matching_coupons = Coupon.objects.filter(code=code, is_active=True) if matching_coupons: return save_registration_code( user, course_id, mode_slug, invoice=invoice, order=order, invoice_item=invoice_item )
try: course_code_number = int(request.POST['total_registration_codes']) except ValueError: course_code_number = int(float(request.POST['total_registration_codes']))
subject = u'Confirmation and Invoice for {course_name}'.format(course_name=course.display_name) message = render_to_string('emails/registration_codes_sale_email.txt', context)
recipient_list.append(finance_email)
registration_codes_list = CourseRegistrationCode.objects.filter( course_id=course_id ).order_by('invoice_item__invoice__company_name')
if all_students and student: return HttpResponseBadRequest( "all_students and unique_student_identifier are mutually exclusive." ) if all_students and delete_module: return HttpResponseBadRequest( "all_students and delete_module are mutually exclusive." )
if all_students or delete_module: if not has_access(request.user, 'instructor', course): return HttpResponseForbidden("Requires instructor access.")
error_msg = _("An error occurred while deleting the score.") return HttpResponse(error_msg, status=500)
if all_students or delete_module: if not has_access(request.user, 'instructor', course): return HttpResponseForbidden(_("Requires instructor access."))
tasks = instructor_task.api.get_instructor_task_history(course_id, task_type=task_type)
emails = instructor_task.api.get_instructor_task_history(course_id, task_type=task_type)
tasks = instructor_task.api.get_instructor_task_history(course_id, module_state_key, student)
tasks = instructor_task.api.get_instructor_task_history(course_id, module_state_key)
tasks = instructor_task.api.get_running_instructor_tasks(course_id)
tasks = instructor_task.api.get_entrance_exam_instructor_task_history(course_id, entrance_exam_key, student)
tasks = instructor_task.api.get_entrance_exam_instructor_task_history(course_id, entrance_exam_key)
if not (has_forum_admin or has_instructor_access): return HttpResponseBadRequest( "Operation requires staff & forum admin or instructor access" )
if rolename == FORUM_ROLE_ADMINISTRATOR and not has_instructor_access: return HttpResponseBadRequest("Operation requires instructor access.")
if rolename not in [FORUM_ROLE_ADMINISTRATOR, FORUM_ROLE_MODERATOR, FORUM_ROLE_COMMUNITY_TA]: return HttpResponseBadRequest(strip_tags( "Unrecognized rolename '{}'.".format(rolename) ))
try: email = CourseEmail.create( course_id, request.user, targets, subject, message, template_name=template_name, from_addr=from_addr ) except ValueError as err: return HttpResponseBadRequest(repr(err))
instructor_task.api.submit_bulk_course_email(request, course_id, email.id)
if not (has_forum_admin or has_instructor_access): return HttpResponseBadRequest( "Operation requires staff & forum admin or instructor access" )
if rolename == FORUM_ROLE_ADMINISTRATOR and not has_instructor_access: return HttpResponseBadRequest("Operation requires instructor access.")
return JsonResponse( _("Successfully removed invalid due date extension (unit has no due date).") )
try: certificate_exception, student = parse_request_data_and_get_user(request, course_key) except ValueError as error: return JsonResponse({'success': False, 'message': error.message}, status=400)
pass
students = 'all_whitelisted'
return JsonResponse( { 'success': False, 'message': _('Invalid data, generate_for must be "new" or "all".'), }, status=400 )
if len(student) != 2: if len(student) > 0: build_row_errors('data_format_error', student[user_index], row_num)
try: certificate_invalidation_data = parse_request_data(request) certificate = validate_request_data_and_get_certificate(certificate_invalidation_data, course_key) except ValueError as error: return JsonResponse({'message': error.message}, status=400)
elif request.method == 'DELETE': try: re_validate_certificate(request, course_key, certificate) except ValueError as error: return JsonResponse({'message': error.message}, status=400)
certificate_invalidation = CertificateInvalidation.objects.get(generated_certificate=generated_certificate)
certificate_invalidation.deactivate()
student = certificate_invalidation.generated_certificate.user instructor_task.api.generate_certificates_for_students( request, course_key, student_set="specific_student", specific_student_id=student.id )
sections.append(_section_analytics(course, access))
if BulkEmailFlag.feature_enabled(course_key): sections.append(_section_send_email(course, access))
if settings.FEATURES['CLASS_DASHBOARD'] and access['staff']: sections.append(_section_metrics(course, access))
if course_mode_has_price and (access['finance_admin'] or access['sales_admin']): sections.append(_section_e_commerce(course, access, paid_modes[0], is_white_label, is_white_label))
certs_enabled = CertificateGenerationConfiguration.current().enabled if certs_enabled and access['admin']: sections.append(_section_certificates(course))
log.info('deleting redemption entry (%s) from the database.', code_redemption.id) code_redemption.delete()
with disable_overrides(): original_due_date = getattr(unit, 'due', None)
if not get_override_for_user(student, unit, 'due'): raise DashboardError(_("No due date extension is set for that student and unit."))
cls.audit_course = CourseFactory.create() CourseModeFactory.create(course_id=cls.audit_course.id, mode_slug=CourseMode.AUDIT)
self.white_label_course = CourseFactory.create() self.white_label_course_mode = CourseModeFactory.create( course_id=self.white_label_course.id, mode_slug=CourseMode.HONOR, min_price=10, suggested_prices='10', )
info_log.assert_called_with('email sent to new created user at %s', 'test_student@example.com')
info_log.assert_called_with('email sent to new created user at %s', 'test_student@example.com')
info_log.assert_called_with( u"user already exists with username '%s' and email '%s'", 'test_student_1', 'test_student@example.com' )
self.client.login(username=self.audit_course_instructor.username, password='test')
for enrollment in manual_enrollments: self.assertEqual(enrollment.enrollment.mode, CourseMode.AUDIT)
self.white_label_course_mode.min_price = 0 self.white_label_course_mode.suggested_prices = ''
self.client.login(username=self.white_label_course_instructor.username, password='test')
for enrollment in manual_enrollments: self.assertEqual(enrollment.enrollment.mode, CourseMode.HONOR)
self.client.login(username=self.white_label_course_instructor.username, password='test')
for enrollment in manual_enrollments: self.assertEqual(enrollment.enrollment.mode, CourseMode.DEFAULT_SHOPPINGCART_MODE_SLUG)
cea = CourseEnrollmentAllowed(email='robot-allowed@robot.org', course_id=self.course.id) cea.save() self.allowed_email = 'robot-allowed@robot.org'
user = User.objects.get(email=self.notenrolled_student.email) self.assertTrue(CourseEnrollment.is_enrolled(user, self.course.id))
self.assertEqual(len(mail.outbox), 0)
user = User.objects.get(email=self.notenrolled_student.email) self.assertTrue(CourseEnrollment.is_enrolled(user, self.course.id))
user = User.objects.get(email=self.enrolled_student.email) self.assertFalse(CourseEnrollment.is_enrolled(user, self.course.id))
self.assertEqual(len(mail.outbox), 0)
user = User.objects.get(email=self.enrolled_student.email) self.assertFalse(CourseEnrollment.is_enrolled(user, self.course.id))
mock_uses_shib.return_value = True
course_enrollment.mode = u'verified' course_enrollment.save() self.assertEqual(course_enrollment.mode, u'verified')
self._change_student_enrollment(self.enrolled_student, self.course, 'enroll')
course_enrollment.mode = u'verified' course_enrollment.save() self.assertEqual(course_enrollment.mode, u'verified')
expected = { "action": "add", "results": [ { "identifier": identifier, "error": False, "userDoesNotExist": False } ] }
self.assertEqual(len(mail.outbox), 0)
self.assertEqual(len(mail.outbox), 0)
if hasattr(self.beta_tester, '_roles'): del self.beta_tester._roles self.assertFalse(CourseBetaTesterRole(self.course.id).has_user(self.beta_tester))
self.assertEqual(len(mail.outbox), 0)
if hasattr(self.beta_tester, '_roles'): del self.beta_tester._roles self.assertFalse(CourseBetaTesterRole(self.course.id).has_user(self.beta_tester))
seed_permissions_roles(self.course.id)
self.assertEqual(response.status_code, 200)
self.assertTrue('Activate Course Enrollment' in response.content)
response = self.assert_request_status_code(400, url, method="POST", data=data) self.assertIn("The sale associated with this invoice has already been invalidated.", response.content)
data['event_type'] = "re_validate" self.assert_request_status_code(200, url, method="POST", data=data)
response = self.assert_request_status_code(400, url, method="POST", data=data) self.assertIn("This invoice is already active.", response.content)
coupon = Coupon( code='test_code', description='test_description', course_id=self.course.id, percentage_discount='10', created_by=self.instructor, is_active=True ) coupon.save()
CourseFinanceAdminRole(self.course.id).add_users(self.instructor)
resp = self.client.post(reverse('shoppingcart.views.use_code'), {'code': coupon.code}) self.assertEqual(resp.status_code, 200)
self.cart.purchase() resp = self.client.get(instructor_dashboard) self.assertEqual(resp.status_code, 200)
test_user = UserFactory() self.register_with_redemption_code(test_user, course_registration_code.code)
changed_module = StudentModule.objects.get(pk=self.module_to_reset.pk) self.assertEqual( json.loads(changed_module.state)['attempts'], 0 )
self.assertEqual( StudentModule.objects.filter( student=self.module_to_reset.student, course_id=self.module_to_reset.course_id, ).count(), 0 )
CourseInstructorRole(self.course_with_invalid_ee.id).add_users(self.instructor) self.client.login(username=self.instructor.username, password='test')
changed_modules = StudentModule.objects.filter(module_state_key__in=self.ee_modules) for changed_module in changed_modules: self.assertEqual( json.loads(changed_module.state)['attempts'], 0 )
changed_modules = StudentModule.objects.filter(module_state_key__in=self.ee_modules) self.assertEqual(changed_modules.count(), 0)
tasks = json.loads(response.content)['tasks'] self.assertEqual(len(tasks), 0)
response = self.client.post(url, { 'unique_student_identifier': self.student.email, })
message = _('This student (%s) is already allowed to skip the entrance exam.') % self.student.email self.assertContains(response, message)
self.duration_sec = 'unknown'
self.assertEqual(len(email_info), 1)
expected_message = self.emails[0].html_message returned_email_info = email_info[0] received_message = returned_email_info[u'email'][u'html_message'] self.assertEqual(expected_message, received_message)
self.assertEqual(len(email_info), 0)
certificate_count = 3 for __ in xrange(certificate_count): self.generate_certificate(course_id=self.course.id, mode='honor', status=CertificateStatuses.generating)
certificate_count = 3 for __ in xrange(certificate_count): self.generate_certificate(course_id=self.course.id, mode='honor', status=CertificateStatuses.downloadable)
for __ in xrange(certificate_count): self.generate_certificate( course_id=self.course.id, mode='verified', status=CertificateStatuses.downloadable )
self.assertEqual(len(res_json['certificates']), 2)
certificate = res_json['certificates'][1] self.assertEqual(certificate.get('total_issued_certificate'), 3) self.assertEqual(certificate.get('mode'), 'verified')
certificate_count = 3 for __ in xrange(certificate_count): self.generate_certificate(course_id=self.course.id, mode='honor', status=CertificateStatuses.downloadable)
for i in range(5): i += 1 registration_code_redemption = RegistrationCodeRedemption( registration_code_id=i, redeemed_by=self.instructor ) registration_code_redemption.save()
self.assertEqual(mail.outbox[-1].to[0], 'finance@example.com')
url_user_invoice_preference = reverse('get_user_invoice_preference', kwargs={'course_id': self.course.id.to_deprecated_string()})
url_user_invoice_preference = reverse('get_user_invoice_preference', kwargs={'course_id': self.course.id.to_deprecated_string()})
for i in range(9): i += 13 registration_code_redemption = RegistrationCodeRedemption( registration_code_id=i, redeemed_by=self.instructor ) registration_code_redemption.save()
reg_code = CourseRegistrationCode.objects.get(code=reg_code.code) self.assertEqual(reg_code.is_valid, False)
enrollment = CourseEnrollment.get_enrollment(student, self.course.id) self.assertEqual(enrollment.is_active, False)
enrollment = CourseEnrollment.get_enrollment(student, self.course.id) self.assertEqual(enrollment.is_active, False)
reg_code = CourseRegistrationCode.objects.get(code=reg_code.code) self.assertEqual(reg_code.is_valid, True)
self.update_enrollement("enroll", "newuser@hotmail.com") self.check_outbox("You have been")
self.assertTrue('Coupon Code List' in response.content)
CourseFinanceAdminRole(self.course.id).remove_users(self.instructor)
course_honor_mode = CourseMode.mode_for_course(self.course.id, 'honor')
CourseFinanceAdminRole(self.course.id).remove_users(self.instructor)
url = reverse('instructor_dashboard', kwargs={'course_id': self.course.id.to_deprecated_string()}) response = self.client.get(url)
response = self.client.post(set_course_price_url, data) self.assertTrue("Please Enter the numeric value for the course price" in response.content)
data['course_price'] = 100 response = self.client.post(set_course_price_url, data) self.assertTrue("CourseMode price updated successfully" in response.content)
response = self.client.get(self.url) self.assertTrue(self.e_commerce_link in response.content) self.assertFalse('Coupons List' in response.content)
super(BadImplementationAbstractEnrollmentReportProvider, self)
instructor = AdminFactory.create() self.client.login(username=instructor.username, password="test")
def test_email_flag_false_mongo_true(self): BulkEmailFlag.objects.create(enabled=False) response = self.client.get(self.url) self.assertFalse(self.email_link in response.content)
cauth = CourseAuthorization(course_id=self.course.id, email_enabled=True) cauth.save()
self.assertTrue(BulkEmailFlag.feature_enabled(self.course.id)) response = self.client.get(self.url) self.assertTrue(self.email_link in response.content)
def test_course_authorized_feature_off(self): BulkEmailFlag.objects.create(enabled=False, require_course_email_auth=True) cauth = CourseAuthorization(course_id=self.course.id, email_enabled=True) cauth.save()
cls.url = reverse('instructor_dashboard', kwargs={'course_id': cls.course_key.to_deprecated_string()}) cls.email_link = '<a href="" data-section="send_email">Email</a>'
instructor = AdminFactory.create() self.client.login(username=instructor.username, password="test")
self.url = reverse('instructor_dashboard', kwargs={'course_id': self.course_key.to_deprecated_string()}) self.email_link = '<a href="" data-section="send_email">Email</a>'
eobjs = mes.create_user(self.course_key) ees = EmailEnrollmentState(self.course_key, eobjs.email) self.assertEqual(mes, ees)
print "checking initialization..." eobjs = before_ideal.create_user(self.course_key) before = EmailEnrollmentState(self.course_key, eobjs.email) self.assertEqual(before, before_ideal)
print "running action..." action(eobjs.email)
print "checking effects..." after = EmailEnrollmentState(self.course_key, eobjs.email) self.assertEqual(after, after_ideal)
StudentModule.objects.create( student=user, course_id=self.course_key, module_state_key=problem_location, state=json.dumps({}) )
reset_student_attempts( self.course_key, user, problem_location, requesting_user=user, delete_module=True, )
score = sub_api.get_score(student_item) self.assertIs(score, None)
result = get_email_params( self.course, True, course_key=self.course_key, display_name=self.ccx.display_name )
result = get_email_params(self.course, False)
with mock.patch.dict('django.conf.settings.FEATURES', {'ENABLE_MKTG_SITE': True}): result = get_email_params(self.course, True)
self.instructor = AdminFactory.create() self.client.login(username=self.instructor.username, password="test")
response = render_to_response(path, context) response.mako_context = context response.mako_template = path return response
self.instructor = AdminFactory.create() self.client.login(username=self.instructor.username, password="test")
self.url = reverse('instructor_dashboard', kwargs={'course_id': self.course.id.to_deprecated_string()})
self.assertFalse('<h2>Enrollment Information</h2>' in response.content)
self.assertFalse(self.get_dashboard_enrollment_message() in response.content)
self.assertContains(response, "<td>Professional</td><td>2</td>")
expected_message = self.get_dashboard_enrollment_message() self.assertTrue(expected_message in response.content)
expected_message = self.get_dashboard_analytics_message() self.assertTrue(expected_message in response.content)
self.instructor = AdminFactory.create() self.client.login(username=self.instructor.username, password="test")
course = ItemFactory.create( parent_location=self.course.location, category="course", display_name="Test course", )
kwargs = {} if cls.grading_policy is not None: kwargs['grading_policy'] = cls.grading_policy cls.course = CourseFactory.create(**kwargs)
self.assertEquals(7, self.response.content.count('grade_Pass'))
self.assertEquals(22, self.response.content.count('grade_F'))
self.assertEquals(293, self.response.content.count('grade_None'))
self.assertEquals(5, self.response.content.count('grade_A'))
self.assertEquals(3, self.response.content.count('grade_B'))
self.assertEquals(3, self.response.content.count('grade_C'))
self.assertEquals(3, self.response.content.count('grade_C'))
self.assertEquals(11, self.response.content.count('grade_F'))
self.assertEquals(3, self.response.content.count('grade_None'))
cache.clear()
CertificateGenerationConfiguration.objects.create(enabled=True)
self.client.login(username=self.instructor.username, password="test") self._assert_certificates_visible(False)
self.client.login(username=self.global_staff.username, password="test") self._assert_certificates_visible(True)
CertificateGenerationConfiguration.objects.create(enabled=False) cache.clear()
self.client.login(username=self.global_staff.username, password="test") self._assert_certificates_visible(False)
self._assert_enable_certs_button_is_disabled()
self._assert_enable_certs_button(True)
certs_api.set_cert_generation_enabled(self.course.id, True)
self._assert_enable_certs_button(False)
certs_api.set_cert_generation_enabled(self.course.id, False) self._assert_enable_certs_button_is_disabled()
certs_api.set_cert_generation_enabled(self.course.id, True) self._assert_enable_certs_button(False)
cache.clear() CertificateGenerationConfiguration.objects.create(enabled=True)
self.client.login(username=self.instructor.username, password='test') response = self.client.post(url) self.assertEqual(response.status_code, 403)
self.client.login(username=self.global_staff.username, password='test') response = self.client.post(url) self.assertEqual(response.status_code, 302)
self._assert_redirects_to_instructor_dash(response)
status = certs_api.example_certificates_status(self.course.id) self.assertIsNot(status, None)
self._assert_redirects_to_instructor_dash(response)
actual_enabled = certs_api.cert_generation_enabled(self.course.id) self.assertEqual(is_enabled, actual_enabled)
GeneratedCertificateFactory.create( user=self.user, course_id=self.course.id, status=CertificateStatuses.downloadable, mode='honor' )
self.assertEqual(response.status_code, 200) res_json = json.loads(response.content)
self.assertTrue(res_json['success'])
dummy_course = CourseFactory.create() GeneratedCertificateFactory.create( user=self.user, course_id=dummy_course.id, status=CertificateStatuses.generating, mode='honor' )
self.assertEqual(response.status_code, 400) res_json = json.loads(response.content)
self.assertEqual( res_json['message'], u'Please select one or more certificate statuses that require certificate regeneration.' )
self.assertEqual(response.status_code, 400) res_json = json.loads(response.content)
self.assertEqual(res_json['message'], u'Please select certificate statuses from the list only.')
cache.clear() CertificateGenerationConfiguration.objects.create(enabled=True) self.client.login(username=self.global_staff.username, password='test')
self.assertEqual(response.status_code, 200) certificate_exception = json.loads(response.content)
self.assertEqual(certificate_exception['user_email'], self.user.email) self.assertEqual(certificate_exception['user_name'], self.user.username)
self.assertEqual(response.status_code, 400) res_json = json.loads(response.content)
self.assertFalse(res_json['success'])
self.assertEqual( res_json['message'], u"{user} does not exist in the LMS. Please check your spelling and retry.".format(user=invalid_user) )
self.assertEqual(response.status_code, 400) res_json = json.loads(response.content)
self.assertFalse(res_json['success'])
self.assertEqual(response.status_code, 400) res_json = json.loads(response.content)
self.assertFalse(res_json['success'])
self.assertEqual( res_json['message'], u"Student (username/email={user_name}) already in certificate exception list.".format(user_name=user) )
self.assertEqual(certificate_exception['user_email'], self.user.email) self.assertEqual(certificate_exception['user_name'], self.user.username)
self.client.post( url_course2, data=json.dumps(self.certificate_exception), content_type='application/json' )
self.assertEqual(certificate_exception['user_email'], self.user.email) self.assertEqual(certificate_exception['user_name'], self.user.username)
self.assertEqual(response.status_code, 400) res_json = json.loads(response.content)
self.assertFalse(res_json['success'])
self.assertEqual( res_json['message'], "{user} is not enrolled in this course. Please check your spelling and retry.".format( user=self.certificate_exception['user_name'] ) )
self.assertEqual(response.status_code, 204)
response = self.client.post( self.url, data='Test Invalid data', content_type='application/json', REQUEST_METHOD='DELETE' ) self.assertEqual(response.status_code, 400)
self.assertEqual(response.status_code, 400)
cache.clear() CertificateGenerationConfiguration.objects.create(enabled=True) self.client.login(username=self.global_staff.username, password='test')
self.assertEqual(response.status_code, 200)
self.assertTrue(res_json['success']) self.assertEqual( res_json['message'], u"Certificate generation started for white listed students." )
self.assertEqual(response.status_code, 200)
self.assertTrue(res_json['success']) self.assertEqual( res_json['message'], u"Certificate generation started for white listed students." )
self.assertEqual(response.status_code, 400)
self.assertFalse(res_json['success']) self.assertEqual( res_json['message'], u'Invalid data, generate_for must be "new" or "all".' )
self.client.login(username=self.global_staff.username, password="test")
self.client.login(username=self.global_staff.username, password="test")
self.assertEqual(response.status_code, 200) result = json.loads(response.content)
try: CertificateInvalidation.objects.get( generated_certificate=self.generated_certificate, invalidated_by=self.global_staff, notes=self.notes, active=True, ) except ObjectDoesNotExist: self.fail("The certificate is not invalidated.")
generated_certificate = GeneratedCertificate.eligible_certificates.get( user=self.enrolled_user_1, course_id=self.course.id, ) self.assertFalse(generated_certificate.is_valid())
self.assertEqual(response.status_code, 400) res_json = json.loads(response.content)
self.assertEqual(response.status_code, 400) res_json = json.loads(response.content)
self.assertEqual( res_json['message'], u"{user} does not exist in the LMS. Please check your spelling and retry.".format(user=invalid_user), )
self.assertEqual(response.status_code, 400) res_json = json.loads(response.content)
self.assertEqual(response.status_code, 400) res_json = json.loads(response.content)
self.generated_certificate.invalidate()
self.assertEqual(response.status_code, 400) res_json = json.loads(response.content)
self.generated_certificate.invalidate()
self.assertEqual(response.status_code, 400) res_json = json.loads(response.content)
self.generated_certificate.invalidate()
self.assertEqual(response.status_code, 204)
with self.assertRaises(ObjectDoesNotExist): CertificateInvalidation.objects.get( generated_certificate=self.generated_certificate, invalidated_by=self.global_staff, active=True, )
self.generated_certificate.invalidate()
self.assertEqual(response.status_code, 400) res_json = json.loads(response.content)
self.assertEqual( res_json['message'], u"Certificate Invalidation does not exist, Please refresh the page and try again.", )
self.assertEqual( StudentModule.objects.filter( student=self.module_to_reset.student, course_id=self.course.id, module_state_key=self.module_to_reset.module_state_key, ).count(), 1 )
self.assertEqual( StudentModule.objects.filter( student=self.module_to_reset.student, course_id=self.course.id, module_state_key=self.module_to_reset.module_state_key, ).count(), 0 )
assert_in(role, ['instructor', 'staff'])
world.clear_courses()
course = world.CourseFactory.create( org='edx', number='999', display_name='Test Course' )
if role == 'instructor': world.instructor = InstructorFactory(course_key=world.course_key) world.enroll_user(world.instructor, world.course_key)
world.staff = StaffFactory(course_key=world.course_key) world.enroll_user(world.staff, world.course_key)
go_to_section("data_download")
world.css_click('input[name="calculate-grades-csv"]')
go_to_section("data_download")
go_to_section("data_download")
go_to_section("data_download")
world.wait_for_visible('#data-student-profiles-table')
world.wait_for(lambda _: world.css_text('#data-student-profiles-table') not in [u'', u'Loading'])
kwargs = {'course_id': self.course_id.to_deprecated_string(), 'note_id': str(self.pk)} return reverse('notes_api_note', kwargs=kwargs)
self.assertFalse(self.has_notes_tab(self.course, self.user))
self.assertFalse(self.has_notes_tab(self.course, self.user))
self.course.advanced_modules = ["notes"] self.assertFalse(self.has_notes_tab(self.course, self.user))
patcher = patch.object(api, 'api_enabled', Mock(return_value=True)) patcher.start() self.addCleanup(patcher.stop)
self.NOTE_ID_DOES_NOT_EXIST = 99999
from __future__ import unicode_literals
'MAX_NOTE_LIMIT': 1000,
ApiResponse = collections.namedtuple('ApiResponse', ['http_response', 'data'])
if not api_enabled(request, course_key): log.debug('Notes are disabled for course: {0}'.format(course_id)) raise Http404
resource_map = API_SETTINGS.get('RESOURCE_MAP', {}) resource_name = kwargs.pop('resource') resource_method = request.method resource = resource_map.get(resource_name)
if api_response.data is not None and api_response.data != '': content = json.dumps(api_response.data)
if offset.isdigit(): offset = int(offset) else: offset = 0
filters = {'course_id': course_key, 'user': request.user} if uri != '': filters['uri'] = uri
CourseEnrollmentFactory(user=other_user, course_id=self.courses[0].id)
with self.assertNumQueries(6): self._get_list()
queryset = User.objects.filter( preferences__key=NOTIFICATION_PREF_KEY ).select_related( "profile" ).prefetch_related( "preferences", "courseenrollment_set", "course_groups", "roles__permissions" )
self.assertEqual(str(user.username.encode('utf-8')), UsernameCipher().decrypt(str(pref.value)))
test_invalid_token("AAAAAAAAAAA=", "initialization_vector")
test_invalid_token(self.tokens[self.user][:-4], "aes")
test_invalid_token("AAAAAAAAAAAAAAAAAAAAAMoazRI7ePLjEWXN1N7keLw=", "padding")
test_invalid_token("AAAAAAAAAAAAAAAAAAAAAC6iLXGhjkFytJoJSBJZzJ4=", "padding")
test_invalid_token("AAAAAAAAAAAAAAAAAAAAANRGw8HDEmlcLVFawgY9wI8=", "padding")
test_invalid_token("AAAAAAAAAAAAAAAAAAAAACpyUxTGIrUjnpuUsNi7mAY=", "username")
self.assertFalse(UserPreference.objects.filter(user=user, key=NOTIFICATION_PREF_KEY)) request = self.request_factory.get("dummy") request.user = AnonymousUser()
UserPreference.objects.get_or_create( user=user, key=NOTIFICATION_PREF_KEY, defaults={ "value": UsernameCipher.encrypt(user.username) } )
if self.instance and self.instance.get("pinned") is None: self.instance["pinned"] = False
for field_name in remove_fields: self.fields.pop(field_name)
if not ( self._is_anonymous(self.context["thread"]) and not self._is_user_privileged(endorser_id) ): return DjangoUser.objects.get(id=endorser_id).username
if 'parent_id' not in data: data["parent_id"] = None
raise ThreadNotFoundError("Thread not found.")
if paginated_results.page != page: raise PageNotFoundError("Page not found (No results on this page).")
if cc_thread['closed']: raise PermissionDenied
ret |= {"voted"} if _is_author_or_privileged(cc_content, context): ret |= {"raw_body"}
self.register_get_threads_response([], page=3, num_pages=3) with self.assertRaises(PageNotFoundError): get_thread_list(self.request, self.course.id, page=4, page_size=10)
thread = self.make_minimal_cs_thread({ "thread_type": thread_type, response_field: [make_minimal_cs_comment()], response_total_field: 5, })
assert_page_correct( page=1, page_size=10, expected_start=0, expected_stop=10, expected_next=None, expected_prev=None )
assert_page_correct( page=1, page_size=4, expected_start=0, expected_stop=4, expected_next=2, expected_prev=None )
assert_page_correct( page=2, page_size=4, expected_start=4, expected_stop=8, expected_next=3, expected_prev=1 )
assert_page_correct( page=3, page_size=4, expected_start=8, expected_stop=10, expected_next=None, expected_prev=2 )
with self.assertRaises(PageNotFoundError): self.get_comment_list(thread, endorsed=True, page=2, page_size=10)
url(r'^programs/(?P<program_id>\d+)/[\w\-]*/?$', views.program_details, name='program_details_view'),
self.mock_programs_api() self.mock_credentials_api(self.student, data=self.CREDENTIALS_API_RESPONSE, reset_url=False)
self.mock_programs_api() self.mock_credentials_api(self.student, data={"results": []}, reset_url=False)
('cart', 'cart'),
('paying', 'paying'),
('purchased', 'purchased'),
('refunded', 'refunded'),
('defunct-cart', 'defunct-cart'),
('defunct-paying', 'defunct-paying'),
ORDER_STATUS_MAP = { 'cart': 'defunct-cart', 'paying': 'defunct-paying', }
OrderItemSubclassPK = namedtuple('OrderItemSubclassPK', ['cls', 'pk'])
processor_reply_dump = models.TextField(blank=True)
return cart.has_items()
for item_type in item_types: if cart.has_items(item_type): return True
CouponRedemption.remove_code_redemption_from_item(item, user)
if is_order_type_business: email.content_subtype = "html"
self.save() orderitems = OrderItem.objects.filter(order=self).select_subclasses() site_name = microsite.get_value('SITE_NAME', settings.SITE_NAME)
csv_file, courses_info = self.generate_registration_codes_csv(orderitems, site_name)
log.exception('Error occurred while sending payment confirmation email')
log.exception( u'Unable to emit {event} event for user {user} and order {order}'.format( event=event_name, user=self.user.id, order=self.id) )
if self.status in ORDER_STATUS_MAP.values(): return
report_comments = models.TextField(default="")
total_amount = models.FloatField()
course_id = CourseKeyField(max_length=255, db_index=True)
('started', 'started'),
('completed', 'completed'),
('cancelled', 'cancelled')
snapshot = models.TextField(blank=True)
invoice = models.ForeignKey(Invoice, null=True) invoice_item = models.ForeignKey(CourseRegistrationCodeInvoiceItem, null=True)
reg_codes = cls.objects.filter(course_enrollment=course_enrollment).order_by('-redeemed_at') if reg_codes: return reg_codes[0]
course_mode = CourseMode.DEFAULT_SHOPPINGCART_MODE
self.course_enrollment = CourseEnrollment.enroll(user=self.user, course_key=self.course_id, mode=self.mode) self.save()
course_mode = CourseMode.DEFAULT_SHOPPINGCART_MODE
from instructor.views.api import save_registration_code
return u"{} : {}".format(self.course_id.to_deprecated_string(), self.annotation)
return u"{} : {}".format(self.course_id.to_deprecated_string(), self.annotation)
if (not course_enrollment.refundable()) or skip_refund: return
return u"{verification_reminder} {refund_reminder}".format( verification_reminder=verification_reminder, refund_reminder=refund_reminder )
DONATION_TYPES = ( ("general", "A general donation"), ("course", "A donation to a particular course") )
donation_type = models.CharField(max_length=32, default="general", choices=DONATION_TYPES)
course_id = CourseKeyField(max_length=255, db_index=True)
super(Donation, cls).add_to_order(order, currency=currency)
description = cls._line_item_description(course_id=course_id)
else: return _(u"Donation for {platform_name}").format(platform_name=settings.PLATFORM_NAME)
request.user.is_authenticated() and is_shopping_cart_enabled() and Order.does_user_have_cart(request.user) and Order.user_cart_has_items( request.user, [PaidCourseRegistration, CourseRegCodeItem] )
self.pdf.drawString(horizontal_padding_from_border, y_pos,
self.pdf.drawString( horizontal_padding_from_border, y_pos, _(u'Date: {date}').format(date=self.date) )
('ALIGN', (5, 0), (5, 0), 'RIGHT'),
('RIGHTPADDING', (5, 0), (5, -1), 7 * mm),
('ALIGN', (2, 0), (4, 0), 'CENTER'),
('ALIGN', (1, 0), (1, -1), 'LEFT'),
('ALIGN', (2, 1), (2, -1), 'CENTER'),
('INNERGRID', (1, 1), (-2, -1), 0.50, '#cccccc'),
split_table = split_tables[0] __, rendered_height = split_table.wrap(0, 0) split_table.drawOn(self.pdf, table_left_padding, y_pos - rendered_height)
course_items_table.drawOn(self.pdf, table_left_padding, y_pos - rendered_height)
totals_data.append( ['', '{tax_label}: {tax_id}'.format(tax_label=self.tax_label, tax_id=self.tax_id)] )
self.prepare_new_page() totals_table.drawOn(self.pdf, self.margin + left_padding, self.second_page_start_y_pos - rendered_height) return self.second_page_start_y_pos - rendered_height - self.min_clearance
('LEFTPADDING', (0, 1), (0, 1), 5 * mm),
('BACKGROUND', (1, 2), (1, 2), '#EEEEEE'),
('BACKGROUND', (1, 4), (1, 4), '#EEEEEE'),
footer_style.append(('BACKGROUND', (1, 6), (1, 6), '#EEEEEE'))
AUDIT_LOG.info("Redemption of a invalid RegistrationCode %s", registration_code) limiter.tick_bad_request_counter(request) raise Http404()
embargo_redirect = embargo_api.redirect_if_blocked( course.id, user=request.user, ip_address=get_ip(request), url=request.path ) if embargo_redirect is not None: return redirect(embargo_redirect)
embargo_redirect = embargo_api.redirect_if_blocked( course.id, user=request.user, ip_address=get_ip(request), url=request.path ) if embargo_redirect is not None: return redirect(embargo_redirect)
cart = Order.get_cart_for_user(request.user) try: cart_items = cart.find_item_by_course_id(course_registration.course_id)
if amount < decimal.Decimal('0.01'): return HttpResponseBadRequest("Amount must be greater than 0")
cart = Order.get_cart_for_user(request.user) cart.clear()
Donation.add_to_order(cart, amount, course_id=course_id)
cart.start_purchase()
callback_url = request.build_absolute_uri( reverse("shoppingcart.views.postpay_callback") )
extra_data = [ unicode(course_id) if course_id else "", "donation_course" if course_id else "donation_general" ]
"payment_url": get_purchase_endpoint(),
"payment_params": get_signed_purchase_params( cart, callback_url=callback_url, extra_data=extra_data ),
cert_items = CertificateItem.objects.filter(order=order)
url += '?payment-order-num={order_num}'.format(order_num=order.id) return HttpResponseRedirect(url)
return HttpResponseRedirect(reverse('shoppingcart.views.show_receipt', args=[result['order'].id]))
if order_items.count() == 1: receipt_template = order_items[0].single_item_receipt_template context.update(order_items[0].single_item_receipt_context)
return _render_report_form(start_date, end_date, start_letter, end_letter, report_type, date_fmt_error=True)
self.cart, __ = self._create_cart()
record_purchase(params, result['order']) return {'success': True, 'order': result['order'], 'error_html': ''}
charged_amt = Decimal(params['ccAuthReply_amount'])
payment_support_email = microsite.get_value('payment_support_email', settings.PAYMENT_SUPPORT_EMAIL)
return '<p class="error_msg">EXCEPTION!</p>'
PROCESSOR_MODULE = __import__( 'shoppingcart.processors.' + settings.CC_PROCESSOR_NAME, fromlist=[ 'render_purchase_form_html', 'process_postpay_callback', 'get_purchase_endpoint', 'get_signed_purchase_params', ] )
DEFAULT_REASON = ugettext_noop("UNKNOWN REASON")
if hasattr(error, 'order'): _record_payment_info(params, error.order) else: log.info(json.dumps(params)) return { 'success': False,
if params.get('decision') == u'CANCEL': raise CCProcessorUserCancelled()
if params.get('decision') == u'DECLINE': raise CCProcessorUserDeclined()
for num, item in enumerate(extra_data, start=1): key = u"merchant_defined_data{num}".format(num=num) params[key] = item
config = settings.CC_PROCESSOR.get( settings.CC_PROCESSOR_NAME, {} )
config_key = microsite.get_value('cybersource_config_key') if config_key: config = config['microsites'][config_key]
self.assertEqual(1, 1)
self.assertEqual(1, 1)
self.assertIn("EXCEPTION!", get_processor_exception_html(CCProcessorException()))
for key in baseline: params = baseline.copy() del params[key] with self.assertRaises(CCProcessorDataException): payment_accepted(params)
for key in wrong: params = baseline.copy() params[key] = wrong[key] with self.assertRaises(CCProcessorDataException): payment_accepted(params)
params_bad_ordernum = params.copy() params_bad_ordernum['orderNumber'] = str(order1.id + 10) with self.assertRaises(CCProcessorDataException): payment_accepted(params_bad_ordernum)
params_wrong_type_amt = params.copy() params_wrong_type_amt['ccAuthReply_amount'] = 'ab' with self.assertRaises(CCProcessorDataException): payment_accepted(params_wrong_type_amt)
params_wrong_amt = params.copy() params_wrong_amt['ccAuthReply_amount'] = '1.00' with self.assertRaises(CCProcessorWrongAmountException): payment_accepted(params_wrong_amt)
params_not_accepted = params.copy() params_not_accepted['decision'] = "REJECT" self.assertFalse(payment_accepted(params_not_accepted)['accepted'])
self.assertTrue(payment_accepted(params)['accepted'])
self.assertEqual(params['override_custom_receipt_page'], self.CALLBACK_URL)
self.assertEqual(params['access_key'], '0123456789012345678901') self.assertEqual(params['profile_id'], 'edx')
self.assertGreater(len(params['signed_date_time']), 0) self.assertGreater(len(params['transaction_uuid']), 0)
self.assertEqual(params['signature'], self._signature(params))
@patch.object(OrderItem, 'purchased_callback')
@patch.object(OrderItem, 'purchased_callback') @patch.object(OrderItem, 'pdf_receipt_display_name')
params = self._signed_callback_params(self.order.id, self.COST, self.COST) result = process_postpay_callback(params)
purchased_callback.assert_called_with()
self.assertEqual(result['order'].status, 'purchased') self.assert_dump_recorded(result['order'])
params = self._signed_callback_params(self.order.id, self.COST, self.COST, decision='REJECT') result = process_postpay_callback(params)
self.assertFalse(result['success']) self.assertIn(u"did not accept your payment", result['error_html']) self.assert_dump_recorded(result['order'])
params = self._signed_callback_params(self.order.id, self.COST, self.COST, signature="invalid!") result = process_postpay_callback(params)
self.assertFalse(result['success']) self.assertIn(u"corrupted message regarding your charge", result['error_html'])
params = self._signed_callback_params("98272", self.COST, self.COST) result = process_postpay_callback(params)
self.assertFalse(result['success']) self.assertIn(u"inconsistent data", result['error_html'])
params = self._signed_callback_params(self.order.id, "145.00", "145.00") result = process_postpay_callback(params)
params = self._signed_callback_params(self.order.id, self.COST, "abcd") result = process_postpay_callback(params)
self.assertFalse(result['success']) self.assertIn(u"badly-typed value", result['error_html'])
params = self._signed_callback_params(self.order.id, self.COST, "abcd") params['decision'] = u'CANCEL' result = process_postpay_callback(params)
self.assertFalse(result['success']) self.assertIn(u"you have cancelled this transaction", result['error_html'])
params = self._signed_callback_params( self.order.id, self.COST, self.COST, card_number='nodigits' ) result = process_postpay_callback(params)
self.assertEqual(result['order'].bill_to_ccnum, '####')
params = self._signed_callback_params(self.order.id, self.COST, self.COST) del params[missing_param]
params['signed_field_names'] = 'reason_code,message' params['signature'] = self._signature(params)
self.assertFalse(result['success']) self.assertIn(u"did not return a required parameter", result['error_html'])
result = process_postpay_callback(params) self.assertTrue(result['success']) self.assert_dump_recorded(result['order'])
if decision in self.FAILED_DECISIONS: signed_field_names.remove("auth_amount")
"decision": decision, "req_reference_number": str(order_id), "req_amount": order_amount, "auth_amount": paid_amount, "req_card_number": card_number,
params['signature'] = signature if signature is not None else self._signature(params) return params
params = self._signed_callback_params(self.order.id, self.COST, self.COST, decision='DECLINE') result = process_postpay_callback(params)
self.assertFalse(result['success']) self.assertIn(u"payment was declined", result['error_html'])
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
course5 = CourseFactory.create(org='otherorg', number='999') course5_key = course5.id
Donation.add_to_order(cart, 10.0, None) cart.purchase(first='FirstNameTesting123', street1='StreetTesting123') self.orderid_courseless_donation = cart.id
self.cart1 = Order.get_cart_for_user(self.first_verified_user) CertificateItem.add_to_order(self.cart1, self.course_key, self.cost, 'verified') self.cart1.purchase()
CourseEnrollment.enroll(self.honor_user, self.course_key, "honor")
num_certs = 0 for cert in refunded_certs: num_certs += 1 self.assertEqual(num_certs, 2)
self.assertEqual(csv.replace('\r\n', '\n').strip(), self.CORRECT_REFUND_REPORT_CSV.strip())
num_purchases = 0 for item in purchases: num_purchases += 1 self.assertEqual(num_purchases, 2)
self.assertEqual(csv.replace('\r\n', '\n').strip(), self.CORRECT_CSV.strip())
self.annotation.delete() self.assertEqual(u"", self.reg.csv_report_comments)
self.testing_cost = 20 self.testing_course_mode = CourseMode( course_id=self.testing_course.id, mode_slug=CourseMode.HONOR, mode_display_name="testing honor cert", min_price=self.testing_cost ) self.testing_course_mode.save()
CourseMode( course_id=self.xss_course_key, mode_slug=CourseMode.HONOR, mode_display_name="honor cert", min_price=self.cost ).save()
self.verified_course_mode = CourseMode( course_id=self.verified_course_key, mode_slug=CourseMode.HONOR, mode_display_name="honor cert", min_price=self.cost ) self.verified_course_mode.save()
resp = self.client.get(billing_url) self.assertEqual(resp.status_code, 404)
self.assertEqual(context['currency'], 'usd') self.assertEqual(context['currency_symbol'], '$')
self.assertEqual(context['currency'], 'PKR') self.assertEqual(context['currency_symbol'], 'Rs')
item = self.cart.orderitem_set.all().select_subclasses()[0] self.assertEquals(item.unit_cost, self.get_discount(self.cost))
self.assertEqual(self.cart.total_cost, self.get_discount(self.cost))
item = self.cart.orderitem_set.all().select_subclasses()[0] self.assertEquals(item.unit_cost, self.get_discount(self.cost))
item = self.cart.orderitem_set.all().select_subclasses()[0] self.assertEquals(item.unit_cost, self.get_discount(self.cost))
self.assertTrue('Activate Course Enrollment' in response.content)
course_key = self.course_key.to_deprecated_string() self._add_course_mode(mode_slug='verified') self.add_reg_code(course_key, mode_slug='verified')
self.assertTrue('Activate Course Enrollment' in response.content)
current_enrollment, __ = CourseEnrollment.enrollment_mode_for_user(self.user, self.course_key) self.assertEquals('verified', current_enrollment)
resp = self.client.post(reverse('shoppingcart.views.remove_item', args=[]), {'id': reg_item.id})
resp = self.client.post(reverse('shoppingcart.views.remove_item', args=[]), {'id': cert_item.id})
self.assertEqual(context['currency'], 'usd') self.assertEqual(context['currency_symbol'], '$')
self.assertEqual(context['currency'], 'PKR') self.assertEqual(context['currency_symbol'], 'Rs')
self.assertEqual(resp.status_code, 200)
self.add_course_to_user_cart(self.xss_course_key) self.assertEquals(self.cart.orderitem_set.count(), 1)
self.assertEqual(resp.status_code, 200)
json_resp = json.loads(resp.content) self.assertEqual(json_resp.get('total_cost'), self.cart.total_cost)
total_amount = PaidCourseRegistration.get_total_amount_of_purchased_item(self.course_key) self.assertEqual(total_amount, 36)
self.add_course_to_user_cart(self.course_key) self.assertEquals(self.cart.orderitem_set.count(), 1)
self.assertTrue('Activate Course Enrollment' in response.content)
self.assertTrue('Activate Course Enrollment' in resp.content)
item2 = PaidCourseRegistration.objects.get(id=item2.id) self.assertIsNotNone(item2.course_enrollment) self.assertEqual(item2.course_enrollment.course_id, self.testing_course.id)
self.assertEqual(context['currency_symbol'], '$') self.assertEqual(context['currency'], 'usd')
self.assertEqual(context['currency_symbol'], 'Rs') self.assertEqual(context['currency'], 'PKR')
self.assertEquals(len(mail.outbox), 3)
course_registration_codes = CourseRegistrationCode.objects.filter(order=self.cart)
redeem_url = reverse('register_code_redemption', args=[context['reg_code_info_list'][0]['code']])
resp = self.client.get(reverse('shoppingcart.views.show_receipt', args=[self.cart.id])) self.assertEqual(resp.status_code, 200)
self.assertTrue(context['reg_code_info_list'][0]['is_redeemed']) self.assertFalse(context['reg_code_info_list'][1]['is_redeemed'])
CourseMode.objects.create( course_id=self.verified_course_key, mode_slug="verified", mode_display_name="verified cert", min_price=self.cost )
self.cart = Order.get_cart_for_user(self.user) CertificateItem.add_to_order(self.cart, self.verified_course_key, self.cost, 'verified') self.cart.start_purchase()
session = self.client.session session['attempting_upgrade'] = True session.save()
Order.get_cart_for_user(self.user).start_purchase() Order.get_cart_for_user(self.user).start_purchase() Order.get_cart_for_user(self.user).start_purchase()
self.cart = Order.get_cart_for_user(self.user) CertificateItem.add_to_order( self.cart, self.course_key, self.COST, 'verified' ) self.cart.start_purchase()
self.testing_course.enrollment_start = self.tomorrow self.testing_course.enrollment_end = self.nextday self.testing_course = self.update_course(self.testing_course, self.user.id)
self.testing_course.enrollment_start = self.tomorrow self.testing_course.enrollment_end = self.nextday self.testing_course = self.update_course(self.testing_course, self.user.id)
self.testing_course.enrollment_start = self.tomorrow self.testing_course.enrollment_end = self.nextday self.testing_course = self.update_course(self.testing_course, self.user.id)
response = self.client.post(url) self.assertEquals(response.status_code, 403)
reset_time = datetime.now(UTC) + timedelta(seconds=300) with freeze_time(reset_time): response = self.client.post(url) self.assertEquals(response.status_code, 404)
response = self.client.get(url) self.assertEquals(response.status_code, 403)
reset_time = datetime.now(UTC) + timedelta(seconds=300) with freeze_time(reset_time): response = self.client.get(url) self.assertEquals(response.status_code, 404)
CourseSalesAdminRole(self.course.id).add_users(instructor)
registration_code = CourseRegistrationCode.objects.all()[0].code redeem_url = reverse('register_code_redemption', args=[registration_code]) self.login_user()
self.assertIn('Activate Course Enrollment', response.content)
reg_code = CourseRegistrationCode.objects.create( code="abcd1234", course_id=self.course.id, created_by=self.user )
is_redeemed = RegistrationCodeRedemption.objects.filter( registration_code=reg_code ).exists() self.assertFalse(is_redeemed)
is_enrolled = CourseEnrollment.is_enrolled(self.user, self.course.id) self.assertFalse(is_enrolled)
config = DonationConfiguration.current() config.enabled = True config.save()
self._donate(self.DONATION_AMOUNT, course_id=self.course.id)
self._assert_receipt_contains("tax purposes") self._assert_receipt_contains(self.course.display_name)
response = self.client.post(reverse('donation')) self.assertEqual(response.status_code, 404)
self.client.logout() response = self.client.post(reverse('donation')) self.assertEqual(response.status_code, 404)
params = {'amount': donation_amount} if course_id is not None: params['course_id'] = course_id
payment_info = json.loads(response.content) self.assertEqual(payment_info["payment_url"], "/shoppingcart/payment_fake")
url = reverse('shoppingcart.views.postpay_callback') response = self.client.post(url, processor_response_params) self.assertRedirects(response, self._receipt_url)
self.assertFalse(any(settings.PDF_RECEIPT_TERMS_AND_CONDITIONS in s for s in pdf_content))
PaymentFakeView.PAYMENT_STATUS_RESPONSE = "success"
post_params = sign(self.CLIENT_POST_PARAMS)
resp = self.client.post( '/shoppingcart/payment_fake', dict(post_params) )
self.assertEqual(resp.status_code, 200)
self.assertIn("Payment Form", resp.content)
post_params = sign(self.CLIENT_POST_PARAMS)
post_params['signature'] = "invalid"
resp = self.client.post( '/shoppingcart/payment_fake', dict(post_params) )
self.assertIn("Error", resp.content)
post_params = sign(self.CLIENT_POST_PARAMS)
resp_params = PaymentFakeView.response_post_params(post_params)
try: verify_signatures(resp_params)
post_params = sign(self.CLIENT_POST_PARAMS)
resp = self.client.put( '/shoppingcart/payment_fake', data="decline", content_type='text/plain' ) self.assertEqual(resp.status_code, 200)
resp_params = PaymentFakeView.response_post_params(post_params) self.assertEqual(resp_params.get('decision'), 'DECLINE')
resp = self.client.put( '/shoppingcart/payment_fake', data="failure", content_type='text/plain' ) self.assertEqual(resp.status_code, 200)
resp_params = PaymentFakeView.response_post_params(post_params) self.assertEqual(resp_params.get('decision'), 'REJECT')
resp = self.client.put( '/shoppingcart/payment_fake', data="success", content_type='text/plain' ) self.assertEqual(resp.status_code, 200)
resp_params = PaymentFakeView.response_post_params(post_params) self.assertEqual(resp_params.get('decision'), 'ACCEPT')
patcher = patch('shoppingcart.models.analytics') self.mock_tracker = patcher.start() self.addCleanup(patcher.stop)
next_cart = Order.get_cart_for_user(user=self.user) self.assertNotEqual(cart, next_cart) self.assertEqual(next_cart.status, 'cart')
cart.purchase() cart.purchase() self.assertEquals(len(mail.outbox), 1)
with patch.object(mail.message.EmailMessage, 'send') as mock_send: mock_send.side_effect = Exception("Kaboom!") cart.purchase()
self.assertEqual(cart.status, 'purchased')
mode, is_active = CourseEnrollment.enrollment_mode_for_user(self.user, self.course_key) self.assertTrue(is_active) self.assertEqual(mode, 'verified')
registration_codes = CourseRegistrationCode.order_generated_registration_codes(self.course_key) self.assertEqual(registration_codes.count(), item.qty)
many_days = datetime.timedelta(days=60)
many_days = datetime.timedelta(days=60)
many_days = datetime.timedelta(days=60)
CourseEnrollment.enroll(self.user, self.course_key, 'verified') ret_val = CourseEnrollment.unenroll(self.user, self.course_key) self.assertFalse(ret_val)
donation = Donation.add_to_order(self.cart, self.COST) self._assert_donation( donation, donation_type="general", unit_cost=self.COST, line_desc="Donation for edX" )
course = CourseFactory.create(display_name="Test Course")
Donation.add_to_order(self.cart, self.COST) self.cart.start_purchase() self.cart.purchase()
self.assertTrue(self.cart.has_items(item_type=Donation)) self.assertEqual(self.cart.total_cost, unit_cost)
self.cart.start_purchase() self.cart.purchase()
donation = Donation.objects.get(pk=donation.id) self.assertEqual(donation.status, "purchased")
first_transaction.delete() second_transaction.delete() self._assert_history_transactions([])
from shoppingcart.processors.CyberSource2 import processor_hash
PAYMENT_STATUS_RESPONSE = "success"
PaymentFakeView.PAYMENT_STATUS_RESPONSE = new_status return HttpResponse()
signed_fields = post_params.get('signed_field_names').split(',')
hash_val = ",".join([ "{0}={1}".format(key, post_params[key]) for key in signed_fields ]) public_sig = processor_hash(hash_val)
"decision": decision,
resp_params['signed_field_names'] = ",".join(signed_fields)
hash_val = ",".join([ "{0}={1}".format(key, resp_params[key]) for key in signed_fields ]) resp_params['signature'] = processor_hash(hash_val)
"callback_url": callback_url,
'post_params_success': post_params_success,
'post_params_decline': post_params_decline
parser = PDFParser(pdf_buffer) document = PDFDocument(parser, password)
layout = device.get_result()
text_content.append(lt_object.get_text().encode('utf-8'))
if matched == ';': return ';;' elif matched == '/': return ';_' else: return matched
func = getattr(block.__class__, handler_name, None) if not func: raise ValueError("{!r} is not a function name".format(handler_name))
#if not getattr(func, "_is_xblock_handler", False):
if not suffix: url = url.rstrip('/')
if query: url += '?' + query
if thirdparty: scheme = "https" if settings.HTTPS == "on" else "http" url = '{scheme}://{host}{path}'.format( scheme=scheme, host=settings.SITE_NAME, path=url )
tag = self.runtime.service(self.mock_block, 'user_tags').get_tag(self.scope, self.key) self.assertIsNone(tag)
with self.assertRaises(ValueError): self.runtime.service(self.mock_block, 'user_tags').set_tag('fake_scope', self.key, set_value)
with self.assertRaises(ValueError): self.runtime.service(self.mock_block, 'user_tags').get_tag('fake_scope', self.key)
if isinstance(authored_data, LmsFieldData):
from __future__ import unicode_literals
_ = lambda text: text
help=_("What format this module is in (used for deciding which " "grader to apply, and what to show in the TOC)"), scope=Scope.settings,
continue
merged_access[partition_id] = group_ids
user_partitions = UserPartitionList( help=_("The list of group configurations for partitioning students in content experiments."), default=[], scope=Scope.settings )
if user_partition.active: for group_id in group_ids: try: user_partition.get_group(group_id) except NoSuchUserPartitionGroupError: has_invalid_groups = True
courses = CourseOverview.get_all_courses( org=org, filter_=filter_, ) if org == microsite_org else []
target_org = org or microsite_org courses = CourseOverview.get_all_courses(org=target_org, filter_=filter_)
if microsite_org: return courses
filtered_visible_ids = None
microsite_orgs = microsite.get_all_orgs() return [course for course in courses if course.location.org not in microsite_orgs]
if domain and 'edge.edx.org' in domain: return redirect(reverse("signin_user"))
return student.views.index(request, user=request.user)
return courseware.views.views.courses(request)
accepts = request.META.get('HTTP_ACCEPT', '*/*')
show_openedx_logo = bool(request.GET.get('show-openedx-logo', False))
include_dependencies = bool(request.GET.get('include-dependencies', False))
language = request.GET.get('language', translation.get_language())
from __future__ import unicode_literals
u"\u00A9 {org_name}. All rights reserved except where noted. " u"EdX, Open edX and the edX and Open EdX logos are registered trademarks " u"or trademarks of edX Inc."
title = _("Powered by Open edX") return { "url": settings.FOOTER_OPENEDX_URL, "title": title, "image": settings.FOOTER_OPENEDX_LOGO_IMAGE, }
if urlparse.urlparse(url_path).netloc: return url_path
return _absolute_url(is_secure, url_path)
microsite_url = get_microsite_url(name) if microsite_url != EMPTY_URL: return microsite_url
url = marketing_link(name)
image_url = microsite.get_value('logo_image_url') if image_url: return '{static_url}{image_url}'.format( static_url=settings.STATIC_URL, image_url=image_url )
university = microsite.get_value('university')
resp = self.client.get('/') self.assertEquals(resp['X-Frame-Options'], 'ALLOW')
resp = self.client.get('/') self.assertEquals(resp['X-Frame-Options'], 'DENY')
request.META["HTTP_HOST"] = "edge.edx.org" response = index(request)
self.assertIsInstance(response, HttpResponseRedirect)
self.assertIn('pre requisite course', resp.content) self.assertIn('course that has pre requisite', resp.content)
self.assertNotIn('Search for a course', response.content)
response = self.client.get(reverse('branding.views.courses')) self.assertEqual(response.status_code, 200)
self.assertNotIn('Search for a course', response.content) self.assertNotIn('<aside aria-label="Refine Your Search" class="search-facets phone-menu">', response.content)
self.assertIn('<div class="courses no-course-discovery"', response.content)
self.assertIn('Search for a course', response.content)
response = self.client.get(reverse('branding.views.courses')) self.assertEqual(response.status_code, 200)
response = self.client.get(reverse('branding.views.courses')) self.assertEqual(response.status_code, 200)
response = self.client.get(reverse('branding.views.courses')) self.assertEqual(response.status_code, 200)
self.assertIn("logo_image", json_data)
self.assertIn("copyright", json_data)
resp = self._get_footer(params={'language': language}) self.assertEqual(resp.status_code, 200) json_data = json.loads(resp.content)
self.assertIn(expected_copyright, json_data['copyright'])
(False, "en", "lms-footer.css"), (False, "ar", "lms-footer-rtl.css"),
(True, "en", "lms-footer-edx.css"), (True, "ar", "lms-footer-edx-rtl.css"),
(False, True), (False, False),
(True, True), (True, False),
(False, False), (False, True),
(True, False), (True, True),
with patch_edxnotes_api_settings("http://example.com/"): self.assertEqual("http://example.com/", get_endpoint_function())
with patch_edxnotes_api_settings("http://example.com"): self.assertEqual("http://example.com/", get_endpoint_function())
with patch_edxnotes_api_settings("http://example.com"): self.assertEqual("http://example.com/some_path/", get_endpoint_function("/some_path"))
with patch_edxnotes_api_settings("http://example.com"): self.assertEqual("http://example.com/some_path/", get_endpoint_function("some_path/"))
with patch_edxnotes_api_settings(None): self.assertRaises(ImproperlyConfigured, get_endpoint_function)
if expected is None: self.assertEqual(expected, constructed) else: self.assertTrue(constructed.startswith(notes_url))
self.assertNotIn('user', constructed)
allowed_params = ('page', 'page_size', 'text')
parsed = urlparse.urlparse(constructed) params = urlparse.parse_qs(parsed.query)
for param, value in params.items(): self.assertIn(param, allowed_params) self.assertIn('{}={}'.format(param, value[0]), expected)
self.course.edxnotes = False self.assertFalse(has_notes_tab(self.user, self.course))
self.course.edxnotes = True self.assertTrue(has_notes_tab(self.user, self.course))
CLIENT_NAME = "edx-notes" DEFAULT_PAGE = 1 DEFAULT_PAGE_SIZE = 25
def default(self, obj): if isinstance(obj, datetime): return get_default_time_display(obj) return json.JSONEncoder.default(self, obj)
usage_key = usage_key.replace(course_key=store.fill_in_run(usage_key.course_key))
course = item.get_parent() item_dict['index'] = get_index(item_dict['location'], course.children)
section = get_current_child(chapter, min_depth=1) if section is None: log.debug("No section found when loading current position in course") return None
CONFIG_FILE = open(settings.REPO_ROOT / "docs" / "lms_config.ini") CONFIG = ConfigParser.ConfigParser() CONFIG.readfp(CONFIG_FILE)
course_id = serializers.CharField(source='id', read_only=True)
return CourseDetails.fetch_about_attribute(course_overview.id, 'overview')
if requesting_user.username == target_username: return True elif not target_username: raise TypeError("target_username must be specified") else: staff = GlobalStaff() return staff.has_user(requesting_user)
url( r'^v1/blocks/{}'.format(settings.USAGE_KEY_PATTERN), BlocksView.as_view(), name="blocks_in_block_tree" ),
url( r'^v1/blocks/', BlocksInCourseView.as_view(), name="blocks_in_course" ),
course_key_string = request.query_params.get('course_id', None) if not course_key_string: raise ValidationError('course_id is required.')
return (requested_fields or set()) | {'type', 'display_name'}
additional_requested_fields = [ 'student_view_data', 'block_counts', 'nav_depth', 'block_types_filter', ] for additional_field in additional_requested_fields: field_value = cleaned_data.get(additional_field)
if not cleaned_data.get('all_blocks', None): raise ValidationError({'username': ['This field is required unless all_blocks is requested.']})
return None
try: return User.objects.get(username=requested_username) except User.DoesNotExist: raise Http404( "Requested user '{requested_username}' does not exist.".format(requested_username=requested_username) )
transformers = BlockStructureTransformers() if user is not None: transformers += COURSE_BLOCK_ACCESS_TRANSFORMERS + [ProctoredExamTransformer()] transformers += [ BlocksAPITransformer( block_counts, student_view_data, depth, nav_depth ) ]
blocks = get_course_blocks(user, usage_key, transformers)
serializer_context = { 'request': request, 'block_structure': blocks, 'requested_fields': requested_fields or [], }
return serializer.data
block_structure.request_xblock_fields('category')
block_structure.request_xblock_fields('hide_from_toc')
for parent_desc_list in parents_descendants_list: if parent_desc_list is not None: parent_desc_list.items.append(unicode(block_key))
user_exam_summary = get_attempt_status_summary( usage_info.user.id, unicode(block_key.course_key), unicode(block_key), ) return user_exam_summary and user_exam_summary['status'] != ProctoredExamStudentAttemptStatus.declined
SupportedFieldType(StudentViewTransformer.STUDENT_VIEW_DATA, StudentViewTransformer), SupportedFieldType(StudentViewTransformer.STUDENT_VIEW_MULTI_DEVICE, StudentViewTransformer),
SupportedFieldType(None, BlockCountsTransformer, BlockCountsTransformer.BLOCK_COUNTS),
SupportedFieldType( 'merged_visible_to_staff_only', VisibilityTransformer, requested_field_name='visible_to_staff_only', )
block_structure.request_xblock_fields('category')
block_structure.request_xblock_fields('graded', 'format', 'display_name', 'category')
StudentViewTransformer.collect(block_structure) BlockCountsTransformer.collect(block_structure) BlockDepthTransformer.collect(block_structure) BlockNavigationTransformer.collect(block_structure)
BlockDepthTransformer.collect(block_structure) BlockNavigationTransformer.collect(block_structure) block_structure._collect_requested_xblock_fields()
BlockDepthTransformer().transform(usage_info=None, block_structure=block_structure) BlockNavigationTransformer(0).transform(usage_info=None, block_structure=block_structure) block_structure._prune_unreachable()
StudentViewTransformer.collect(self.block_structure) self.block_structure._collect_requested_xblock_fields()
StudentViewTransformer('video').transform(usage_info=None, block_structure=self.block_structure)
from openedx.core.lib.block_structure.factory import BlockStructureFactory from xmodule.modulestore.tests.django_utils import ModuleStoreTestCase from xmodule.modulestore.tests.factories import SampleCourseFactory
BlockCountsTransformer.collect(self.block_structure) self.block_structure._collect_requested_xblock_fields()
BlockCountsTransformer(['problem', 'chapter']).transform(usage_info=None, block_structure=self.block_structure)
self.assertEquals(block_counts_for_course['chapter'], 2)
self.assertEquals(block_counts_for_course['problem'], 6) self.assertEquals(block_counts_for_chapter_x['problem'], 3)
for block_type in ['course', 'html', 'video']: self.assertNotIn(block_type, block_counts_for_course) self.assertNotIn(block_type, block_counts_for_chapter_x)
self.course_hierarchy = self.get_course_hierarchy() self.blocks = self.build_course(self.course_hierarchy) self.course = self.blocks['course']
CourseEnrollmentFactory.create(user=self.user, course_id=self.course.id, is_active=True)
self.form_data.setlist('requested_fields', ['field1', 'field2'])
block_types_list = {'block_type1', 'block_type2'} for field_name in ['block_counts', 'student_view_data']: self.form_data.setlist(field_name, block_types_list) self.cleaned_data[field_name] = block_types_list
self.cleaned_data['requested_fields'] |= {'field1', 'field2', 'student_view_data', 'block_counts'} self.assert_equals_cleaned_data()
self.user = UserFactory.create() self.client.login(username=self.user.username, password='test') CourseEnrollmentFactory.create(user=self.user, course_id=self.course_key)
if serialized_block['type'] == 'video': self.assertIn('student_view_data', serialized_block)
if serialized_block['type'] == 'html': self.assertIn('student_view_multi_device', serialized_block) self.assertTrue(serialized_block['student_view_multi_device'])
staff_user = UserFactory.create() CourseStaffRole(self.course.location.course_key).add_users(staff_user)
self.assertEquals(serializer.data['root'], unicode(self.block_structure.root_block_usage_key))
for block_key_string, serialized_block in serializer.data['blocks'].iteritems(): self.assertEquals(serialized_block['id'], block_key_string) self.assert_basic_block(block_key_string, serialized_block)
alternate_course = self.create_course( org=md5(self.course.org).hexdigest() )
alternate_course = self.create_course(course='mobile', mobile_available=True)
alternate_course = self.create_course( org=md5(self.course.org).hexdigest() )
unfiltered_response = self.verify_response(params={'username': self.staff_user.username}) for org in [self.course.org, alternate_course.org]: self.assertTrue(
filtered_response = self.verify_response(params={'org': self.course.org, 'username': self.staff_user.username}) self.assertTrue(
alternate_course = self.create_course(course='mobile', mobile_available=True)
'course_id': u'edX/toy/2012_Fall',
expected_mongo_calls = 1 serializer_class = CourseDetailSerializer
about_descriptor = XBlock.load_class('about') overview_template = about_descriptor.get_template('overview.yaml') self.expected_data['overview'] = overview_template.get('data')
from safe_lxml import defuse_xml_libs defuse_xml_libs()
import contracts contracts.disable_all()
modulestore()
from django.core.wsgi import get_wsgi_application
dashboard = DashboardPage(self.browser) dashboard.wait_for_page() return dashboard
if self._course_id is not None: url += "?{params}".format( params=urlencode({ "course_id": self._course_id, "enrollment_action": "enroll" }) )
EmptyPromise( lambda: self.current_form != old_form, "Finish toggling to the other form" ).fulfill()
self.q(css=".register-button").click()
self.q(css=".login-button").click()
self.q(css="a.forgot-password").click()
EmptyPromise( lambda: self.current_form != login_form, "Finish toggling to the password reset form" ).fulfill()
self.wait_for_element_visibility('#password-reset-email', 'Email field is shown') self.q(css="#password-reset-email").fill(email)
self.q(css="button.js-reset").click()
active_script = "return " + title_selector + " === document.activeElement;" return self.browser.execute_script(active_script)
self.current_view = self.MAPPING["search"](self.browser) if text.strip(): self.current_view.wait_for_page()
chapter_index = self._chapter_index(chapter) if chapter_index is None: return None
return self._section_scores(chapter_index, section_index)
return chapter_titles.index(title.lower()) + 1
section_titles = [t.split('\n')[0] for t in section_titles]
section_titles = [t for t in section_titles if t]
return section_titles.index(title.lower()) + 1
score_css = "div.chapters>section:nth-of-type({0}) div.sections>div:nth-of-type({1}) div.scores>ol>li".format( chapter_index, section_index )
return [tuple(map(int, score.split('/'))) for score in text_scores]
course_listing = self.q(css=".course").filter(lambda el: course_name in el.text).results
course_listing = self.q(css=".course").filter(lambda el: course_name in el.text).results
el = course_listing[0]
el.find_element_by_css_selector('#upgrade-to-verified').click()
all_links = self.q(css='a.enter-course').map(lambda el: el.get_attribute('href')).results
link_index = None for index in range(len(all_links)): if course_id in all_links[index]: link_index = index break
tab_css = self._tab_css(tab_name)
return EmptyPromise( lambda: self._is_on_tab(tab_name), "{0} is the current tab".format(tab_name) )
BASE_URL = os.environ.get('test_url', 'http://localhost:8003')
AUTH_BASE_URL = os.environ.get('test_url', 'http://localhost:8031')
Promise(_check_func, "The 'Next Step' button is enabled.").fulfill()
for idx, text in enumerate(text_options): if text == POLL_ANSWER: self.q(css=text_selector).nth(idx).click()
cohort_management_section.wait_for_ajax() cohort_management_section.wait_for_page() return cohort_management_section
folders_list_in_path = folders_list_in_path[:-4]
folders_list_in_path.extend(['data', 'uploads', file_name])
return os.sep.join(folders_list_in_path)
return self.q(css='.cohorts-state-section').visible or self.q(css='.new-cohort-form').visible
self.wait_for( lambda: "Add a New Cohort" in self.q(css=self._bounded_selector(".form-title")).text, "Create cohort form is visible" )
if assignment_type: self.set_assignment_type(assignment_type)
self.wait_for( lambda: "added to this cohort" in self.get_cohort_confirmation_messages(wait_for_messages=True)[0], "Student(s) added confirmation message." )
self.wait_for_element_visibility(email_selector, 'Email field is visible') self.q(css=email_selector).fill(email)
EmptyPromise( lambda: self.q(css=enrollment_button).present, "Enrollment button" ).fulfill() self.q(css=enrollment_button).click()
self.q(css="select#allowance_type").present or self.q(css="label#timed_exam_allowance_type").present
dashboard = DashboardPage(self.browser) dashboard.wait_for_page() return dashboard
self.q(css=".contribution-option > input").first.click() self.q(css="input[name='verified_mode']").click()
self.wait_for_element_visibility(ccx_name_selector, 'CCX name field is visible') self.q(css=ccx_name_selector).fill(ccx_name)
EmptyPromise( lambda: self.q(css=create_ccx_button).present, "Create a new Custom Course for edX" ).fulfill() self.q(css=create_ccx_button).click()
url_path = ""
self._discussion_page = InlineDiscussionPage(self.browser, self.discussion_id)
EmptyPromise(lambda: self.is_button_shown('transcript_button'), "transcript button is shown").fulfill()
if self.is_captions_visible() != captions_new_state: self.click_player_button('transcript_button')
EmptyPromise(lambda: self.is_captions_visible() == captions_new_state, "Transcripts are {state}".format(state=state)).fulfill()
EmptyPromise(lambda: self.is_closed_captions_visible() == closed_captions_new_state, "Closed captions are {state}".format(state=state)).fulfill()
self.wait_for_ajax()
if button == 'pause': self.wait_for(lambda: self.state != 'buffering', 'Player is Ready for Pause')
wrapper_width = 75 if is_transcript_visible else 100 initial = self.browser.get_window_size()
time.sleep(0.2)
time.sleep(0.2)
self.browser.set_window_size( initial['width'], initial['height'] )
if '.' + transcript_format not in self.q(css=transcript_selector).text[0]: return False
cc_button_selector = self.get_element_selector(VIDEO_BUTTONS["transcript"]) element_to_hover_over = self.q(css=cc_button_selector).results[0] ActionChains(self.browser).move_to_element(element_to_hover_over).perform()
if self.current_language() != code: self.select_language(code)
self.wait_for_ajax()
logging.debug("Current state of '{}' element is '{}'".format(state_selector, current_state))
all_times = self.q(css=selector).text[0]
nav_dict = dict()
for sec_index, sec_title in enumerate(section_titles):
nav_dict[sec_title] = self._subsection_titles(sec_index + 1)
self.browser.execute_script("jQuery.fx.off = true;")
try: sec_index = self._section_titles().index(section_title) except ValueError: self.warning("Could not find section '{0}'".format(section_title)) return
section_css = '.course-navigation .chapter:nth-of-type({0})'.format(sec_index + 1) self.q(css=section_css).first.click()
subsection_css = ( ".course-navigation .chapter-content-container:nth-of-type({0}) " ".menu-item:nth-of-type({1})" ).format(sec_index + 1, subsec_index + 1)
self.q(css=subsection_css).first.click() self._on_section_promise(section_title, subsection_title).fulfill()
all_items = self.sequence_items
seq_css = "ol#sequence-list>li:nth-of-type({0})>.nav-item".format(seq_index + 1) self.q(css=seq_css).first.click() self.wait_for_ajax()
subsection_css = ( ".course-navigation .chapter-content-container:nth-of-type({0}) " ".menu-item a p:nth-of-type(1)" ).format(section_index)
REMOVE_SPAN_TAG_RE = re.compile(r'</span>(.+)<span')
return self.q(css=".badges-modal").visible
self._user_info = None
self._params = {}
BASE_URL = os.environ.get('test_url', 'http://localhost:8003')
AUTH_BASE_URL = os.environ.get('test_url', 'http://localhost:8031')
footer_el = footer_nav.find_element_by_xpath('..') return 'hidden' not in footer_el.get_attribute('class').split()
return element.is_displayed() and all(size > 0 for size in element.size.itervalues())
disable_animations(page) page.q(css=css).filter(_is_visible).nth(source_index).click()
page.wait_for_ajax()
return ( self.q(css='{} .acid-block'.format(self.context_selector)).present and wait_for_xblock_initialization(self, self.context_selector) and self._ajax_finished() )
is_done = page.browser.execute_script("return $({!r}).data('initialized')".format(xblock_css)) return (is_done, is_done)
type_in_codemirror(self, 0, content)
test_dir = path(__file__).abspath().dirname().dirname().dirname() file_path = test_dir + '/data/uploads/' + file_name
self.wait_for_ajax()
def is_browser_on_page(self): wait_for_ajax_or_reload(self.browser) return self.q(css='body.view-settings').visible
license_text = self.q(css='section.license span.license-text') if license_text.is_present(): return license_text.text[0] return None
folders_list_in_path = folders_list_in_path[:-4]
folders_list_in_path.extend(['data', 'uploads', file_name])
return os.sep.join(folders_list_in_path)
self.wait_for_element_visibility(upload_btn_selector, 'upload button is present')
self.wait_for_element_presence(self.upload_image_popup_window_selector, 'upload dialog is present')
filepath = SettingsPage.get_asset_path(file_to_upload) self.q(css=self.upload_image_browse_button_selector).results[0].send_keys(filepath) self.q(css=self.upload_image_upload_button_selector).results[0].click()
self.wait_for_element_absence(self.upload_image_popup_window_selector, 'upload dialog is hidden')
BASE_URL = os.environ.get('test_url', 'http://localhost:8031')
return None
return None
return os.sep.join(__file__.split(os.sep)[:-4]) + '/data/uploads/' + filename
self.q(css='button.signatory-panel-save').click() self.mode = 'details' self.wait_for_ajax() self.wait_for_signatory_detail_view()
return self.q(css='h1.page-header')[0].text.split('\n')[-1]
return "/".join([BASE_URL, self.url_path, unicode(self.locator)])
return os.sep.join(__file__.split(os.sep)[:-4]) + '/data/imports/' + filename
completed = True
try:
self.q(css=self._bounded_selector(self.NAME_INPUT_SELECTOR)).results[0].send_keys(Keys.ENTER) self.wait_for_ajax()
return "is-editing" in self.q( css=self._bounded_selector(self.NAME_FIELD_WRAPPER_SELECTOR) )[0].get_attribute("class")
return self.q(css=self._bounded_selector(child_class.BODY_SELECTOR)).map( lambda el: child_class(self.browser, el.get_attribute('data-locator'))).results
self.browser.execute_script("jQuery.fx.off = true;")
grandkids = [] for descendant in descendants: grandkids.extend(descendant.children)
if not self.q(css="input.no_special_exam").present: return False
if not self.q(css="input.timed_exam").present: return False
if not self.q(css="input.proctored_exam").present: return False
if not self.q(css="input.practice_exam").present: return False
return self.q(css="#is_prereq").visible
return self.q(css="#is_prereq:checked").present
return self.q(css="#prereq").visible
return self.q(css="#prereq_min_score").visible
self.wait_for_ajax()
elem.clear() elem.send_keys(value) elem.send_keys(Keys.TAB) self.save()
EmptyPromise( lambda: "login" not in self.browser.current_url, "redirected from the login page" ).fulfill()
key = self.q(css=KEY_CSS).nth(i).text[0] if key == expected_key: return i
self.browser.switch_to_window(browser_window_handles[-1])
click_css(self, 'a.delete-button', source_index, require_notification=False) confirm_prompt(self)
self.wait_for_page()
grandkids = [] for descendant in descendants: grandkids.extend(descendant.children)
url_path = ""
page.wait_for_component_menu() click_css(page, 'button>span.large-advanced-icon', menu_index, require_notification=False)
page.wait_for_element_visibility('.new-component-advanced', 'Advanced component menu is visible')
component_css = 'button[data-category={}]'.format(name) page.wait_for_element_visibility(component_css, 'Advanced component {} is visible'.format(name))
click_css(page, component_css, 0)
if is_advanced_problem: advanced_tab = page.q(css='.problem-type-tabs a').filter(text='Advanced').first advanced_tab.click()
page.wait_for_component_menu() click_css(page, 'button>span.large-html-icon', menu_index, require_notification=False)
page.wait_for_element_visibility('.new-component-html', 'HTML component menu is visible')
component_css = 'button[data-category=html]' if boilerplate: component_css += '[data-boilerplate={}]'.format(boilerplate) else: component_css += ':not([data-boilerplate])'
click_css(page, component_css, 0)
input_element.click() input_element.send_keys(Keys.CONTROL + 'a') input_element.send_keys(value) return input_element
LIBRARY_LABEL = "Library" COUNT_LABEL = "Count" SCORED_LABEL = "Scored" PROBLEM_TYPE_LABEL = "Problem Type"
self.wait_for_ajax() self.wait_for_element_absence(btn_selector, 'Wait for the XBlock to finish reloading')
[DISPLAY_NAME, 'Video', False], ['Default Video URL', 'https://www.youtube.com/watch?v=3_yD_cEKoCk, , ', False],
DELAY = 0.5
self.click_button('create_video', require_notification=True) self.wait_for_video_component_render()
self._params = {}
PAGES_PACKAGE_DIR = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'pages')
course_fix = CourseFixture( self.course_info['org'], self.course_info['number'], self.course_info['run'], self.course_info['display_name'] )
LogoutPage(self.browser).visit() self._make_har_file(login_page)
STUDIO_BASE_URL = os.environ.get('studio_url', 'http://localhost:8031')
LMS_BASE_URL = os.environ.get('lms_url', 'http://localhost:8003')
XQUEUE_STUB_URL = os.environ.get('xqueue_url', 'http://localhost:8040')
ORA_STUB_URL = os.environ.get('ora_url', 'http://localhost:8041')
COMMENTS_STUB_URL = os.environ.get('comments_url', 'http://localhost:4567')
EDXNOTES_STUB_URL = os.environ.get('edxnotes_url', 'http://localhost:8042')
PROGRAMS_STUB_URL = os.environ.get('programs_url', 'http://localhost:8090')
session = requests.Session() response = session.get(LMS_BASE_URL + "/auto_auth?superuser=true")
self.user = {}
session = requests.Session() response = session.get(STUDIO_BASE_URL + "/auto_auth?staff=true")
response = self.session.post( STUDIO_BASE_URL + '/xblock/', data=json.dumps(create_payload), headers=self.headers, )
response = self.session.post( STUDIO_BASE_URL + '/xblock/' + loc, data=xblock_desc.serialize(), headers=self.headers, )
response = self.session.put( "{}/xblock/{}".format(STUDIO_BASE_URL, locator), data=json.dumps(data), headers=self.headers, )
CourseUpdateDesc = namedtuple("CourseUpdateDesc", ['date', 'content'])
if start_date is None: start_date = datetime.datetime(1970, 1, 1)
response = self.session.post( STUDIO_BASE_URL + '/course/', data=self._encode_post_dict(self._course_dict), headers=self.headers )
if err is not None: raise FixtureError("Could not create course {0}. Error message: '{1}'".format(self, err))
response = self.session.get(url, headers=self.headers)
details.update(self._course_details)
response = self.session.post( url, data=self._encode_post_dict(details), headers=self.headers, )
payload = json.dumps({ 'children': None, 'data': handouts_html, 'id': self._handouts_loc, 'metadata': dict(), })
response = self.session.post( url, data=self._encode_post_dict(self._advanced_settings), headers=self.headers, )
payload = {self._pattern: json.dumps(self._response_dict)} response = requests.put(url, data=payload)
xblock_desc.publish = "not-applicable"
if 'user_id' in kwargs: kwargs['user_id'] = str(kwargs['user_id']) return kwargs
self.instructor_dashboard_page = InstructorDashboardPage(self.browser, self.course_id) self.instructor_dashboard_page.visit() self.cohort_management_page = self.instructor_dashboard_page.select_cohort_management()
self.cohort_name = "OnlyCohort" self.setup_cohort_config(self.course_fixture) self.cohort_id = self.add_manual_cohort(self.course_fixture, self.cohort_name)
self.instructor_dashboard_page = InstructorDashboardPage(self.browser, self.course_id) self.instructor_dashboard_page.visit() self.cohort_management_page = self.instructor_dashboard_page.select_cohort_management() self.cohort_management_page.wait_for_page()
self.cohort_management_page.save_discussion_topics(key)
confirmation_message = self.cohort_management_page.get_cohort_discussions_message(key=key) self.assertEqual("Your changes have been saved.", confirmation_message)
self.assertTrue(self.cohort_management_page.is_save_button_disabled(key))
self.cohort_management_page.select_always_inline_discussion()
self.cohort_management_page.select_cohort_some_inline_discussion() self.assertFalse(self.cohort_management_page.is_save_button_disabled(self.inline_key)) self.assertFalse(self.cohort_management_page.inline_discussion_topics_disabled())
self.cohort_management_page.select_cohort_some_inline_discussion()
self.cohort_management_page.select_discussion_topic(self.inline_key)
self.assertFalse(self.cohort_management_page.is_save_button_disabled(self.inline_key))
self.save_and_verify_discussion_topics(key=self.inline_key)
self.cohort_management_page.select_cohort_some_inline_discussion()
self.assertFalse(self.cohort_management_page.is_category_selected())
self.cohort_management_page.select_discussion_topic(self.inline_key)
self.assertTrue(self.cohort_management_page.is_category_selected())
self.cohort_management_page.select_cohort_some_inline_discussion()
self.assertFalse(self.cohort_management_page.is_category_selected())
self.cohort_management_page.select_discussion_topic(self.inline_key)
self.assertTrue(self.cohort_management_page.is_category_selected())
self.cohort_management_page.select_discussion_topic(self.inline_key)
self.assertFalse(self.cohort_management_page.is_category_selected())
self.cohort_management_page.select_cohort_some_inline_discussion()
self.assertFalse(self.cohort_management_page.is_category_selected())
self.save_and_verify_discussion_topics(key=self.inline_key)
self.verify_discussion_topics_after_reload(self.inline_key, cohorted_topics_after)
self.instructor_dashboard_page = InstructorDashboardPage(self.browser, self.course_id) self.instructor_dashboard_page.visit() self.cohort_management_page = self.instructor_dashboard_page.select_cohort_management()
self.disable_cohorting(self.course_fixture) self.refresh_thread_page(self.thread_id) self.assertEquals(self.thread_page.get_group_visibility_label(), "This post is visible to everyone.")
def refresh_thread_page(self, thread_id): self.browser.refresh() self.thread_page.wait_for_page()
pass
pass
pass
pass
self.assertTrue( self.thread_page_1.check_threads_rendered_successfully(thread_count=self.thread_count) )
self.thread_page_1.click_and_open_thread(thread_id=self.thread_ids[1]) self.assertTrue(self.thread_page_2.is_browser_on_page())
self.thread_page_2.check_focus_is_set(selector=".discussion-article")
self.assertFalse(thread_page.check_if_selector_is_focused(selector='.thread-wrapper'))
for i in range(current_page, total_pages): _check_page() if current_page < total_pages: page.click_on_page(current_page + 1) current_page += 1
for i in range(current_page, 0, -1): _check_page() if current_page > 1: page.click_on_page(current_page - 1) current_page -= 1
CourseFixture(**self.course_info).install()
self.cohort_a_student_username = "cohort_a_student" self.cohort_a_student_email = "cohort_a_student@example.com" StudioAutoAuthPage( self.browser, username=self.cohort_a_student_username, email=self.cohort_a_student_email, no_login=True ).visit()
self.cohort_b_student_username = "cohort_b_student" self.cohort_b_student_email = "cohort_b_student@example.com" StudioAutoAuthPage( self.browser, username=self.cohort_b_student_username, email=self.cohort_b_student_email, no_login=True ).visit()
self.cohort_default_student_username = "cohort_default_student" self.cohort_default_student_email = "cohort_default_student@example.com" StudioAutoAuthPage( self.browser, username=self.cohort_default_student_username, email=self.cohort_default_student_email, no_login=True ).visit()
StudioAutoAuthPage( self.browser, username=self.staff_user["username"], email=self.staff_user["email"] ).visit()
EmptyPromise( lambda: cohort_name == cohort_management_page.get_selected_cohort(), "Waiting for new cohort" ).fulfill() cohort_management_page.add_students_to_selected_cohort([student])
with open(self.TEST_INDEX_FILENAME, "w+") as index_file: json.dump({}, index_file)
course_outline.visit() subsection = course_outline.section_at(0).subsection_at(0) subsection.expand_subsection() subsection.add_unit()
self._auto_auth(self.USERNAME, self.EMAIL, False) self.dashboard.visit()
super(BaseLmsIndexTest, self).setUp()
self.page = IndexPage(self.browser)
self.page.visit()
self.now = datetime.datetime.now()
self.assertFalse(self.page.intro_video_element.visible)
time.sleep(time_between_creation)
self.teams_page.verify_my_team_count(expected_number_of_teams)
url = self.browser.current_url fragment_index = url.find('#') if fragment_index >= 0: url = url[0:fragment_index]
self.assertEqual(self.team_page.team_name, self.team['name']) self.assertTrue(self.team_page.edit_team_button_present)
self.teams_page.click_specific_topic("Example Topic") self.teams_page.verify_topic_team_count(1)
self.verify_my_team_count(1)
self.teams_page.click_all_topics() self.verify_my_team_count(1)
self.teams_page.click_all_topics() self.verify_my_team_count(0)
self._auto_auth("STAFF_TESTER", "staff101@example.com", True)
self.course_outline.visit() self.course_outline.open_subsection_settings_dialog(0) self.course_outline.select_access_tab() self.course_outline.make_gating_prerequisite()
self._auto_auth("STAFF_TESTER", "staff101@example.com", True)
self.course_outline.visit() self.course_outline.open_subsection_settings_dialog(1) self.course_outline.select_access_tab() self.course_outline.add_prerequisite_to_subsection("80")
self.course_fixture = CourseFixture( self.course_info['org'], self.course_info['number'], self.course_info['run'], self.course_info['display_name'] )
children_headers = self._set_library_content_settings(count=2, capa_type="Custom Evaluated Script") self.assertEqual(children_headers, set())
super(BaseLmsDashboardTest, self).setUp()
self.dashboard_page = DashboardPage(self.browser)
self.dashboard_page.visit()
self.now = datetime.datetime.now()
self.dashboard_page.visit()
self.assertEqual(course_date, expected_course_date)
self.dashboard_page.visit()
self.assertEqual(course_date, expected_course_date)
self.dashboard_page.visit()
self.assertEqual(course_date, expected_course_date)
self.dashboard_page.visit()
self.assertEqual(course_date, expected_course_date)
with open(self.TEST_INDEX_FILENAME, "w+") as index_file: json.dump({}, index_file) self.addCleanup(remove_file, self.TEST_INDEX_FILENAME)
course_fixture = CourseFixture( self.course_info['org'], self.course_info['number'], self.course_info['run'], self.course_info['display_name'] )
AutoAuthPage( self.browser, username=self.username, email=self.email, password=self.password, course_id=self.course_id, staff=False ).visit()
problem_page.click_hint() self.assertIn("Hint (1 of 2): mathjax should work1", problem_page.extract_hint_text_from_html) problem_page.verify_mathjax_rendered_in_hint()
problem_page.click_hint()
self.assertTrue(self.coach_dashboard_page.is_browser_on_enrollment_page())
super(XBlockAcidBase, self).setUp()
self.oauth_page.confirm() self.oauth_page.wait_for_element_absence( 'input[name=authorize]', 'Authorization button is not present' )
self.submission = "a=1" + self.unique_id[0:5]
if self.xqueue_grade_response is not None: XQueueResponseFixture(self.submission, self.xqueue_grade_response).install()
time.sleep(5)
self.course_fixture = CourseFixture( self.course_info['org'], self.course_info['number'], self.course_info['run'], self.course_info['display_name'] )
AutoAuthPage(self.browser, username=self.USERNAME, email=self.EMAIL, course_id=self.course_id, staff=True).visit()
EmptyPromise( lambda: cohort_name == cohort_management_page.get_selected_cohort(), "Waiting for new cohort" ).fulfill() cohort_management_page.add_students_to_selected_cohort([student])
course_page = self._goto_staff_page() course_page.set_staff_view_mode_specific_student(student_a_username) verify_expected_problem_visibility(self, course_page, [self.alpha_text, self.everyone_text])
course_page.set_staff_view_mode_specific_student(student_b_username) verify_expected_problem_visibility(self, course_page, [self.beta_text, self.everyone_text])
self.account_settings_page = AccountSettingsPage(self.browser) self.account_settings_page.visit() self.account_settings_page.wait_for_ajax()
self.expected_settings_change_initiated_event( 'email', email, 'you@there.com', username=username, user_id=user_id),
self.assert_no_setting_changed_event()
self.assert_no_setting_changed_event()
self.assertEqual(self.account_settings_page.value_for_dropdown_field('year_of_birth', ''), '')
self.reset_password_page.visit()
self.assertTrue(self.reset_password_page.is_form_visible())
self.reset_password_page.visit()
self.reset_password_page.fill_password_reset_form(self.user_info['email'])
self.assertIn("Password Reset Email Sent", self.reset_password_page.get_success_message())
CourseFixture( self.course_info['org'], self.course_info['number'], self.course_info['run'], self.course_info['display_name'] ).install()
email, password = self._create_unique_user()
self.login_page.visit().login(email=email, password=password)
course_names = self.dashboard_page.wait_for_page().available_courses self.assertIn(self.course_info["display_name"], course_names)
self.login_page.visit()
self.login_page.login(email="nobody@nowhere.com", password="password")
self.assertIn("Email or password is incorrect.", self.login_page.wait_for_errors())
self.login_page.visit().password_reset(email=email)
self.assertIn("Password Reset Email Sent", self.login_page.wait_for_success())
self.login_page.visit()
self.login_page.password_reset(email="nobody@nowhere.com")
self.assertIn( "No user with the provided email address exists.", self.login_page.wait_for_errors() )
email, password = self._create_unique_user()
self.login_page.visit() self.assertScreenshot('#login .login-providers', 'login-providers-{}'.format(self.browser.name))
self.login_page.click_third_party_dummy_provider()
self.login_page.login(email=email, password=password)
course_names = self.dashboard_page.wait_for_page().available_courses self.assertIn(self.course_info["display_name"], course_names)
LogoutPage(self.browser).visit()
AutoAuthPage(self.browser, course_id=self.course_id).visit() self._link_dummy_account() LogoutPage(self.browser).visit()
course_page = CoursewarePage(self.browser, self.course_id) self.browser.get(course_page.url + '?tpa_hint=oa2-dummy')
course_page.wait_for_page()
account_settings.switch_account_settings_tabs('accounts-tab')
account_settings.switch_account_settings_tabs('accounts-tab') account_settings.wait_for_link_title_for_link_field(field_id, "Unlink This Account")
account_settings = AccountSettingsPage(self.browser).visit() account_settings.switch_account_settings_tabs('accounts-tab')
AutoAuthPage( self.browser, username=username, email=email, password=password ).visit()
LogoutPage(self.browser).visit()
CourseFixture( self.course_info['org'], self.course_info['number'], self.course_info['run'], self.course_info['display_name'] ).install()
self.register_page.visit()
course_names = self.dashboard_page.wait_for_page().available_courses self.assertIn(self.course_info["display_name"], course_names)
self.register_page.visit()
self.register_page.visit() self.assertScreenshot('#register .login-providers', 'register-providers-{}'.format(self.browser.name))
self.register_page.click_third_party_dummy_provider()
self.register_page.register(country="US", favorite_movie="Battlestar Galactica", terms_of_service=True)
course_names = self.dashboard_page.wait_for_page().available_courses self.assertIn(self.course_info["display_name"], course_names)
LogoutPage(self.browser).visit()
account_settings = AccountSettingsPage(self.browser).visit() account_settings.switch_account_settings_tabs('accounts-tab')
CourseFixture( self.course_info['org'], self.course_info['number'], self.course_info['run'], self.course_info['display_name'] ).install()
ModeCreationPage(self.browser, self.course_id).visit()
ModeCreationPage(self.browser, self.course_id, mode_slug=u'verified', mode_display_name=u'Verified Certificate', min_price=10, suggested_prices='10,20').visit()
student_id = AutoAuthPage(self.browser).visit().get_user_id()
self.track_selection_page.visit()
self.track_selection_page.enroll('verified')
self.payment_and_verification_flow.proceed_to_payment()
self.fake_payment_page.submit_payment()
self.payment_and_verification_flow.immediate_verification()
self.payment_and_verification_flow.webcam_capture() self.payment_and_verification_flow.next_verification_step(self.immediate_verification_page)
self.payment_and_verification_flow.webcam_capture() self.payment_and_verification_flow.next_verification_step(self.immediate_verification_page)
self.payment_and_verification_flow.next_verification_step(self.immediate_verification_page)
self.dashboard_page.visit()
enrollment_mode = self.dashboard_page.get_enrollment_mode(self.course_info["display_name"]) self.assertEqual(enrollment_mode, 'verified')
student_id = AutoAuthPage(self.browser).visit().get_user_id()
self.track_selection_page.visit()
self.track_selection_page.enroll('verified')
self.payment_and_verification_flow.proceed_to_payment()
self.fake_payment_page.submit_payment()
self.dashboard_page.visit()
enrollment_mode = self.dashboard_page.get_enrollment_mode(self.course_info["display_name"]) self.assertEqual(enrollment_mode, 'verified')
student_id = AutoAuthPage(self.browser, course_id=self.course_id).visit().get_user_id()
self.dashboard_page.visit()
enrollment_mode = self.dashboard_page.get_enrollment_mode(self.course_info["display_name"]) self.assertEqual(enrollment_mode, 'honor')
self.dashboard_page.upgrade_enrollment(self.course_info["display_name"], self.upgrade_page)
self.upgrade_page.indicate_contribution()
self.upgrade_page.proceed_to_payment()
self.fake_payment_page.submit_payment()
self.dashboard_page.visit()
enrollment_mode = self.dashboard_page.get_enrollment_mode(self.course_info["display_name"]) self.assertEqual(enrollment_mode, 'verified')
self.course_info['number'] = self.unique_id[0:6]
AutoAuthPage(self.browser, course_id=self.course_id).visit()
self.course_info_page.visit() self.tab_nav.go_to_tab('Wiki')
self.course_info['number'] = self.unique_id[0:6]
course_fix = CourseFixture( self.course_info['org'], self.course_info['number'], self.course_info['run'], self.course_info['display_name'] )
AutoAuthPage(self.browser, course_id=self.course_id).visit()
self.progress_page.visit() self.tab_nav.go_to_tab('Home')
self.assertEqual(self.course_info_page.num_updates, 1)
handout_links = self.course_info_page.handout_links self.assertEqual(len(handout_links), 1) self.assertIn('demoPDF.pdf', handout_links[0])
self.course_info_page.visit() self.tab_nav.go_to_tab('Progress')
CHAPTER = 'Test Section' SECTION = 'Test Subsection' EXPECTED_SCORES = [(0, 3), (0, 1)]
self.course_info_page.visit() self.tab_nav.go_to_tab('Test Static Tab') self.assertTrue(self.tab_nav.is_on_tab('Test Static Tab'))
self.course_info_page.visit() self.tab_nav.go_to_tab('Test Static Tab') self.assertTrue(self.tab_nav.is_on_tab('Test Static Tab'))
self.tab_nav.mathjax_has_rendered()
self.course_info_page.visit() self.tab_nav.go_to_tab('Wiki') self.assertTrue(self.tab_nav.is_on_tab('Wiki'))
self.course_info_page.visit() self.tab_nav.go_to_tab('Course')
EXPECTED_SECTIONS = { 'Test Section': ['Test Subsection'], 'Test Section 2': ['Test Subsection 2', 'Test Subsection 3'] }
self.course_nav.go_to_section('Test Section', 'Test Subsection')
EXPECTED_ITEMS = ['Test Problem 1', 'Test Problem 2', 'Test HTML']
course_fix = CourseFixture( self.course_info['org'], self.course_info['number'], self.course_info['run'], self.course_info['display_name'] )
AutoAuthPage(self.browser, course_id=self.course_id).visit()
for i in range(1, 3): self.tab_nav.go_to_tab("PDF Book {}".format(i))
AutoAuthPage(self.browser, course_id=self.course_id).visit()
AutoAuthPage(self.browser, course_id=self.course_id).visit()
self.dashboard_page.visit() self.assertFalse(self.dashboard_page.pre_requisite_message_displayed())
LogoutPage(self.browser).visit() AutoAuthPage(self.browser, course_id=self.course_id, staff=True).visit()
self.settings_page.visit() self._set_pre_requisite_course()
LogoutPage(self.browser).visit() AutoAuthPage(self.browser, course_id=self.course_id, staff=False).visit()
course_fix = CourseFixture( self.course_info['org'], self.course_info['number'], self.course_info['run'], self.course_info['display_name'] )
AutoAuthPage(self.browser, course_id=self.course_id).visit()
self.course_info_page.visit() self.tab_nav.go_to_tab('Course') self.course_nav.go_to_section('Test Section', 'Test Subsection')
self.assertIn("What is the sum of 17 and 3?", problem_page.problem_text)
problem_page.fill_answer("20") problem_page.click_check() self.assertTrue(problem_page.is_correct())
problem_page.fill_answer("4") problem_page.click_check() self.assertFalse(problem_page.is_correct())
AutoAuthPage(self.browser, course_id=self.course_id).visit()
self.courseware_page.visit() self.courseware_page.wait_for_page() self.assertFalse(element_has_text( page=self.courseware_page, css_selector=entrance_exam_link_selector, text='Entrance Exam' ))
LogoutPage(self.browser).visit() AutoAuthPage(self.browser, course_id=self.course_id, staff=True).visit()
LogoutPage(self.browser).visit() AutoAuthPage(self.browser, course_id=self.course_id, staff=False).visit()
self.courseware_page.visit() self.courseware_page.wait_for_page() self.assertTrue(element_has_text( page=self.courseware_page, css_selector=entrance_exam_link_selector, text='Entrance Exam' ))
ModeCreationPage(self.browser, self.course_id).visit()
ModeCreationPage( self.browser, self.course_id, mode_slug=u'verified', mode_display_name=u'Verified Certificate', min_price=10, suggested_prices='10,20' ).visit()
select_option_by_text(language_selector, 'English') self.account_settings.wait_for_ajax() self.assertEqual(self.account_settings.value_for_dropdown_field('pref-lang'), u'English')
self.register_page = CombinedLoginAndRegisterPage(self.browser, start_page="register") self.dashboard_page = DashboardPage(self.browser)
ModeCreationPage( self.browser, self.course_id, mode_slug=u'verified', mode_display_name=u'Verified Certificate', min_price=10, suggested_prices='10,20' ).visit()
self._auto_auth(self.USERNAME, self.EMAIL, False)
self.track_selection_page.visit()
self.track_selection_page.enroll('verified')
self.payment_and_verification_flow.proceed_to_payment()
self.fake_payment_page.submit_payment()
LogoutPage(self.browser).visit() self._auto_auth("STAFF_TESTER", "staff101@example.com", True) self.course_outline.visit()
self.course_outline.open_subsection_settings_dialog()
self.course_outline.select_advanced_tab()
LogoutPage(self.browser).visit() self._login_as_a_verified_user() self.courseware_page.visit()
self.courseware_page.start_proctored_exam()
LogoutPage(self.browser).visit() self._auto_auth("STAFF_TESTER", "staff101@example.com", True) self.course_outline.visit()
self.course_outline.open_subsection_settings_dialog()
self.course_outline.select_advanced_tab()
LogoutPage(self.browser).visit() self._login_as_a_verified_user() self.courseware_page.visit()
self.courseware_page.start_timed_exam()
self.courseware_page.stop_timed_exam()
self._create_a_timed_exam_and_attempt()
__, __ = self.log_in_as_instructor()
instructor_dashboard_page = self.visit_instructor_dashboard() allowance_section = instructor_dashboard_page.select_special_exams().select_allowance_section()
self.assertTrue(allowance_section.is_add_allowance_button_visible)
allowance_section.click_add_allowance_button()
self.assertTrue(allowance_section.is_add_allowance_popup_visible)
allowance_section.submit_allowance_form('10', self.USERNAME)
self.assertTrue(allowance_section.is_allowance_record_visible)
self._create_a_timed_exam_and_attempt()
__, __ = self.log_in_as_instructor()
instructor_dashboard_page = self.visit_instructor_dashboard() exam_attempts_section = instructor_dashboard_page.select_special_exams().select_exam_attempts_section()
self.assertTrue(exam_attempts_section.is_search_text_field_visible)
self.assertTrue(exam_attempts_section.is_student_attempt_visible)
exam_attempts_section.remove_student_attempt() self.assertFalse(exam_attempts_section.is_student_attempt_visible)
AutoAuthPage( self.browser, username="johndoe_saee", email=self.student_identifier, course_id=self.course_id, staff=False ).visit()
self.log_in_as_instructor() self.student_admin_section = self.visit_instructor_dashboard().select_student_admin()
alert = get_modal_alert(self.student_admin_section.browser) alert.dismiss()
self.certificates_section.refresh()
self.certificates_section.wait_for_certificate_exceptions_section()
self.assertIn(self.user_name, self.certificates_section.last_certificate_exception.text) self.assertIn(notes, self.certificates_section.last_certificate_exception.text)
self.certificates_section.remove_first_certificate_exception() self.assertNotIn(self.user_name, self.certificates_section.last_certificate_exception.text) self.assertNotIn(notes, self.certificates_section.last_certificate_exception.text)
self.certificates_section.refresh()
self.certificates_section.wait_for_certificate_exceptions_section()
self.assertNotIn(self.user_name, self.certificates_section.last_certificate_exception.text) self.assertNotIn(notes, self.certificates_section.last_certificate_exception.text)
self.certificates_section.add_certificate_exception(self.user_name, '')
self.certificates_section.add_certificate_exception(self.user_name, '')
self.certificates_section.wait_for_certificate_exceptions_section() self.certificates_section.click_add_exception_button()
self.certificates_section.wait_for_certificate_exceptions_section()
self.certificates_section.wait_for_certificate_exceptions_section()
self.certificates_section.add_certificate_exception(self.user_name, '')
self.certificates_section.click_generate_certificate_exceptions_button() self.certificates_section.wait_for_ajax()
self.certificates_section.refresh()
self.certificates_section.wait_for_certificate_exceptions_section()
self.assertIn(self.user_name, self.certificates_section.last_certificate_exception.text) self.assertIn(expected_notes, self.certificates_section.last_certificate_exception.text)
CourseFixture( org='test_org', number='335535897951379478207964576572017930000', run='test_run', display_name='Test Course 335535897951379478207964576572017930000', ).install()
self.course_info['number'] = "335535897951379478207964576572017930000"
self.student_id = "99" self.student_name = "testcert" self.student_email = "cert@example.com"
AutoAuthPage( self.browser, username=self.student_name, email=self.student_email, course_id=self.course_id, ).visit()
self.assertIn( "Certificate has been successfully invalidated for {user}.".format(user=self.student_name), self.certificates_section.certificate_invalidation_message.text )
self.certificates_section.refresh()
self.certificates_section.wait_for_certificate_invalidations_section()
self.assertIn(self.student_name, self.certificates_section.last_certificate_invalidation.text) self.assertIn(notes, self.certificates_section.last_certificate_invalidation.text)
self.certificates_section.refresh()
self.certificates_section.wait_for_certificate_invalidations_section()
self.certificates_section.remove_first_certificate_invalidation()
self.assertNotIn(self.student_name, self.certificates_section.last_certificate_invalidation.text) self.assertNotIn(notes, self.certificates_section.last_certificate_invalidation.text)
self.certificates_section.fill_certificate_invalidation_user_name_field("") self.certificates_section.click_invalidate_certificate_button() self.certificates_section.wait_for_ajax()
self.certificates_section.fill_certificate_invalidation_user_name_field(invalid_user) self.certificates_section.click_invalidate_certificate_button() self.certificates_section.wait_for_ajax()
self.certificates_section.wait_for_certificate_invalidations_section()
self.certificate_page = CertificatePage(self.browser, self.user_id, self.course_id)
self.course_info['number'] = "3355358979513794782079645765720179311111"
self.assertEqual(actual_padding, expected_padding)
self.course_nav.go_to_section('Test Section', 'Test Subsection')
self.course_nav.go_to_vertical('Test Problem 1')
self.course_nav.q(css='select option[value="{}"]'.format('blue')).first.click()
self.course_nav.q(css='fieldset label:nth-child(3) input').nth(0).click()
self.course_nav.q(css='button.check.Check').click() self.course_nav.wait_for_ajax()
self.course_nav.go_to_section('Test Section 2', 'Test Subsection 2')
self.course_nav.go_to_vertical('Test Problem 2')
self.course_nav.q(css='input[id^=input_][id$=_2_1]').fill('A*x^2 + sqrt(y)')
self.course_nav.q(css='button.check.Check').click() self.course_nav.wait_for_ajax()
with open(self.TEST_INDEX_FILENAME, "w+") as index_file: json.dump({}, index_file) self.addCleanup(remove_file, self.TEST_INDEX_FILENAME)
self.cohort_default_student_username = "cohort_default_student" self.cohort_default_student_email = "cohort_default_student@example.com" StudioAutoAuthPage( self.browser, username=self.cohort_default_student_username, email=self.cohort_default_student_email, no_login=True ).visit()
EmptyPromise( lambda: cohort_name == cohort_management_page.get_selected_cohort(), "Waiting for new cohort" ).fulfill() cohort_management_page.add_students_to_selected_cohort([student])
self.course_fix = CourseFixture( self.course_info['org'], self.course_info['number'], self.course_info['run'], self.course_info['display_name'] )
self._auto_auth(self.USERNAME, self.EMAIL, False)
self._goto_problem_page()
LogoutPage(self.browser).visit() self._auto_auth("STAFF_TESTER", "staff101@example.com", True)
self.course_outline.visit()
self.course_outline.change_problem_release_date()
LogoutPage(self.browser).visit() self._auto_auth(self.USERNAME, self.EMAIL, False)
self.courseware_page.visit() self.assertEqual(self.problem_page.problem_name, 'Test Problem 2')
ModeCreationPage( self.browser, self.course_id, mode_slug=u'verified', mode_display_name=u'Verified Certificate', min_price=10, suggested_prices='10,20' ).visit()
self._auto_auth(self.USERNAME, self.EMAIL, False)
self.track_selection_page.visit()
self.track_selection_page.enroll('verified')
self.payment_and_verification_flow.proceed_to_payment()
self.fake_payment_page.submit_payment()
course_fix = CourseFixture( self.course_info['org'], self.course_info['number'], self.course_info['run'], self.course_info['display_name'] )
self.assert_navigation_state('Test Section 1', 'Test Subsection 1,1', 0, next_enabled=True, prev_enabled=False)
self.courseware_page.click_next_button_on_top() self.assert_navigation_state('Test Section 1', 'Test Subsection 1,1', 1, next_enabled=True, prev_enabled=True)
self.courseware_page.go_to_sequential_position(4) self.assert_navigation_state('Test Section 1', 'Test Subsection 1,1', 3, next_enabled=True, prev_enabled=True)
self.courseware_page.click_next_button_on_bottom() self.assert_navigation_state('Test Section 1', 'Test Subsection 1,2', 0, next_enabled=True, prev_enabled=True)
self.courseware_page.click_next_button_on_top() self.assert_navigation_state('Test Section 2', 'Test Subsection 2,1', 0, next_enabled=False, prev_enabled=True)
self.courseware_page.click_previous_button_on_top() self.assert_navigation_state('Test Section 1', 'Test Subsection 1,2', 0, next_enabled=True, prev_enabled=True)
self.courseware_page.click_previous_button_on_bottom() self.assert_navigation_state('Test Section 1', 'Test Subsection 1,1', 3, next_enabled=True, prev_enabled=True)
self.courseware_page.click_previous_button_on_bottom() self.assert_navigation_state('Test Section 1', 'Test Subsection 1,1', 2, next_enabled=True, prev_enabled=True)
filter_sequence_ui_event = lambda event: event.get('name', '').startswith('edx.ui.lms.sequence.')
filter_selected_events = lambda event: event.get('name', '') == 'edx.ui.lms.outline.selected' selected_events = self.wait_for_events(event_filter=filter_selected_events, timeout=2)
self.courseware_page.a11y_audit.config.set_scope( include=['div.sequence-nav']) self.courseware_page.a11y_audit.check_for_accessibility_errors()
course_fix = CourseFixture( self.course_info['org'], self.course_info['number'], self.course_info['run'], self.course_info['display_name'] )
AutoAuthPage( self.browser, username=self.USERNAME, email=self.EMAIL, course_id=self.course_id, staff=False ).visit()
self.go_to_tab_and_assert_problem(1, self.problem1_name)
self.problem_page.click_choice('choice_choice_1') self.problem_page.click_check() self.problem_page.wait_for_expected_status('label.choicegroup_incorrect', 'incorrect')
problem1_content_before_switch = self.problem_page.problem_content
self.go_to_tab_and_assert_problem(2, self.problem2_name)
self.go_to_tab_and_assert_problem(1, self.problem1_name) problem1_content_after_coming_back = self.problem_page.problem_content self.assertEqual(problem1_content_before_switch, problem1_content_after_coming_back)
self.go_to_tab_and_assert_problem(1, self.problem1_name)
self.problem_page.click_choice('choice_choice_1') self.problem_page.click_save() self.problem_page.wait_for_expected_status('div.capa_alert', 'saved')
problem1_content_before_switch = self.problem_page.problem_content
self.go_to_tab_and_assert_problem(2, self.problem2_name)
self.go_to_tab_and_assert_problem(1, self.problem1_name) problem1_content_after_coming_back = self.problem_page.problem_content self.assertIn(problem1_content_after_coming_back, problem1_content_before_switch)
self.go_to_tab_and_assert_problem(1, self.problem1_name)
problem1_content_before_switch = self.problem_page.problem_content
self.go_to_tab_and_assert_problem(2, self.problem2_name)
self.go_to_tab_and_assert_problem(1, self.problem1_name) problem1_content_after_coming_back = self.problem_page.problem_content self.assertEqual(problem1_content_before_switch, problem1_content_after_coming_back)
if privacy is not None: profile_page.visit()
profile_page.privacy = privacy
if privacy == self.PRIVACY_PUBLIC: self.assertEqual(profile_page.privacy, 'all_users') else: self.assertEqual(profile_page.privacy, 'private')
profile_page.visit()
if privacy is None: privacy = self.PRIVACY_PUBLIC self.visit_profile_page(username, privacy=privacy)
if birth_year: self.set_birth_year(birth_year)
LogoutPage(self.browser).visit()
self.browser.refresh() profile_page.wait_for_page() self.verify_profile_page_is_public(profile_page)
self.browser.refresh() profile_page.wait_for_page() self.verify_profile_page_is_private(profile_page)
self.notes_page.search("note") self.assertFalse(self.notes_page.is_error_visible) self.assertIn(u"Search Results", self.notes_page.tabs)
self.assertGroupContent( groups[0], title=u"cool (2)", notes=[u"Third note", None] )
self.assertGroupContent( groups[1], title=u"review (2)", notes=[u"Fourth note", None] )
self.assertGroupContent( groups[3], title=u"[no tags] (2)", notes=["Fifth note", "First note"] )
self.notes_page.wait_for_ajax() note = self.notes_page.notes[0] assert_page(note, self.raw_note_list[4]['usage_id'], "Recent Activity")
self.notes_page.wait_for_ajax() note = self.notes_page.notes[1] assert_page(note, self.raw_note_list[2]['usage_id'], "Location in Course")
self.notes_page.wait_for_ajax() note = self.notes_page.notes[0] assert_page(note, self.raw_note_list[2]['usage_id'], "Tags")
self.notes_page.wait_for_ajax()
pear_group = self.notes_page.tag_groups[group_index] self.assertEqual(tag_name + " (3)", pear_group.title) self.assertTrue(pear_group.scrolled_to_top(group_index))
self.notes_page.go_to_page(2) self._verify_pagination_info( notes_count_on_current_page=1, header_text='Showing 26-26 out of 26 total', previous_button_enabled=True, next_button_enabled=False, current_page_number=2, total_pages=2 )
self.notes_page.go_to_page(3) self._verify_pagination_info( notes_count_on_current_page=1, header_text='Showing 26-26 out of 26 total', previous_button_enabled=True, next_button_enabled=False, current_page_number=2, total_pages=2 )
self.notes_page.go_to_page(2) self._verify_pagination_info( notes_count_on_current_page=1, header_text='Showing 26-26 out of 26 total', previous_button_enabled=True, next_button_enabled=False, current_page_number=2, total_pages=2 )
self.notes_page.go_to_page(3) self._verify_pagination_info( notes_count_on_current_page=1, header_text='Showing 26-26 out of 26 total', previous_button_enabled=True, next_button_enabled=False, current_page_number=2, total_pages=2 )
with open(self.TEST_INDEX_FILENAME, "w+") as index_file: json.dump({}, index_file) self.addCleanup(remove_file, self.TEST_INDEX_FILENAME)
self.course_outline.visit() subsection = self.course_outline.section_at(section_index).subsection_at(0) subsection.expand_subsection() subsection.add_unit()
self._studio_add_content(0)
self.assertFalse(self._search_for_content(self.SEARCH_STRING))
self._studio_publish_content(0)
self.assertTrue(self._search_for_content(self.SEARCH_STRING))
self._studio_add_content(1)
self.assertFalse(self._search_for_content(self.EDITED_SEARCH_STRING))
self._studio_publish_content(1)
self._studio_reindex()
self.assertFalse(self._search_for_content(self.EDITED_SEARCH_STRING))
self._studio_reindex()
self.assertTrue(self._search_for_content(self.EDITED_SEARCH_STRING))
return XBlockFixtureDesc( 'problem', self.problem_name, data=self.factory.build_xml(**self.factory_kwargs), metadata={'rerandomize': 'always'} )
self.assertEqual(self.problem_page.problem_name, self.problem_name)
self.answer_problem(correct=True) self.problem_page.click_check() self.wait_for_status('correct')
self.answer_problem(correct=False) self.problem_page.click_check() self.wait_for_status('incorrect')
self.problem_page.a11y_audit.config.set_scope( include=['div#seq_content'])
self.problem_page.a11y_audit.check_for_accessibility_errors()
first_addend = random.randint(-100, 100) second_addend = 10 - first_addend
if not correct: second_addend += random.randint(1, 10)
AutoAuthPage(self.browser, username=self.USERNAME, email=self.EMAIL, course_id=self.course_id, staff=False).visit()
with open(self.TEST_INDEX_FILENAME, "w+") as index_file: json.dump({}, index_file)
conditional_page = ConditionalPage(self.browser) conditional_page.fill_in_poll() self.courseware_page.visit() self.assertTrue(conditional_page.is_content_visible())
LmsAutoAuthPage(self.browser, username=self.USERNAME, email=self.EMAIL, course_id=self.course_id).visit()
LogoutPage(self.browser).visit() StudioAutoAuthPage( self.browser, username=self.USERNAME, email=self.EMAIL, course_id=self.course_id, staff=True ).visit()
self.course_outline_page.visit() self.course_outline_page.wait_for_page()
LogoutPage(self.browser).visit() LmsAutoAuthPage(self.browser, username=self.USERNAME, email=self.EMAIL, course_id=self.course_id).visit()
self.courseware_page.visit() self.courseware_page.wait_for_page()
breadcrumbs = self._breadcrumb(num_units=num_units, modified_name=modified_name) breadcrumbs.reverse() self.assertEqual(bookmarked_breadcrumbs, breadcrumbs)
course_fix = CourseFixture( self.course_info['org'], self.course_info['number'], self.course_info['run'], self.course_info['display_name'] )
AutoAuthPage(self.browser, username=self.USERNAME, email=self.EMAIL, course_id=self.course_id, staff=False).visit()
disable_animations(annotation_component_page)
'transcript': 'http://video.google.com/timedtext?lang=en&v=3_yD_cEKoCk',
if all_options_selected and not has_option: all_options_selected = False return all_options_selected
EmptyPromise(options_selected, "Option is selected").fulfill()
continue
AutoAuthPage(self.browser, username=self.USERNAME, email=self.EMAIL, course_id=self.course_id, staff=False).visit()
self.assert_payload_contains_ids(load_video_event)
self.assert_field_type(load_video_event, 'time', datetime.datetime) del load_video_event['time']
sources, duration = self.video.sources[0], self.video.duration self.assert_bumper_payload_contains_ids(load_video_event, sources, duration)
self.assert_field_type(load_video_event, 'time', datetime.datetime) del load_video_event['time']
self.navigate_to_video()
self.navigate_to_video()
self.video.wait_for_state('pause')
self.navigate_to_video()
self.video.wait_for_state('pause')
self.navigate_to_video()
self.video.wait_for_state('pause')
if lang_code == 'zh_HANT': self.video.select_language(lang_code) unicode_text = lang_text.decode('utf-8') self.assertIn(unicode_text, self.video.captions_text)
self.addCleanup(YouTubeStubConfig.reset)
if not _contents_of_verticals: _contents_of_verticals = [[{'display_name': 'Video', 'metadata': self.metadata}]]
self.assertTrue(self.video.is_video_rendered('youtube'))
unicode_text = "好 各位同学".decode('utf-8') self.assertIn(unicode_text, self.video.captions_text)
self.video.hide_closed_captions() self.video.wait_for_closed_captions_to_be_hidden() self.video.reload_page() self.video.wait_for_closed_captions_to_be_hidden()
self.assertIn('Welcome to edX.', self.video.captions_text)
self.video.click_player_button('fullscreen')
self.assertTrue(self.video.is_aligned(False))
self.navigate_to_video()
unicode_text = "好 各位同学".decode('utf-8') self.assertTrue(self.video.downloaded_transcript_contains_text('srt', unicode_text))
self.navigate_to_video()
self.assertIn('Welcome to edX.', self.video.captions_text)
self.assertTrue(self.video.downloaded_transcript_contains_text('srt', 'Welcome to edX.'))
self.assertTrue(self.video.select_language('zh'))
unicode_text = "好 各位同学".decode('utf-8') self.assertIn(unicode_text, self.video.captions_text)
unicode_text = "好 各位同学".decode('utf-8') self.assertTrue(self.video.downloaded_transcript_contains_text('srt', unicode_text))
self.navigate_to_video()
self.video.show_captions()
self.video.click_player_button('fullscreen')
self.assertTrue(self.video.is_aligned(True))
self.video.click_player_button('transcript_button')
self.assertTrue(self.video.is_aligned(False))
self.youtube_configuration['time_to_response'] = 0.4 self.metadata = self.metadata_for_mode('youtube_html5')
self.youtube_configuration['time_to_response'] = 2.0 self.metadata = self.metadata_for_mode('youtube_html5')
self.youtube_configuration.update({ 'youtube_api_blocked': True, })
self.assertEqual(len(self.video.q(css='video')), 1)
self.youtube_configuration.update({ 'time_to_response': 2.0, 'youtube_api_blocked': True, })
self.assertEqual(len(self.video.q(css='video')), 1)
self.youtube_configuration.update({ 'time_to_response': 2.0, 'youtube_api_blocked': True, })
self.assertTrue(self.video.is_button_shown('transcript_button')) self._verify_caption_text('Welcome to edX.')
self.navigate_to_video()
self.assertTrue(self.video.downloaded_transcript_contains_text('srt', '00:00:00,260'))
self.assertTrue(self.video.select_transcript_format('txt'))
self.assertTrue(self.video.downloaded_transcript_contains_text('txt', 'Welcome to edX.'))
self.course_nav.go_to_vertical('Test Vertical-1')
self.assertTrue(self.video.downloaded_transcript_contains_text('txt', 'Equal transcripts'))
self.course_nav.go_to_vertical('Test Vertical-2')
self.assertFalse(self.video.is_menu_present('download_transcript'))
self.navigate_to_video()
self.navigate_to_video() self.video.show_closed_captions()
self.video.click_player_button('play') self.video.wait_for_position('0:03') self.video.click_player_button('pause')
self.navigate_to_video() execute_video_steps(tab1_video_names)
self.go_to_sequential_position(2) execute_video_steps(tab2_video_names)
self.go_to_sequential_position(1) execute_video_steps(tab1_video_names)
self.course_nav.go_to_vertical('Test Vertical-0') self.video.wait_for_video_player_render() self.video.speed = '2.0'
self.course_nav.go_to_vertical('Test Vertical-1') self.video.wait_for_video_player_render() self.video.speed = '0.50'
self.course_nav.go_to_vertical('Test Vertical-2') self.video.wait_for_video_player_render()
self.course_nav.go_to_vertical('Test Vertical-0')
self.assertEqual(self.video.speed, '2.0x')
self.video.reload_page()
self.course_nav.go_to_vertical('Test Vertical-0')
self.assertEqual(self.video.speed, '2.0x')
self.video.speed = '1.0'
self.course_nav.go_to_vertical('Test Vertical-1')
self.assertEqual(self.video.speed, '0.50x')
self.course_nav.go_to_vertical('Test Vertical-2')
self.video.verify_speed_changed('1.0x')
self.navigate_to_video()
self.video.wait_for_video_player_render() self.assertIn(self.video.state, ['playing', 'buffering', 'finished'])
self.courseware.go_to_sequential_position(2)
self.courseware.go_to_sequential_position(1) execute_video_steps(tab1_video_names)
self.assertTrue(self.video.is_video_rendered('youtube'))
self.assertFalse(self.video.is_autoplay_enabled)
self.assertTrue(self.video.is_error_message_shown)
correct_error_message_text = 'No playable video sources found.' self.assertIn(correct_error_message_text, self.video.error_message_text)
self.assertFalse(self.video.is_spinner_shown)
self.navigate_to_video()
unicode_text = "好 各位同学".decode('utf-8') self.assertIn(unicode_text, self.video.captions_text)
unicode_text = "好 各位同学".decode('utf-8') self.assertTrue(self.video.downloaded_transcript_contains_text('srt', unicode_text))
self.navigate_to_video()
self.assertIn('Welcome to edX.', self.video.captions_text)
self.assertTrue(self.video.downloaded_transcript_contains_text('srt', 'Welcome to edX.'))
self.assertTrue(self.video.select_language('zh'))
unicode_text = "好 各位同学".decode('utf-8')
unicode_text = "好 各位同学".decode('utf-8') self.assertTrue(self.video.downloaded_transcript_contains_text('srt', unicode_text))
self.navigate_to_video()
self.video.show_captions()
self.video.click_player_button('fullscreen')
self.assertTrue(self.video.is_aligned(True))
self.navigate_to_video()
self.video.show_captions()
self.assertIn("Welcome to edX.", self.video.captions_text)
self.navigate_to_video()
self.video.show_captions()
unicode_text = "好 各位同学".decode('utf-8') self.assertIn(unicode_text, self.video.captions_text)
self.navigate_to_video() self.video.show_captions()
self.video.a11y_audit.config.set_scope( include=["div.video"] ) self.video.a11y_audit.check_for_accessibility_errors()
self.unit_page = None
self.assertTrue(video_xblocks == 2)
self.outline.visit()
self.unit_page = self.outline.section('Test Section').subsection('Test Subsection').expand_subsection().unit( 'Test Unit').go_to()
self.unit_page.xblocks[1].open_advanced_tab()
self.unit_page.xblocks[1].open_basic_tab()
self.unit_page.xblocks[1].save_settings()
self._create_video()
self.edit_component(1) self.open_advanced_tab() self.video.set_field_value('YouTube ID', 'sampleid123') self.save_unit_settings()
self._navigate_to_course_unit_page() self.assertTrue(self.video.is_controls_visible())
self.assertFalse(self.video.is_error_message_shown)
self._create_course_unit(subtitles=True) self.edit_component() self.video.upload_transcript('english_single_transcript.srt')
self.outline.a11y_audit.config.set_scope( include=["div.video"] ) self.outline.a11y_audit.check_for_accessibility_errors()
for page in self.pages: page.visit()
ModeCreationPage( self.browser, self.course_id, mode_slug=u'verified', mode_display_name=u'Verified Certificate', min_price=10, suggested_prices='10,20' ).visit()
certificate.course_title = course_title_override
self.assertEqual(certificate.get_text('.action-primary'), "Create") certificate.click_create_certificate_button() self.assertIn(course_title_override, certificate.course_title) return certificate
certificate.click_edit_certificate_button() certificate.course_title = "Updated Course Title Override 2" self.assertEqual(certificate.get_text('.action-primary'), "Save") certificate.click_save_certificate_button()
certificate.click_delete_certificate_button() self.certificates_page.click_confirmation_prompt_primary_button()
self.certificates_page.visit() self.assertEqual(len(self.certificates_page.certificates), 0)
signatory = certificate.signatories[0] signatory.edit()
self.assertEqual(len(self.certificates_page.certificates), 1)
self.assertEqual(len(self.certificates_page.certificates), 1) course_number = self.certificates_page.get_course_number() self.assertEqual(self.course_info['number'], course_number)
self.assertEqual(len(self.certificates_page.certificates), 1)
self.advanced_settings_page.visit() self.advanced_settings_page.set_values(self.course_advanced_settings) self.advanced_settings_page.wait_for_ajax()
add_discussion(container, group_a_menu) container.duplicate(self.group_a_item_1_action_index)
drag(container, first_handle + 3, first_handle, 40) drag(container, first_handle + 2, first_handle, 40)
group_a_item_1_delete_index = 1 self.delete_and_verify(group_a_item_1_delete_index, expected_ordering)
if expected_labels != [self.VISIBILITY_LABEL_ALL]: expected_labels.append(self.VISIBILITY_LABEL_SPECIFIC) self.assertItemsEqual(expected_labels, [option.text for option in visibility_editor.selected_options])
visibility_editor = self.edit_component_visibility(component) for label in labels: visibility_editor.select_option(label, save=False) visibility_editor.save()
visibility_editor = self.edit_component_visibility(component) self.verify_selected_labels(visibility_editor, expected_labels) visibility_editor.save()
self._verify_components_visible(['problem']) self._verify_student_view_locked()
self._verify_components_visible(['discussion']) self._verify_student_view_visible(['discussion'])
unit = self.go_to_unit_page() test_block = unit.xblocks[1] title_on_unit_page = test_block.name container = test_block.go_to_container() self.assertEqual(container.name, title_on_unit_page)
lib_page = LibraryEditPage(self.browser, LibraryLocator(org, number)) lib_page.wait_for_page()
self.dashboard_page.visit() self.assertTrue(self.dashboard_page.has_library(name=name, org=org, number=number))
self.assertFalse(library_container.has_validation_not_configured_warning)
expected_text = "This component is out of date. The library has new content." library_block = self._get_library_xblock_wrapper(self.unit_page.xblocks[1])
#self.assertIn("3 matching components", library_block.author_content)
#self.assertIn("4 matching components", library_block.author_content)
self.assertFalse(library_container.has_validation_error) self.assertFalse(library_container.has_validation_warning)
self.assertFalse(library_container.has_validation_error) self.assertFalse(library_container.has_validation_warning)
self.assertFalse(library_container.has_validation_error) self.assertFalse(library_container.has_validation_warning)
self.library_fixture.create_xblock(self.library_fixture.library_location, XBlockFixtureDesc("html", "Html4"))
block.edit() block.reset_field_val("Display Name") block.save_settings() self.assertEqual(block.name, name_default)
upload_start_time = datetime.utcnow().replace(microsecond=0, second=0) self.import_page.upload_tarball(self.tarball_name) self.import_page.wait_for_upload()
upload_finish_time = datetime.utcnow().replace(microsecond=0, second=0)
course_outline_page = CourseOutlinePage( self.browser, self.course_org, self.course_number, self.course_run ) course_outline_page.visit() course_outline_page.wait_for_page()
self.dashboard_page.visit() self.assertTrue(self.dashboard_page.has_course( org=self.course_org, number=self.course_number, run=self.course_run ))
course_outline_page = CourseOutlinePage( self.browser, new_org, self.course_number, self.course_run ) course_outline_page.visit() course_outline_page.wait_for_page()
self.dashboard_page.visit() self.assertTrue(self.dashboard_page.has_course( org=new_org, number=self.course_number, run=self.course_run ))
outline_page = self.course_outline_page.visit() outline_page.q(css='.outline-item.outline-subsection.is-collapsed .ui-toggle-expansion').click() verify_ordering(self, outline_page, expected_ordering)
self.chap_1_handle = 0 self.chap_1_seq_1_handle = 1
self.seq_1_vert_1_handle = 2 self.seq_1_vert_2_handle = 3 self.chap_1_seq_2_handle = 4
course_outline_page.q(css='.outline-item.outline-subsection.is-collapsed .ui-toggle-expansion').first.click()
features = [
course_fixture.add_children(*[ self._build_fixture(self.UnitState(*state)) for state in itertools.product(*features) ])
self.assertTrue(subsection.release_date) self.assertFalse(subsection.due_date) self.assertFalse(subsection.policy)
modal.release_date = '3/12/1972' modal.release_time = '04:01' modal.due_date = '7/21/2014' modal.due_time = '23:39' modal.policy = 'Lab'
self.assertTrue(section.release_date) self.assertFalse(section.due_date) self.assertFalse(section.policy)
self.assertTrue(modal.has_release_date()) self.assertFalse(modal.has_due_date()) self.assertFalse(modal.has_policy())
self.assertEqual(modal.release_date, u'1/1/1970')
modal.release_date = '5/14/1969'
self.assertFalse(section.due_date) self.assertFalse(section.policy)
modal.policy = 'Lab' modal.save()
self.course_fixture.create_xblock( parent_vertical.locator, XBlockFixtureDesc(category='poll', display_name="", data=load_data_str('poll_markdown.xml')) ) self.course_outline_page.visit()
second_config.edit() second_config.name = "Updated Second Content Group" self.assertEqual(second_config.get_text('.action-primary'), "Save") second_config.save()
config.delete() self.assertEqual(len(self.group_configurations_page.content_groups), 0)
EmptyPromise( lambda: self.outline_page.is_browser_on_page(), "loaded page {!r}".format(self.outline_page), timeout=30 ).fulfill()
self.advanced_settings.visit() self.assertTrue(self.advanced_settings.is_browser_on_page())
course_display_name = self.advanced_settings.get('Course Display Name') self.advanced_settings.set('Course Display Name', 1) self.advanced_settings.wait_for_modal_load()
self.check_modal_shows_correct_contents(['Course Display Name']) self.advanced_settings.refresh_and_wait_for_load()
original_values_map = self.get_settings_fields_of_each_type() self.set_wrong_inputs_to_fields() self.advanced_settings.wait_for_modal_load()
self.check_modal_shows_correct_contents(self.type_fields) self.advanced_settings.refresh_and_wait_for_load()
original_values_map = self.get_settings_fields_of_each_type() self.set_wrong_inputs_to_fields()
self.advanced_settings.wait_for_modal_load()
self.advanced_settings.undo_changes_via_modal()
for key, val in original_values_map.iteritems(): self.assertEquals( self.advanced_settings.get(key), val, 'Undoing Should revert back to original value' )
self.assertFalse(self.advanced_settings.is_validation_modal_present())
self.assertTrue(self.advanced_settings.is_validation_modal_present())
error_item_names = self.advanced_settings.get_error_item_names() self.assertEqual(set(wrong_settings_list), set(error_item_names))
self.assertIn("Some Rights Reserved", self.lms_courseware.course_license)
self.settings_page.a11y_audit.config.set_rules({ "ignore": [
self.course_outline.a11y_audit.config.set_scope( include=['section.edit-settings-timed-examination'] ) self.course_outline.a11y_audit.check_for_accessibility_errors()
self.settings_page.visit()
self.settings_page.wait_for_ajax() self.settings_page.wait_for_jquery_value('input#course-name:text', 'test_run')
file_to_upload = 'image.jpg' self.settings_page.upload_image('#upload-course-image', file_to_upload) self.assertIn(file_to_upload, self.settings_page.get_uploaded_image_path('#course-image'))
file_to_upload = 'image.jpg' self.settings_page.upload_image('#upload-banner-image', file_to_upload) self.assertIn(file_to_upload, self.settings_page.get_uploaded_image_path('#banner-image'))
file_to_upload = 'image.jpg' self.settings_page.upload_image('#upload-video-thumbnail-image', file_to_upload) self.assertIn(file_to_upload, self.settings_page.get_uploaded_image_path('#video-thumbnail-image'))
return (len(active_groups) + len(inactive_groups) == len(container.xblocks) - 1, len(active_groups))
check_xblock_names(active_groups + inactive_groups, container.xblocks[1:]) if verify_missing_groups_not_present: self.verify_add_missing_groups_button_not_present(container)
wait_for_xblock_initialization(self, '.xblock[data-block-type="split_test"]')
container.add_missing_groups() self.verify_groups(container, ['alpha', 'gamma'], ['beta'])
container = self.go_to_nested_container_page() self.verify_groups(container, ['alpha', 'gamma'], ['beta'])
container.delete(0) self.verify_groups(container, ['alpha'], [], verify_missing_groups_not_present=False)
self.assertTrue(config.id)
config.toggle()
config.toggle()
self.page.visit() config = self.page.experiment_group_configurations[0]
if publish: unit.publish_action.click() unit.view_published_version() self.assertEqual(len(self.browser.window_handles), 2) courseware_page.wait_for_page()
self.assertEqual(config.get_text('.action-primary'), "Create") self.assertFalse(config.delete_button_is_present) config.save()
config.groups[1].remove() config.groups[0].name = "First Group" config.save()
split_test = self._add_split_test_to_vertical(number=0, group_configuration_metadata={'user_partition_id': 0})
config.groups[2].name = "Second Group"
config.groups[0].remove() config.save()
container.add_missing_groups() self.verify_groups(container, ['Group B', 'Second Group', 'Group D'], ['Group ID 0'])
self.page.create_experiment_group_configuration()
config.cancel()
config.cancel()
config.save() self.assertEqual(config.mode, 'edit') self.assertEqual(message, config.validation_message)
self.page.create_experiment_group_configuration() config = self.page.experiment_group_configurations[0] config.description = "Description of the group configuration."
config.save()
self.page.visit() config = self.page.experiment_group_configurations[0] config.toggle() config.click_outline_anchor()
EmptyPromise( lambda: self.outline_page.is_browser_on_page(), "loaded page {!r}".format(self.outline_page), timeout=30 ).fulfill()
self.page.visit() config = self.page.experiment_group_configurations[0] config.toggle() usage = config.usages[0] config.click_unit_anchor()
EmptyPromise( lambda: unit.is_browser_on_page(), "loaded page {!r}".format(unit), timeout=30 ).fulfill()
config.delete() self.assertEqual(len(self.page.experiment_group_configurations), 1)
config.delete() self.assertEqual(len(self.page.experiment_group_configurations), 0)
self.assertFalse(self.page.experiment_group_configurations[0].is_expanded) self.assertTrue(self.page.experiment_group_configurations[1].is_expanded)
config, _ = self.create_group_configuration_experiment([Group("0", "Group A"), Group("1", "Group B")], True)
config.toggle() self.assertFalse(config.details_error_icon_is_present) self.assertFalse(config.details_message_is_present)
config.toggle() config.edit() config.add_group() config.save()
config.toggle() self.assertFalse(config.details_warning_icon_is_present) self.assertFalse(config.details_message_is_present)
config.toggle() config.edit() config.groups[2].remove() config.save()
courseware_page = CoursewarePage(self.browser, self.course_id) self.publish_unit_and_verify_groups_in_lms(courseware_page, [u'Group A', u'Group B', u'Group C'])
self.publish_unit_and_verify_groups_in_lms( courseware_page, [u'Group A', u'Group B', u'Group ID 2 (inactive)'], publish=False )
container.visit() container.delete(0)
self.publish_unit_and_verify_groups_in_lms(courseware_page, [u'Group A', u'Group B'])
add_component(self.lib_page, "html", "Text") self.assertEqual(len(self.lib_page.xblocks), 1) first_block_id = self.lib_page.xblocks[0].locator
self.lib_page.click_delete_button(first_block_id, confirm=True) self.assertEqual(len(self.lib_page.xblocks), 1) self.assertEqual(self.lib_page.xblocks[0].locator, second_block_id)
self.assertEqual(len(self.lib_page.xblocks), 1) problem_block = self.lib_page.xblocks[0] self.assertIn("Laura Roslin", problem_block.author_content)
AutoAuthPage(self.browser, username="second", email="second@example.com", no_login=True).visit()
lib_page.a11y_audit.config.set_rules({ "ignore": [
super(ContainerBase, self).setUp(is_staff=is_staff)
container = unit.xblocks[1].go_to_container() return container
container = self.go_to_nested_container_page() verify_ordering(self, container, expected_ordering)
self.settings_detail.visit() self.assertTrue(self.settings_detail.is_browser_on_page())
self.settings_detail.refresh_page() self.settings_detail.wait_for_prerequisite_course_options() self.assertTrue(is_option_value_selected( browser_query=self.settings_detail.pre_requisite_course_options, value=pre_requisite_course_id ))
self.settings_detail.refresh_page() self.settings_detail.wait_for_prerequisite_course_options() self.assertTrue(is_option_value_selected( browser_query=self.settings_detail.pre_requisite_course_options, value='' ))
select_option_by_value( browser_query=self.settings_detail.pre_requisite_course_options, value=pre_requisite_course_id ) self.settings_detail.save_changes() self.assertEqual( 'Your changes have been saved.', self.settings_detail.alert_confirmation_title.text )
self.settings_detail.refresh_page() self.settings_detail.wait_for_prerequisite_course_options() dropdown_status = is_option_value_selected( browser_query=self.settings_detail.pre_requisite_course_options, value=pre_requisite_course_id ) self.assertTrue(dropdown_status)
course_outline_page = CourseOutlinePage( self.browser, self.course_info['org'], self.course_info['number'], self.course_info['run'] ) course_outline_page.visit()
self.assertTrue(element_has_text( page=course_outline_page, css_selector='span.section-title', text='Entrance Exam' ))
self.settings_detail.visit() self.settings_detail.require_entrance_exam(required=False) self.settings_detail.save_changes()
self.assertTrue(element_has_text( page=course_outline_page, css_selector='.add-item a.button-new', text='New Unit' ))
self.assertFalse(element_has_text( page=course_outline_page, css_selector='.add-item a.button-new', text='New Subsection' ))
self.course_fixture.add_course_details({'start_date': datetime.now() + timedelta(days=1)})
super(XBlockAcidBase, self).setUp()
_ = lambda text: text
if (('python2.7/site-packages/gunicorn/workers/sync.py' in exc_str) and ('[Errno 11] Resource temporarily unavailable' in exc_str)): exc_str = ''
from crum import get_current_request
Score = namedtuple("Score", "earned possible graded section module_id")
NOT_CONFIGURED = "not-configured"
class_priority = ['video', 'problem']
DEPRECATION_VSCOMPAT_EVENT = 'deprecation.vscompat'
STUDENT_VIEW = 'student_view'
AUTHOR_VIEW = 'author_view'
STUDIO_VIEW = 'studio_view'
PREVIEW_VIEWS = [STUDENT_VIEW, AUTHOR_VIEW]
coffee = cls.js.setdefault('coffee', []) js = cls.js.setdefault('js', [])
cls.js.setdefault('xmodule_js', resource_string(__name__, 'js/src/xmodule.js'))
default=None
has_score = False
show_in_read_only_mode = False
self.save()
if usage_id_filter is None and usage_key_filter is not None: usage_id_filter = usage_key_filter
if self.scope_ids.user_id is not None and user_id == self.scope_ids.user_id: if getattr(xmodule_runtime, 'position', None):
self.save()
self.scope_ids = self.scope_ids._replace(user_id=user_id)
self.clear_child_cache()
for field in self.fields.values(): if field.scope in (Scope.parent, Scope.children): continue
if field in self._dirty_fields: del self._dirty_fields[field]
self.xmodule_runtime = xmodule_runtime
return [XBlock.tags, XBlock.name]
fields = getattr(self, 'unmixed_class', self.__class__).fields
self.descriptor = descriptor self._runtime = None super(XModule, self).__init__(*args, **kwargs) self.runtime.xmodule_instance = self
child_descriptor = self.descriptor.get_child(usage_id) child_block = None if child_descriptor is not None: child_block = self.system.get_module(child_descriptor)
metadata_translations = { 'slug': 'url_name', 'name': 'display_name', }
self.previous_version = self.update_version = self.definition_locator = None self.xmodule_runtime = None
xml = etree.tostring(node) block = cls.from_xml(xml, runtime, id_generator) return block
self.xmodule_runtime.xmodule_instance = None
self.system.error_tracker(msg) return 'Oops, couldn't load grommet'
self.export_fs = None
return descriptor_global_local_resource_url(block, uri)
pass
service = super(DescriptorSystem, self).service(block=block, service_name=service_name) if callable(service): return service(block) return service
node.attrib.pop('xblock-family', None)
xblock_family = child.attrib.pop('xblock-family', None) if xblock_family: xblock_family = self._family_id_to_superclass(xblock_family) if issubclass(xblock_family, XBlockAside): aside_children.append(child)
kwargs.pop('_view_name')
service = super(ModuleSystem, self).service(block=block, service_name=service_name) if callable(service): return service(block) return service
try: return getattr(self._module_system, name) except AttributeError: return getattr(self._descriptor_system, name)
return u''.join(filter(None, parts))
(n, d) = a.frac() (n2, d2) = b.frac() return Progress(n + n2, d + d2)
return course.display_name_with_default.replace('<', '&lt;').replace('>', '&gt;')
return _('TBD')
announcement, start, now = sorting_dates(start, advertised_start, announcement)
if self.transcript_language == 'en': return Transcript.asset(self.location, youtube_id).data
if self.transcript_language == 'en':
if not self.transcript_language == 'en': return response
if not isinstance(self.course_id, CourseLocator): return response
return self.get_static_transcript(request, transcripts)
_ = lambda text: text
module = __name__.replace('.video_module', '', 2)
sorted_languages = sorted(languages.items(), key=itemgetter(1))
cdn_url = getattr(settings, 'VIDEO_CDN_URL', {}).get(self.system.user_location)
if self.edx_video_id and edxval_api: try: val_profiles = ["youtube", "desktop_webm", "desktop_mp4"] val_video_urls = edxval_api.get_urls_for_profiles(self.edx_video_id, val_profiles)
if val_video_urls["youtube"]: youtube_streams = "1.00:{}".format(val_video_urls["youtube"])
if getattr(self, 'video_speed_optimizations', True) and cdn_url: branding_info = BrandingInfoConfig.get_config().get(self.system.user_location)
if not download_video_link and self.download_video: if self.source: download_video_link = self.source elif self.html5_sources: download_video_link = self.html5_sources[0]
'captionDataDir': getattr(self, 'data_dir', None),
'recordedYoutubeIsAvailable': self.youtube_is_available,
if self.data: field_data = self._parse_video_xml(etree.fromstring(self.data)) self._field_data.set_many(self, field_data) del self.data
if self.source in self.html5_sources:
if not self.fields['download_video'].is_set_on(self): self.download_video = self.download_video self.force_save_fields(['download_video'])
if not self.fields['download_track'].is_set_on(self) and self.track: self.download_track = True
ScopeIds(None, block_type, definition_id, usage_id), field_data,
self.add_license_to_xml(xml)
if self.edx_video_id and edxval_api: val_youtube_id = edxval_api.get_url_for_profile(self.edx_video_id, "youtube") if val_youtube_id: video_id = val_youtube_id
youtube_id = deserialize_field(cls.youtube_id_1_0, pieces[1]) ret[speed] = youtube_id
conversions = { }
field_data[attr] = deserialize_field(cls.fields[attr], value)
if 'download_video' not in field_data and sources: field_data['source'] = field_data['html5_sources'][0]
if 'download_track' not in field_data and track is not None: field_data['download_track'] = True
edxval_api.import_from_xml( video_asset_elem, field_data['edx_video_id'], course_id=course_id )
field_data = LicenseMixin.parse_license_from_xml(field_data, xml)
if self.transcripts: for language in self.transcripts.keys(): _update_transcript_for_index(language)
if self.only_on_web: return {"only_on_web": True}
if self.edx_video_id: video_profile_names = context.get("profiles", ["mobile_low"])
val_course_data = self.get_cached_val_data_for_course(video_profile_names, self.location.course_key) val_video_data = val_course_data.get(self.edx_video_id, {})
if val_video_data: encoded_videos = val_video_data.get('profiles', {})
if not encoded_videos: video_url = self.html5_sources[0] if self.html5_sources else self.source if video_url: encoded_videos["fallback"] = { "url": video_url,
if self.youtube_id_1_0: encoded_videos["youtube"] = { "url": self.create_youtube_url(self.youtube_id_1_0),
_ = lambda text: text
source = String( help=_("The external URL to download the video."), display_name=_("Download Video"), scope=Scope.settings, default="" ) download_video = Boolean(
transcripts = Dict(
(it is done to allow user to enter both /static/filename.srt and filename.srt)
if generate_translation: for lang, filename in item.transcripts.items(): item.transcripts[lang] = os.path.split(filename)[-1]
if generate_translation: old_langs = set(old_metadata.get('transcripts', {})) if old_metadata else set() new_langs = set(item.transcripts)
item.transcripts.pop(lang) reraised_message += ' ' + ex.message
generate_subs_from_source( result_subs_dict, os.path.splitext(user_filename)[1][1:], srt_transcripts.data.decode('utf-8-sig'), item, lang )
if not verify_assets: if other_langs: translations = list(other_langs) if not translations or sub: translations += ['en'] return translations
for lang, transcript_url in bumper_settings.get('transcripts', {}).items(): bumper_settings['transcripts'][lang] = transcript_url.replace("/static/", "")
log.warning( "Could not retrieve information from VAL for Bumper edx Video ID: %s.", video.bumper['edx_video_id'] ) return []
rewritten_url = cdn_base_url.rstrip("/") + "/" + parsed.path.lstrip("/") validator = URLValidator()
return None
if user is not None and password is not None: mongo_conn.authenticate(user, password)
_ = lambda text: text
type = ''
title = None
is_hideable = False
is_hidden = False
priority = None
is_movable = True
is_collection = False
is_dynamic = False
is_default = True
allow_multiple = False
view_name = None
return False
name_is_eq = (other.get('name') is None or self.name == other['name'])
return self.type == other.get('type') and name_is_eq
if hasattr(course, 'syllabus_present') and course.syllabus_present: course.tabs.append(CourseTab.load('syllabus'))
if course.discussion_link: discussion_tab = CourseTab.load( 'external_discussion', name=_('External Discussion'), link=course.discussion_link ) else: discussion_tab = CourseTab.load('discussion')
if course.discussion_link: return CourseTab.load( 'external_discussion', name=_('External Discussion'), link=course.discussion_link )
for tab in course.tabs: if tab.type == 'discussion' or tab.type == 'external_discussion': return tab return None
if inline_collections: for item in tab.items(course): yield item elif len(list(tab.items(course))) > 0: yield tab
_ = lambda text: text
default=_("Text")
if self.system.anonymous_student_id: return self.data.replace("%%USER_ID%%", self.system.anonymous_student_id) return self.data
new_candidates = [] for candidate in candidates: if candidate.endswith('.xml'): new_candidates.append(candidate[:-4] + '.html') return candidates + new_candidates
_context.update({ 'base_asset_url': StaticContent.get_base_url_path_for_course_assets(self.location.course_key), 'enable_latex_compiler': self.use_latex_compiler, 'editor': self.editor }) return _context
for candidate in candidates: if system.resources_fs.exists(candidate): filepath = candidate break
raise Exception(msg), None, sys.exc_info()[2]
pathname = name_to_pathname(self.url_name) filepath = u'{category}/{pathname}.html'.format( category=self.category, pathname=pathname )
relname = path(pathname).basename()
STATUS_VISIBLE = 'visible' STATUS_DELETED = 'deleted' TEMPLATE_DIR = 'courseware'
dog_stats_api.increment( DEPRECATION_VSCOMPAT_EVENT, tags=["location:customtag_descriptor_render_template"] )
template_loc = self.location.replace(category='custom_tag_template', name=template_name)
VERSION = 1
USER_PARTITION_SCHEME_NAMESPACE = 'openedx.user_partition_scheme'
scheme_extensions = None
VERSION_1_SCHEME = "random"
scheme_id = UserPartition.VERSION_1_SCHEME
elif value["version"] >= 2: if "scheme" not in value: raise TypeError("UserPartition dict {0} missing value key 'scheme'".format(value))
self.addCleanup(self.cleanup_scheme_extensions)
self.user_partition = UserPartition( self.TEST_ID, self.TEST_NAME, self.TEST_DESCRIPTION, self.TEST_GROUPS, extensions[0].plugin, self.TEST_PARAMETERS, )
self.user_partition.get_scheme(self.non_random_scheme.name) self.user_partition.get_scheme(self.random_scheme.name)
user_id = abs(hash(username))
user_partition_id = self.user_partition.id groups = self.user_partition.groups self.user_partition.scheme.current_group = groups[0]
group1_id = self.partition_service.get_user_group_id_for_partition(user_partition_id) self.assertEqual(group1_id, groups[0].id)
self.user_partition.scheme.current_group = groups[1] group2_id = self.partition_service.get_user_group_id_for_partition(user_partition_id) self.assertEqual(group2_id, groups[1].id)
ps_shared_cache_1 = self._create_service(username, shared_cache) ps_shared_cache_2 = self._create_service(username, shared_cache)
ps_diff_cache = self._create_service(username, {})
ps_uncached = self._create_service(username)
first_group = self.user_partition.groups[0] self.user_partition.scheme.current_group = first_group
for part_svc in [ps_shared_cache_1, ps_diff_cache, ps_uncached]: self.assertEqual( first_group.id, part_svc.get_user_group_id_for_partition(user_partition_id) )
second_group = self.user_partition.groups[1] self.user_partition.scheme.current_group = second_group
for part_svc in [ps_shared_cache_1, ps_shared_cache_2, ps_diff_cache]: self.assertEqual( first_group.id, part_svc.get_user_group_id_for_partition(user_partition_id) )
self.assertEqual( second_group.id, ps_uncached.get_user_group_id_for_partition(user_partition_id) )
ps_new_cache = self._create_service(username, {}) self.assertEqual( second_group.id, ps_new_cache.get_user_group_id_for_partition(user_partition_id) )
self.user_partition.scheme.current_group = groups[0] group1 = self.partition_service.get_group(self.user_partition) self.assertEqual(group1, groups[0])
self.user_partition.scheme.current_group = groups[1] group2 = self.partition_service.get_group(self.user_partition) self.assertEqual(group2, groups[1])
_ = lambda text: text
user_partition_values = [] no_partition_selected = {'display_name': _("Not Selected"), 'value': -1}
user_partitions = UserPartitionList( help=_("The list of group configurations for partitioning students in content experiments."), default=[], scope=Scope.settings )
group_id_to_child = ReferenceValueDict( help=_("Which child module students in a particular group_id should see"), scope=Scope.content )
log.debug("configuration error in split test module: no such child") return []
sorted_active_contents = sorted(active_contents, key=itemgetter('group_name')) sorted_inactive_contents = sorted(inactive_contents, key=itemgetter('group_name'))
return Fragment(content=u"<div>Nothing here. Move along.</div>")
module_class = SplitTestModule
if 'user_partition_id' not in old_content or old_content['user_partition_id'] != self.user_partition_id: selected_partition = self.get_selected_partition() if selected_partition is not None:
else: for str_group_id, usage_key in self.group_id_to_child.items():
SplitTestFields.build_partition_values(self.user_partitions, self.get_selected_partition())
editable_fields[SplitTestFields.user_partition_id.name] = self._create_metadata_editor_info( SplitTestFields.user_partition_id )
inactive_children = [child for child in children if child not in active_children]
self.system.modulestore.update_item(self, None)
_ = lambda text: text
last_el = self.table_of_contents[-1] while last_el.getchildren(): last_el = last_el[-1]
log.exception("Couldn't load textbook ({0}, {1})".format(title, book_url)) continue
default="images_course_image.jpg"
default="images_course_image.jpg"
default="images_course_image.jpg"
display_name=_("Certificate Web/HTML View Overrides"), help=_("Enter course-specific overrides for the Web/HTML template parameters here (JSON format)"), scope=Scope.settings,
certificates = Dict( display_name=_("Certificate Configuration"), help=_("Enter course-specific configuration information here (JSON format)"), scope=Scope.settings, )
if self.system.resources_fs is None: self.syllabus_present = False else: self.syllabus_present = self.system.resources_fs.exists(path('syllabus'))
grading_policy.update(course_policy)
policy_str = '{}'
break
course_file = StringIO(xml_data.encode('ascii', 'ignore')) xml_obj = etree.parse(course_file, parser=edx_xml_parser).getroot()
paths = ['grading_policy.json'] if policy_dir: paths = [policy_dir + '/grading_policy.json'] + paths
instance.set_grading_policy(policy)
wiki_slug = None wiki_tag = xml_object.find("wiki") if wiki_tag is not None: wiki_slug = wiki_tag.attrib.get("slug", default=None) xml_object.remove(wiki_tag)
definition = LicenseMixin.parse_license_from_xml(definition, xml_object)
self.add_license_to_xml(xml_object, default="all-rights-reserved")
self.grading_policy['GRADER'] return self._grading_policy['RAW_GRADER']
self._grading_policy['RAW_GRADER'] = value self.grading_policy['GRADER'] = value
policy = self.grading_policy policy['GRADE_CUTOFFS'] = value self.grading_policy = policy
try: module = getattr(self, '_xmodule', None) if not module: module = self except UndefinedContext: module = self
section_description = { 'section_descriptor': section, 'xmoduledescriptors': [child for child in xmoduledescriptors if child.has_score] }
CLASS_PRIORITY = ['video', 'problem']
for child in self.get_display_items(): rendered_child = child.render(STUDENT_VIEW, child_context) fragment.add_frag_resources(rendered_child)
_ = lambda text: text
_ = lambda text: text
valid_block_keys = set([(c.block_type, c.block_id) for c in children]) invalid_block_keys = (selected - valid_block_keys) if invalid_block_keys: selected -= invalid_block_keys
overlimit_block_keys = set() while len(selected) > max_count: overlimit_block_keys.add(selected.pop())
num_to_add = max_count - len(selected)
publish_event( "removed", result=format_block_keys(block_keys['selected']), removed=format_block_keys(block_keys['invalid']), reason="invalid" )
selected = block_keys['selected']
fragment.add_javascript_url(self.runtime.local_resource_url(self, 'public/js/library_content_edit.js')) fragment.initialize_js('LibraryContentAuthorView') return fragment
non_editable_fields.extend([LibraryContentFields.mode, LibraryContentFields.source_library_version]) return non_editable_fields
user_id = user_service.get_current_user().opt_attrs.get('edx-platform.user_id', None)
return True
_ = lambda text: text
display_name = String(help="Display name for this module", scope=Scope.settings)
answers = List(help="Poll answers from xml", scope=Scope.content, default=[])
temp_poll_answers = self.poll_answers temp_poll_answers[dispatch] += 1 self.poll_answers = temp_poll_answers
temp_poll_answers = self.poll_answers temp_poll_answers[self.poll_answer] -= 1 self.poll_answers = temp_poll_answers
if self.poll_answers is None: self.poll_answers = {}
temp_poll_answers = self.poll_answers
if len(xml_object.xpath(cls._child_tag_name)) == 0: raise ValueError("Poll_question definition must include \ at least one 'answer' tag")
_ = lambda text: text
metadata_translations = dict(RawDescriptor.metadata_translations) metadata_translations['id'] = 'discussion_id' metadata_translations['for'] = 'discussion_target'
non_editable_fields.extend([DiscussionDescriptor.discussion_id, DiscussionDescriptor.sort_key]) return non_editable_fields
_ = lambda text: text
for field in InheritanceMixin.fields.values(): if field.is_set_on(descriptor): parent_metadata[field.name] = field.read_json(descriptor)
self._fields[key.field_name] = value
EXCLUDE_ALL = '*'
self.modules = defaultdict(dict) self.definitions = {} self.definitions_in_db = set() self.course_key = None
if self.index is None: return []
if self.initial_index is None: return self.index.get('versions', {}).keys()
if course_key is None: return self._bulk_ops_record_type()
if course_key.org is None or course_key.course is None or course_key.run is None: return self._active_bulk_ops.records[ course_key.replace(org=None, course=None, run=None, branch=None) ]
return super(SplitBulkWriteMixin, self)._get_bulk_ops_record( course_key.replace(branch=None, version_guid=None), ignore_case )
bulk_write_record.index = copy.deepcopy(bulk_write_record.initial_index) bulk_write_record.course_key = course_key
for _id in bulk_write_record.structures.viewkeys() - bulk_write_record.structures_in_db: dirty = True
if structure is None: structure = self.db_connection.get_structure(version_guid, course_key) bulk_write_record.structures[version_guid] = structure if structure is not None: bulk_write_record.structures_in_db.add(version_guid)
version_guid = course_key.as_object_id(version_guid) return self.db_connection.get_structure(version_guid, course_key)
if definition is None: definition = self.db_connection.get_definition(definition_guid, course_key) bulk_write_record.definitions[definition_guid] = definition if definition is not None: bulk_write_record.definitions_in_db.add(definition_guid)
definition_guid = course_key.as_object_id(definition_guid) return self.db_connection.get_definition(definition_guid, course_key)
for definition in bulk_write_record.definitions.values(): definition_id = definition.get('_id') if definition_id in ids: ids.remove(definition_id) definitions.append(definition)
if bulk_write_record.active and course_key.branch in bulk_write_record.dirty_branches: return bulk_write_record.structure_for_branch(course_key.branch)
if bulk_write_record.active: bulk_write_record.set_structure_for_branch(course_key.branch, new_structure)
for _, record in self._active_records: if branch and branch not in record.index.get('versions', {}): continue
if org_target: if record.index['org'] != org_target: continue
super(SplitMongoModuleStore, self)._drop_database(database, collections, connections)
if not lazy: descendent_definitions = self.get_definitions( course_key, [ block.definition for block in new_module_data.itervalues() ] ) definitions = {definition['_id']: definition for definition in descendent_definitions}
block.fields.update(definition.get('fields')) block.definition_loaded = True
index = self.get_course_index(course_key)
raise VersionConflictError(course_key, version_guid)
version_guids = [] id_version_map = defaultdict(list) for course_index in matching_indexes: version_guid = course_index['versions'][branch] version_guids.append(version_guid) id_version_map[version_guid].append(course_index) return version_guids, id_version_map
return self._get_structures_for_branch_and_locator(branch, self._create_course_locator, **kwargs)
raise ItemNotFoundError(course_id)
raise ItemNotFoundError(library_id)
return False
return False
return False
raise ItemNotFoundError(usage_key)
return []
if 'children' in qualifiers: settings['children'] = qualifiers.pop('children')
path_cache = None parents_cache = None
if path_cache is not None: path_cache[block_key] = True
raise ItemNotFoundError(locator)
parent_ids = [ valid_parent for valid_parent in all_parent_ids if self.has_path_to_root(valid_parent, course) ]
parent_ids.sort(key=lambda parent: (parent.type, parent.id)) return BlockUsageLocator.make_relative( locator, block_type=parent_ids[0].type, block_id=parent_ids[0].id, )
raise ItemNotFoundError(course_key)
raise ItemNotFoundError(course_key)
raise ItemNotFoundError(course_key)
raise ItemNotFoundError(definition_locator)
raise ItemNotFoundError(course_locator)
old_definition = self.get_definition(course_key, definition_locator.definition_id) if old_definition is None: raise ItemNotFoundError(definition_locator)
index_entry = self._get_index_if_valid(course_key, force) structure = self._lookup_course(course_key).structure
new_structure = self.version_structure(course_key, structure, user_id)
return self.get_item(item_loc)
new_structure = self._lookup_course(xblock.location.course_key).structure
block_id = BlockKey.from_usage_key(parent_usage_key) if block_id not in new_structure['blocks']: raise ItemNotFoundError(parent_usage_key)
self.version_block(parent, user_id, new_structure['_id'])
self.update_structure(parent_usage_key.course_key, new_structure)
return xblock
super(SplitMongoModuleStore, self).clone_course(source_course_id, dest_course_id, user_id, fields, **kwargs) return new_course
if versions_dict is None or master_branch not in versions_dict: definition_id = self.create_definition_from_data(locator, definition_fields, root_category, user_id).definition_id
settings = partitioned_fields[Scope.settings] settings = self._serialize_fields(block_key.type, settings) if not is_updated: is_updated = self._compare_settings(settings, original_entry.fields)
if is_updated or asides_updated: new_structure = self.version_structure(course_key, original_structure, user_id) block_data = self._get_block_from_structure(new_structure, block_key)
block_data.edit_info.source_version = None
new_locator = course_key.make_usage_key(block_key.type, block_key.id) return self.get_item(new_locator, **kwargs)
inherited_settings = {}
parent_xblock.save()
if index_entry is not None: self._update_head(course_key, index_entry, xblock.location.branch, new_id)
return self.get_item(xblock.location.for_version(new_id))
with self.bulk_operations(source_course): source_structure = self._lookup_course(source_course).structure
raise ItemNotFoundError(destination_course)
self.update_structure(destination_course, destination_structure) self._update_head(destination_course, index_entry, destination_course.branch, destination_structure['_id'])
dest_info = dest_structure['blocks'][block_key]
dest_info.edit_info.previous_version = dest_info.edit_info.update_version dest_info.edit_info.update_version = old_dest_structure_version dest_info.edit_info.edited_by = user_id dest_info.edit_info.edited_on = datetime.datetime.now(UTC)
return [ destination_course.make_usage_key(*k) for k in dest_structure['blocks'][block_key].fields['children'] ]
new_block_info = copy.deepcopy(source_block_info) existing_block_info = dest_structure['blocks'].get(new_block_key, BlockData()) new_block_info.defaults = new_block_info.fields
new_children.append(new_block_key)
dest_structure['blocks'][new_parent_block_key].fields['children'] = new_children
raise ItemNotFoundError(usage_locator)
parent_block.edit_info.source_version = None self.decache_block(usage_locator.course_key, new_id, parent_block_key)
self.update_structure(usage_locator.course_key, new_structure)
self._update_head(usage_locator.course_key, index_entry, usage_locator.branch, new_id) result = usage_locator.course_key.for_version(new_id)
if parents.issubset(to_delete): next_tier.add(child_block_key)
log.info(u"deleting course from split-mongo: %s", course_key) self.delete_course_index(course_key)
inherited_settings_map.setdefault(block_key, {}).update(inheriting_settings)
inheriting_settings = inherited_settings_map[block_key].copy() block_fields = block_data.fields for field_name in inheritance.InheritanceMixin.fields: if field_name in block_fields: inheriting_settings[field_name] = block_fields[field_name]
pass
all_assets.extend(course_assets.setdefault(asset_type, [])) asset_idx = all_assets.find(asset_key)
self.update_structure(asset_key.course_key, new_structure)
self._update_head(asset_key.course_key, index_entry, asset_key.branch, new_structure['_id'])
asset_key = asset_metadata_list[0].asset_id course_key = asset_key.course_key
self.update_structure(course_key, new_structure)
self._update_head(course_key, index_entry, asset_key.branch, new_structure['_id'])
mdata = AssetMetadata(asset_key, asset_key.path) mdata.from_storable(all_assets[asset_idx]) mdata.update(attr_dict)
all_assets[asset_idx] = mdata.to_storable() return all_assets
self.update_structure(dest_course_key, new_structure)
self._update_head(dest_course_key, index_entry, dest_course_key.branch, new_structure['_id'])
self._update_head(course_locator, index_entry, course_locator.branch, new_structure['_id'])
if isinstance(block_key, BlockUsageLocator): return block_key.map_into_course(course_key) elif not isinstance(block_key, BlockKey): block_key = BlockKey(*block_key)
result = defaultdict(dict) for field in xblock.fields.itervalues(): if field.is_set_on(xblock): result[field.scope][field.name] = field.read_from(xblock) return result
if isinstance(reference, basestring): reference = BlockUsageLocator.from_string(reference) elif isinstance(reference, BlockKey): return reference return BlockKey.from_usage_key(reference)
for key, val in new_block.edit_info.to_storable().iteritems(): if getattr(destination_block.edit_info, key) is None: setattr(destination_block.edit_info, key, val)
destination_block.edit_info.source_version = ( new_block.edit_info.source_version or new_block.edit_info.update_version )
self._auto_publish_no_children(item.location, item.location.category, user_id, **kwargs)
self.publish(location.version_agnostic(), user_id, blacklist=EXCLUDE_ALL, **kwargs)
self._auto_publish_no_children(item.location, item.location.category, user_id, **kwargs) self._auto_publish_no_children(parent_usage_key, item.location.category, user_id, **kwargs) return item
draft_branch = ModuleStoreEnum.BranchName.library published_branch = ModuleStoreEnum.BranchName.library
if self._get_version(draft_block) != self._get_version(published_block): return True
if 'children' in draft_block.fields: return any( [has_changes_subtree(child_block_id) for child_block_id in draft_block.fields['children']] )
draft_course_structure = self._lookup_course(draft_course_key).structure new_structure = self.version_structure(draft_course_key, draft_course_structure, user_id)
self._remove_subtree(BlockKey.from_usage_key(location), new_structure['blocks'])
pass
return None
if block_type == 'course': block_id = self.DEFAULT_ROOT_COURSE_BLOCK_ID elif block_type == 'library': block_id = self.DEFAULT_ROOT_LIBRARY_BLOCK_ID new_usage_key = course_key.make_usage_key(block_type, block_id)
partitioned_fields = self.partition_fields_by_scope(block_type, fields)
xblock._published_by = published_block.edit_info.edited_by xblock._published_on = published_block.edit_info.edited_on
tagger.sample_rate = 1 return None
compressed_pickled_data = zlib.compress(pickled_data, 1) tagger.measure('compressed_size', len(compressed_pickled_data))
self.cache.set(key, compressed_pickled_data, None)
kwargs['w'] = 1
tagger_get_structure.sample_rate = 1
if 'last_update' in from_index: query['last_update'] = from_index['last_update']
self.course_id = course_entry.course_key self.lazy = lazy self.module_data = module_data self.default_class = default_class self.local_modules = {} self._services['library_tools'] = LibraryToolsService(modulestore)
if isinstance(usage_key, BlockUsageLocator):
course_key = usage_key.course_key
cached_module = self.modulestore.get_cached_block(course_key, version_guid, block_key) if cached_module: return cached_module
self.modulestore.cache_items(self, [block_key], course_key, lazy=self.lazy) json_data = self.module_data.get(block_key) if json_data is None: raise ItemNotFoundError(block_key)
self.course_entry = CourseEnvelope(course_entry_override.course_key, self.course_entry.structure)
if block_key is None: block_key = BlockKey(block_data.block_type, LocalId())
if definition_id is None: definition_id = LocalId()
block_locator = course_key.make_usage_key( block_type=block_key.type, block_id=block_key.id, )
try: if block_data.asides: aside_fields = {block_key.type: {}} for aside in block_data.asides: aside_fields[block_key.type].update(aside['fields']) except AttributeError: pass
module.save()
if isinstance(block_locator.block_id, LocalId): self.local_modules[block_locator] = module
max_date = block_data.edit_info.edited_on max_date_by = block_data.edit_info.edited_by
return self._cds.local_modules[usage_id].scope_ids.def_id
SplitMongoKVSid = namedtuple('SplitMongoKVSid', 'id, def_id') new_contract('BlockUsageLocator', BlockUsageLocator)
super(SplitMongoKVS, self).__init__(copy.deepcopy(initial_values))
if field_decorator is None: self.field_decorator = lambda x: x else: self.field_decorator = field_decorator
self._load_definition() if key.block_scope_id.block_type not in self.aside_fields: raise KeyError()
if key.field_name not in aside_fields: self._load_definition()
return self.field_decorator(field_value)
if key.scope not in self.VALID_SCOPES: raise InvalidScopeError(key, self.VALID_SCOPES) if key.scope == Scope.content: self._load_definition()
self._fields[key.field_name] = value
if key.scope not in self.VALID_SCOPES: raise InvalidScopeError(key, self.VALID_SCOPES) if key.scope == Scope.content: self._load_definition()
if key.field_name in self._fields: del self._fields[key.field_name]
if key.scope not in self.VALID_SCOPES: return False
return key.field_name in self._fields
return super(SplitMongoKVS, self).default(key)
DIRECT_ONLY_CATEGORIES = ['course', 'chapter', 'sequential', 'about', 'static_tab', 'course_info']
self.thread_cache = threading.local()
thread_local_branch_setting = getattr(self.thread_cache, 'branch_setting', None) if thread_local_branch_setting: return thread_local_branch_setting else: return self.default_branch_setting_func()
self.signal_handler.send("course_published", course_key=course_key.for_branch(None))
original_course = self.source_modulestore.get_course(source_course_key, **kwargs) if original_course is None: raise ItemNotFoundError(unicode(source_course_key))
self._add_draft_modules_to_course(new_course.location, source_course_key, user_id, **kwargs)
self.split_modulestore.fix_not_found(course_version_locator, user_id)
if any(new_locator.block_id == child.block_id for child in new_parent.children): continue new_parent_cursor = 0 for old_child_loc in old_parent.children: if old_child_loc.block_id == draft_location.block_id:
for idx in range(new_parent_cursor, len(new_parent.children)): if new_parent.children[idx].block_id == old_child_loc.block_id: new_parent_cursor = idx + 1
draft_node_list = []
parent_url = None if parent_loc is not None: parent_url = parent_loc.to_deprecated_string()
if not hasattr(draft_node.module, 'xml_attributes'): draft_node.module.xml_attributes = {}
if draft_node.parent_location is None: continue
xml_centric_courselike_key = self.get_key() adapt_references(courselike, xml_centric_courselike_key, export_fs) courselike.add_xml_to_node(root)
self.process_root(root, export_fs)
root_courselike_dir = self.root_dir + '/' + self.target_dir self.process_extra(root, courselike, root_courselike_dir, xml_centric_courselike_key, export_fs)
self.post_process(root, export_fs)
export_extra_content( export_fs, self.modulestore, self.courselike_key, xml_centric_courselike_key, 'static_tab', 'tabs', '.html' )
export_extra_content( export_fs, self.modulestore, self.courselike_key, xml_centric_courselike_key, 'custom_tag_template', 'custom_tags' )
export_extra_content( export_fs, self.modulestore, self.courselike_key, xml_centric_courselike_key, 'course_info', 'info', '.html' )
export_extra_content( export_fs, self.modulestore, self.courselike_key, xml_centric_courselike_key, 'about', 'about', '.html' )
course_policy_dir_name = courselike.url_name
with course_run_policy_dir.open('grading_policy.json', 'w') as grading_policy: grading_policy.write(dumps(courselike.grading_policy, cls=EdxJSONEncoder, sort_keys=True, indent=4))
export_fs.makeopendir('policies')
xml_file = export_fs.open(LIBRARY_ROOT, 'w') xml_file.write(lxml.etree.tostring(root, pretty_print=True, encoding='utf-8')) xml_file.close()
[adapt_references(child, destination_course_key, export_fs) for child in subtree.get_children()]
_export_field_content(item, item_dir)
phase_data = self.run_data.setdefault(test_phase, {}) amount_data = phase_data.setdefault(amount_md, {}) __ = amount_data.setdefault(modulestores, time_taken)
for phase in self.run_data.keys(): if phase in ('fake_assets',): continue per_phase = self.run_data[phase] html.add_header(1, phase)
html.add_header(2, title_map[table_type]) html.add_to_body(phase_table.table)
ASSET_XSD_FILE = 'assets.xsd'
NAME_CHARS = u'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_-' NAME_CHARS_W_UNICODE = NAME_CHARS + u'àĚŘǅΦШΩΣӔ'
validate_xml(input_xsd, output_xml)
try: from code_block_timer import CodeBlockTimer except ImportError: CodeBlockTimer = None
ASSET_AMOUNT_PER_TEST = (0, 1, 10, 100, 1000, 10000)
COURSE_NAME = 'manual-testing-complete'
TEST_COURSE = (COURSE_NAME, )
TEST_DIR = path(__file__).dirname() PLATFORM_ROOT = TEST_DIR.parent.parent.parent.parent.parent.parent TEST_DATA_ROOT = PLATFORM_ROOT / TEST_DATA_DIR COURSE_DATA_DIR = TEST_DATA_ROOT / COURSE_NAME
ASSET_XML_PATH = COURSE_DATA_DIR / AssetMetadata.EXPORTED_ASSET_DIR / AssetMetadata.EXPORTED_ASSET_FILENAME
ASSET_XSD_PATH = PLATFORM_ROOT / "common" / "lib" / "xmodule" / "xmodule" / "assetstore" / "tests" / ASSET_XSD_FILE
perf_test = True
make_asset_xml(num_assets, ASSET_XML_PATH) validate_xml(ASSET_XSD_PATH, ASSET_XML_PATH)
perf_test = True
make_asset_xml(num_assets, ASSET_XML_PATH) validate_xml(ASSET_XSD_PATH, ASSET_XML_PATH)
__ = source_store.find_asset_metadata(asset_key)
perf_test = True
make_asset_xml(num_assets, ASSET_XML_PATH) validate_xml(ASSET_XSD_PATH, ASSET_XML_PATH)
if asset_collection.name in asset_collection.database.collection_names():
XMODULE_FIELDS_WITH_USAGE_KEYS = ['location', 'parent']
draft_preferred = 'rev-opt-draft-preferred'
draft_only = 'rev-opt-draft-only'
published_only = 'rev-opt-published-only'
all = 'rev-opt-all'
mgmt_command = -1
primitive_command = -2
test = -3
system = -4
_bulk_ops_record_type = BulkOpsRecord
bulk_ops_record.nest()
if bulk_ops_record.is_root: self._start_outermost_bulk_operation(bulk_ops_record, course_key)
bulk_ops_record = self._get_bulk_ops_record(structure_key) if not bulk_ops_record.active: return
if emit_signals and bulk_ops_record.is_root: self.send_pre_publish_signal(bulk_ops_record, structure_key)
if bulk_ops_record.active: return
bulk_ops_record.nest()
bulk_ops_record.unnest()
self.signal_handler.send("course_published", course_key=course_id.for_branch(None)) bulk_ops_record.has_publish_item = False
self._subtree_edited_on = kwargs.get('_subtree_edited_on', None) self._subtree_edited_by = kwargs.get('_subtree_edited_by', None)
self.previous_version = edit_info.get('previous_version', None)
self.update_version = edit_info.get('update_version', None)
self.edited_on = edit_info.get('edited_on', None) self.edited_by = edit_info.get('edited_by', None)
self.original_usage = edit_info.get('original_usage', None) self.original_usage_version = edit_info.get('original_usage_version', None)
self.definition_loaded = False self.from_storable(kwargs)
self.fields = block_data.get('fields', {})
self.block_type = block_data.get('block_type', None)
self.definition = block_data.get('definition', None)
self.defaults = block_data.get('defaults', {})
self.asides = block_data.get('asides', {})
self.edit_info = EditInfo(**block_data.get('edit_info', {}))
self.add(metadata_to_insert)
self[asset_idx] = metadata_to_insert
all_assets.extend(course_assets.setdefault(asset_key.block_type, [])) idx = all_assets.find(asset_key)
all_assets = SortedAssetList(iterable=[], key=key_func) for asset_type, val in course_assets.iteritems(): all_assets.update(val)
all_assets = SortedAssetList(iterable=course_assets.get(asset_type, []), key=key_func)
end_idx = num_assets
step_incr = -1 start_idx = (num_assets - 1) - start_idx end_idx = (num_assets - 1) - end_idx
assets_by_type = defaultdict(lambda: SortedAssetList(iterable=course_assets.get(asset_type, [])))
log.warning("Asset's course {} does not match other assets for course {} - not saved.".format( asset_md.asset_id.course_key, course_key )) continue
xblock, fields = (block, block.fields)
xblock, fields = (None, block.__dict__)
return any(self._value_matches(target, test_val) for test_val in criteria['$in'])
return not any(self._value_matches(target, test_val) for test_val in criteria['$nin'])
def __init__( self, contentstore=None,
db=None, collection=None, host=None, port=None, tz_aware=True, user=None, password=None, ** kwargs
return {'default_impl': True}
about_location = self.make_course_key(org, course, run).make_usage_key('about', 'overview')
if self.contentstore: self.contentstore.copy_all_course_assets(source_course_id, dest_course_id) return dest_course_id
if self.contentstore: self.contentstore.delete_all_course_assets(course_key) super(ModuleStoreWriteBase, self).delete_course(course_key, user_id)
from xmodule.modulestore.mongo.draft import DraftModuleStore as DraftMongoModuleStore
if revision == ModuleStoreEnum.RevisionOption.published_only: return get_published()
elif usage_key.category in DIRECT_ONLY_CATEGORIES: return get_published()
elif revision == ModuleStoreEnum.RevisionOption.draft_only: return get_draft()
try: return get_draft() except ItemNotFoundError: return get_published()
super(DraftModuleStore, self).delete_course(course_key, user_id)
course_query = self._course_key_to_son(course_key) self.collection.remove(course_query, multi=True) self.delete_all_asset_metadata(course_key, user_id)
if not self.has_course(source_course_id): raise ItemNotFoundError("Cannot find a course at {0}. Aborting".format(source_course_id))
super(DraftModuleStore, self).clone_course(source_course_id, dest_course_id, user_id, fields)
if module.has_children: new_children = [] for child_loc in module.children: child_loc = child_loc.map_into_course(dest_course_id) new_children.append(child_loc)
query = self._course_key_to_son(location.course_key) query['definition.children'] = location.to_deprecated_string()
parents = self.collection.find(query, {'_id': True}, sort=[SORT_REVISION_FAVOR_DRAFT])
draft_items_locations = {item.location for item in draft_items} return [ item for item in base_get_items(MongoRevisionKey.published) if item.location not in draft_items_locations ]
return self.get_item(location)
self._verify_branch_setting(ModuleStoreEnum.Branch.draft_preferred) _verify_revision_is_published(location)
if location.category in DIRECT_ONLY_CATEGORIES: raise InvalidVersionError(location)
if delete_published: item['_id']['revision'] = MongoRevisionKey.published to_be_deleted.append(item['_id'])
self._breadth_first(convert_item, [location])
self._convert_to_draft(xblock.location, user_id, ignore_if_draft=True)
if not (allow_not_found and exception.args[0] == xblock.location): raise
if draft_only: revision = MongoRevisionKey.draft else: revision = ModuleStoreEnum.RevisionOption.all
self.refresh_cached_metadata_inheritance_tree(location.course_key)
item = self.get_item(item_location)
if item.has_children: for child_loc in item.children: _internal_depth_first(child_loc, False)
return
try: original_published = super(DraftModuleStore, self).get_item(item_location) except ItemNotFoundError: original_published = None
super(DraftModuleStore, self).update_item( item, user_id, isPublish=True, is_publish_root=is_root, allow_not_found=True ) to_be_deleted.append(as_draft(item_location).to_deprecated_son())
self._verify_branch_setting(ModuleStoreEnum.Branch.draft_preferred) _verify_revision_is_published(location)
if location.category in DIRECT_ONLY_CATEGORIES: raise InvalidVersionError(location)
to_process_non_drafts = super(DraftModuleStore, self)._query_children_for_cache_children(course_key, items)
for draft in to_process_drafts: draft_loc = Location._from_deprecated_son(draft["_id"], course_key.run) draft_as_non_draft_loc = as_published(draft_loc)
if draft_as_non_draft_loc in to_process_dict: to_process_dict[draft_as_non_draft_loc] = draft
queried_children = to_process_dict.values()
SORT_REVISION_FAVOR_DRAFT = ('_id.revision', pymongo.DESCENDING)
SORT_REVISION_FAVOR_PUBLISHED = ('_id.revision', pymongo.ASCENDING)
_OSFS_INSTANCE = {}
self.course_id = course_key self.cached_metadata = cached_metadata
try: category = json_data['location']['category'] class_ = self.load_block_type(category)
parent = self.modulestore.get_parent_location( as_published(location), ModuleStoreEnum.RevisionOption.published_only if location.revision is None else ModuleStoreEnum.RevisionOption.draft_preferred )
non_draft_loc = as_published(location)
metadata_to_inherit = self.cached_metadata.get(unicode(non_draft_loc), {}) inherit_metadata(module, metadata_to_inherit)
module.save() return module
return []
if value is None and key != '_id.revision': del query[key]
bulk_ops_record.dirty = False
DEFAULT_ASSET_COLLECTION_NAME = 'assetstore'
kwargs['w'] = 1
if asset_collection is None: asset_collection = self.DEFAULT_ASSET_COLLECTION_NAME self.asset_collection = self.database[asset_collection]
super(MongoModuleStore, self)._drop_database(database, collections, connections)
for field_name in InheritanceMixin.fields: record_filter['metadata.{0}'.format(field_name)] = 1
resultset = self.collection.find(query, record_filter)
results_by_url = {} root = None
for result in resultset: location = as_published(Location._from_deprecated_son(result['_id'], course_id.run))
metadata_to_inherit = {}
tree = self._compute_metadata_inheritance_tree(course_id)
if self.metadata_inheritance_cache_subsystem is not None: self.metadata_inheritance_cache_subsystem.set(unicode(course_id), tree)
cached_metadata = self._get_cached_metadata_inheritance_tree(course_id, force_refresh=True) if runtime: runtime.cached_metadata = cached_metadata
to_process = [] if children: to_process = self._query_children_for_cache_children(course_key, children)
if depth is not None: depth -= 1
return [ self._load_item( course_key, item, data_cache, using_descriptor_system=using_descriptor_system, apply_cached_metadata=self._should_apply_cached_metadata(item, depth), for_parent=for_parent, ) for item in items ]
super(MongoModuleStore, self).create_course( org, course, run, user_id, runtime=xblock.runtime, **kwargs )
if block_id is None: if block_type == 'course': block_id = course_key.run else: block_id = u'{}_{}'.format(block_type, uuid4().hex[:5])
ScopeIds(None, block_type, location, location), dbmodel, for_parent=kwargs.get('for_parent'),
xmodule.save() return xmodule
parent = None
result = self.collection.update( {'_id': location.to_deprecated_son()}, {'$set': update}, multi=False, upsert=allow_not_found,
xblock._edit_info = payload['edit_info']
self.refresh_cached_metadata_inheritance_tree(xblock.scope_ids.usage_id.course_key, xblock.runtime)
bulk_record = self._get_bulk_ops_record(location.course_key)
query = self._course_key_to_son(location.course_key) query['definition.children'] = unicode(location)
if revision == ModuleStoreEnum.RevisionOption.published_only: query['_id.revision'] = MongoRevisionKey.published
parents = list( self.collection.find(query, {'_id': True}, sort=[SORT_REVISION_FAVOR_DRAFT]) ) if len(parents) == 0: return cache_and_return(None)
return cache_and_return(None)
raise ReferentialIntegrityError( u"{} parents claim {}".format(len(parents), location) )
return cache_and_return(Location._from_deprecated_son(parents[0]['_id'], location.course_key.run))
all_parents = [] published_parents = 0 for parent in parents: if parent['_id']['revision'] is None: published_parents += 1 all_parents.append(parent)
if published_parents > 1: non_orphan_parents = self._get_non_orphan_parents(location, all_parents, revision) return cache_and_return(non_orphan_parents[0].replace(run=location.course_key.run))
return cache_and_return(Location._from_deprecated_son(found_id, location.course_key.run))
item_locs.add( unicode(as_published(Location._from_deprecated_son(item['_id'], course_key.run))) )
return [ Location._from_deprecated_son(course['_id'], course['_id']['name']).course_key for course in courses ]
assert len(course_assets['assets']) == 0 self.asset_collection.update( {'_id': doc_id}, {'$set': {'assets': {}}} )
return CourseAssetsFromStorage(course_key, doc_id, course_assets['assets'])
updates_by_type = {} for asset_type, assets in assets_by_type.iteritems(): updates_by_type[self._make_mongo_asset_key(asset_type)] = assets.as_list()
self.asset_collection.update( {'_id': course_assets.doc_id}, {'$set': updates_by_type} ) return True
self.asset_collection.insert(dest_assets)
all_assets = course_assets[asset_key.asset_type] md = AssetMetadata(asset_key, asset_key.path) md.from_storable(all_assets[asset_idx]) md.update(attr_dict)
all_assets[asset_idx] = md.to_storable()
self.asset_collection.update( {'_id': course_assets.doc_id}, {'$set': {self._make_mongo_asset_key(asset_key.asset_type): all_asset_info}} ) return 1
try: course_assets = self._find_course_assets(course_key) self.asset_collection.remove(course_assets.doc_id) except ItemNotFoundError: pass
create_collection_index(self.collection, '_id.category', background=True)
create_collection_index(self.collection, 'definition.children', sparse=True, background=True)
create_collection_index(self.collection, '_id.revision', background=True)
def convert_to_draft(self, location, user_id): raise NotImplementedError()
from xmodule.modulestore.mongo.draft import DraftModuleStore
continue
fullname_with_subpath = content_path.replace(static_dir, '') if fullname_with_subpath.startswith('/'): fullname_with_subpath = fullname_with_subpath[1:] asset_key = StaticContent.compute_location(target_id, fullname_with_subpath)
if not mime_type or mime_type not in mimetypes_list:
thumbnail_content, thumbnail_location = static_content_store.generate_thumbnail(content)
try: static_content_store.save(content) except Exception as err: log.exception(u'Error importing {0}, error={1}'.format( fullname_with_subpath, err ))
remap_dict[fullname_with_subpath] = asset_key
if self.target_id: assert len(self.xml_module_store.modules) == 1
import_static_content( data_path, self.static_content_store, dest_id, subpath='static', verbose=self.verbose )
asset_key = make_asset_id(course_id, asset) asset_md = AssetMetadata(asset_key) asset_md.from_xml(asset) all_assets.append(asset_md)
if len(all_assets) > 0: self.store.save_asset_metadata_list(all_assets, all_assets[0].edited_by, import_only=True)
course_data_path = path(self.data_dir) / source_courselike.data_dir
source_courselike.static_asset_path = source_courselike.data_dir source_courselike.save() log.debug('course static_asset_path=%s', source_courselike.static_asset_path)
pass
with self.store.bulk_operations(dest_id): source_courselike, courselike, data_path = self.get_courselike(courselike_key, runtime, dest_id)
self.import_static(data_path, dest_id)
self.import_asset_metadata(data_path, dest_id)
self.import_children(source_courselike, courselike, courselike_key, dest_id)
course, course_data_path = self.import_courselike( runtime, courselike_key, dest_id, source_course, ) return source_course, course, course_data_path
dest_id = self.store.make_course_key(courselike_key.org, courselike_key.course, courselike_key.run)
if existing_id: dest_id = existing_id
with self.store.branch_setting(ModuleStoreEnum.Branch.published_only, dest_id): self.recursive_build(source_courselike, courselike, courselike_key, dest_id)
return self.store.get_course(courselike.id.replace(branch=None, version_guid=None))
if 'parent_url' in value: del value['parent_url'] if 'parent_sequential_url' in value: del value['parent_sequential_url']
module.data = rewrite_nonportable_content_links( source_course_id, dest_course_id, module.data )
errorlog = make_error_tracker()
module_location = module.location.map_into_course(target_id) _update_module_location(module, module_location.replace(revision=MongoRevisionKey.draft))
if parent_url is not None and index is not None: course_key = descriptor.location.course_key parent_location = course_key.make_usage_key_from_deprecated_string(parent_url)
parent_location = parent_location.map_into_course(target_id)
drafts.sort(key=lambda x: x.index)
return 0
'parent_url', module.xml_attributes.get('parent_sequential_url')
if attr == 'parent_sequential_url': attr = 'parent_url' xml_attrs[attr] = val
module.xml_attributes = xml_attrs
for module in module_store.modules[course_id].itervalues(): if module.location.category == parent_category: parents.append(module)
for course_dir in source_dirs: _err_cnt, _warn_cnt = validate_data_source_paths(path(data_dir), course_dir) err_cnt += _err_cnt warn_cnt += _warn_cnt
self.course_id = course_id self.load_error_modules = load_error_modules self.modulestore = xmlstore
need_uniq_names = ('problem', 'sequential', 'video', 'course', 'chapter', 'videosequence', 'poll_question', 'vertical')
orig_name = orig_name[len(tag) + 1:-12]
if url_name is None or url_name == "": url_name = fallback_name()
)
if child.parent is None or child.parent > descriptor.scope_ids.usage_id: child.parent = descriptor.location child.save()
descriptor.save() return descriptor
def add_node_as_child(self, block, node, id_generator): child_block = self.process_xml(etree.tostring(node)) block.children.append(child_block.scope_ids.usage_id)
self.field_data = inheriting_field_data(kvs=DictKeyValueStore())
errorlog = make_error_tracker() course_descriptor = None try: course_descriptor = self.load_course(course_dir, course_ids, errorlog.tracker, target_course_id)
self.errored_courses[course_dir] = errorlog
courselike_label = self.parent_xml.split('.')[0]
if policy == {}:
if course_data.get('name'):
if isinstance(course_descriptor, ErrorDescriptor): return course_descriptor
compute_inherited_metadata(course_descriptor)
self.load_extra_content( system, course_descriptor, 'course_info', self.data_dir / course_dir / 'info', course_dir, url_name )
self.load_extra_content( system, course_descriptor, 'static_tab', self.data_dir / course_dir / 'tabs', course_dir, url_name )
return SlashSeparatedCourseKey(org, course, url_name)
if os.path.isdir(base_dir / url_name): self._load_extra_content(system, course_descriptor, category, base_dir / url_name, course_dir)
data_content = None
ScopeIds(None, category, loc, loc), DictFieldData(data_content),
if category == "static_tab": dog_stats_api.increment( DEPRECATION_VSCOMPAT_EVENT, tags=( "location:xml_load_extra_content_static_tab", u"course_dir:{}".format(course_dir), ) )
if mod_loc.name not in name: return False
raise NotImplementedError
return None
lazy.invalidate(library_descriptor, '_unwrapped_field_data') library_descriptor._field_data = inheriting_field_data(InheritanceKeyValueStore(init_dict))
raise NotImplementedError
if not settings.configured: settings.configure()
from request_cache.middleware import RequestCache
try: from xblock_django.user_service import DjangoXBlockUserService from crum import get_current_user
_MIXED_MODULESTORE = None
pass
mappings = getattr(settings, 'HOSTNAME_MODULESTORE_DEFAULT_MAPPINGS', None)
if mappings: for key in mappings.iterkeys(): if re.match(key, hostname): return mappings[key]
return get_branch_setting()
queue = [(usage_key, ())] while len(queue) > 0:
parent = modulestore.get_parent_location(next_usage)
if next_usage.block_type == "course": path = (next_usage, path) return flatten(path) elif parent is None: return None
newpath = (next_usage, path) queue.append((parent, newpath))
chapter = path[1].name if n > 1 else None section = path[2].name if n > 2 else None vertical = path[3].name if n > 3 else None position = None
modulestore = XMLModuleStore( DATA_DIR, source_dirs=['toy'], xblock_mixins=(XModuleMixin,), load_error_modules=False)
errors = modulestore.get_course_errors(SlashSeparatedCourseKey("edX", "toy", "2012_Fall")) assert errors == []
toy_course = store.get_course(SlashSeparatedCourseKey('edX', 'toy', '2012_Fall')) toy_course.wiki_slug = 'simple'
with store.branch_setting(ModuleStoreEnum.Branch.published_only, course.id): store.get_item(course.location)
with self.assertRaises(ValueError): with store.branch_setting(ModuleStoreEnum.Branch.draft_preferred, course.id):
other_parent_loc = course_key.make_usage_key('vertical', 'zeta') other_parent = store.get_item(other_parent_loc) self.assertIn(shared_item_loc, other_parent.children)
TESTABLE_BLOCK_TYPES = set(DIRECT_ONLY_CATEGORIES) TESTABLE_BLOCK_TYPES.discard('course')
self.assertEqual(len(course_summaries), 1)
self.assertCourseSummaryFields(course_summaries)
self.assertTrue(all(isinstance(course, CourseSummary) for course in course_summaries))
self.assertBlockHasContent(child_usage_key, 'data', child_data)
new_mixed_setting = convert_module_store_setting_if_needed(copy.deepcopy(old_setting))
self.assertEqual(new_mixed_setting["default"]["ENGINE"], "xmodule.modulestore.mixed.MixedModuleStore")
new_stores = get_mixed_stores(new_mixed_setting) self.assertIsInstance(new_stores, list)
new_stores = [store for store in get_mixed_stores(new_mixed_setting) if store['NAME'] != 'split'] old_stores = get_mixed_stores(self.OLD_MIXED_CONFIG_WITH_DICT)
self.assertEqual(len(new_stores), len(old_stores)) for new_store in new_stores: self.assertStoreValuesEqual(new_store, old_stores[new_store['NAME']])
old_mixed_setting = self.ALREADY_UPDATED_MIXED_CONFIG new_mixed_setting, new_default_store_setting = self.assertMigrated(old_mixed_setting) self.assertTrue(self.is_split_configured(new_mixed_setting)) self.assertEquals(old_mixed_setting, new_mixed_setting)
self.writable_chapter_location = self.store = self.fake_location = None self.course_locations = {}
chapter = self.store.create_child(self.user_id, self.course.location, 'chapter', block_id='Overview', asides=asides) self.writable_chapter_location = chapter.location
self.assertEqual(self.store.get_modulestore_type( SlashSeparatedCourseKey('foo', 'bar', '2012_Fall')), default_ms )
self.store.mappings = {} course_key = self.course_locations[self.MONGO_COURSEID].course_key with check_exact_number_of_calls(self.store.default_modulestore, 'has_course', 1):
with check_mongo_calls(max_find.pop(0), max_send): self.assertFalse(self.store.has_item(self.fake_location))
with self.assertRaises(UnsupportedRevisionError): self.store.has_item(self.fake_location, revision=ModuleStoreEnum.RevisionOption.draft_preferred)
with check_mongo_calls(max_find.pop(0), max_send): with self.assertRaises(ItemNotFoundError): self.store.get_item(self.fake_location)
with self.assertRaises(UnsupportedRevisionError): self.store.get_item(self.fake_location, revision=ModuleStoreEnum.RevisionOption.draft_preferred)
with self.assertRaises(UnsupportedRevisionError): self.store.get_items( self.course_locations[self.MONGO_COURSEID].course_key, revision=ModuleStoreEnum.RevisionOption.draft_preferred )
orphans = self.store.get_orphans(course_key) self.assertEqual(len(orphans), 0)
orphan = course_key.make_usage_key('chapter', 'OrphanChapter') self.store.create_item(self.user_id, orphan.course_key, orphan.block_type, block_id=orphan.block_id)
orphans = self.store.get_orphans(course_key) self.assertIn(orphan, orphans) self.assertEqual(len(orphans), 1)
items = self.store.get_items(course_key) self.assertIn(orphan, [item.location for item in items]) self.assertEqual(len(items), 3)
items_in_tree = self.store.get_items(course_key, include_orphans=False)
chapter = self.store.create_item( self.user_id, test_course.id, 'chapter', block_id='vertical_container' )
self.assertFalse(self.store.has_changes(test_course)) self.assertFalse(self.store.has_changes(chapter))
xblock = self.store.create_item( self.user_id, test_course.id, 'vertical', block_id='test_vertical' )
self.assertTrue(self.store.has_changes(xblock))
newXBlock = self.store.publish(xblock.location, self.user_id) self.assertFalse(self.store.has_changes(newXBlock))
component = self.store.get_item(xblock.location) component.display_name = 'Changed Display Name'
component = self.store.publish(component.location, self.user_id) self.assertFalse(self.store.has_changes(component))
xblock = self.store.create_item( self.user_id, test_course.id, 'vertical', block_id='test_vertical' )
self.assertTrue(self.store.has_changes(xblock))
component = self.store.publish(xblock.location, self.user_id) self.assertFalse(self.store.has_changes(component))
component = self.store.publish(component.location, self.user_id) self.assertFalse(self.store.has_changes(component))
xblock = self.store.create_item( self.user_id, test_course.id, 'vertical', block_id='test_vertical' )
self.assertTrue(self.store.has_changes(xblock))
component = self.store.publish(xblock.location, self.user_id) self.assertFalse(self.store.has_changes(component))
self.store.revert_to_published(component.location, self.user_id) component = self.store.get_item(component.location) self.assertFalse(self.store.has_changes(component))
component = self.store.get_item(component.location) component.display_name = 'Changed Display Name' self.store.update_item(component, self.user_id)
self.assertTrue(self.store.has_changes(component))
self.store.publish(vertical.location, self.user_id) self.assertFalse(self._has_changes(vertical.location))
self.store.revert_to_published(vertical.location, self.user_id) self.assertFalse(self._has_changes(vertical.location))
self.store.delete_item(component.location, self.user_id) vertical = self.store.get_item(vertical.location) self.assertTrue(self._has_changes(vertical.location))
self.store.publish(sequential.location, self.user_id) self.assertFalse(self._has_changes(sequential.location))
self.store.delete_item(vertical.location, self.user_id) self.assertFalse(self._has_changes(sequential.location))
self.store.publish(locations['parent_sibling'], self.user_id) self.store.publish(locations['parent'], self.user_id)
for key in locations: self.assertFalse(self._has_changes(locations[key]))
child = self.store.get_item(locations['child']) child.display_name = 'Changed Display Name' self.store.update_item(child, self.user_id)
self.store.publish(locations['parent'], self.user_id)
for key in locations: self.assertFalse(self._has_changes(locations[key]))
for key in locations: self.assertFalse(self._has_changes(locations[key]))
self.assertTrue(self._has_changes(locations['grandparent'])) self.assertTrue(self._has_changes(locations['parent']))
self.store.publish(locations['child_sibling'], self.user_id)
self.assertTrue(self._has_changes(locations['grandparent'])) self.assertTrue(self._has_changes(locations['parent']))
self.store.publish(locations['child'], self.user_id)
self.assertFalse(self._has_changes(locations['grandparent'])) self.assertFalse(self._has_changes(locations['parent']))
self.assertFalse(self._has_changes(locations['grandparent'])) self.assertFalse(self._has_changes(locations['parent']))
self.store.create_child( self.user_id, locations['parent'], 'vertical', block_id='new_child', )
self.assertTrue(self._has_changes(locations['grandparent'])) self.assertTrue(self._has_changes(locations['parent']))
self.assertFalse(self._has_changes(locations['grandparent'])) self.assertFalse(self._has_changes(locations['parent']))
self.assertFalse(self._has_changes(parent.location)) self.assertFalse(self._has_changes(child.location))
child.display_name = 'Changed Display Name' self.store.update_item(child, user_id=self.user_id)
self.assertTrue(self._has_changes(parent.location)) self.assertTrue(self._has_changes(child.location))
self.assertTrue(self.store.has_changes(parent))
with self.assertRaises(ItemNotFoundError): self.store.get_item(self.writable_chapter_location)
with self.assertRaises(UnsupportedRevisionError): self.store.delete_item( private_leaf.location, self.user_id, revision=ModuleStoreEnum.RevisionOption.draft_preferred )
self.store.create_child( self.user_id, self.course.location, 'static_tab' )
mongo_course = self.store.get_course(self.course_locations[self.MONGO_COURSEID].course_key) self.assertEqual(len(mongo_course.children), 1)
library = self.store.get_library(library_key) self.assertEqual(library.location.library_key, library_key)
self.store.mappings.clear() library = self.store.get_library(library_key) self.assertEqual(library.location.library_key, library_key)
self.course = self.store.publish(self.course.location, self.user_id)
self.store.convert_to_draft(self.vertical_x1a, self.user_id) self.store.convert_to_draft(self.vertical_y1a, self.user_id)
child_to_move_location = self.problem_x1a_1 new_parent_location = self.vertical_y1a old_parent_location = self.vertical_x1a
self.store.publish(self.course.location, self.user_id)
self.store.convert_to_draft(self.vertical_y1a, self.user_id)
child_to_delete_location = self.problem_y1a_1 old_parent_location = self.vertical_y1a self.store.delete_item(child_to_delete_location, self.user_id)
(child_to_delete_location, old_parent_location, ModuleStoreEnum.RevisionOption.draft_preferred), (child_to_delete_location, old_parent_location, ModuleStoreEnum.RevisionOption.published_only),
self._create_block_hierarchy() self.store.publish(self.course.location, self.user_id)
mongo_store.collection.update( self.vertical_x1b.to_deprecated_son('_id.'), {'$push': {'definition.children': unicode(self.problem_x1a_1)}} )
self.store.convert_to_draft(self.vertical_x1a, self.user_id) item = self.store.get_item(self.vertical_x1a) self.assertTrue(self.store.has_published_version(item))
with self.store.branch_setting(ModuleStoreEnum.Branch.draft_preferred, course_id): parent = mongo_store.get_parent_location(self.problem_x1a_1) self.assertEqual(parent, self.vertical_x1a)
with check_mongo_calls(num_finds.pop(0), num_sends): path = path_to_location(self.store, location) self.assertEqual(path, expected)
orphan = course_key.make_usage_key('chapter', 'OrphanChapter') self.store.create_item( self.user_id, orphan.course_key, orphan.block_type, block_id=orphan.block_id )
self.store.delete_item(self.problem_x1a_1, self.user_id) self.assertTrue(self._has_changes(self.vertical_x1a))
problem.display_name = "updated before calling revert" self.store.update_item(problem, self.user_id) self.store.revert_to_published(self.vertical_x1a, self.user_id)
self.assertEqual(num_children, len(reverted_parent.children))
self._create_block_hierarchy()
detached_locations = [ course_id.make_usage_key('static_tab', 'StaticTab'), course_id.make_usage_key('course_info', 'updates'), ]
self._create_block_hierarchy() self.store.publish(self.course.location, self.user_id)
with check_mongo_calls(max_find, max_send): wiki_courses = self.store.get_courses_for_wiki('999') self.assertEqual(len(wiki_courses), 1) self.assertIn(
with check_mongo_calls(max_find, max_send): self.store.unpublish(self.vertical_x1a, self.user_id)
draft_xblock = self.store.get_item( self.vertical_x1a, revision=ModuleStoreEnum.RevisionOption.draft_only ) self.assertIsNotNone(draft_xblock)
self.store.publish(item_location, self.user_id) item = self.store.get_item(item_location) self.assertTrue(self.store.has_published_version(item))
self.store.unpublish(item_location, self.user_id) item = self.store.get_item(item_location) self.assertFalse(self.store.has_published_version(item))
self.store.publish(item_location, self.user_id) item = self.store.get_item(item_location) self.assertTrue(self.store.has_published_version(item))
self.store.convert_to_draft(item_location, self.user_id) item = self.store.get_item(item_location) self.assertTrue(self.store.has_published_version(item))
for block in [component, child, sibling]: check_node(block.location, None, after_create, self.user_id, None, after_create, self.user_id)
component.display_name = 'Changed Display Name'
check_node(child.location, None, after_create, self.user_id, None, after_create, self.user_id)
child = self.store.get_item(child.location) child.display_name = 'Changed Display Name' self.store.update_item(child, user_id=editing_user)
check_node(child.location, after_create, after_edit, editing_user, after_create, after_edit, editing_user)
check_node(test_course.location, None, after_create, self.user_id, after_create, after_edit, editing_user)
check_node(sibling.location, None, after_create, self.user_id, None, after_create, self.user_id)
component = self.store.create_child( self.user_id, test_course.location, 'vertical', )
self.assertEqual(component.edited_by, self.user_id) old_edited_on = component.edited_on
component.display_name = 'Changed' self.store.update_item(component, edit_user) updated_component = self.store.get_item(component.location)
self.assertLess(old_edited_on, updated_component.edited_on) self.assertEqual(updated_component.edited_by, edit_user)
component = self.store.create_child( self.user_id, test_course.location, 'vertical', )
old_time = datetime.datetime.now(UTC) self.store.publish(component.location, publish_user) updated_component = self.store.get_item(component.location)
self.assertLessEqual(old_time, updated_component.published_on) self.assertEqual(updated_component.published_by, publish_user)
test_course = self.store.create_course('testx', 'GreekHero', 'test_run', self.user_id) self.assertTrue(self.store.has_published_version(test_course))
sequential.display_name = 'sequential1' sequential = self.store.update_item(sequential, self.user_id) self.assertTrue(self.store.has_published_version(sequential))
problem_child = self.store.create_child(self.user_id, chapter_location, 'problem', 'Problem_Child') self.assertFalse(self.store.has_published_version(problem_child))
problem_item = self.store.create_item(self.user_id, test_course_key, 'problem', 'Problem_Item') self.assertFalse(self.store.has_published_version(problem_item))
problem_item.display_name = 'Problem_Item1' problem_item = self.store.update_item(problem_item, self.user_id) self.assertFalse(self.store.has_published_version(problem_item))
wiki_courses = self.store.get_courses_for_wiki('999') self.assertIn(
mongo_course = self.store.get_course(self.course_locations[self.MONGO_COURSEID].course_key) mongo_course.wiki_slug = 'simple' self.store.update_item(mongo_course, self.user_id)
wiki_courses = self.store.get_courses_for_wiki('999') self.assertEqual(len(wiki_courses), 0)
problem = self.store.get_item(problem_location) self.assertEquals(problem.display_name, expected_display_name)
assertNumProblems(expected_display_name, 1)
with self.store.branch_setting(ModuleStoreEnum.Branch.draft_preferred, course_key): self.assertTrue(self.store.has_item(problem_location)) assertProblemNameEquals(problem_original_name)
self.store.publish(self.vertical_x1a, self.user_id) self.store.publish(problem_location, self.user_id)
with self.store.branch_setting(ModuleStoreEnum.Branch.published_only, course_key): self.assertTrue(self.store.has_item(problem_location)) assertProblemNameEquals(problem_original_name)
with self.store.branch_setting(ModuleStoreEnum.Branch.draft_preferred, course_key): assertProblemNameEquals(problem_original_name)
problem = self.store.get_item(problem_location) problem.display_name = problem_new_name self.store.update_item(problem, self.user_id)
with self.store.branch_setting(ModuleStoreEnum.Branch.draft_preferred, course_key): assertProblemNameEquals(problem_new_name)
with self.store.branch_setting(ModuleStoreEnum.Branch.published_only, course_key): assertProblemNameEquals(problem_original_name) assertNumProblems(problem_new_name, 0)
self.store.publish(problem_location, self.user_id)
with self.store.branch_setting(ModuleStoreEnum.Branch.published_only, course_key): assertProblemNameEquals(problem_new_name) assertNumProblems(problem_original_name, 0)
self._initialize_mixed(mappings={})
self._initialize_mixed(mappings={})
self._initialize_mixed(mappings={})
self._initialize_mixed(contentstore=contentstore, mappings={})
source_store = self.store._get_modulestore_by_type(source_modulestore) dest_store = self.store._get_modulestore_by_type(destination_modulestore) self.assertCoursesEqual(source_store, source_course_key, dest_store, dest_course_id)
course = self.store.create_course('org_x', 'course_y', 'run_z', self.user_id) signal_handler.send.assert_called_with('course_published', course_key=course.id)
course = self.store.create_course('org_x', 'course_y', 'run_z', self.user_id) signal_handler.send.assert_called_with('course_published', course_key=course.id)
course = self.store.create_course('org_x', 'course_y', 'run_z', self.user_id) signal_handler.send.assert_called_with('course_published', course_key=course.id)
signal_handler.reset_mock() section = self.store.create_item(self.user_id, course.id, 'chapter') signal_handler.send.assert_called_with('course_published', course_key=course.id)
signal_handler.reset_mock() unit = self.store.create_child(self.user_id, subsection.location, 'vertical') signal_handler.send.assert_not_called()
course = self.store.create_course('org_x', 'course_y', 'run_z', self.user_id) signal_handler.send.assert_called_with('course_published', course_key=course.id)
course = self.store.create_course('org_x', 'course_y', 'run_z', self.user_id) signal_handler.send.assert_called_with('course_published', course_key=course.id)
unit = self.store.create_child(self.user_id, subsection.location, 'vertical') signal_handler.send.assert_not_called()
course = self.store.create_course('org_x', 'course_y', 'run_z', self.user_id) course_key = course.id
course = self.store.delete_course(course_key, self.user_id)
signal_handler.send.assert_called_with('course_deleted', course_key=course_key)
course_orphans = self.store.get_orphans(course_locator) self.assertEqual(len(course_orphans), 0) self.store.delete_item(vertical.location, self.user_id)
course_publish_orphans = self.store.get_orphans(course_locator_publish)
course_orphans = self.store.get_orphans(course_locator) self.assertEqual(len(course_orphans), 0)
course_publish_orphans = self.store.get_orphans(course_locator_publish)
self.assertTrue(self._has_changes(draft_xblock.location))
self.assertFalse(self._has_changes(published_xblock.location))
self.assertFalse(self._has_changes(published_xblock.location))
self.assertFalse(self._has_changes(published_xblock.location))
self.assertTrue(self._has_changes(published_xblock.location))
with self.store.branch_setting(ModuleStoreEnum.Branch.draft_preferred, source_course_key): component = self.store.get_item(published_xblock.location) self.assertEqual(component.display_name, updated_display_name)
sequential = self.store.create_child( self.user_id, chapter.location, 'sequential', block_id='subsection_one' ) self.store.publish(sequential.location, self.user_id)
vertical = self.store.create_child( self.user_id, sequential.location, 'vertical', block_id='moon_unit' )
self.assertTrue(self._has_changes(chapter.location)) self.assertTrue(self._has_changes(sequential.location)) self.assertTrue(self._has_changes(vertical.location))
sequential = self.store.create_child( self.user_id, chapter.location, 'sequential', block_id='subsection_one' ) self.store.publish(sequential.location, self.user_id)
self._export_import_course_round_trip( self.store, contentstore, source_course_key, self.export_dir )
with self.store.branch_setting(ModuleStoreEnum.Branch.draft_preferred, source_course_key): component = self.store.get_item(unit.location) self.assertEqual(component.display_name, updated_display_name)
sequential = self.store.create_child( self.user_id, chapter.location, 'sequential', block_id='subsection_one' ) self.store.publish(sequential.location, self.user_id)
self._export_import_course_round_trip( self.store, contentstore, source_course_key, self.export_dir )
with self.store.branch_setting(ModuleStoreEnum.Branch.published_only, source_course_key): component = self.store.get_item(unit.location) self.assertEqual(component.display_name, updated_display_name)
new_chapter = self.store.create_child(self.user_id, courses[0].location, 'chapter', 'new_chapter') asides = new_chapter.runtime.get_asides(new_chapter)
chapter_aside.data_field = 'new value' self.store.update_item(new_chapter, self.user_id, asides=[chapter_aside])
chapter_aside.data_field = 'another one value' self.store.update_item(new_chapter, self.user_id, asides=[chapter_aside])
top_level_export_dir = 'exported_source_course_with_asides' export_course_to_xml( self.store, contentstore, dest_course_key, self.export_dir, top_level_export_dir, )
courses2 = import_course_from_xml( self.store, self.user_id, self.export_dir, source_dirs=[top_level_export_dir], static_content_store=contentstore, target_id=dest_course_key2, create_if_not_present=True, raise_on_failure=True, )
top_level_export_dir = 'exported_source_course_with_asides' export_course_to_xml( self.store, contentstore, dest_course_key, self.export_dir, top_level_export_dir, )
courses2 = import_course_from_xml( self.store, self.user_id, self.export_dir, source_dirs=[top_level_export_dir], static_content_store=contentstore, target_id=dest_course_key2, create_if_not_present=True, raise_on_failure=True, )
chapters = courses2[0].get_children() self.assertEquals(2, len(chapters)) self.assertIn(new_chapter_display_name, [item.display_name for item in chapters])
aside1 = AsideFoo(scope_ids=ScopeIds('user', block_type1, def_id, usage_id), runtime=self.runtime) aside1.field11 = 'new_value11' aside1.field12 = 'new_value12'
aside2 = AsideBar(scope_ids=ScopeIds('user', block_type2, def_id, usage_id), runtime=self.runtime) aside2.field21 = 'new_value21'
published_xblock = self.store.create_item( self.user_id, self.course.id, 'vertical', block_id='test_vertical', asides=[aside1, aside2] )
self._initialize_mixed(contentstore=contentstore, mappings={})
actual_items = source_store.get_items(dest_course_id, revision=ModuleStoreEnum.RevisionOption.published_only) chapter_is_found = False
self.store.delete_item(published_xblock.location, self.user_id)
published_xblock2 = self.store.create_item( self.user_id, self.course.id, 'vertical', block_id='test_vertical' )
asides2 = published_xblock2.runtime.get_asides(published_xblock2) self.assertEquals(asides2[0].field11, 'aside1_default_value1') self.assertEquals(asides2[0].field12, 'aside1_default_value2')
self.store.publish(item_location, self.user_id) item = self.store.get_item(item_location) self.assertTrue(self.store.has_published_version(item)) _check_asides(item)
self.store.unpublish(item_location, self.user_id) item = self.store.get_item(item_location) self.assertFalse(self.store.has_published_version(item)) _check_asides(item)
class Meta(object): model = Dummy
kwargs.update(kwargs.pop('metadata', {})) default_store_override = kwargs.pop('default_store', None)
except CyclicDefinitionError: return default_location
parent = kwargs.pop('parent', None) or store.get_item(parent_location)
if display_name is not None: metadata['display_name'] = display_name
@functools.wraps(func) def capture(*args, **kwargs): stacks.capture_stack(args, kwargs) return func(*args, **kwargs)
assert_greater_equal(call_count, minimum_calls)
assert_less_equal(call_count, maximum_calls)
db_config = { 'host': MONGO_HOST, 'port': MONGO_PORT_NUM, 'db': 'test_xmodule', }
self.split_mongo.create_course( self.split_course_key.org, self.split_course_key.course, self.split_course_key.run, self.user_id, fields=fields, root_block_id='runid' )
source_keys = [source_container.children[0]] new_blocks = self.store.copy_from_template(source_keys, dest_key=course.location, user_id=self.user_id) self.assertEqual(len(new_blocks), 1)
self.assertIsNotNone(problem_block.markdown) self.assertIsNone(problem_block_course.markdown)
new_display_name = "The Trouble with Tribbles" new_weight = 20 problem_block_course.display_name = new_display_name problem_block_course.weight = new_weight self.store.update_item(problem_block_course, self.user_id)
extra_block = self.make_block("html", vertical_block_course)
source_course = self.store.get_course( source_course.location.course_key, remove_version=False, remove_branch=False )
self.assertFalse(self.store.has_changes(new_blocks["about"])) self.assertTrue(published_version_exists(new_blocks["chapter"]))
self.create_random_units(False, conditional_loc)
if store is not None and i not in (4, 5): store.save_asset_metadata(asset_md, asset[4])
asset_md = store.find_asset_metadata(new_asset_loc) self.assertIsNone(asset_md)
asset_md = store.get_all_asset_metadata(course.id, 'asset') self.assertEquals(asset_md, [])
with self.assertRaises(ItemNotFoundError): store.find_asset_metadata(new_asset_loc) with self.assertRaises(ItemNotFoundError): store.get_all_asset_metadata(fake_course_id, 'asset')
with storebuilder.build() as (__, store): course = CourseFactory.create(modulestore=store)
with storebuilder.build() as (__, store): course = CourseFactory.create(modulestore=store)
store.save_asset_metadata_list(md_list, ModuleStoreEnum.UserID.test)
with storebuilder.build() as (__, store): course1 = CourseFactory.create(modulestore=store) course2 = CourseFactory.create(modulestore=store)
store.save_asset_metadata_list(md_list, ModuleStoreEnum.UserID.test)
match = re.search(r'(.*?/common)(?:$|/)', path(__file__)) COMMON_ROOT = match.group(1)
SplitModuleTest.modulestore = None
self.assertEqual(len(courses), 3)
self.assertEqual(len(courses), 1)
self.assertEqual(len(courses), 2)
courses = modulestore().get_courses(branch=BRANCH_NAME_DRAFT) self.assertEqual(len(courses), 3)
self.assertEqual(course.edited_by, "testassist@edx.org") self.assertDictEqual(course.grade_cutoffs, {"Pass": 0.55})
self.assertEqual(course.edited_by, "testassist@edx.org") self.assertDictEqual(course.grade_cutoffs, {"Pass": 0.45})
version_history = modulestore().get_block_generations(updated_problem.location) self.assertEqual(version_history.locator.version_guid, first_problem.location.version_guid)
self.cache = caches['default']
self.cache.clear() self.addCleanup(self.cache.clear)
self.user = random.getrandbits(32) self.new_course = modulestore().create_course( 'org', 'course', 'test_run', self.user, BRANCH_NAME_DRAFT, )
mock_get_cache.return_value = self.cache
with check_mongo_calls(0): cached_structure = self._get_structure(self.new_course)
self.assertEqual(cached_structure, not_cached_structure)
with check_mongo_calls(1): cached_structure = self._get_structure(self.new_course)
self.assertEqual(cached_structure, not_cached_structure)
with check_mongo_calls(1): cached_structure = self._get_structure(self.new_course)
self.assertEqual(cached_structure, not_cached_structure)
locator = course.location.map_into_course(CourseLocator(version_guid=previous_version)) self.assertTrue( modulestore().has_item(locator), "couldn't find in %s" % previous_version )
locator = BlockUsageLocator(course_locator, block_type='chapter', block_id='chapter1') self.assertTrue( modulestore().has_item(locator), "couldn't find chapter1" )
self.assertEqual(block.edited_by, "testassist@edx.org") self.assertDictEqual( block.grade_cutoffs, {"Pass": 0.45}, )
with self.assertRaises(ItemNotFoundError): modulestore().get_item(course.location.for_branch(BRANCH_NAME_PUBLISHED))
new_module = modulestore().get_item(chapter_locator)
with self.assertRaises(VersionConflictError): _fail = modulestore().create_child( user, new_course.location, 'chapter', fields={'display_name': 'chapter 3'}, )
self.assertGreater(len(block.children), 0, "meaningless test") moved_child = block.children.pop()
block = modulestore().get_item(locator) pre_def_id = block.definition_locator.definition_id pre_version_guid = block.location.version_guid
nodes = modulestore().get_items(reusable_location, qualifiers={'category': 'chapter'}) new_course_loc = modulestore().delete_item(nodes[0].location, self.user_id)
store.delete_course(refetch_course.id, user)
split_store = modulestore()
courses = split_store.get_courses(BRANCH_NAME_DRAFT)
chapter = modulestore().get_item(chapter.location.version_agnostic()) del chapter.visible_to_staff_only modulestore().update_item(chapter, self.user_id)
SplitModuleTest.modulestore = class_(
def render_to_template_mock(*args): pass
from collections import namedtuple import datetime BlockInfo = namedtuple('BlockInfo', 'block_id, category, fields, sub_tree')
ModuleStoreNoSettings.modulestore = class_(
def render_to_template_mock(*args): pass
self.xblock.location = Location("org", "import", "run", "category", "stubxblock")
self.xblock.test_content_field = "Explicitly set" self.xblock.test_settings_field = "Explicitly set" self.xblock.save()
self.assertEqual(new_version.location.course_key, target_location_namespace)
self.assertEqual(new_version.test_content_field, 'Explicitly set') self.assertEqual(new_version.test_settings_field, 'Explicitly set')
self.assertIn( 'test_content_field', new_version.get_explicitly_set_fields_by_scope(scope=Scope.content) ) self.assertIn( 'test_settings_field', new_version.get_explicitly_set_fields_by_scope(scope=Scope.settings) )
self.xblock.location = Location("org", "import", "run", "category", "stubxblock")
self.xblock.save()
self.assertEqual(new_version.test_content_field, 'default value') self.assertEqual(new_version.test_settings_field, 'default value')
self.assertNotIn( 'test_content_field', new_version.get_explicitly_set_fields_by_scope(scope=Scope.content) ) self.assertNotIn( 'test_settings_field', new_version.get_explicitly_set_fields_by_scope(scope=Scope.settings) )
self.xblock.location = Location("org", "import", "run", "category", "stubxblock") self.xblock.save()
self.assertNotIn( 'start', new_version.get_explicitly_set_fields_by_scope(scope=Scope.settings) ) self.assertNotIn( 'graded', new_version.get_explicitly_set_fields_by_scope(scope=Scope.settings) )
self.xblock.location = Location("org", "import", "run", "category", "stubxblock")
target_location = self.xblock.location.replace(revision='draft') _update_module_location(self.xblock, target_location)
self.assertEqual(new_version.location, target_location)
for field in self.CONTENT_FIELDS + self.SETTINGS_FIELDS + self.CHILDREN_FIELDS: self.assertTrue(new_version.fields[field].is_set_on(new_version))
courses = ['toy', 'simple', 'simple_with_draft', 'test_unicode']
doc_store_config = { 'host': HOST, 'port': PORT, 'db': DB, 'collection': COLLECTION, } cls.add_asset_collection(doc_store_config)
import_course_from_xml( draft_store, 999, DATA_DIR, ['test_import_course'], static_content_store=content_store, do_import_static=False, verbose=True )
import_course_from_xml( draft_store, 999, DATA_DIR, ['test_import_course'], static_content_store=content_store, do_import_static=False, verbose=True, target_id=SlashSeparatedCourseKey('guestx', 'foo', 'bar') )
connection.drop_database(DB)
course_locations = self.draft_store.get_courses_for_wiki('toy') assert_equals(len(course_locations), 0)
self.content_store.find(location)
component = self.draft_store.get_item(location) self.assertEqual(component.published_on, published_date) self.assertEqual(component.published_by, published_by)
self.draft_store.delete_course(course.id, self.dummy_user)
self.assertEquals(self.draft_store.get_all_asset_metadata(course.id, 'asset'), [])
self.assertRaises(ItemNotFoundError, lambda: self.draft_store.get_all_asset_metadata(course_key, 'asset')[:1])
with patch('mongodb_proxy.MongoProxy') as mock_proxy: mock_proxy.return_value.alive.return_value = False useless_conn = MongoConnection('useless', 'useless', 'useless')
asset_deprecated = None ssck_deprecated = None
self.contentstore = MongoContentStore(HOST, DB, port=PORT)
self.contentstore.delete(asset_key)
__, count = self.contentstore.get_all_content_for_course(self.course2_key) self.assertEqual(count, len(self.course2_files))
self.assertEqual(set(subtree_roots_urls), set(expected_roots_urls))
self.bulk.update_definition(self.course_key, self.definition) self.assertConnCalls(call.insert_definition(self.definition, self.course_key))
self.bulk.insert_course_index(self.course_key, self.index_entry) self.assertConnCalls(call.insert_course_index(self.index_entry, self.course_key)) self.assertCacheNotCleared()
self.bulk._end_bulk_operation(self.course_key)
self.assertEquals(self.conn.get_definitions.call_count, 0)
with check_mongo_calls(import_reads, first_import_writes): import_course_from_xml( source_store, 'test_user', TEST_DATA_DIR, source_dirs=['manual-testing-complete'], static_content_store=source_content, target_id=source_course_key, create_if_not_present=True, raise_on_failure=True, )
import_course_from_xml( source_store, 'test_user', TEST_DATA_DIR, source_dirs=['manual-testing-complete'], static_content_store=source_content, target_id=source_course_key, create_if_not_present=True, raise_on_failure=True, )
for xblock in all_blocks: for __, field in xblock.fields.iteritems(): if field.is_set_on(xblock): __ = field.read_from(xblock)
assert_true(modulestore.has_course(locator, ignore_case))
{key_field: 'fake'}, {key_field: getattr(locator, key_field) + 'X'}, {key_field: 'X' + getattr(locator, key_field)},
fs_root = mkdtemp()
rmtree(fs_root, ignore_errors=True)
fs_root = mkdtemp()
rmtree(fs_root, ignore_errors=True)
store_iterator = iter(modulestores) next_modulestore = lambda *args, **kwargs: store_iterator.next()
stores = [{'NAME': name, 'ENGINE': 'This space deliberately left blank'} for name in names]
return store.asset_collection
return store.db_connection.structures
) DIRECT_MS_SETUPS_SHORT = ( 'mongo', #'split', ) MODULESTORE_SETUPS = DIRECT_MODULESTORE_SETUPS + MIXED_MODULESTORE_SETUPS MODULESTORE_SHORTNAMES = DIRECT_MS_SETUPS_SHORT + MIXED_MS_SETUPS_SHORT SHORT_NAME_MAP = dict(zip(MODULESTORE_SETUPS, MODULESTORE_SHORTNAMES))
with check_mongo_calls(4, 2):
item = self.draft_mongo.get_item(vert_location, 0) self.assertFalse(getattr(item, 'is_draft', False), "Item was published. Draft should not exist")
location = self.old_course_key.make_usage_key('discussion', block_id='Discussion1') self.draft_mongo.delete_item(location, self.user_id)
block_type = 'vertical' for idx in xrange(0, 8): block_id = _make_block_id(block_type, idx) self.all_verticals.append((block_type, block_id))
block_type = 'html' for idx in xrange(0, 16): block_id = _make_block_id(block_type, idx) self.all_units.append((block_type, block_id))
self.all_verticals = [] self.all_units = []
self.course_db = {}
if block_type == 'html': self.assertElementAttrsSubset(element, {'filename': filename})
self.assertParentReferences( element, course_key, **kwargs )
child_id_regex = None child_type = None if child_types_ids: child_type = child_types_ids[0][0] child_id_regex = '|'.join([child[1] for child in child_types_ids])
with MongoContentstoreBuilder().build() as self.contentstore: with modulestore_builder.build(contentstore=self.contentstore) as self.store: self._create_course(self.store) yield
export_course_to_xml( self.store, self.contentstore, self.course.id, self.root_export_dir, self.export_dir, )
self.publish((('html', 'html00'),))
with self.assertRaises(ItemNotFoundError): self.publish((('html', 'html00'),))
self.assertOLXIsDraftOnly(block_list_publish) self.assertOLXIsDraftOnly(block_list_untouched)
self.publish(block_list_parents_to_publish)
self.assertOLXIsPublishedOnly(block_list_publish) self.assertOLXIsDraftOnly(block_list_untouched)
self.assertOLXIsDraftOnly(block_list_to_unpublish) with self.assertRaises(ItemNotFoundError): self.unpublish(block_list_to_unpublish)
self.assertOLXIsDraftOnly(block_list_to_unpublish) with self.assertRaises(ItemNotFoundError): self.unpublish(block_list_to_unpublish)
(ModuleStoreEnum.RevisionOption.published_only, 'assertOLXIsDeleted'), (ModuleStoreEnum.RevisionOption.all, 'assertOLXIsDeleted'), (None, 'assertOLXIsDeleted'),
(ModuleStoreEnum.RevisionOption.published_only, 'assertOLXIsDraftOnly'), (ModuleStoreEnum.RevisionOption.all, 'assertOLXIsDeleted'), (None, 'assertOLXIsDeleted'),
self.assertOLXIsPublishedOnly(block_list_to_delete) self.delete_item(block_list_to_delete, revision=revision) self._check_for_item_deletion(block_list_to_delete, result) self.assertOLXIsDeleted(block_list_children)
self.assertOLXIsPublishedOnly(block_list_to_delete) self.delete_item(block_list_to_delete, revision=revision) self._check_for_item_deletion(block_list_to_delete, result) self.assertOLXIsDeleted(autopublished_children) self.assertOLXIsDeleted(block_list_draft_children)
self.assertOLXIsDraftOnly(block_list_to_revert) with self.assertRaises(InvalidVersionError): self.revert_to_published(block_list_to_revert)
self.assertOLXIsDraftOnly(block_list_to_revert) self.publish(block_list_to_revert) self.assertOLXIsPublishedOnly(block_list_to_revert) self.revert_to_published(block_list_to_revert) self.assertOLXIsPublishedOnly(block_list_to_revert)
self.assertOLXIsDraftOnly(block_list_to_revert) self.publish(block_list_to_revert) self.assertOLXIsPublishedOnly(block_list_to_revert)
self.assertOLXIsDraftAndPublished(block_list_to_revert) self.revert_to_published(block_list_to_revert) self.assertOLXIsPublishedOnly(block_list_to_revert)
'ADDITIONAL_OPTIONS': { 'trashcan': { 'bucket': 'trash_fs' } }
mock_create.return_value = None
TEST_DATA_MONGO_MODULESTORE = functools.partial(mixed_store_config, mkdtemp_clean(), {})
TEST_DATA_SPLIT_MODULESTORE = functools.partial( mixed_store_config, mkdtemp_clean(), {}, store_order=[StoreConstructors.split, StoreConstructors.draft] )
multi_db = True
yield super(SharedModuleStoreTestCase, cls).setUpClass()
OverrideFieldData.provider_classes = None super(SharedModuleStoreTestCase, self).setUp()
multi_db = True
OverrideFieldData.provider_classes = None
self.user = User.objects.create_user(uname, email, self.user_password)
self.user.is_staff = True self.user.save()
new_module_store_setting['default']['OPTIONS']['stores'] = convert_old_stores_into_list( module_store_setting ) module_store_setting = new_module_store_setting
elif isinstance(get_mixed_stores(module_store_setting), dict): warnings.warn( "Using a dict for the Stores option in the MixedModuleStore is deprecated. Please use a list instead.", DeprecationWarning )
module_store_setting['default']['OPTIONS']['stores'] = convert_old_stores_into_list( get_mixed_stores(module_store_setting) ) assert isinstance(get_mixed_stores(module_store_setting), list)
mixed_stores.remove(store) mixed_stores.insert(0, store) return
super(CourseKeyField, self).__init__(**kwargs)
return course_key.to_deprecated_string()
rem_vers = kwargs.pop('remove_version', True) rem_branch = kwargs.pop('remove_branch', True)
retval = func(field_decorator=strip_key_collection, *args, **kwargs)
return strip_key_collection(retval)
for course_key, store_name in self.mappings.iteritems(): if store_name == key: self.mappings[course_key] = store self.modulestores.append(store)
return self.default_modulestore
if course_id in course_summaries: log.warning( u"Modulestore %s have duplicate courses %s; skipping from result.", store, course_id ) else: course_summaries[course_id] = course_summary
for course in store.get_courses(**kwargs): course_id = self._clean_locator_for_mapping(course.id) if course_id not in courses: courses[course_id] = course
for library in store.get_libraries(**kwargs): library_id = self._clean_locator_for_mapping(library.location) if library_id not in libraries: libraries[library_id] = library
for course_id, store in self.mappings.iteritems(): candidate_key = store.make_course_key(org, course, run) if candidate_key == course_id: return candidate_key
return self.default_modulestore.make_course_key(org, course, run)
source_store.copy_all_asset_metadata(source_course_key, dest_course_key, user_id)
course_key = self.make_course_key(org, course, run) if course_key in self.mappings and self.mappings[course_key].has_course(course_key): raise DuplicateCourseError(course_key, course_key)
store = self._verify_modulestore_support(None, 'create_course') course = store.create_course(org, course, run, user_id, **kwargs)
self.mappings[course_key] = store
lib_key = LibraryLocator(org=org, library=library) if lib_key in self.mappings: raise DuplicateCourseError(lib_key, lib_key)
store = self._verify_modulestore_support(None, 'create_library') library = store.create_library(org, library, user_id, fields, **kwargs)
self.mappings[lib_key] = store
dest_modulestore = self._get_modulestore_for_courselike(dest_course_id) if source_modulestore == dest_modulestore: return source_modulestore.clone_course(source_course_id, dest_course_id, user_id, fields, **kwargs)
super(MixedModuleStore, self).clone_course(source_course_id, dest_course_id, user_id, fields, **kwargs)
if hasattr(modulestore, '_drop_database'):
return dict( itertools.chain.from_iterable( store.heartbeat().iteritems() for store in self.modulestores ) )
return thread_local_default_store
return self.modulestores[0]
incxml = etree.XML(ifp.read())
parent.insert(parent.index(next_include), incxml)
msg = "Error in problem xml include: %s" % ( etree.tostring(next_include, pretty_print=True)) system.error_tracker(msg)
filename = '_' + fragment_name contents[filename] = fragment
EDX_XML_PARSER = XMLParser(dtd_validation=False, load_dtd=False, remove_comments=True, remove_blank_text=True, encoding='utf-8')
return value
filename_extension = 'xml'
metadata_translations = { 'slug': 'url_name', 'name': 'display_name', }
'course', 'org', 'url_name', 'filename', 'xml_attributes')
msg = 'Unable to load file contents at path %s for item %s: %s ' % ( filepath, def_id, err) raise Exception, msg, sys.exc_info()[2]
definition_xml.attrib.update(xml_object.attrib)
attr = cls._translate(attr)
continue
metadata['xml_attributes'][attr] = value
definition, children = cls.load_definition(definition_xml, runtime, def_id, id_generator)
if is_pointer_tag(node): definition['filename'] = [filepath, filepath]
cls.apply_policy(metadata, runtime.get_policy(usage_id))
ScopeIds(None, node.tag, def_id, usage_id), field_data,
xml_object.tag = self.category node.tag = self.category
if self.category == 'course': node.set('org', self.location.org) node.set('course', self.location.course)
return super(XmlDescriptor, cls).parse_xml( etree.fromstring(xml_data), system,
node = Element(self.category) super(XmlDescriptor, self).add_xml_to_node(node) return etree.tostring(node)
GENERAL_ASSET_TYPE = 'asset'
ALL_ASSETS_XML_TAG = 'assets'
ASSET_XML_TAG = 'asset'
EXPORTED_ASSET_DIR = 'assets'
EXPORTED_ASSET_FILENAME = 'assets.xml'
self.created_by = created_by self.created_by_email = created_by_email self.created_on = created_on or now self.fields = fields or {}
continue
value = True if value == "true" else False
value = None
value = dateutil.parser.parse(value)
value = int(value)
value = json.loads(value)
if attr == self.ASSET_TYPE_ATTR: value = self.asset_id.asset_type elif attr == self.ASSET_BASENAME_ATTR: value = self.asset_id.path else: value = getattr(self, attr)
etree.fromstring(etree.tostring(root), self.xmlparser)
conditions_map = {
'correct': 'is_correct',
log.warn('Error in conditional module: \ required module {module} has no {module_attr}'.format(module=module, module_attr=attr_name)) return False
self.required_html_ids = [descriptor.location.html_id() for descriptor in self.descriptor.get_required_module_descriptors()]
class_priority = ['video', 'problem']
stringified_sources_list = map(lambda loc: loc.to_deprecated_string(), self.sources_list) self.xml_attributes['sources'] = ';'.join(stringified_sources_list) return xml_object
try: import dogstats_wrapper as dog_stats_api except ImportError: dog_stats_api = None
_ = lambda text: text
NUM_RANDOMIZATION_BINS = 20 MAX_RANDOMIZATION_BINS = 1000
return int(r_hash.hexdigest()[:7], 16) % NUM_RANDOMIZATION_BINS
default=_("Blank Advanced Problem")
self.runtime.set('location', self.location.to_deprecated_string())
self.seed = randomization_bin(self.runtime.seed, unicode(self.location).encode('utf-8'))
self.seed %= MAX_RANDOMIZATION_BINS
if self.weight == 0: return None
score = score * self.weight / total total = self.weight
_ = self.runtime.service(self, "i18n").ugettext check = _('Check') final_check = _('Final Check')
if 'custom_check' in self.text_customization:
if 'custom_checking' in self.text_customization: return self.text_customization.get('custom_checking')
if self.closed() or submitted_without_reset: return False else: return True
if self.closed() and not is_survey_question: return False
if self.rerandomize in [RANDOMIZATION.ALWAYS, RANDOMIZATION.ONRESET] and self.is_submitted(): return True else: if self.is_correct(): return False else: return self.show_reset_button
if self.force_save_button: return not self.closed() else: is_survey_question = (self.max_attempts == 0) needs_reset = self.is_submitted() and self.rerandomize == RANDOMIZATION.ALWAYS
elif (self.closed() and not is_survey_question) or needs_reset: return False else: return True
student_answers = self.lcp.student_answers answer_ids = student_answers.keys()
self.lcp = self.new_lcp(None) self.set_state_from_lcp()
warning_msg = _("The problem's state was corrupted by an invalid submission. The submission consisted of:") warning += warning_msg + '<ul>'
log.exception("Unable to generate html from LoncapaProblem") raise
prefix = _('Hint ({hint_num} of {hints_count}): ').format(hint_num=hint_index + 1, hints_count=len(demand_hints))
return { 'success': True, 'contents': prefix + hint_text, 'hint_index': hint_index }
if self.should_show_check_button(): check_button = self.check_button_name() check_button_checking = self.check_button_checking_name() else: check_button = False check_button_checking = False
demand_hints = self.lcp.tree.xpath("//problem/demandhint/hint") demand_hint_possible = len(demand_hints) > 0
return html
return self.lcp.done
return True
return self.lcp.done
self.lcp.ungraded_response(score_msg, queuekey) self.set_state_from_lcp() return dict()
self.set_state_from_lcp() return response
if not name: raise ValueError(u"{key} must contain at least one underscore".format(key=key))
except(KeyError, ValueError): raise ValueError( u"Invalid submission: {val} for {key}".format(val=data[key], key=key) )
if name in answers: raise ValueError(u"Key {name} already exists in answers dict".format(name=name)) else: answers[name] = val
current_time = datetime.datetime.now(UTC()) if override_time is not False: current_time = override_time
if self.lcp.is_queued(): prev_submit_time = self.lcp.get_recentmost_queuetime()
self.set_state_from_lcp()
if self.runtime.user_is_staff: msg = u"Staff debug info: {tb}".format(tb=cgi.escape(traceback.format_exc()))
else: msg = _(u"Error: {msg}").format(msg=inst.message)
self.set_state_from_lcp()
success = 'correct' for answer_id in correct_map: if not correct_map.is_correct(answer_id): success = 'incorrect'
html = self.get_problem_html(encapsulate=False)
event_unmasked = copy.deepcopy(event_info) self.unmask_event(event_unmasked) self.runtime.publish(self, title, event_unmasked)
answer = event_info.get('answers', {}).get(response.answer_id) if answer is not None: event_info['answers'][response.answer_id] = response.unmask_name(answer)
permutation_option = None if response.has_shuffle(): permutation_option = 'shuffle' elif response.has_answerpool(): permutation_option = 'answerpool'
if 'permutation' not in event_info: event_info['permutation'] = {} event_info['permutation'][response.answer_id] = (permutation_option, response.unmask_order())
log.exception('Unable to gather submission metadata, it will not be included in the event.')
raise NotImplementedError(_("Problem's definition does not support rescoring."))
orig_score = self.lcp.get_score() event_info['orig_score'] = orig_score['score'] event_info['orig_total'] = orig_score['total']
self.set_state_from_lcp()
success = 'correct' for answer_id in correct_map: if not correct_map.is_correct(answer_id): success = 'incorrect'
event_info['correct_map'] = correct_map.get_dict() event_info['success'] = success event_info['attempts'] = self.attempts self.track_function_unmask('problem_rescore', event_info)
'error': _("Problem is closed."),
'error': _("Refresh the page and make an attempt before resetting."),
self.choose_new_seed()
self.lcp = self.new_lcp(None)
self.set_state_from_lcp()
_ = lambda text: text
custom_parameters = {}
if param_name not in PARAMETERS: param_name = 'custom_' + param_name
u'resource_link_id': self.get_resource_link_id(), u'lis_result_sourcedid': self.get_lis_result_sourcedid(),
body.update(custom_parameters)
'Content-Type': 'application/x-www-form-urlencoded',
params = dict([param.strip().replace('"', '').split('=') for param in params.split(',')])
params[u'oauth_signature'] = urllib.unquote(params[u'oauth_signature']).decode('utf8')
params.update(body) return params
score = float(score) if not 0 <= score <= 1: raise LTIError('score value outside the permitted range of 0-1.')
num_choices = len(self.descriptor.get_children())
self.choice = None
log.debug("children of randomize module (should be only 1): %s", self.child)
return Fragment(content=u"<div>Nothing to randomize between</div>")
module_class = RandomizeModule resources_dir = None
return self.system.render_template('module-error.html', { 'staff_access': True, 'data': self.contents, 'error': self.error_msg, })
return self.system.render_template('module-error.html', { 'staff_access': False, 'data': "", 'error': "", })
error_msg = 'Error not available'
error_msg = exc_info_to_str(sys.exc_info())
assert library.location.library_key.version_guid is not None return library.location.library_key.version_guid
source_blocks.extend(self._problem_type_filter(library, dest_block.capa_type))
import logging
metadata_translations = dict(RawDescriptor.metadata_translations) metadata_translations['attempts'] = 'max_attempts'
fragment = Fragment( self.system.render_template(self.mako_template, self.get_context()) ) shim_xmodule_js(self, fragment) return fragment
request.url = 'http://testurl/' self.xmodule.verify_oauth_body_sign(request)
"Test for Annotation Xmodule functional logic."
def test_real_user(useless): useless_user = Mock(email='fake@fake.com', id=useless) return useless_user
def test_user_role(): return 'staff'
response = self.ajax_request('bad_answer', {}) self.assertDictEqual(response, {'error': 'Unknown Command!'})
response = self.ajax_request('No', {})
overflow_grader = graders.AssignmentFormatGrader("Lab", 3, 2) lab_grader = graders.AssignmentFormatGrader("Lab", 7, 3)
homework_grader = graders.AssignmentFormatGrader("Homework", 12, 2) homework_grader2 = graders.grader_from_conf(homework_grader)
course = xml.CourseFactory.build() sequence = xml.SequenceFactory.build(parent=course) vertical = xml.VerticalFactory.build(parent=sequence)
self.assertTrue(self.lc_block.has_dynamic_children())
self.assertEqual(len(self.lc_block.get_child_descriptors()), 1) self.assertEqual(len(self.lc_block.get_content_titles()), 1)
self.lc_block.source_library_id = "" result = self.lc_block.validate()
self.lc_block.source_library_id = "library-v1:BAD+WOLF" result = self.lc_block.validate()
self.lc_block.source_library_id = unicode(self.library.location.library_key) result = self.lc_block.validate()
self.lc_block.refresh_children() self.assertTrue(self.lc_block.validate())
self.lc_block.max_count = 50 self.lc_block.refresh_children() result = self.lc_block.validate()
self.lc_block.max_count = 1 self._create_capa_problems() self.lc_block.refresh_children() self.assertTrue(self.lc_block.validate())
self.lc_block.max_count = 1 self.lc_block.capa_type = 'multiplechoiceresponse' self.lc_block.refresh_children() self.assertTrue(self.lc_block.validate())
self.lc_block.max_count = 10 self.lc_block.capa_type = 'multiplechoiceresponse' self.lc_block.refresh_children() result = self.lc_block.validate()
self.lc_block.max_count = 1 self.lc_block.capa_type = 'customresponse' self.lc_block.refresh_children() result = self.lc_block.validate()
self.lc_block = self.store.get_item(self.lc_block.location) self._bind_course_module(self.lc_block) self.lc_block.xmodule_runtime.publish = self.publisher
course_usage_main_vertical = self.lc_block.children[0] course_usage_inner_vertical = self.store.get_item(course_usage_main_vertical).children[0] inner_vertical_in_course = self.store.get_item(course_usage_inner_vertical) course_usage_html = inner_vertical_in_course.children[0] course_usage_problem = inner_vertical_in_course.children[1]
self.lc_block.get_child_descriptors() event_data = self._assert_event_was_published("assigned")
del self.lc_block._xmodule._selected_set
del self.lc_block._xmodule._selected_set initial_blocks_assigned = self.lc_block.get_child_descriptors() self.assertEqual(len(initial_blocks_assigned), 2)
"original_usage_version": None, "descendants": [],
system = self.get_system() descriptor = system.process_xml(xml_str_in)
node = etree.Element('unknown') descriptor.add_xml_to_node(node)
self.assertEqual(node.tag, 'sequential')
print(descriptor, descriptor._field_data) self.assertEqual(descriptor.due, ImportTestCase.date.from_json(from_date_string))
descriptor.runtime.export_fs = MemoryFS() node = etree.Element('unknown') descriptor.add_xml_to_node(node)
with descriptor.runtime.export_fs.open('course/{url_name}.xml'.format(url_name=url_name)) as f: course_xml = etree.fromstring(f.read())
self.assertNotIn('course', course_xml.attrib) self.assertNotIn('org', course_xml.attrib)
self.assertNotIn('url_name', course_xml.attrib)
descriptor = descriptor.get_children()[0] self.course_descriptor_no_inheritance_check(descriptor)
child = descriptor.get_children()[0] self.assertEqual(child.due, None)
self.assertLessEqual( datetime.datetime.now(UTC()), child.start )
self.assertEqual( ImportTestCase.date.to_json(ImportTestCase.date.from_json(course_due)), child.xblock_kvs.inherited_settings['due'] )
child._field_data.set(child, 'due', child_due) compute_inherited_metadata(descriptor) self.override_metadata_check(descriptor, child, course_due, child_due)
self.assertEqual(two_toys.grade_cutoffs['C'], 0.5999)
self.assertEqual(toy.graded, True)
modulestore = XMLModuleStore(DATA_DIR, source_dirs=['toy']) courses = modulestore.get_courses() self.assertEquals(len(courses), 1) course = courses[0]
print "video {0} url_name: {1}".format(i, video.url_name)
self.assertFalse(course.is_cohorted)
course.cohort_config = {} self.assertFalse(course.is_cohorted)
course.cohort_config = {'cohorted': False} self.assertFalse(course.is_cohorted)
course.cohort_config = {'cohorted': True} self.assertTrue(course.is_cohorted)
self.xmodule.is_condition_satisfied = lambda: True self.xmodule.descriptor.get_children = lambda: []
assert_equals(out.count("But it is "), 1)
TestScenario( (self.demo_course.id, '='), "course_MVSFQL2EMVWW6WBOGEXUMYLMNRPTEMBRGQ======" ), TestScenario( (self.html_course.id, '~'), "course_MNXXK4TTMUWXMMJ2KVXGS5TFOJZWS5DZLAVUGUZNGIYDGK2ZGIYDSNQ~" ),
TestScenario((self.demo_course,), "Empty"), TestScenario((self.html_course,), "Intro to &lt;html&gt;"),
TestScenario((self.demo_course,), "Empty"), TestScenario((self.html_course,), "Intro to <html>"),
with self.assertRaises(ValueError): mock_strftime_localized(test_datetime, 'BAD_FORMAT_SPECIFIER')
DATA_DIR = MODULE_DIR.parent.parent.parent.parent / "test" / "data"
def get_asides(self, block): return []
module_system = get_test_system()
self.__manager = None
manager = self.__manager self.__manager = None yield
exc_type, exc_value, exc_tb = sys.exc_info()
relevant_frames = 0 for frame_record in inspect.stack(): frame = frame_record[0] if '__unittest' in frame.f_globals: break relevant_frames += 1
with self._capture_assertion_errors(): context = assertion(*args, **kwargs)
if context is not None: return nested(self._capture_assertion_errors(), context)
self.assertIn(map_key(actual_item_location), actual_item_map.keys())
self.assertEqual(expected_item.fields, actual_item.fields)
if field_name == 'children': continue
continue
with assert_raises(ValueError): course = self.process_xml(CourseFactory.build(policy={'days_early_for_beta': 'null'}))
XML_IMPORT_ARGS = inspect.getargspec(XmlImportData.__init__).args
inline_xml = kwargs.pop('inline_xml')
root = CourseFactory.build(days_early_for_beta="null") sequence = SequenceFactory.build(parent=root) ProblemFactory.build(parent=sequence)
return getattr(super(BulkAssertionTest, self), 'assert' + assertion_name)(*args, **kwargs)
course = xml.CourseFactory.build() sequence = xml.SequenceFactory.build(parent=course) split_test = SplitTestModuleFactory( parent=sequence, attribs={ 'user_partition_id': '0',
self.assertIn(self.split_test_module.child_descriptor.url_name, ['split_test_cond0', 'split_test_cond1'])
@patch('xmodule.html_module.HtmlDescriptor.definition_to_xml') def test_export_import_round_trip(self, def_to_xml): def_to_xml.return_value = lxml.etree.Element('html')
self.module_system.process_xml = Mock()
xml_obj = self.split_test_module.definition_to_xml(MemoryFS())
self.assertIn(SplitTestDescriptor.user_partition_id.name, editable_metadata_fields)
self.split_test_module.user_partition_id = SplitTestFields.no_partition_selected['value']
self.split_test_module.user_partition_id = 0
self.split_test_module.user_partition_id = 999
split_test_module.user_partition_id = -1 [active_children, inactive_children] = split_test_module.active_and_inactive_children() self.assertEqual(active_children, []) self.assertEqual(inactive_children, children)
self.split_test_module.user_partition_id = 2 [active_children, inactive_children] = split_test_module.active_and_inactive_children() self.assertEqual(active_children, []) self.assertEqual(inactive_children, children)
field_data['attempts'] = int(attempts)
self.two_day_delta_str = "2 days"
problem = CapaFactory.create() self.assertFalse(problem.answer_available())
used_all_attempts = CapaFactory.create(showanswer='closed', max_attempts="1", attempts="1", due=self.tomorrow_str) self.assertTrue(used_all_attempts.answer_available())
after_due_date = CapaFactory.create(showanswer='closed', max_attempts="1", attempts="0", due=self.yesterday_str)
attempts_left_open = CapaFactory.create(showanswer='closed', max_attempts="1", attempts="0", due=self.tomorrow_str) self.assertFalse(attempts_left_open.answer_available())
still_in_grace = CapaFactory.create(showanswer='closed', max_attempts="1", attempts="0", due=self.yesterday_str, graceperiod=self.two_day_delta_str) self.assertFalse(still_in_grace.answer_available())
answer_correct = CapaFactory.create(showanswer='correct_or_past_due', max_attempts="1", attempts="0", due=self.tomorrow_str, correct=True) self.assertTrue(answer_correct.answer_available())
past_due_date = CapaFactory.create(showanswer='correct_or_past_due', max_attempts="1", attempts="0", due=self.yesterday_str) self.assertTrue(past_due_date.answer_available())
past_due_date_correct = CapaFactory.create(showanswer='correct_or_past_due', max_attempts="1", attempts="0", due=self.yesterday_str, correct=True) self.assertTrue(past_due_date_correct.answer_available())
still_in_grace = CapaFactory.create(showanswer='correct_or_past_due', max_attempts="1", attempts="1", due=self.yesterday_str, graceperiod=self.two_day_delta_str) self.assertFalse(still_in_grace.answer_available())
used_all_attempts = CapaFactory.create(showanswer='past_due', max_attempts="1", attempts="1", due=self.tomorrow_str) self.assertFalse(used_all_attempts.answer_available())
past_due_date = CapaFactory.create(showanswer='past_due', max_attempts="1", attempts="0", due=self.yesterday_str) self.assertTrue(past_due_date.answer_available())
attempts_left_open = CapaFactory.create(showanswer='past_due', max_attempts="1", attempts="0", due=self.tomorrow_str) self.assertFalse(attempts_left_open.answer_available())
still_in_grace = CapaFactory.create(showanswer='past_due', max_attempts="1", attempts="1", due=self.yesterday_str, graceperiod=self.two_day_delta_str) self.assertFalse(still_in_grace.answer_available())
used_all_attempts = CapaFactory.create(showanswer='finished', max_attempts="1", attempts="1", due=self.tomorrow_str) self.assertTrue(used_all_attempts.answer_available())
past_due_date = CapaFactory.create(showanswer='finished', max_attempts="1", attempts="0", due=self.yesterday_str) self.assertTrue(past_due_date.answer_available())
attempts_left_open = CapaFactory.create(showanswer='finished', max_attempts="1", attempts="0", due=self.tomorrow_str) self.assertFalse(attempts_left_open.answer_available())
correct_ans = CapaFactory.create(showanswer='finished', max_attempts="1", attempts="0", due=self.tomorrow_str, correct=True) self.assertTrue(correct_ans.answer_available())
still_in_grace = CapaFactory.create(showanswer='finished', max_attempts="1", attempts="1", due=self.yesterday_str, graceperiod=self.two_day_delta_str) self.assertTrue(still_in_grace.answer_available())
module = CapaFactory.create(max_attempts="1", attempts="0") self.assertFalse(module.closed())
module = CapaFactory.create(max_attempts="2", attempts="1") self.assertFalse(module.closed())
module = CapaFactory.create(max_attempts="1", attempts="1") self.assertTrue(module.closed())
module = CapaFactory.create(max_attempts="1", attempts="2") self.assertTrue(module.closed())
module = CapaFactory.create(max_attempts="0", attempts="2") self.assertTrue(module.closed())
module = CapaFactory.create(max_attempts="1", attempts="0", due=self.yesterday_str) self.assertTrue(module.closed())
valid_get_dict = MultiDict({'input_1[]': 'test'}) result = CapaModule.make_dict_of_responses(valid_get_dict) self.assertEqual(result['1'], ['test'])
invalid_get_dict = MultiDict({'input': 'test'}) with self.assertRaises(ValueError): result = CapaModule.make_dict_of_responses(invalid_get_dict)
get_request_dict = {CapaFactory.input_key(): '3.14'} result = module.check_problem(get_request_dict)
self.assertEqual(result['success'], 'correct')
self.assertEqual(result['contents'], 'Test HTML')
self.assertEqual(module.attempts, 2)
with patch('capa.correctmap.CorrectMap.is_correct') as mock_is_correct: mock_is_correct.return_value = False
get_request_dict = {CapaFactory.input_key(): '0'} result = module.check_problem(get_request_dict)
self.assertEqual(result['success'], 'incorrect')
self.assertEqual(module.attempts, 1)
self.assertEqual(module.attempts, 3)
module = CapaFactory.create(rerandomize=rerandomize, attempts=0)
module.done = True
with self.assertRaises(xmodule.exceptions.NotFoundError): get_request_dict = {CapaFactory.input_key(): '3.14'} module.check_problem(get_request_dict)
self.assertEqual(module.attempts, 0)
module = CapaFactory.create(rerandomize=rerandomize, attempts=0, done=True)
get_request_dict = {CapaFactory.input_key(): '3.14'} result = module.check_problem(get_request_dict)
self.assertEqual(module.attempts, 1)
self.assertIn('You must wait', result['success'])
self.assertEqual(module.attempts, 1)
get_request_dict = { CapaFactoryWithFiles.input_key(response_num=2): fileobjs, CapaFactoryWithFiles.input_key(response_num=3): 'None', }
exception_classes = [StudentInputError, LoncapaProblemError, ResponseError] for exception_class in exception_classes:
module = CapaFactory.create(attempts=1)
module.system.user_is_staff = False
with patch('capa.capa_problem.LoncapaProblem.grade_answers') as mock_grade: mock_grade.side_effect = exception_class('test error')
expected_msg = 'Error: test error' self.assertEqual(expected_msg, result['success'])
self.assertEqual(module.attempts, 1)
module = CapaFactory.create(attempts=1)
module.system.user_is_staff = False
module.system.DEBUG = True
with patch('capa.capa_problem.LoncapaProblem.grade_answers') as mock_grade: error_msg = u"Superterrible error happened: ☠" mock_grade.side_effect = Exception(error_msg)
self.assertIn(error_msg, result['success'])
module = CapaFactory.create(attempts=1)
module.lcp.get_score = lambda: {'score': 0, 'total': 0}
get_request_dict = {CapaFactory.input_key(): '3.14'} module.check_problem(get_request_dict)
exception_classes = [StudentInputError, LoncapaProblemError, ResponseError] for exception_class in exception_classes:
module = CapaFactory.create(attempts=1)
module.system.user_is_staff = False
with patch('capa.capa_problem.LoncapaProblem.grade_answers') as mock_grade: mock_grade.side_effect = exception_class(u"ȧƈƈḗƞŧḗḓ ŧḗẋŧ ƒǿř ŧḗşŧīƞɠ")
expected_msg = u'Error: ȧƈƈḗƞŧḗḓ ŧḗẋŧ ƒǿř ŧḗşŧīƞɠ' self.assertEqual(expected_msg, result['success'])
self.assertEqual(module.attempts, 1)
for exception_class in [StudentInputError, LoncapaProblemError, ResponseError]:
module = CapaFactory.create(attempts=1)
module.system.user_is_staff = True
with patch('capa.capa_problem.LoncapaProblem.grade_answers') as mock_grade: mock_grade.side_effect = exception_class('test error')
self.assertIn('test error', result['success'])
self.assertIn('Traceback', result['success'])
self.assertEqual(module.attempts, 1)
with patch('xmodule.capa_module.CapaModule.get_problem_html') as mock_html: mock_html.return_value = "<div>Test HTML</div>"
get_request_dict = {} result = module.reset_problem(get_request_dict)
self.assertTrue('success' in result and result['success'])
self.assertIn('html', result) self.assertEqual(result['html'], "<div>Test HTML</div>")
module.new_lcp.assert_called_once_with(None)
module = CapaFactory.create(rerandomize=RANDOMIZATION.ALWAYS)
with patch('xmodule.capa_module.CapaModule.closed') as mock_closed: mock_closed.return_value = True
get_request_dict = {} result = module.reset_problem(get_request_dict)
self.assertTrue('success' in result and not result['success'])
module = CapaFactory.create(done=False)
get_request_dict = {} result = module.reset_problem(get_request_dict)
self.assertTrue('success' in result and not result['success'])
with patch('capa.responsetypes.LoncapaResponse.evaluate_answers') as mock_evaluate_answers: mock_evaluate_answers.return_value = CorrectMap(CapaFactory.answer_key(), 'correct') result = module.rescore_problem()
self.assertEqual(result['success'], 'correct')
self.assertNotIn('contents', result)
self.assertEqual(module.attempts, 1)
module = CapaFactory.create(attempts=0, done=True)
with patch('capa.responsetypes.LoncapaResponse.evaluate_answers') as mock_evaluate_answers: mock_evaluate_answers.return_value = CorrectMap(CapaFactory.answer_key(), 'incorrect') result = module.rescore_problem()
self.assertEqual(result['success'], 'incorrect')
self.assertEqual(module.attempts, 0)
module = CapaFactory.create(done=False)
with self.assertRaises(xmodule.exceptions.NotFoundError): module.rescore_problem()
with patch('capa.capa_problem.LoncapaProblem.supports_rescoring') as mock_supports_rescoring: mock_supports_rescoring.return_value = False with self.assertRaises(NotImplementedError): module.rescore_problem()
module = CapaFactory.create(attempts=1, done=True)
with patch('capa.capa_problem.LoncapaProblem.rescore_existing_answers') as mock_rescore: mock_rescore.side_effect = exception_class(u'test error \u03a9') result = module.rescore_problem()
expected_msg = u'Error: test error \u03a9' self.assertEqual(result['success'], expected_msg)
self.assertEqual(module.attempts, 1)
get_request_dict = {CapaFactory.input_key(): '3.14'} result = module.save_problem(get_request_dict)
expected_answers = {CapaFactory.answer_key(): '3.14'} self.assertEqual(module.lcp.student_answers, expected_answers)
self.assertTrue('success' in result and result['success'])
with patch('xmodule.capa_module.CapaModule.closed') as mock_closed: mock_closed.return_value = True
get_request_dict = {CapaFactory.input_key(): '3.14'} result = module.save_problem(get_request_dict)
self.assertTrue('success' in result and not result['success'])
module = CapaFactory.create(rerandomize=rerandomize, done=True)
get_request_dict = {CapaFactory.input_key(): '3.14'} result = module.save_problem(get_request_dict)
self.assertTrue('success' in result and not result['success'])
module = CapaFactory.create(rerandomize=rerandomize, done=True)
get_request_dict = {CapaFactory.input_key(): '3.14'} result = module.save_problem(get_request_dict)
self.assertTrue('success' in result and result['success'])
attempts = random.randint(1, 10) module = CapaFactory.create(attempts=attempts - 1, max_attempts=attempts) self.assertEqual(module.check_button_name(), "Final Check")
module = CapaFactory.create(attempts=attempts - 2, max_attempts=attempts) self.assertEqual(module.check_button_name(), "Check")
module = CapaFactory.create(attempts=attempts - 3) self.assertEqual(module.check_button_name(), "Check")
module = CapaFactory.create(due=self.yesterday_str) self.assertFalse(module.should_show_check_button())
module = CapaFactory.create(attempts=attempts, max_attempts=attempts) self.assertFalse(module.should_show_check_button())
module = CapaFactory.create(max_attempts=0) self.assertFalse(module.should_show_check_button())
module = CapaFactory.create(rerandomize=RANDOMIZATION.ALWAYS, done=True) self.assertFalse(module.should_show_check_button())
module = CapaFactory.create() self.assertTrue(module.should_show_check_button())
module = CapaFactory.create(rerandomize=RANDOMIZATION.NEVER, done=True) self.assertTrue(module.should_show_check_button())
module = CapaFactory.create(due=self.yesterday_str, done=True) self.assertFalse(module.should_show_reset_button())
module = CapaFactory.create(attempts=attempts, max_attempts=attempts, done=True) self.assertFalse(module.should_show_reset_button())
module = CapaFactory.create(rerandomize=RANDOMIZATION.ALWAYS, done=True) self.assertTrue(module.should_show_reset_button())
module = CapaFactory.create(rerandomize=RANDOMIZATION.ALWAYS, max_attempts=0, done=True) self.assertTrue(module.should_show_reset_button())
module = CapaFactory.create(rerandomize=RANDOMIZATION.ALWAYS, max_attempts=0, done=True, correct=False) self.assertTrue(module.should_show_reset_button())
module = CapaFactory.create(rerandomize=RANDOMIZATION.NEVER, max_attempts=0, done=True, correct=True) self.assertFalse(module.should_show_reset_button())
module = CapaFactory.create(rerandomize=RANDOMIZATION.ALWAYS, max_attempts=0, done=True, correct=True) self.assertTrue(module.should_show_reset_button())
module = CapaFactory.create(rerandomize=RANDOMIZATION.ALWAYS, show_reset_button=False, done=False) self.assertFalse(module.should_show_reset_button())
module = CapaFactory.create(rerandomize=RANDOMIZATION.ALWAYS, show_reset_button=False, done=True) self.assertTrue(module.should_show_reset_button())
module = CapaFactory.create(due=self.yesterday_str, done=True) self.assertFalse(module.should_show_save_button())
module = CapaFactory.create(attempts=attempts, max_attempts=attempts, done=True) self.assertFalse(module.should_show_save_button())
module = CapaFactory.create(rerandomize=RANDOMIZATION.ALWAYS, done=True) self.assertFalse(module.should_show_save_button())
module = CapaFactory.create(max_attempts=None, rerandomize=RANDOMIZATION.NEVER, done=False) self.assertFalse(module.should_show_save_button())
module = CapaFactory.create(rerandomize=RANDOMIZATION.ALWAYS, done=False) self.assertTrue(module.should_show_save_button())
module = CapaFactory.create(rerandomize=RANDOMIZATION.NEVER, max_attempts=2, done=True) self.assertTrue(module.should_show_save_button())
module = CapaFactory.create(max_attempts=0, done=False) self.assertTrue(module.should_show_save_button())
module = CapaFactory.create(due=self.yesterday_str, force_save_button="true", done=True) self.assertFalse(module.should_show_save_button())
attempts = random.randint(1, 10) module = CapaFactory.create(attempts=attempts, max_attempts=attempts, force_save_button="true", done=True) self.assertFalse(module.should_show_save_button())
module = CapaFactory.create(force_save_button="true", rerandomize=RANDOMIZATION.ALWAYS, done=True) self.assertTrue(module.should_show_save_button())
module.system.render_template = Mock(return_value="<div>Test Template HTML</div>")
with patch('capa.capa_problem.LoncapaProblem.get_html') as mock_html: mock_html.return_value = "<div>Test Problem HTML</div>"
html = module.get_problem_html(encapsulate=False)
html_encapsulated = module.get_problem_html(encapsulate=True)
self.assertEqual(html, "<div>Test Template HTML</div>")
render_args, _ = module.system.render_template.call_args self.assertEqual(len(render_args), 2)
self.assertIn(html, html_encapsulated)
module = CapaFactory.create(xml=self.demand_xml)
module.location = Mock(module.location) module.location.to_deprecated_string.return_value = 'i4x://edX/capa_test/problem/meh'
module1.set_state_from_lcp() self.assertEqual(module1.lcp.inputs.keys(), module1.input_state.keys())
original_problem = module.lcp
module.lcp.get_html = Mock(side_effect=Exception("Test"))
module.system.render_template = Mock(return_value="<div>Test Template HTML</div>")
module.system.DEBUG = False
html = module.get_problem_html()
render_args, _ = module.system.render_template.call_args context = render_args[1] self.assertIn("error", context['problem']['html'])
self.assertNotEqual(original_problem, module.lcp)
error_msg = u"Superterrible error happened: ☠" module.lcp.get_html = Mock(side_effect=Exception(error_msg))
module.system.render_template = Mock(return_value="<div>Test Template HTML</div>")
module.system.DEBUG = True
html = module.get_problem_html()
render_args, _ = module.system.render_template.call_args context = render_args[1] self.assertIn(error_msg, context['problem']['html'])
seed = module.seed self.assertTrue(seed is not None)
if rerandomize == RANDOMIZATION.NEVER: self.assertEqual(seed, 1, msg="Seed should always be 1 when rerandomize='%s'" % rerandomize)
get_request_dict = {CapaFactory.input_key(): '3.14'} module.check_problem(get_request_dict)
self.assertEqual(seed, module.seed)
module.save_problem(get_request_dict)
self.assertEqual(seed, module.seed)
module.reset_problem({})
return module.seed
seed = module.seed self.assertTrue(seed is not None)
if rerandomize in [RANDOMIZATION.NEVER, 'false', RANDOMIZATION.PER_STUDENT]: self.assertEqual(seed, _reset_and_get_seed(module))
else:
success = _retry_and_check(5, lambda: _reset_and_get_seed(module) != seed)
module.reset_problem({})
return module.seed
seed = module.seed self.assertTrue(seed is not None)
i = 200 while i > 0: module = CapaFactory.create(rerandomize=rerandomize) assert 0 <= module.seed < 1000 i -= 1
xml = ''.join(line.strip() for line in xml.split('\n')) factory = self.capa_factory_for_problem_xml(xml) module = factory.create()
mock_call = mock_track_function.mock_calls[-1] event = mock_call[1][2]
xqueue_interface = XQueueInterface("http://example.com/xqueue", Mock())
field_data['attempts'] = int(attempts)
module.get_score = lambda: {'score': 1, 'total': 1}
self.assertEqual(result['success'], 'correct') self.assertEqual(module.attempts, num_attempts + 1)
try: info_module.get_html() except ValueError: self.fail("CourseInfoModule could not parse an invalid date!")
self.assertRaises(ValueError, Progress, 0, 0) self.assertRaises(ValueError, Progress, 2, 0) self.assertRaises(ValueError, Progress, 1, -2)
self.assertRaises(TypeError, Progress, 2j, 3)
self.assertFalse(self.done.inprogress()) self.assertFalse(self.not_started.inprogress())
self.assertEqual(f(None), "0")
self.assertNotEqual(prg1, prg2) self.assertEqual(prg1, prg3)
'html5_sources': ['http://www.example.com/source.mp4'], 'data': '',
'html5_sources': ['http://www.example.com/source.mp4'], 'data': ''
'html5_sources': ['http://www.example.com/source.mp4'], 'data': ''
mock_val_api.ValVideoNotFoundError = _MockValVideoNotFoundError mock_val_api.export_to_xml = Mock(side_effect=mock_val_api.ValVideoNotFoundError) self.descriptor.edx_video_id = 'test_edx_video_id'
expected = '<video url_name="SampleProblem" download_video="false"/>\n' self.assertEquals(expected, etree.tostring(xml, pretty_print=True))
'API': 'www.youtube.com/iframe_api',
'METADATA_URL': 'www.googleapis.com/youtube/v3/videos/',
'ADDITIONAL_OPTIONS': { 'trashcan': { 'bucket': 'trash_fs' } }
d = get_dummy_course('2012-12-02T12:00') self.assertEqual('', d.end_datetime_text())
course = get_dummy_course('2012-12-02T12:00') self.assertEqual('', course.end_datetime_text("DATE_TIME"))
self.assertFalse(self.course.teams_enabled)
self.add_team_configuration(max_team_size=4, topics=[self.make_topic()]) self.assertTrue(self.course.teams_enabled)
self.add_team_configuration(max_team_size=4, topics=[]) self.assertFalse(self.course.teams_enabled)
ItemFactory.create( category="html", parent_location=library.location, user_id=self.user_id, publish_item=False, modulestore=self.store, data=message ) library = self.store.get_library(library.location.library_key)
randomize_module = RandomizeModule( randomize_descriptor, self.system, scope_ids=ScopeIds(None, None, self.course.id, self.course.id) )
print "Starting export" file_system = OSFS(root_dir) initial_course.runtime.export_fs = file_system.makeopendir(course_dir) root = lxml.etree.Element('root')
strip_filenames(initial_course) strip_filenames(exported_course)
NOT_STUDIO_EDITABLE = ( PollDescriptor, )
if depth == 0: self.get_module.side_effect = lambda x: LeafModuleFactory(descriptor_cls=HtmlDescriptor) else: self.get_module.side_effect = lambda x: ContainerModuleFactory( descriptor_cls=VerticalBlock, depth=depth - 1 )
if depth == 0: self.load_item.side_effect = lambda x: LeafModuleFactory(descriptor_cls=HtmlDescriptor) else: self.load_item.side_effect = lambda x: ContainerModuleFactory( descriptor_cls=VerticalBlock, depth=depth - 1 )
@ddt.data(*flatten(CONTAINER_XMODULES))
@ddt.data(*flatten(CONTAINER_XMODULES))
"Test for Annotation Xmodule functional logic."
def test_real_user(useless): useless_user = Mock(email='fake@fake.com', id=useless) return useless_user
def test_user_role(): return 'staff'
self.assertTrue(self.xmodule.verify_oauth_body_sign.called)
([
u'{"@type": "Result", "resultScore": 0.1}',
def test_real_user(useless): useless_user = Mock(email='fake@fake.com', id=useless) return useless_user
def test_user_role(): return 'staff'
block = self.get_a_block() self.assertEqual(block.inherited, "the default") self.assertEqual(block.not_inherited, "nothing")
parent = self.get_a_block(usage_id="parent") parent.inherited = "Changed!" self.assertEqual(parent.inherited, "Changed!")
parent = self.get_a_block(usage_id="parent") parent.not_inherited = "Changed!" self.assertEqual(parent.not_inherited, "Changed!")
self.assertEqual(1, len(editable_fields), editable_fields) self.assert_field_values( editable_fields, 'display_name', XModuleMixin.display_name, explicitly_set=False, value=None, default_value=None )
def get_xml_editable_fields(self, field_data): runtime = get_test_descriptor_system() return runtime.construct_xblock_from_class( XmlDescriptor, scope_ids=Mock(), field_data=field_data, ).editable_metadata_fields
self.assertDeserializeEqual(False, 'false') self.assertDeserializeEqual(True, 'true') self.assertDeserializeEqual(-2.78, '-2.78')
self.assertDeserializeEqual(False, 'false') self.assertDeserializeEqual(True, 'true')
self.assertDeserializeEqual('"false"', '"false"') self.assertDeserializeNonString()
self.assertDeserializeEqual(False, 'false') self.assertDeserializeEqual(True, 'true')
self.assertDeserializeEqual('False', 'False') self.assertDeserializeEqual('True', 'True')
self.assertDeserializeEqual(-2.78, '-2.78')
self.assertDeserializeEqual('10:20:30', '"10:20:30"')
assert_false(hasattr(SequenceDescriptor, 'rerandomize'))
assert_equals('never', seq.rerandomize)
assert_not_in('rerandomize', seq.xml_attributes)
assert_false(hasattr(SequenceDescriptor, 'attempts'))
assert_false(hasattr(seq, 'attempts'))
assert_in('attempts', seq.xml_attributes)
assert_false(hasattr(SequenceDescriptor, attribute))
assert_true(hasattr(InheritanceMixin, attribute))
assert_in(InheritanceMixin, root.xblock_mixins)
assert_equals(value, getattr(seq, attribute))
assert_not_in(attribute, seq.xml_attributes)
content = StaticContent('loc', 'name', 'content_type', 'data', None, None, None) self.assertIsNone(content.thumbnail_location)
return {'cond_module': cond_descriptor, 'source_module': source_descriptor, 'child_module': child_descriptor}
location = Location("HarvardX", "ER22x", "2013_Spring", "conditional", "condone")
'ajax_url': '{}/xmodule_handler'.format(location.to_deprecated_string()), 'element_id': u'i4x-HarvardX-ER22x-conditional-condone', 'depends': u'i4x-HarvardX-ER22x-problem-choiceprob'
inner_module = inner_get_module(location.replace(category="problem", name='choiceprob')) inner_module.attempts = 1 inner_module.save()
import collections import json import logging from pkg_resources import resource_string
class_priority = ['video', 'problem']
_ = lambda text: text
position = getattr(self.system, 'position', None) if position is not None: assert isinstance(position, int) self.position = self.system.position
self._capture_basic_metrics()
if self.is_time_limited: view_html = self._time_limited_student_view(context)
if view_html: fragment.add_content(view_html) return fragment
return fragment
newrelic.agent.add_custom_parameter('seq.num_units', len(display_items))
all_item_keys = self._locations_in_subtree(self) newrelic.agent.add_custom_parameter('seq.num_items', len(all_item_keys))
view_html = None
if credit_service: credit_state = credit_service.get_credit_state(user_id, course_id) if credit_state: context.update({ 'credit_state': credit_state })
view_html = proctoring_service.get_student_view( user_id=user_id, course_id=course_id, content_id=content_id, context=context, user_role=user_role_in_course )
_ = lambda text: text
msg = "No valid user id found in endpoint URL" log.info("[LTI]: {}".format(msg)) raise LTIError(msg)
base_json_obj['resultScore'] = round(self.module_score, 2) base_json_obj['comment'] = self.score_comment return Response(json.dumps(base_json_obj), content_type=LTI_2_0_JSON_CONTENT_TYPE)
if score is None: self.clear_user_module_score(real_user) return Response(status=200)
self.set_user_module_score(real_user, score, self.max_score(), comment) return Response(status=200)
try:
return time.strftime('%Y-%m-%dT%H:%M:%SZ', value)
return value.isoformat()
MUTABLE = False
MUTABLE = False
if isinstance(value, float): return datetime.timedelta(seconds=value)
_ = lambda text: text
raw_student_words = data.getall('student_words[]') student_words = filter(None, map(self.good_word, raw_student_words))
temp_all_words = self.all_words
for word in self.student_words: temp_all_words[word] = temp_all_words.get(word, 0) + 1
self.top_words = self.top_dict( temp_all_words, self.num_top_words )
self.all_words = temp_all_words
self.import_path = import_path self.locked = locked
url_path = StaticContent.serialize_asset_key_with_slash( course_key.make_asset_key('asset', placeholder_id).for_branch(None) ) return url_path.replace(placeholder_id, '')
if path.startswith('/static/'): path = path[len('/static/'):]
return StaticContent.compute_location(course_key, path)
_, _, relative_path, params, query_string, fragment = urlparse(path)
asset_key = StaticContent.get_asset_key_from_path(course_key, relative_path)
if any(relative_path.lower().endswith(excluded_ext.lower()) for excluded_ext in excluded_exts): serve_from_cdn = False
thumbnail_name = StaticContent.generate_thumbnail_name( content.location.name, dimensions=dimensions ) thumbnail_file_location = StaticContent.compute_location( content.location.course_key, thumbnail_name, is_thumbnail=True )
im = im.convert('RGB')
thumbnail_content = StaticContent(thumbnail_file_location, thumbnail_name, 'image/jpeg', thumbnail_file)
logging.exception(u"Failed to generate thumbnail for {0}. Exception: {1}".format(content.location, str(e)))
proxy = False mongo_db = connect_to_mongodb( db, host, port=port, tz_aware=tz_aware, user=user, password=password, proxy=proxy, **kwargs )
locked=getattr(content, 'locked', False)) as fp:
self.fs.delete(location_or_id)
export_name = escape_invalid_characters(name=filename, invalid_char_list=['/', '\\'])
thumbnail_location=asset['thumbnail_location'], import_path=asset['import_path'], locked=asset.get('locked', False)
ordered_key_fields = ['category', 'name', 'course', 'tag', 'org', 'revision']
dbkey['run'] = location.run content_id = unicode(location.for_branch(None))
dbkey['run'] = _id_field['run']
thumbs = store.get_all_content_thumbnails_for_course(course_loc) for thumb in thumbs: print "Deleting {0}...".format(thumb) store.delete(thumb['_id'])
assets, __ = store.get_all_content_for_course(course_loc) for asset in assets: print "Deleting {0}...".format(asset) store.delete(asset['_id'])
store.save(content)
if content.thumbnail_location is not None: try: thumbnail_content = trash.find(content.thumbnail_location) store.save(thumbnail_content) except Exception:
_ = lambda text: text
_ = lambda text: text
return {'ok': False, 'msg': msg}
msg = '<div class="capa_alert">%s</div>' % msg return msg
if options is None: options = '' do_matrix = 'matrix' in options do_qubit = 'qubit' in options do_numerical = 'numerical' in options
try: fans = my_sympify(str(ans), matrix=do_matrix, do_qubit=do_qubit) except Exception, err: fans = None
if fexpect == fsym: return {'ok': True, 'msg': msg}
return {'ok': False, 'msg': msg}
expr_s = re.sub( r'script([a-zA-Z0-9]+)', '\\mathcal{\\1}', expr_s )
if symtab: varset = symtab else: varset = { 'p': sympy.Symbol('p'), 'g': sympy.Symbol('g'),
if ( tag == 'msup' and len(k) == 2 and gettag(k[1]) == 'mrow' and
if ( tag == 'msubsup' and len(k) == 3 and gettag(k[2]) == 'mrow' and
try: xml = self.preprocess_pmathml(self.expr)
self.the_cmathml = self.GetContentMathML(self.asciimath, pmathml) return self.the_cmathml
tag = gettag(xml)
result = symmath_check(expected_str, expected_str, dynamath=[dynamath]) self.assertTrue('ok' in result and result['ok'])
result = symmath_check(expected_str, input_str, dynamath=[dynamath]) self.assertTrue('ok' in result and result['ok'])
mathml_start = '<math xmlns="http://www.w3.org/1998/Math/MathML"><mstyle displaystyle="true">' mathml_end = '</mstyle></math>'
expr = stripXML(self.mathml_start + expr + self.mathml_end) expected = stripXML(self.mathml_start + expected + self.mathml_end)
xml = etree.fromstring(expr) xml = self.formulaInstance.preprocess_pmathml(xml) test = etree.tostring(xml)
self.assertEqual(test, expected)
expr = stripXML(self.mathml_start + expr + self.mathml_end) expected = stripXML(self.mathml_start + expected + self.mathml_end)
xml = etree.fromstring(expr) xml = self.formulaInstance.preprocess_pmathml(xml) test = etree.tostring(xml)
self.assertEqual(test, expected)
expr = stripXML(self.mathml_start + expr + self.mathml_end) expected = stripXML(self.mathml_start + expected + self.mathml_end)
xml = etree.fromstring(expr) xml = self.formulaInstance.preprocess_pmathml(xml) test = etree.tostring(xml)
self.assertEqual(test, expected)
expr = stripXML(self.mathml_start + expr + self.mathml_end) expected = stripXML(self.mathml_start + expected + self.mathml_end)
xml = etree.fromstring(expr) xml = self.formulaInstance.preprocess_pmathml(xml) test = etree.tostring(xml)
self.assertEqual(test, expected)
self.start_time = datetime.now(UTC) - timedelta(seconds=1)
result = next(k for k in parse_result if isinstance(k, numbers.Number)) return result
parse_result = reversed( [k for k in parse_result
power = reduce(lambda a, b: b ** a, parse_result) return power
if math_expr.strip() == "": return float('nan')
math_interpreter = ParseAugmenter(math_expr, case_sensitive) math_interpreter.parse_algebra()
all_variables, all_functions = add_defaults(variables, functions, case_sensitive)
math_interpreter.check_variables(all_variables, all_functions)
if case_sensitive: casify = lambda x: x else:
number_part = Word(nums) inner_number = (number_part + Optional("." + Optional(number_part))) | ("." + number_part) inner_number = Combine(inner_number)
number_suffix = MatchFirst(Literal(k) for k in SUFFIXES.keys())
expr = Forward()
inner_varname = Word(alphas + "_", alphanums + "_") varname = Group(inner_varname)("variable") varname.setParseAction(self.variable_parse_action)
function = Group(inner_varname + Suppress("(") + expr + Suppress(")"))("function") function.setParseAction(self.function_parse_action)
pow_term = atom + ZeroOrMore("^" + atom) pow_term = Group(pow_term)("power")
if terminal_converter is None: return node else: return terminal_converter(node)
return handle_node(self.tree)
if parens is not None: left_parens = parens if left_parens == '{': left_parens = r'\{'
greek += [x.capitalize() for x in greek]
greek.append('hbar')
greek.append('infty')
varname = ur"{a}_{{{b}}}".format( a=enrich_varname(first), b=enrich_varname(second) )
latex = fname + inner return LatexRendered(latex, tall=children[1].tall)
fraction_mode_ever = True position = "denominator"
latex += render_frac(numerator, denominator) + r"\cdot "
position = "numerator" numerator = [] denominator = []
if position == "denominator": latex += render_frac(numerator, denominator) else: num_latex = r"\cdot ".join(k.latex for k in numerator) latex += num_latex
if math_expr.strip() == "": return ""
latex_interpreter = ParseAugmenter(math_expr, case_sensitive) latex_interpreter.parse_algebra()
variables, functions = add_defaults(variables, functions, case_sensitive)
if case_sensitive: casify = lambda x: x else:
inputs = inputs[1:] neg_inputs = neg_inputs[1:]
self.assert_function_values( 'sqrt',
self.assert_function_values('abs', [-1, 0, 1, 'j'], [1, 0, 1, 1])
self.assertAlmostEqual( calc.evaluator(variables, {}, '3*x-y'),
self.assertAlmostEqual( calc.evaluator(variables, {}, "T", case_sensitive=True), 298, delta=0.2 )
self.assertEquals( preview.latex_preview('-x+2-3+4', variables=['x']), '-x+2-3+4' )
bad_exceptions[math] = None
complex_value_list = [] v_value = value while isinstance(v_value, dict): v_key = v_value.keys()[0] v_value = v_value.values()[0] complex_value_list.append(v_key)
if not self or not other: return False
return False
return False
user_answer = json.loads(user_answer)
user_answer = flat_user_answer(user_answer)
if not isinstance(r, numbers.Number) or \ r < 0 or \ math.isnan(r) or \ math.isinf(r): return False
if r == 0: return True
while r < 100: r = r * 10 while r >= 1000: r = r / 10
if abs(r - round(r)) > 0.01: return False r = int(round(r))
for type_list in valid_types: if r in type_list: return True if int(r / 10.) in type_list and (r % 10) == 0: return True
print not iseia(2200, (E48, E96, E192)) print iseia(5490e2, (E48, E96, E192)) print iseia(2200) print not iseia(5490e2)
if len(cls.tags) == 0: raise ValueError("No tags specified for class {0}".format(cls.__name__))
continue
for t in cls.tags: self._mapping[t] = cls
return cls
solution_tags = ['solution']
response_properties = ["codeparam", "responseparam", "answer", "openendedparam"]
html_problem_semantics = [ "codeparam", "responseparam", "answer", "script", "hintgroup", "openendedparam", "openendedrubric", ]
self.seed = state.get('seed', seed) assert self.seed is not None, "Seed must be provided for LoncapaProblem."
problem_text = re.sub(r"startouttext\s*/", "text", problem_text) problem_text = re.sub(r"endouttext\s*/", "/text", problem_text) self.problem_text = problem_text
self.tree = etree.XML(problem_text)
self._process_includes()
self.context = self._extract_context(self.tree)
self.inputs = {}
responder.update_score(score_msg, cmap, queuekey)
for the_input in self.inputs.values(): if hasattr(the_input, 'ungraded_response'): the_input.ungraded_response(xqueue_msg, queuekey)
self.student_answers = convert_files_to_filenames(answers) return self._grade_answers(answers)
oldcmap = self.correct_map
answer_map = dict() for response in self.responders.keys(): results = self.responder_answers[response] answer_map.update(results)
if hasattr(self, 'has_targeted'): return
choicegroup = mult_choice_response.xpath('./choicegroup[@type="MultipleChoice"]')[0] choices_list = list(choicegroup.iter('choice'))
student_answer = self.student_answers.get(choicegroup.get('id')) expl_id_for_student_answer = None
if not show_explanation or not self.done: continue
if solution_element is None: continue
parent_element.remove(solution_element)
solution_element.tag = 'targetedfeedback' targetedfeedbackset.append(solution_element)
ifp = self.capa_system.filestore.open(filename)
incxml = etree.XML(ifp.read())
raw_path = script.get('system_path', '').split(":") + DEFAULT_PATH
path = []
zip_lib = self.capa_system.get_python_lib_zip() if zip_lib is not None: extra_files.append(("python_lib.zip", zip_lib)) python_path.append("python_lib.zip")
context['script_code'] = all_code context['python_path'] = python_path context['extra_files'] = extra_files or None return context
return
return deepcopy(problemtree)
self.inputs[input_id] = input_type_cls(self.capa_system, problemtree, state) return self.inputs[input_id].get_html()
if problemtree in self.responders: overall_msg = self.correct_map.get_overall_message() return self.responders[problemtree].render_html( self._extract_html, response_msg=overall_msg )
tree = etree.Element(problemtree.tag) for item in problemtree: item_xhtml = self._extract_html(item) if item_xhtml is not None: tree.append(item_xhtml)
for (key, value) in problemtree.items(): tree.set(key, value)
response.set('id', response_id_str) response_id += 1
responsetype_cls = responsetypes.registry.get_class_for_tag(response.tag) responder = responsetype_cls(response, inputfields, self.context, self.capa_system, self.capa_module) self.responders[response] = responder
old_stdout, old_stderr = sys.stdout, sys.stderr try: sys.stdout = StringIO() sys.stderr = StringIO()
real_answers = problem.get_question_answers()
all_answer_ids = problem.get_answer_ids() all_answers = dict((answer_id, real_answers.get(answer_id, "")) for answer_id in all_answer_ids)
_ = lambda text: text
multi_device_support = False
self.answer_ids = [x.get('id') for x in self.inputfields] if self.max_inputfields == 1: self.answer_id = self.answer_ids[0]
partial_credit = xml.xpath('.')[0].get('partial_credit', default=False)
tree = etree.Element('span')
if self.xml.get('inline', ''): tree.set('class', 'inline')
if response_msg: tree.append(self._render_response_msg_html(response_msg))
event_info = dict() event_info['module_id'] = self.capa_module.location.to_deprecated_string() event_info['problem_part_id'] = self.id
if correct: style = QUESTION_HINT_CORRECT_STYLE else: style = QUESTION_HINT_INCORRECT_STYLE
return u'<div class="{0}">{1}{2}</div>'.format(style, label_wrap, hints_wrap)
CORRECTMAP_PY = inspect.getsource(correctmap)
aid = self.answer_ids[-1] new_cmap.set_hint_and_mode(aid, hint_text, hintmode)
self.get_extended_hints(student_answers, new_cmap)
try: response_msg_div = etree.XML('<div>%s</div>' % str(response_msg))
response_msg_div.set("class", "response_message")
self.parse_xml()
if not choice.get('id'): choice.set("id", chr(ord("A") + index))
if not self.has_partial_credit: return self.grade_without_partial_credit(student_answer=student_answer)
graders = { 'edc': self.grade_via_every_decision_counts, 'halves': self.grade_via_halves, 'false': self.grade_without_partial_credit }
if len(self.credit_type) > 1: raise LoncapaProblemError('Only one type of partial credit is allowed for Checkbox problems.')
if self.credit_type[0] not in graders: raise LoncapaProblemError('partial_credit attribute should be one of: ' + ','.join(graders))
return graders[self.credit_type[0]]( all_choices=all_choices, student_answer=student_answer, student_non_answers=student_non_answers )
if self.get_compound_hints(new_cmap, student_answers): return
selectors = compound_hint.get('value').upper().split() selector_set = set(selectors)
self.mc_setup_response()
xml = self.xml cxml = xml.xpath('//*[@id=$id]//choice', id=xml.get('id'))
if isinstance(student_answer, list): student_answer = student_answer[0]
if not self.has_partial_credit: return self.grade_without_partial_credit(student_answers=student_answers)
graders = { 'points': self.grade_via_points, 'false': self.grade_without_partial_credit }
if len(self.credit_type) > 1: raise LoncapaProblemError('Only one type of partial credit is allowed for Multiple Choice problems.')
if self.credit_type[0] not in graders: raise LoncapaProblemError('partial_credit attribute should be one of: ' + ','.join(graders))
return graders[self.credit_type[0]]( student_answers=student_answers )
choices = self.xml.xpath('choicegroup/choice') return [choice.get("name") for choice in choices]
msg = _("answer-pool value should be an integer") raise LoncapaProblemError(msg)
if self.has_answerpool(): return
for choice in choices_list: choicegroup.remove(choice)
(solution_id, subset_choices) = self.sample_from_answer_pool(choices_list, rng, num_choices)
for choice in subset_choices: choicegroup.append(choice)
num_incorrect = num_pool - 1 num_incorrect = min(num_incorrect, len(incorrect_choices))
index = rng.randint(0, len(correct_choices) - 1) correct_choice = correct_choices[index] solution_id = correct_choice.get('explanation-id')
subset_choices = [correct_choice] rng.shuffle(incorrect_choices) subset_choices += incorrect_choices[:num_incorrect] rng.shuffle(subset_choices)
if unicode(val) == student_answers[aid]: return '$' + key
tolerance_xml = xml.xpath( '//*[@id=$id]//responseparam[@type="tolerance"]/@default', id=xml.get('id') )
has_partial_range = tree.xpath('responseparam[@partial_range]') if has_partial_range: partial_range = float(has_partial_range[0].get('partial_range', default='2')) else: partial_range = 2
_("There was a problem with the staff answer to this problem: complex boundary.")
_("There was a problem with the staff answer to this problem: empty boundary.")
if self.backward: self.setup_response_backward() return
responses = self.xml.xpath('//stringresponse[@id=$id]', id=self.id) if responses: response = responses[0]
regex = re.compile('^' + answer + '$', flags=flags | re.UNICODE) return re.search(regex, given)
if not given: return False
if self.backward: return self.check_string_backward(expected, given)
separator = u' <b>{}</b> '.format(_('or')) return {self.answer_id: separator.join(self.correct_answer)}
default_pc = 0.5
self.expect = contextualize_text(xml.get('expect') or xml.get('answer'), self.context)
self.code = None answer = None try: answer = xml.xpath('//*[@id=$id]//answer', id=xml.get('id'))[0] except IndexError:
cfn = xml.get('cfn') if cfn: log.debug("cfn = %s", cfn)
dynamath = [student_answers.get(k + '_dynamath', None) for k in idset]
correct = ['unknown'] * len(idset) messages = [''] * len(idset) overall_message = ""
self.context.update({ 'response_id': self.id,
'expect': self.expect,
'submission': submission,
'idset': idset,
'dynamath': dynamath,
'answers': student_answers,
'correct': correct,
'messages': messages,
'overall_message': overall_message,
'options': self.xml.get('options'), 'testdat': 'hello world',
self.context['debug'] = self.capa_system.DEBUG
self.execute_check_function(idset, submission)
if len(idset) > 1: self.context['overall_message'] = msg else: self.context['messages'][0] = msg
else: log.error(traceback.format_exc()) _ = self.capa_system.i18n.ugettext raise ResponseError( _("CustomResponse: check function returned an invalid dictionary!") )
if msg:
msg = '<html>' + msg + '</html>'
msg = msg.replace('&#60;', '&lt;')
msg = etree.tostring(fromstring_bs(msg, convertEntities=None), pretty_print=True)
msg = re.sub('(?ms)<html>(.*)</html>', '\\1', msg)
return msg.strip()
else: return ""
msg = 'Error occurred while evaluating CustomResponse' log.warning(msg, exc_info=True)
_, _, traceback_obj = sys.exc_info() raise ResponseError(err.message, traceback_obj)
self.xml.set('cfn', 'symmath_check')
super(SymbolicResponse, self).setup_response()
answer_given = submission[0]
msg = _(u"An error occurred with SymbolicResponse. The error was: {error_msg}").format( error_msg=err, ) raise Exception(msg)
codeparam = self.xml.find('codeparam') assert codeparam is not None, "Unsupported old format! <coderesponse> without <codeparam>" self._parse_coderesponse_xml(codeparam)
submission = student_answers[self.answer_id]
#
queuestate = {'key': queuekey, 'time': qtime, }
cmap.set(self.answer_id, queuestate=queuestate, correctness='incomplete', msg=msg)
error_msg = _('Invalid grader reply. Please contact the course staff.') oldcmap.set(self.answer_id, msg=error_msg) return oldcmap
self.url = xml.get('url') or "http://qisx.mit.edu:8889/pyloncapa"
rxml = etree.fromstring(req.text)
tolerance_xml = xml.xpath( '//*[@id=$id]//responseparam[@type="tolerance"]/@default', id=xml.get('id') )
self.case_sensitive = False
self.case_sensitive = True
self.case_sensitive = False
try: correctness = self.check_formula( correct_answer, given, samples ) except Exception: correctness = 'incorrect' if correctness == 'correct': hints_to_show.append(name)
self.code = self.capa_system.filestore.open('src/' + answer_src).read()
msg = _('Error in evaluating SchematicResponse. The error was: {error_msg}').format(error_msg=err) raise ResponseError(msg)
return self.default_answer_map
if (llx <= ans_x <= urx) and (lly <= ans_y <= ury): correct_map.set(aid, 'correct') break
choices_correct = self._check_student_choices(binary_choices) inputs_correct = self._check_student_inputs(numtolerance_inputs) correct = choices_correct and inputs_correct
numtolerance_choices = {} binary_choices = {}
selected_choices = [key for key in a_dict if key.endswith("bc")] for key in selected_choices: binary_choices[key] = a_dict[key]
selected_numtolerance_inputs = [ key for key in a_dict if key.partition("_numtolerance_input_")[0] + "bc" in selected_choices ]
params = self.correct_inputs.get(answer_name, {'answer': 0})
if answer_name in self.correct_inputs and not partial_correct: inputs_correct = False
__all__ = [ CodeResponse, NumericalResponse, FormulaResponse, CustomResponse, SchematicResponse, ExternalResponse, ImageResponse, OptionResponse, SymbolicResponse, StringResponse, ChoiceResponse, MultipleChoiceResponse, TrueFalseResponse, JavascriptResponse, AnnotationResponse, ChoiceTextResponse, ]
import hashlib import json import logging import requests import dogstats_wrapper as dog_stats_api
(error, msg) = self._send_to_queue(header, body, files_to_upload)
default_tolerance = '0.001%'
return student_complex == instructor_complex
student_complex = complex(student_complex) instructor_complex = complex(instructor_complex)
return abs(student_complex - instructor_complex) <= tolerance
if is_list_of_files(answer): new_answers[answer_id] = [f.name for f in answer] else: new_answers[answer_id] = answers[answer_id]
self.cmap = dict() self.items = self.cmap.items self.keys = self.cmap.keys self.overall_message = "" self.set(*args, **kwargs)
def set( self, answer_id=None, correctness=None, npoints=None, msg='', hint='', hintmode=None, queuestate=None,
self.__init__()
return 0
TEMPLATE_NAME = None
context_dict.setdefault("STATIC_URL", "/dummy-static/") try: xml_str = self.template.render_unicode(**context_dict) except: raise TemplateError(exceptions.text_error_template().render())
xml = self.render_to_xml(self.context) xpath = "//div[@class='indicator-container']/span[@class='status correct']" self.assert_has_xpath(xml, xpath, self.context)
self.assert_no_xpath(xml, "//label[@class='choicegroup_incorrect']", self.context)
self.assert_no_xpath(xml, "//label[@class='choicegroup_incorrect']", self.context)
self.assert_no_xpath(xml, "//label[@class='choicegroup_incorrect']", self.context)
xpath = "//div[@class='indicator-container']/span" self.assert_no_xpath(xml, xpath, self.context)
xpath = "//div[@class='indicator-container']/span" self.assert_no_xpath(xml, xpath, self.context)
xpath = "//div[@class='indicator-container']/span[@class='status correct']" self.assert_no_xpath(xml, xpath, self.context)
self.assert_no_xpath(xml, "//label[@class='choicegroup_incorrect']", self.context)
self.assert_has_text(xml, "//div[@class='capa_alert']", self.context['submitted_message'])
self.assert_no_xpath(xml, "//div[@class='capa_alert']", self.context)
xpath = "//div[@class='%s ']" % div_class self.assert_has_xpath(xml, xpath, self.context)
self.assert_has_text(xml, "//span[@class='status']/span[@class='sr']", status_mark, exact=False)
xpath = "//div[@class='%s inline']" % div_class self.assert_has_xpath(xml, xpath, self.context)
self.context['return_to_annotation'] = True xml = self.render_to_xml(self.context) self.assert_has_xpath(xml, xpath, self.context)
self.context['return_to_annotation'] = False xml = self.render_to_xml(self.context) self.assert_no_xpath(xml, xpath, self.context)
xpath = "//span[contains(@class,'selected')]/p/b" self.assert_has_text(xml, xpath, 'HTML 2', exact=False)
test_cases = [('unsubmitted', 'unanswered'), ('incomplete', 'incorrect'), ('incorrect', 'incorrect')]
xpath = "//div[@class='block']/p/b" self.assert_has_text(xml, xpath, 'prompt HTML')
xpath = "//div[@class='block']/p/b" self.assert_has_text(xml, xpath, 'HTML')
xpath = "//section[@class='math-string']/span[2]/p/b" self.assert_has_text(xml, xpath, 'tail')
self.context['options'] = [(id_num, '<b>Option {0}</b>'.format(id_num)) for id_num in range(5)] self.context['value'] = 2
xpath = "//option[@value='option_2_dummy_default']" self.assert_has_xpath(xml, xpath, self.context)
xpath = "//option[@selected='true']/b" self.assert_has_text(xml, xpath, 'Option 2')
xpath = "//div[@class='{0}']".format(expected_css_class) self.assert_has_xpath(xml, xpath, self.context)
xpath = "//p[@class='status']" self.assert_has_text(xml, xpath, expected_text, exact=False)
xpath = "//div[@class='drag_and_drop_problem_json']/p/b" self.assert_has_text(xml, xpath, 'HTML')
xml = self.render_to_xml(self.context) xpath = "//div[@class='indicator-container']/span[@class='status correct']" self.assert_has_xpath(xml, xpath, self.context)
self.assert_no_xpath(xml, "//label[@class='choicetextgroup_incorrect']", self.context)
grouping_tag = grouping_tags[test_conditions['input_type']] self.assert_no_xpath(xml, "//{0}[@class='choicetextgroup_incorrect']".format(grouping_tag), self.context)
grouping_tag = grouping_tags[test_conditions['input_type']] self.assert_no_xpath(xml, "//{0}[@class='choicetextgroup_incorrect']".format(grouping_tag), self.context)
xpath = "//div[@class='indicator-container']/span" self.assert_no_xpath(xml, xpath, self.context)
xpath = "//div[@class='indicator-container']/span" self.assert_no_xpath(xml, xpath, self.context)
return capa_module
lookup_tag = inputtypes.registry.get_class_for_tag
check(u"('hasnt','hasn't')", [u'hasnt', u'hasn\'t'])
'msg': '', 'value': '3', 'params': params, 'display_file': display_file, 'display_class': display_class, 'problem_state': problem_state,
self.the_input.capa_system.render_template = lambda *args: "<aaa" with self.assertRaises(etree.XMLSyntaxError): self.the_input.get_html()
self.check('[50,40]', 35, 25)
'msg': '', 'width': width, 'height': height,
'msg': '', 'drag_and_drop_json': json.dumps(user_input)
statobj = inputtypes.Status('queued', func) self.assertEqual(statobj.display_name, u'PROCESSING')
the_html2 = problem.get_html() self.assertEquals(the_html, the_html2)
problem = new_loncapa_problem(xml_str) problem.done = True
the_html2 = problem.get_html() self.assertEquals(the_html, the_html2)
problem = new_loncapa_problem(xml_str) problem.done = True problem.student_answers = {'1_2_1': 'choice_1'}
problem = new_loncapa_problem(xml_str) problem.done = True problem.student_answers = {'1_2_1': 'choice_1'}
self.assertRegexpMatches( without_new_lines, r'<targetedfeedbackset.*?>.*?explanation-id="feedback1".*?</targetedfeedbackset>.*' + r'<targetedfeedbackset.*?>\s*</targetedfeedbackset>' )
maxDiff = None
correct_map = problem.grade_answers({'1_2_1': 'choice_0'}) self.assertAlmostEqual(correct_map.get_npoints('1_2_1'), 1)
self.assert_grade(problem, 'choice_3', 'incorrect') self.assert_grade(problem, 'not_a_choice', 'incorrect')
self.assert_grade(problem, 'choice_foil_4', 'incorrect') self.assert_grade(problem, 'not_a_choice', 'incorrect')
problem = self.build_problem(rectangle="(10,10)-(20,20)")
rectangle_str = "(10,10)-(20,20);(100,100)-(200,200)"
region_str = "[ [1,1], [5,10], [0,10] ]"
region_str = "[[[10,10], [20,10], [20, 30]], [[100,100], [120,100], [120,150]]]"
with self.assertRaises(Exception): self.build_problem(math_display=True, expect="2*x+3*y", num_inputs=3)
with mock.patch.object(requests, 'post') as mock_post: mock_post.return_value.text = snuggletex_resp
self.assert_grade(problem, "invalid_option", "incorrect")
problem = self.build_problem(options=["hasnot", "hasn't", "has'nt"], correct_option="hasn't")
sample_dict = {'x': (-10, 10), 'y': (-10, 10)}
problem = self.build_problem(sample_dict=sample_dict, num_samples=10, tolerance=0.01, answer="x+2*y")
input_formula = "2*x - x + y + y" self.assert_grade(problem, input_formula, "correct")
input_formula = "x + y" self.assert_grade(problem, input_formula, "incorrect")
sample_dict = {'x': (-10, 10), 'y': (-10, 10)}
problem = self.build_problem(sample_dict=sample_dict, num_samples=10, tolerance=0.01, answer="x+2*y", hints=hints)
script = "calculated_ans = 'x+x'"
sample_dict = {'x': (-10, 10)}
problem = self.build_problem(sample_dict=sample_dict, num_samples=10, tolerance=0.01, answer="$calculated_ans", script=script)
self.assert_grade(problem, '2*x', 'correct') self.assert_grade(problem, '3*x', 'incorrect')
self.assert_grade(problem, answer, "correct")
self.assert_grade(problem, answer, "correct") self.assert_grade(problem, answer.lower(), "correct")
problem = self.build_problem(answer=".*tre+", regexp=True) self.assert_grade(problem, "There is a tree", "correct")
problem_specified = self.build_problem(answer="Second", case_sensitive=True)
problem_not_specified = self.build_problem(answer="Second") problems = [problem_specified, problem_not_specified]
self.assert_grade(problem, "Second", "correct")
self.assert_grade(problem, "Other String", "incorrect") self.assert_grade(problem, "second", "incorrect")
answers = ["Second", "Third", "Fourth"]
self.assert_grade(problem, "Other String", "incorrect") self.assert_grade(problem, "second", "incorrect")
problem = self.build_problem(answer=u"\\\\", case_sensitive=False, regexp=True) self.assert_grade(problem, u"\\", "correct")
problem = self.build_problem(answer="Second", case_sensitive=False)
self.assert_grade(problem, "Second", "correct") self.assert_grade(problem, "second", "correct")
self.assert_grade(problem, "Other String", "incorrect")
answers = ["Second", "Third", "Fourth"] problem = self.build_problem(answer="sample_answer", case_sensitive=False, additional_answers=answers)
self.assert_grade(problem, answer, "correct") self.assert_grade(problem, answer.lower(), "correct")
self.assert_grade(problem, "Other String", "incorrect")
input_dict = {'1_2_1': 'Michigan'} correct_map = problem.grade_answers(input_dict) self.assertEquals(correct_map.get_hint('1_2_1'), "")
input_dict = {'1_2_1': 'California'} correct_map = problem.grade_answers(input_dict) self.assertEquals(correct_map.get_hint('1_2_1'), "")
input_dict = {'1_2_1': 'Michigan'} correct_map = problem.grade_answers(input_dict) self.assertEquals(correct_map.get_hint('1_2_1'), "")
input_dict = {'1_2_1': 'California'} correct_map = problem.grade_answers(input_dict) self.assertEquals(correct_map.get_hint('1_2_1'), "")
cmap = CorrectMap() for answer_id in answer_ids: cmap.update(CorrectMap(answer_id=answer_id, queuestate=None)) self.problem.correct_map.update(cmap)
for correctness in ['correct', 'incorrect']: self.problem.correct_map = CorrectMap()
cmap = CorrectMap() for answer_id in answer_ids: cmap.update(CorrectMap(answer_id=answer_id, queuestate=None)) self.problem.correct_map.update(cmap)
latest_timestamp = datetime.strptime( datetime.strftime(latest_timestamp, dateformat), dateformat ).replace(tzinfo=UTC)
self.assert_grade(problem, 'choice_3', 'incorrect')
self.assert_grade(problem, 'choice_3', 'incorrect')
problem = self.build_problem( choice_type='checkbox', choices=[False, False, True, True], credit_type='edc' )
problem = self.build_problem( choice_type='checkbox', choices=[False, False, True, True], credit_type='halves' )
problem = self.build_problem( choice_type='checkbox', choices=[False, False, True, True, False], credit_type='halves' )
problem = self.build_problem( choice_type='checkbox', choices=[False, False, True, True], credit_type='edc' )
problem = self.build_problem( choice_type='checkbox', choices=[False, False, True, True], credit_type='halves' )
problem = self.build_problem( choice_type='checkbox', choices=[False, False, True, True, False], credit_type='halves' )
coffee_file_path = os.path.dirname(__file__) + "/test_files/js/*.coffee" os.system("node_modules/.bin/coffee -c %s" % (coffee_file_path))
self.assert_grade(problem, json.dumps({0: 4}), "correct") self.assert_grade(problem, json.dumps({0: 5}), "incorrect")
capa_system = test_capa_system() capa_system.can_execute_unsafe_code = lambda: False
problem = self.build_problem(answer='[1j, 5]') input_dict = {'1_2_1': '3'} with self.assertRaises(StudentInputError): problem.grade_answers(input_dict)
with self.assertRaises(StudentInputError): problem = self.build_problem(answer='(1 5)')
problem = self.build_problem(answer='(1, ]') input_dict = {'1_2_1': '3'} with self.assertRaises(StudentInputError): problem.grade_answers(input_dict)
self.assert_grade(problem, '42', 'correct') self.assert_grade(problem, '0', 'incorrect')
input_msg = correctmap.get_msg('1_2_1') self.assertEqual(input_msg, "Test Message")
overall_msg = correctmap.get_overall_message() self.assertEqual(overall_msg, "Overall message")
inline_script = "messages[0] = {code}".format(code=self._get_random_number_code()) problem = self.build_problem(answer=inline_script)
input_dict = {'1_2_1': '42'} correct_map = problem.grade_answers(input_dict)
input_dict = {'1_2_1': '21'} correct_map = problem.grade_answers(input_dict)
input_dict = {'1_2_1': '0'} correct_map = problem.grade_answers(input_dict)
self.assertEqual(problem.context['expect'], '42')
correctness = correctmap.get_correctness('1_2_1') self.assertEqual(correctness, 'correct')
input_dict = {'1_2_1': '42', '1_2_2': '42'} correct_map = problem.grade_answers(input_dict)
input_dict = {'1_2_1': '0', '1_2_2': '42'} correct_map = problem.grade_answers(input_dict)
input_dict = {'1_2_1': '0', '1_2_2': '0'} correct_map = problem.grade_answers(input_dict)
self.assertEqual(correct_map.get_overall_message(), "Overall message")
input_dict = {'1_2_1': '42'} correct_map = problem.grade_answers(input_dict)
input_dict = {'1_2_1': '21'} correct_map = problem.grade_answers(input_dict)
input_dict = {'1_2_1': '0'} correct_map = problem.grade_answers(input_dict)
input_dict = {'1_2_1': '-999', '1_2_2': '2', '1_2_3': '3'} correct_map = problem.grade_answers(input_dict)
input_dict = {'1_2_1': '-1', '1_2_2': '2', '1_2_3': '3'} correct_map = problem.grade_answers(input_dict)
input_dict = {'1_2_1': '1', '1_2_2': '2', '1_2_3': '3'} correct_map = problem.grade_answers(input_dict)
self.assertEqual(correct_map.get_overall_message(), 'Message text')
with self.assertRaises(ResponseError): problem.grade_answers({'1_2_1': '42'})
script = 'raise Exception("Test")' problem = self.build_problem(answer=script)
with self.assertRaises(ResponseError): problem.grade_answers({'1_2_1': '42'})
with self.assertRaises(ResponseError): problem.grade_answers({'1_2_1': '42'})
problem = self.build_problem(answer=script)
try: problem.grade_answers({'1_2_1': '42'})
problem = self.build_problem(script=script, cfn="check_func")
try: problem.grade_answers({'1_2_1': '42'})
self.assertListEqual(problem.responders.values()[0].context['idset'], correct_order)
script = "correct = ['correct' if 'test' in submission[0] else 'incorrect']" problem = self.build_problem(answer=script)
submission_dict = {'test': 'the_answer'} input_dict = {'1_2_1': json.dumps(submission_dict)} correct_map = problem.grade_answers(input_dict)
self.assertEqual(correct_map.get_correctness('1_2_1'), 'correct')
script = "raise Exception('test')" problem = self.build_problem(answer=script)
with self.assertRaises(ResponseError): submission_dict = {'test': 'test'} input_dict = {'1_2_1': json.dumps(submission_dict)} problem.grade_answers(input_dict)
choice, answers = choice_answers_pair
choice_id = "1_2_1_choiceinput_{index}bc".format(index=index) choice_value = "choiceinput_{index}".format(index=index) answer_dict[choice_id] = choice_value
answer_id = "1_2_1_choiceinput_{index}_numtolerance_input_{ind}".format( index=index, ind=ind ) answer_dict[answer_id] = answer
self.assert_grade( two_choice_two_input, self._make_answer_dict([(True, ["Platypus"])]), "correct" )
self.assert_grade( two_choice_two_input, self._make_answer_dict([(True, ["1"]), (True, ["Platypus"])]), "correct" )
scenarios = { "2_choices_correct": ("checkbox_two_choices", "correct"), "2_choices_incorrect": ("checkbox_two_choices", "incorrect"),
problems = { "checkbox_two_choices": checkbox_two_choices, "checkbox_2_choices_2_inputs": checkbox_two_choices_two_inputs }
problem_name, correctness = scenarios[name] problem = problems[problem_name]
self.assert_grade( problem, submission, correctness, msg="{0} should be {1}".format(name, correctness) )
response = problem.responders.values()[0] self.assertFalse(response.has_mask()) self.assertFalse(response.has_answerpool())
response = problem.responders.values()[0] self.assertFalse(response.has_mask()) self.assertFalse(response.has_answerpool())
from lxml import etree import unittest import xml.sax.saxutils as saxutils
lookup_tag = customrender.registry.get_class_for_tag
xml = renderer.get_html() context = extract_context(xml) self.assertEqual(context, {'id': 'solution_12'})
root = etree.Element("problem")
if script: script_element = etree.SubElement(root, "script") script_element.set("type", "loncapa/python") script_element.text = str(script)
question = etree.SubElement(root, "p") question.text = question_text
for __ in range(int(num_responses)): response_element = self.create_response_element(**kwargs)
if credit_type is not None: response_element.set('partial_credit', str(credit_type))
for __ in range(int(num_inputs)): input_element = self.create_input_element(**kwargs) if not None == input_element: response_element.append(input_element)
group_element_names = { 'checkbox': 'checkboxgroup', 'radio': 'radiogroup', 'multiple': 'choicegroup' }
assert choice_type in group_element_names group_element = etree.Element(group_element_names[choice_type])
if name: choice_element.text = str(name) choice_element.set("name", str(name))
if pointval: choice_element.set("point_value", str(pointval))
responseparam_element = etree.SubElement(response_element, 'responseparam') responseparam_element.set('partial_answers', partial_answers)
response_element = etree.Element("customresponse")
response_element = etree.Element("schematicresponse")
if answer_script: answer_element = etree.SubElement(response_element, "answer") answer_element.set("type", "loncapa/python") answer_element.text = str(answer_script)
kwargs['explanation_text'] = None return super(CodeResponseXMLFactory, self).build_xml(**kwargs)
response_element = etree.Element("coderesponse")
codeparam_element = etree.SubElement(response_element, "codeparam")
initial_element = etree.SubElement(codeparam_element, "initial_display") initial_element.text = str(initial_display)
answer_element = etree.SubElement(codeparam_element, "answer_display") answer_element.text = str(answer_display)
grader_element = etree.SubElement(codeparam_element, "grader_payload") grader_element.text = str(grader_payload)
if not has_files: input_element = etree.SubElement(response_element, "textbox") input_element.set("mode", "python")
return None
response_element = etree.Element("formularesponse")
sample_str = self._sample_str(sample_dict, num_samples, tolerance) response_element.set("samples", sample_str)
responseparam_element = etree.SubElement(response_element, "responseparam") responseparam_element.set("type", "tolerance") responseparam_element.set("default", str(tolerance))
response_element.set("answer", str(answer))
if hint_list: hintgroup_element = etree.SubElement(response_element, "hintgroup")
formulahint_element = etree.SubElement(hintgroup_element, "formulahint")
formulahint_element.set("samples", sample_str)
assert((display_src and display_class) or (not display_src and not display_class))
response_element = etree.Element("javascriptresponse")
optioninput_element = etree.Element("optioninput")
optioninput_element.set('correct', str(correct_option))
response_element = etree.Element("stringresponse")
response_element.set("answer", unicode(answer))
expect = kwargs.get('expect', '') options = kwargs.get('options', [])
options_str = ",".join(options)
response_element = etree.Element('symbolicresponse')
if not isinstance(choices[0], (list, tuple)): choices = [choices]
if answers:
if not isinstance(answers, (list, tuple)): answers = [answers]
input_type = kwargs.get('type', 'radiotextgroup') input_element = etree.Element(input_type)
choice.text = "choice_{0}".format(ind) input_element.append(choice)
choice_element.append(inp)
problem = new_loncapa_problem(xml_str)
self._create_test_file( 'test_include.xml', '<test>Test include</test>' )
problem = new_loncapa_problem(xml_str, capa_system=self.capa_system)
rendered_html = etree.XML(problem.get_html())
test_element = rendered_html.find("test") self.assertEqual(test_element.tag, "test") self.assertEqual(test_element.text, "Test include")
problem = new_loncapa_problem(xml_str)
rendered_html = etree.XML(problem.get_html())
span_element = rendered_html.find('span') self.assertEqual(span_element.text, 'Test text')
problem = new_loncapa_problem(xml_str)
rendered_html = etree.XML(problem.get_html())
span_element = rendered_html.find('span') self.assertEqual(span_element.text, 'Welcome student')
problem = new_loncapa_problem(xml_str)
rendered_html = etree.XML(problem.get_html())
script_element = rendered_html.find('script') self.assertEqual(None, script_element)
problem = new_loncapa_problem(xml_str)
rendered_html = etree.XML(problem.get_html())
self.assertIn( "<script type=\"text/javascript\">function(){}</script>", etree.tostring(rendered_html) )
the_system = test_capa_system() the_system.render_template = mock.Mock() the_system.render_template.return_value = "<div>Input Template Render</div>"
problem = new_loncapa_problem(xml_str, capa_system=the_system) rendered_html = etree.XML(problem.get_html())
self.assertEqual(rendered_html.tag, "div")
question_element = rendered_html.find("p") self.assertEqual(question_element.text, "Test question")
response_element = rendered_html.find("span") self.assertEqual(response_element.tag, "span")
textline_element = response_element.find("div") self.assertEqual(textline_element.text, 'Input Template Render')
solution_element = rendered_html.find("div") self.assertEqual(solution_element.text, 'Input Template Render')
kwargs = {'script': script, 'cfn': 'check_func'} xml_str = CustomResponseXMLFactory().build_xml(**kwargs)
problem = new_loncapa_problem(xml_str)
problem.grade_answers({'1_2_1': 'test'})
rendered_html = etree.XML(problem.get_html())
msg_p_elements = msg_div_element.findall('p') self.assertEqual(msg_p_elements[0].tag, "p") self.assertEqual(msg_p_elements[0].text, "Test message 1")
problem = new_loncapa_problem(xml_str) rendered_html = etree.XML(problem.get_html())
span_element = rendered_html.find('span') self.assertEqual(span_element.get('attr'), "TEST")
problem = new_loncapa_problem(xml_str)
the_html = problem.get_html() self.assertRegexpMatches(the_html, r"<div>\s+</div>")
self.assertFalse(self.cmap.is_partially_correct('9_2_1'))
self.assertEqual(self.cmap.get_overall_message(), "")
self.cmap.set_overall_message("Test message")
self.assertEqual(self.cmap.get_overall_message(), "Test message")
self.cmap.set_overall_message(None) self.assertEqual(self.cmap.get_overall_message(), "")
other_cmap = CorrectMap() other_cmap.update(self.cmap)
self.assertEqual( other_cmap.get_overall_message(), self.cmap.get_overall_message() )
invalid_list = [None, "string", 5, datetime.datetime.today()]
lazymod_py_file = lazymod.__file__ if lazymod_py_file.endswith("c"): lazymod_py_file = lazymod_py_file[:-1]
code_prolog = CODE_PROLOG % random_seed
if unsafely: exec_fn = codejail_not_safe_exec else: exec_fn = codejail_safe_exec
try: exec_fn( code_prolog + LAZY_IMPORTS + code, globals_dict, python_path=python_path, extra_files=extra_files, slug=slug, ) except SafeExecException as e: emsg = e.message else: emsg = None
if cache: cleaned_results = json_safe(globals_dict) cache.set(key, (emsg, cleaned_results))
if emsg: raise e
safe_exec("a = 1/2", g) self.assertEqual(g['a'], 0.5)
safe_exec("a = int(math.pi)", g) self.assertEqual(g['a'], 3)
safe_exec("rnums = [random.randint(0, 999) for _ in xrange(100)]", g) self.assertNotEqual(g['rnums'], rnums)
safe_exec("rnums = [random.randint(0, 999) for _ in xrange(100)]", g, random_seed=17) self.assertEqual(g['rnums'], rnums)
if not is_configured("python"): raise SkipTest
assert len(key) <= 250 return self.cache.get(key)
assert len(key) <= 250 self.cache[key] = value
cache[cache.keys()[0]] = (None, {'a': 17})
code = "a = 0\n" + ("a += 1\n" * 12345)
code = "1/0" g = {} cache = {} with self.assertRaises(SafeExecException): safe_exec(code, g, cache=DictCache(cache))
self.assertEqual(len(cache), 1) cache_exc_msg, cache_globals = cache.values()[0] self.assertIn("ZeroDivisionError", cache_exc_msg)
cache[cache.keys()[0]] = ("Hey there!", {})
for code in [129, 500, 2 ** 8 - 1, 2 ** 16 - 1]:
self.assertEqual(d1, d2) self.assertNotEqual(d1.keys(), d2.keys())
self.mods = set(sys.modules)
new_mods = [m for m in sys.modules if m not in self.mods] for m in new_mods: del sys.modules[m]
self.addCleanup(ModuleIsolation().clean_up)
self.assertNotIn("wsgiref.util", sys.modules) wsgiref_util = LazyModule("wsgiref.util") self.assertEqual(wsgiref_util.guess_scheme({}), "http")
xml.tail = self.tail return xml
'unsubmitted': 'unanswered', 'incomplete': 'incorrect', 'queued': 'processing',
_sentinel = object()
return self.default
if self.hintmode == 'always': self.msg = self.hint + ('<br/>' if self.msg else '') + self.msg
self.process_requirements()
self.setup()
msg = u"Error in xml '{x}': {err} ".format( x=etree.tostring(xml), err=err.message) raise Exception, msg, sys.exc_info()[2]
try: output = html5lib.parseFragment(html, treebuilder='lxml', namespaceHTMLElements=False)[0] except IndexError: raise ex
options = re.sub(r"([a-zA-Z])('|\\')([a-zA-Z])", r"\1&#39;\3", options)
lexer = shlex.shlex(options[1:-1].encode('utf8')) lexer.quotes = "'" lexer.whitespace = ", "
tokens = [x[1:-1].decode('utf8').replace("&#39;", "'") for x in lexer]
return [(t, t) for t in tokens]
error_message=_('Expected a <choice> or <compoundhint> tag; got {given_tag} instead').format( given_tag=choice.tag )
if self.value == "": self.value = 'null'
]
self.queue_len = 0 if self.status == 'incomplete': self.status = 'queued' self.queue_len = self.msg self.msg = self.submitted_msg
]
Attribute('mode', 'python'), Attribute('linenumbers', 'true'), Attribute('tabsize', 4, transform=int),
if not self.value and self.xml.text: self.value = self.xml.text.strip()
queue_msg = u"<span>{0}</span>".format(_("Error running code."))
if self.capa_system.xqueue is None: return {'success': False, 'message': _('Cannot connect to the queue')}
response = data['submission']
if error == 0: self.input_state['queuekey'] = queuekey self.input_state['queuestate'] = 'queued' self.input_state['queuetime'] = time.time()
(self.gx, self.gy) = [int(x) - 15 for x in m.groups()]
log.warning( "Error while previewing chemical formula", exc_info=True) result['error'] = _("Error while rendering preview")
log.warning( "Error while previewing formula", exc_info=True ) result['error'] = _("Error while rendering preview")
self.no_labels = Attribute('no_labels', default="False").parse_from_xml(self.xml)
to_js['base_image'] = Attribute('img').parse_from_xml(self.xml)
label_bg_color = Attribute('label_bg_color', default=None).parse_from_xml(self.xml) if label_bg_color: to_js['label_bg_color'] = label_bg_color
if self.value == '': self.value = 'null'
self.value = {}
_("Expected a {expected_tag} tag; got {given_tag} instead").format( expected_tag=u"<choice>", given_tag=choice.tag, )
adder = { 'type': 'text', 'contents': choice_text, 'tail_text': '', 'value': '' } components.append(adder)
adder['tail_text'] = elt.tail if elt.tail else '' components.append(adder)
choices.append((choice.get("name"), components))
import codecs from fractions import Fraction import unittest
self.assertFalse(chemical_equations_equal('H2 + O2 -> H2O2', 'O2 + H2 -> 2H2O2'))
self.assertTrue(chemical_equations_equal('H2 + O2 -> H2O2', 'O2 + H2 -> H2O2', exact=True))
def test_compare_phases_ignored(self): self.assertTrue(compare_chemical_expression( "H2O(s) + CO2", "H2O+CO2", ignore_state=True))
points = [round0_25(point) for point in points]
return nltk.tree.Tree(n.node, n[2:])
children = [] for child in tree: children.append(_merge_children(child, tags))
if len(tree) == 1: return tree[0][0] if len(tree) == 3: return " <sup>{num}</sup>&frasl;<sub>{den}</sub> ".format(num=tree[0][0], den=tree[2][0]) return "Error"
return arrow
return spanify(render_expression(left))
list1.sort() list2.sort() return list1 == list2
treedic = {} treedic['1'] = _get_final_tree(s1) treedic['2'] = _get_final_tree(s2)
if not _check_equality(treedic['1 cleaned_mm_list'], treedic['2 cleaned_mm_list']): return False
return False
return Fraction(treedic['1 factors'][0] / treedic['2 factors'][0])
for arrow in ARROWS: left, a, right = eq.partition(arrow) if a != '': return left, a, right
return False
return False
return False
return False
return False
from defusedxml.lxml import parse, fromstring, XML
if user.is_anonymous(): return None
pass
PROFILE_COUNTRY_CACHE_KEY = u"user.{user_id}.profile.country"
language = models.CharField(blank=True, max_length=255, db_index=True) location = models.CharField(blank=True, max_length=255, db_index=True)
return year - year_of_birth - 1
if user_profile.requires_parental_consent() and user_profile.has_profile_image: user_profile.profile_image_uploaded_at = None
user_profile._changed_fields = get_changed_fields_dict(user_profile, sender)
emit_field_changed_events( user_profile, user_profile.user, sender._meta.db_table, excluded_fields=['meta'] )
emit_field_changed_events( user, user, sender._meta.db_table, excluded_fields=['last_login', 'first_name', 'last_name'], hidden_fields=['password'] )
return anonymous_id_for_user(user, None, save=save)
time_last_reset = history[0].time_set
time_last_reset = user.date_joined
history = PasswordHistory.objects.filter(user=user).order_by('-time_set')[:min_diff_passwords_required]
if record.failure_count >= max_failures_allowed: lockout_period_secs = settings.MAX_FAILED_LOGIN_ATTEMPTS_LOCKOUT_PERIOD_SECS record.lockout_until = datetime.now(UTC) + timedelta(seconds=lockout_period_secs)
from student.roles import CourseCcxCoachRole, CourseInstructorRole, CourseStaffRole course_locator = course_id
is_active = models.BooleanField(default=True)
mode = models.CharField(default=CourseMode.DEFAULT_MODE_SLUG, max_length=100)
history = HistoricalRecords()
COURSE_ENROLLMENT_CACHE_KEY = u"enrollment.{}.{}.mode"
self._course_overview = None
if created: enrollment.mode = CourseMode.DEFAULT_MODE_SLUG enrollment.is_active = False enrollment.save()
if self.is_active != is_active and is_active is not None: self.is_active = is_active activation_changed = True
if self.mode != mode and mode is not None: self.mode = mode mode_changed = True
self.emit_event(EVENT_NAME_ENROLLMENT_MODE_CHANGED)
enrollment = cls.get_or_create_enrollment(user, course_key) enrollment.update_enrollment(is_active=True, mode=mode) if badges_enabled(): from lms.djangoapps.badges.events.course_meta import award_enrollment_badge award_enrollment_badge(user)
if GeneratedCertificate.certificate_for_student(self.user, self.course_id) is not None: return False
refund_cutoff_date = self.refund_cutoff_date() if refund_cutoff_date and datetime.now(UTC) > refund_cutoff_date: return False
return self.course_overview
org = models.CharField(max_length=64, db_index=True, blank=True) course_id = CourseKeyField(max_length=255, db_index=True, blank=True) role = models.CharField(max_length=64, db_index=True)
return
dashboard_tracking_code = models.TextField(default="", blank=True)
skip_entrance_exam = models.BooleanField(default=True)
unique_together = (('user', 'name',), )
STUDIO_EDIT_ROLES = 8 STUDIO_VIEW_USERS = 4 STUDIO_EDIT_CONTENT = 2 STUDIO_VIEW_CONTENT = 1
if (isinstance(role, (CourseStaffRole, CourseBetaTesterRole)) and CourseInstructorRole(role.course_key).has_user(user)): return True return False
if not(len(users) == 1 and caller == users[0]): _check_caller_authority(caller, role) role.remove_users(*users)
if GlobalStaff().has_user(caller): return
from notification_prefs.views import enable_notifications
context['show_partners'] = microsite.get_value('show_partners', True)
context['show_homepage_promo_video'] = microsite.get_value('show_homepage_promo_video', False)
youtube_video_id = microsite.get_value('homepage_promo_video_youtube_id', "your-youtube-id") context['homepage_promo_video_youtube_id'] = youtube_video_id
context['courses_list'] = microsite.get_template_path('courses_list.html')
context.update(extra_context)
for status in statuses: if reverifications[status]: reverifications[status].sort(key=lambda x: x.date) return reverifications
course_overview = enrollment.course_overview if not course_overview: log.error( "User %s enrolled in broken or non-existent course %s", user.username, enrollment.course_id ) continue
if org_to_include and course_overview.location.org != org_to_include: continue
elif course_overview.location.org in orgs_to_exclude: continue
else: yield enrollment
linkedin_config = LinkedInAddToProfileConfiguration.current()
redirect_to = get_next_url_for_login_page(request) if request.user.is_authenticated(): return redirect(redirect_to)
redirect_to = get_next_url_for_login_page(request) if request.user.is_authenticated(): return redirect(redirect_to)
course_org_filter = microsite.get_value('course_org_filter')
org_filter_out_set = microsite.get_all_orgs()
if course_org_filter: org_filter_out_set.remove(course_org_filter)
course_enrollments = list(get_course_enrollments(user, course_org_filter, org_filter_out_set))
course_enrollments.sort(key=lambda x: x.created, reverse=True)
enrollment_message = _create_recent_enrollment_message( course_enrollments, course_modes_by_course )
staff_access = False errored_courses = {} if has_access(user, 'staff', 'global'): staff_access = True errored_courses = modulestore().get_errored_courses()
course_programs = _get_course_programs(user, [enrollment.course_id for enrollment in course_enrollments])
course_mode_info = { enrollment.course_id: complete_course_mode_info( enrollment.course_id, enrollment, modes=course_modes_by_course[enrollment.course_id] ) for enrollment in course_enrollments }
show_email_settings_for = frozenset( enrollment.course_id for enrollment in course_enrollments if ( BulkEmailFlag.feature_enabled(enrollment.course_id) ) )
verification_status, verification_msg = SoftwareSecurePhotoVerification.user_status(user)
statuses = ["approved", "denied", "pending", "must_reverify"] reverifications = reverification_info(statuses)
denied_banner = any(item.display for item in reverifications["denied"])
order_history_list = order_history(user, course_org_filter=course_org_filter, org_filter_out_set=org_filter_out_set)
courses_having_prerequisites = frozenset( enrollment.course_id for enrollment in course_enrollments if enrollment.course_overview.pre_requisite_courses ) courses_requirements_not_met = get_pre_requisite_courses_not_completed(user, courses_having_prerequisites)
if enrollment.is_active and enrollment.created > time_delta
if not settings.FEATURES.get("ENABLE_CREDIT_ELIGIBILITY"): return {}
user = request.user
if not user.is_authenticated(): return HttpResponseForbidden()
action = request.POST.get("enrollment_action") if 'course_id' not in request.POST: return HttpResponseBadRequest(_("Course id not specified"))
if settings.FEATURES.get('ENABLE_MKTG_EMAIL_OPT_IN'): _update_email_opt_in(request, course_id.org)
redirect_url = embargo_api.redirect_if_blocked( course_id, user=user, ip_address=get_ip(request), url=request.path ) if redirect_url: return HttpResponse(redirect_url)
return HttpResponse()
@ensure_csrf_cookie
AUDIT_LOG.info(u"User %s w/o external auth attempting login", user)
username = user.username if user else ""
except RateLimitException: return JsonResponse({ "success": False, "value": _('Too many failed login attempts. Try again later.'),
if user_found_by_email_lookup and LoginFailures.is_feature_enabled(): LoginFailures.increment_lockout_counter(user_found_by_email_lookup)
if LoginFailures.is_feature_enabled(): LoginFailures.clear_lockout_counter(user)
return set_logged_in_cookies(request, response, user)
password_history_entry = PasswordHistory() password_history_entry.create(user)
params = dict(params.items())
extra_fields = microsite.get_value( 'REGISTRATION_EXTRA_FIELDS', getattr(settings, 'REGISTRATION_EXTRA_FIELDS', {}) )
with transaction.atomic(): (user, profile, registration) = _do_create_account(form, custom_form)
preferences_api.set_user_preference(user, LANGUAGE_KEY, get_language())
third_party_provider = None running_pipeline = None if third_party_auth.is_enabled() and pipeline.running(request): running_pipeline = pipeline.get(request) third_party_provider = provider.Registry.get_from_pipeline(running_pipeline)
if hasattr(settings, 'LMS_SEGMENT_KEY') and settings.LMS_SEGMENT_KEY: tracking_context = tracker.get_tracker().resolve_context() identity_args = [
subject = render_to_string('emails/activation_email_subject.txt', context) subject = ''.join(subject.splitlines()) message = render_to_string('emails/activation_email.txt', context)
new_user = authenticate(username=user.username, password=params['password']) login(request, new_user) request.session.set_expiry(0)
ManualEnrollmentAudit.create_manual_enrollment_audit( manual_enrollment_audit.enrolled_by, student.email, ALLOWEDTOENROLL_TO_ENROLLED, manual_enrollment_audit.reason, enrollment )
if third_party_auth.is_enabled() and pipeline.running(request): running_pipeline = pipeline.get(request) redirect_url = pipeline.get_complete_url(running_pipeline['backend'])
unique_name = uuid.uuid4().hex[0:30]
enrollment_mode = request.GET.get('enrollment_mode', 'honor')
if is_staff is not None: user.is_staff = (is_staff == "true") user.save()
reg.activate() reg.save()
year = datetime.date.today().year age_limit = settings.PARENTAL_CONSENT_AGE_LIMIT profile.year_of_birth = (year - age_limit) - 1 profile.save()
if course_key is not None: CourseEnrollment.enroll(user, course_key, mode=enrollment_mode)
for role_name in role_names: role = Role.objects.get(name=role_name, course_id=course_key) user.roles.add(role)
if login_when_done: user = authenticate(username=username, password=password) login(request, user)
_enroll_user_in_pending_courses(regs[0].user)
limiter = BadRequestRateLimiter() if limiter.is_rate_limit_exceeded(request): AUDIT_LOG.warning("Rate limit exceeded in password_reset") return HttpResponseForbidden()
tracker.emit( SETTING_CHANGE_INITIATED, { "setting": "password", "old": None, "new": None, "user_id": request.user.id, } )
AUDIT_LOG.info("Bad password_reset user passed in.") limiter.tick_bad_request_counter(request)
try: uid_int = base36_to_int(uidb36) user = User.objects.get(id=uid_int) user.is_active = True user.save() except (ValueError, User.DoesNotExist): pass
err_msg = None
extra_context = {"platform_name": microsite.get_value('platform_name', settings.PLATFORM_NAME)}
try: uidb64 = force_text(urlsafe_base64_encode(force_bytes(base36_to_int(uidb36)))) except ValueError:
old_password_hash = user.password
updated_user = User.objects.get(id=uid_int)
if updated_user.password != old_password_hash: entry = PasswordHistory() entry.create(updated_user)
if not activation_key: activation_key = uuid.uuid4().hex
try: user.email_user( subject, message, theming_helpers.get_value('default_from_email', settings.DEFAULT_FROM_EMAIL) )
try: user.email_user( subject, message, theming_helpers.get_value('default_from_email', settings.DEFAULT_FROM_EMAIL) )
exclude = ('dashboard_tracking_code',)
admin.site.register(User, UserAdmin)
REGISTERED_ACCESS_ROLES = {}
user.is_staff = False user.save()
if not hasattr(user, '_roles'): user._roles = RoleCache(user)
if self.course_key is None: self.course_key = CourseKeyField.Empty entries = User.objects.filter( courseaccessrole__role=self._role_name, courseaccessrole__org=self.org, courseaccessrole__course_id=self.course_key ) return entries
if not hasattr(self.user, '_roles'): self.user._roles = RoleCache(self.user)
from django.core.management.base import BaseCommand from django.contrib.auth.models import User
manage.py ... transfer_students -f edX/Open_DemoX/edx_demo_course -t edX/Open_DemoX/new_demoX
manage.py ... transfer_students -f edX/Open_DemoX/edx_demo_course -t edX/Open_DemoX/new_demoX -c true
manage.py ... transfer_students -f edX/Open_DemoX/edx_demo_course -t edX/Open_DemoX/new_demoX,edX/Open_DemoX/edX_Insider
enrollment = CourseEnrollment.objects.get( user=user, course_id=source_key )
msg = u"Skipping {}, already enrolled in destination course {}" print msg.format(user.username, unicode(dest_key))
if not old_is_active: new_enrollment.update_enrollment(is_active=False, skip_refund=True)
if options['course']: try: course_key = CourseKey.from_string(options['course']) except InvalidKeyError: course_key = SlashSeparatedCourseKey.from_deprecated_string(options['course'])
output_filename = course_key.to_deprecated_string().replace('/', '-') + ".csv"
students = User.objects.filter(courseenrollment__course_id=course_key) if len(students) == 0: self.stdout.write("No students enrolled in %s" % course_key.to_deprecated_string()) return
if options['course']: try: course = CourseKey.from_string(options['course']) except InvalidKeyError: course = SlashSeparatedCourseKey.from_deprecated_string(options['course'])
raise CommandError( _( 'Skipping user "{}" because the specified and existing email ' 'addresses do not match.' ).format(user.username) )
for group_name in groups or set():
self.stderr.write(_('Could not find a group named "{}" - skipping.').format(group_name))
group.full_clean()
raise CommandError( _( 'Invalid group name: "{group_name}". {messages}' ).format( group_name=group_name, messages=exc.messages[0] ) )
raise CommandError(_( 'Invalid permission option: "{}". Please specify permissions ' 'using the format: app_label:model_name:permission_codename.' ).format(permission))
call_command('manage_group', TEST_GROUP) self.check_groups([TEST_GROUP])
call_command('manage_group', TEST_GROUP, '--remove') self.check_groups([])
call_command('manage_group', TEST_GROUP, '--permissions', 'auth:Group:add_group') self.check_groups([TEST_GROUP]) self.check_permissions(TEST_GROUP, ['add_group'])
call_command('manage_group', TEST_GROUP, '--permissions', 'auth:Group:change_group') self.check_groups([TEST_GROUP]) self.check_permissions(TEST_GROUP, ['change_group'])
call_command('manage_group', TEST_GROUP) self.check_groups([TEST_GROUP]) self.check_permissions(TEST_GROUP, [])
call_command('manage_user', TEST_USERNAME, TEST_EMAIL, '--remove') self.assertEqual([], list(User.objects.all()))
self.assertEqual( len(CourseEnrollment.objects.filter(mode='honor', user_id__in=user_ids)), 0 )
self.assertEqual( len(CourseEnrollment.objects.filter(mode='honor', user_id__in=user_ids)), expected_conversions )
self.assertEqual( len(CourseEnrollment.objects.filter(mode='honor', user_id__in=real_user_ids)), 0 )
self.assertEqual( len(CourseEnrollment.objects.filter(mode='honor', user_id__in=real_user_ids)), expected_success )
self._create_and_purchase_verified(student, course.id)
course_location_one = locator.CourseLocator('Org1', 'Course1', 'Run1') new_course_one = self._create_course(course_location_one)
transfer_students.Command().handle( source_course=original_key, dest_course_list=new_key_one + "," + new_key_two ) self.assertTrue(self.signal_fired)
cart = Order.get_cart_for_user(user=student) CertificateItem.add_to_order(cart, course_id, 50, 'verified') cart.purchase()
VERIFY_STATUS_NEED_TO_VERIFY = "verify_need_to_verify" VERIFY_STATUS_SUBMITTED = "verify_submitted" VERIFY_STATUS_APPROVED = "verify_approved" VERIFY_STATUS_MISSED_DEADLINE = "verify_missed_deadline" VERIFY_STATUS_NEED_TO_REVERIFY = "verify_need_to_reverify"
verifications = SoftwareSecurePhotoVerification.objects.filter(user=user)
has_active_or_pending = SoftwareSecurePhotoVerification.user_has_valid_or_pending( user, queryset=verifications )
enrolled_course_keys = [enrollment.course_id for enrollment in course_enrollments] course_deadlines = VerificationDeadline.deadlines_for_courses(enrolled_course_keys)
if enrollment.mode in CourseMode.VERIFIED_MODES:
deadline = course_deadlines.get(enrollment.course_id)
if relevant_verification is not None and relevant_verification.status == "approved": recent_verification_datetime = max( recent_verification_datetime if recent_verification_datetime is not None else relevant_verification.expiration_datetime, relevant_verification.expiration_datetime )
status = None
if relevant_verification is not None: if relevant_verification.status == "approved": status = VERIFY_STATUS_APPROVED elif relevant_verification.status == "submitted": status = VERIFY_STATUS_SUBMITTED
else: status = VERIFY_STATUS_MISSED_DEADLINE
if status is not None: days_until_deadline = None
POST_AUTH_PARAMS = ('course_id', 'enrollment_action', 'course_mode', 'email_opt_in')
params = [(param, request.GET[param]) for param in POST_AUTH_PARAMS if param in request.GET]
return redirect_to
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
self.assertTrue(PasswordHistory.is_allowable_password_reuse(user, "test")) self.assertTrue(PasswordHistory.is_allowable_password_reuse(staff, "test"))
grandfathered_student = UserFactory() grandfathered_student.date_joined = timezone.now()
self.assertEqual(response.status_code, 400)
self.assertEqual(response.status_code, 400)
from config_models.models import cache
course2.certificates_display_behavior = 'early_no_info' cert_status = {'status': 'unavailable'} self.assertEqual(_cert_info(user, course2, cert_status, course_mode), {})
attempt = SoftwareSecurePhotoVerification.objects.create(user=self.user) attempt.mark_ready() attempt.submit() attempt.approve()
attempt = SoftwareSecurePhotoVerification.objects.create(user=self.user) attempt.mark_ready() attempt.submit() attempt.approve()
self.assertEqual(pq(response.content)(".sts-enrollment").length, 0)
self.assertTrue('Activate Course Enrollment' in response.content)
invoice = shoppingcart.models.Invoice.objects.get(id=sale_invoice_1.id) invoice.is_valid = True invoice.save()
self.client.login(username="jack", password="test")
self.client.login(username="jack", password="test") LinkedInAddToProfileConfiguration( company_identifier='0_mC_o2MizqdtZEmkVXjH4eYwMj4DnkCWrZP_D9', enabled=True ).save()
test_course = CourseFactory.create(default_store=modulestore_type, emit_signals=True) self.client.login(username="jack", password="test")
self.assertContains(response, "Explore courses")
self.assertNotContains(response, "How it Works") self.assertNotContains(response, "Schools & Partners")
CourseEnrollment.enroll(user, course_id) self.assertTrue(CourseEnrollment.is_enrolled(user, course_id)) self.assertTrue(CourseEnrollment.is_enrolled_by_partial(user, course_id_partial)) self.assert_no_events_were_emitted()
CourseEnrollment.unenroll(user, course_id) self.assertFalse(CourseEnrollment.is_enrolled(user, course_id)) self.assertFalse(CourseEnrollment.is_enrolled_by_partial(user, course_id_partial)) self.assert_no_events_were_emitted()
enrollment_record = CourseEnrollment.objects.get( user=user, course_id=course_id ) self.assertFalse(enrollment_record.is_active)
user = User(username="rusty", email="rusty@fake.edx.org") course_id = SlashSeparatedCourseKey("edX", "Test101", "2013")
CourseEnrollment.unenroll(user, course_id) self.assert_no_events_were_emitted()
CourseEnrollment.enroll(user, course_id) self.assertTrue(CourseEnrollment.is_enrolled(user, course_id)) self.assert_enrollment_event_was_emitted(user, course_id)
self.assertIsNone( CourseEnrollment.enroll_by_email("not_jack@fake.edx.org", course_id) ) self.assert_no_events_were_emitted()
CourseEnrollment.unenroll_by_email("jack@fake.edx.org", course_id) self.assertFalse(CourseEnrollment.is_enrolled(user, course_id)) self.assert_unenrollment_event_was_emitted(user, course_id)
CourseEnrollment.unenroll_by_email("jack@fake.edx.org", course_id) self.assertFalse(CourseEnrollment.is_enrolled(user, course_id)) self.assert_no_events_were_emitted()
CourseEnrollment.unenroll_by_email("not_jack@fake.edx.org", course_id) self.assert_no_events_were_emitted()
enrollment = CourseEnrollment.get_or_create_enrollment(user, course_id) self.assertFalse(CourseEnrollment.is_enrolled(user, course_id)) self.assert_no_events_were_emitted()
enrollment.activate() self.assertTrue(CourseEnrollment.is_enrolled(user, course_id)) self.assert_enrollment_event_was_emitted(user, course_id)
enrollment.activate() self.assertTrue(CourseEnrollment.is_enrolled(user, course_id)) self.assert_no_events_were_emitted()
enrollment.deactivate() self.assertFalse(CourseEnrollment.is_enrolled(user, course_id)) self.assert_unenrollment_event_was_emitted(user, course_id)
enrollment.deactivate() self.assertFalse(CourseEnrollment.is_enrolled(user, course_id)) self.assert_no_events_were_emitted()
CourseEnrollment.enroll(user, course_id) self.assertTrue(CourseEnrollment.is_enrolled(user, course_id)) self.assert_enrollment_event_was_emitted(user, course_id)
CourseEnrollment.enroll(user, course_id, "honor") self.assert_no_events_were_emitted()
response = self._enroll_through_view(self.course) self.assertEqual(response.status_code, 400)
self.assertContains(response, 'course-container', 2) self._assert_responses(response, 1)
if course_mode == 'verified': self.assertIn('xseries-base-btn', response.content) else: self.assertIn('xseries-border-btn', response.content)
__, course_ids = mock_get_programs.call_args[0] self.assertEqual(list(course_ids), [self.course_1.id]) self._assert_responses(response, 1)
self.assertContains(response, 'course-container', 3) self._assert_responses(response, program_count)
self.assertContains(response, 'course-container', 1) self.assertIn('Pursue a Certificate of Achievement to highlight', response.content)
role.add_users(self.student) role.remove_users(self.student) self.assertFalse(role.has_user(self.student))
courses_list = list(get_course_enrollments(self.student, None, [])) self.assertEqual(len(courses_list), 1) self.assertEqual(courses_list[0].course_id, course_location)
courses_list = list(get_course_enrollments(self.student, None, [])) self.assertEqual(len(courses_list), 0)
CourseOverview.objects.filter(id=course_key).delete()
self.user.is_staff = True self.assertTrue(user_has_role(self.user, CourseCreatorRole()))
user_not_added = User.objects.create_user('testuser2', 'test+courses2@edx.org', 'foo2') self.assertFalse(user_has_role(user_not_added, CourseCreatorRole()))
remove_users(self.admin, CourseCreatorRole(), self.user) self.assertFalse(user_has_role(self.user, CourseCreatorRole()))
add_users(self.admin, CourseCreatorRole(), self.user)
self.assertFalse(user_has_role(self.user, CourseCreatorRole()))
self.user.is_staff = True self.assertTrue(user_has_role(self.user, CourseCreatorRole()))
remove_users(self.admin, CourseCreatorRole(), self.user) self.assertTrue(user_has_role(self.user, CourseCreatorRole()))
from config_models.models import cache
self.enrollment.can_refund = True self.assertTrue(self.enrollment.refundable())
self.enrollment.can_refund = True self.assertTrue(self.enrollment.refundable())
response = self.client.post(reverse('admin:student_courseaccessrole_add'), data=data) self.assertRedirects(response, reverse('admin:student_courseaccessrole_changelist'))
response = self.client.post(reverse('admin:student_courseaccessrole_add'), data=data) self.assertRedirects(response, reverse('admin:student_courseaccessrole_changelist'))
response = self.client.post(reverse('admin:student_courseaccessrole_add'), data=data) self.assertRedirects(response, reverse('admin:student_courseaccessrole_changelist'))
response = self.client.post(reverse('admin:student_courseaccessrole_add'), data=data) self.assertRedirects(response, reverse('admin:student_courseaccessrole_changelist'))
age = years_ago - 1 self.assertEqual(self.profile.age, age)
course_location = locator.CourseLocator('Org1', 'Course1', 'Run1') self.course, self.enrollment = self._create_course_and_enrollment(course_location)
courses_list = list(get_course_enrollments(self.student, None, [])) self.assertEqual(len(courses_list), 2)
self.assert_no_xss(response, xss_content)
self._configure_message_timeout(10000)
DonationConfiguration(enabled=True).save()
for mode, min_price in course_modes: CourseModeFactory.create(mode_slug=mode, course_id=self.course.id, min_price=min_price)
self.client.login(username=self.student.username, password=self.PASSWORD) response = self.client.get(reverse("dashboard"))
self._configure_message_timeout(10000) DonationConfiguration(enabled=True).save()
CourseModeFactory.create(mode_slug="honor", course_id=self.course.id, min_price=100)
student = UserFactory.create() CourseEnrollmentFactory.create(user=student, course_id=self.course.id) self.client.login(username=student.username, password="test")
response = self.client.get(self.url) self.assertTrue(self.email_modal_link in response.content)
response = self.client.get(self.url) self.assertNotIn(self.email_modal_link, response.content)
self.assertFalse(BulkEmailFlag.feature_enabled(self.course.id)) response = self.client.get(self.url) self.assertNotIn(self.email_modal_link, response.content)
student = UserFactory.create() CourseEnrollmentFactory.create( user=student, course_id=SlashSeparatedCourseKey.from_deprecated_string(self.course_name) ) self.client.login(username=student.username, password="test")
response = self.client.get(self.url) self.assertFalse(self.email_modal_link in response.content)
response = self.client.get(self.url) self.assertFalse(self.email_modal_link in response.content)
super(TestCourseVerificationStatus, self).setUp()
self._assert_course_verification_status(None)
CourseEnrollmentFactory( course_id=self.course.id, user=self.user, mode="verified" )
self._assert_course_verification_status(VERIFY_STATUS_NEED_TO_VERIFY)
attempt = SoftwareSecurePhotoVerification.objects.create(user=self.user) self._assert_course_verification_status(VERIFY_STATUS_NEED_TO_VERIFY)
attempt.mark_ready() self._assert_course_verification_status(VERIFY_STATUS_NEED_TO_VERIFY)
attempt = SoftwareSecurePhotoVerification.objects.create(user=self.user) attempt.mark_ready() attempt.submit()
self._assert_course_verification_status(VERIFY_STATUS_SUBMITTED)
attempt = SoftwareSecurePhotoVerification.objects.create(user=self.user) attempt.mark_ready() attempt.submit() attempt.approve()
self._assert_course_verification_status(VERIFY_STATUS_APPROVED)
response = self.client.get(self.dashboard_url) self.assertContains(response, attempt.expiration_datetime.strftime("%m/%d/%Y"))
self._setup_mode_and_enrollment(self.PAST, "verified")
self._assert_course_verification_status(VERIFY_STATUS_MISSED_DEADLINE)
self._setup_mode_and_enrollment(self.PAST, "verified")
self._assert_course_verification_status(VERIFY_STATUS_MISSED_DEADLINE)
self._setup_mode_and_enrollment(self.PAST, "verified")
self._assert_course_verification_status(VERIFY_STATUS_MISSED_DEADLINE)
self._setup_mode_and_enrollment(self.FUTURE, "verified")
attempt = SoftwareSecurePhotoVerification.objects.create(user=self.user) attempt.mark_ready() attempt.submit() attempt.deny("Not valid!")
self._assert_course_verification_status(None)
self._setup_mode_and_enrollment(self.FUTURE, "verified")
attempt = SoftwareSecurePhotoVerification.objects.create(user=self.user) attempt.status = "must_retry" attempt.system_error("Error!")
self._assert_course_verification_status(None)
self._setup_mode_and_enrollment(self.FUTURE, "verified")
attempt = SoftwareSecurePhotoVerification.objects.create(user=self.user) attempt.mark_ready() attempt.submit()
attempt.created_at = attempt.created_at - timedelta(days=364) attempt.save()
self._assert_course_verification_status(VERIFY_STATUS_NEED_TO_REVERIFY)
self._setup_mode_and_enrollment(self.PAST, "verified")
attempt = SoftwareSecurePhotoVerification.objects.create(user=self.user) attempt.mark_ready() attempt.submit()
self._assert_course_verification_status(VERIFY_STATUS_APPROVED)
self._assert_course_verification_status(VERIFY_STATUS_APPROVED)
response = self.client.get(self.dashboard_url) self.assertContains(response, attempt.expiration_datetime.strftime("%m/%d/%Y"))
course2 = CourseFactory.create() CourseModeFactory.create( course_id=course2.id, mode_slug="verified", expiration_datetime=self.PAST ) CourseEnrollmentFactory( course_id=course2.id, user=self.user, mode="verified" )
attempt2 = SoftwareSecurePhotoVerification.objects.create(user=self.user) attempt2.mark_ready() attempt2.submit() attempt2.approve() attempt2.save()
self.assertContains(response, unicode(self.course.id))
alt_text = self.BANNER_ALT_MESSAGES.get(status) if alt_text: self.assertContains(response, alt_text)
self.assertContains( response, "<article class=\"course {}\">".format(self.MODE_CLASSES[status]) )
if status is not None: if status in self.NOTIFICATION_MESSAGES: found_msg = False for message in self.NOTIFICATION_MESSAGES[status]: if message in response.content: found_msg = True break
all_messages = [] for msg_group in self.NOTIFICATION_MESSAGES.values(): all_messages.extend(msg_group)
for msg in all_messages: self.assertNotContains(response, msg)
self.bad_user_client = Client() self.good_user_client = Client() self.non_staff_client = Client() self.admin_client = Client()
self.some_url = '/'
UserProfile.objects.exists() return HttpResponse(mock_render_to_string(template_name, context))
request = RequestFactory().post('unused_url') request.user = self.user request.META['HTTP_HOST'] = "aGenericValidHostName" self.append_allowed_hosts("aGenericValidHostName")
user1_new_email = "valid_user1_email@example.com" user2_new_email = "valid_user2_email@example.com"
user2 = UserFactory.create(email=self.new_email, password="test2")
self.assertIsNone(self.do_email_change(self.user, user1_new_email)) self.assertIsNone(self.do_email_change(user2, user2_new_email))
request = RequestFactory().post('unused_url') request.user = self.user request.META['HTTP_HOST'] = "aGenericValidHostName" self.append_allowed_hosts("aGenericValidHostName")
self.registration = Registration() self.registration.register(self.user) self.registration.save()
self.assertFalse(self.user.is_active)
self.registration.activate() self.assertTrue(self.user.is_active) self.assertFalse(mock_segment_identify.called)
self.assertFalse(self.user.is_active)
self.registration.activate() self.assertTrue(self.user.is_active) mock_segment_identify.assert_called_with( self.user.id, expected_segment_payload, expected_segment_mailchimp_list )
([], '', CourseMode.DEFAULT_MODE_SLUG),
(['verified', 'audit'], 'course_modes_choose', CourseMode.DEFAULT_MODE_SLUG),
(['honor', 'verified', 'audit'], 'course_modes_choose', CourseMode.HONOR),
for mode_slug in course_modes: CourseModeFactory.create( course_id=self.course.id, mode_slug=mode_slug, mode_display_name=mode_slug, )
full_url = ( reverse(next_url, kwargs={'course_id': unicode(self.course.id)}) if next_url else next_url )
resp = self._change_enrollment('enroll') self.assertEqual(resp.status_code, 200) self.assertEqual(resp.content, full_url)
if enrollment_mode is None: self.assertFalse(CourseEnrollment.is_enrolled(self.user, self.course.id))
CourseEnrollment.enroll(self.user, self.course.id, mode="honor")
resp = self._change_enrollment('unenroll') self.assertEqual(resp.status_code, 200)
self.assertFalse(CourseEnrollment.is_enrolled(self.user, self.course.id))
for mode_slug in course_modes: CourseModeFactory.create( course_id=self.course.id, mode_slug=mode_slug, mode_display_name=mode_slug, )
self._change_enrollment('enroll', email_opt_in=email_opt_in)
if email_opt_in is not None: opt_in = email_opt_in == 'true' mock_update_email_opt_in.assert_called_once_with(self.user, self.course.org, opt_in) else: self.assertFalse(mock_update_email_opt_in.called)
is_enrolled = CourseEnrollment.is_enrolled(self.user, self.course.id) self.assertFalse(is_enrolled)
is_enrolled = CourseEnrollment.is_enrolled(self.user, self.course.id) self.assertTrue(is_enrolled)
self.client.logout()
resp = self._change_enrollment('enroll') self.assertEqual(resp.status_code, 403)
resp = self._change_enrollment('unenroll') self.assertEqual(resp.status_code, 400)
staff = UserFactory.create(username="staff", email="staff@e.com", password="test") role = CourseStaffRole(self.course_limited.id) role.add_users(staff)
instructor = UserFactory.create(username="instructor", email="instructor@e.com", password="test") role = CourseInstructorRole(self.course_limited.id) role.add_users(instructor)
current_year = datetime.datetime.now().year self.set_year_of_birth(current_year - 10) self.assertFalse(self.profile.requires_parental_consent())
self.profile.profile_image_uploaded_at = datetime.datetime.now() self.profile.save() self.assertFalse(self.profile.has_profile_image)
self.set_year_of_birth(current_year - 10) self.profile.save() self.assertFalse(self.profile.has_profile_image)
with self.assertRaises(AttributeError): getattr(self.profile, '_changed_fields')
super(AutoAuthEnabledTestCase, self).setUp() self.url = '/auto_auth' self.client = Client()
user_profile = UserProfile.objects.get(user=user) self.assertEqual(user_profile.name, "Robot Name")
self.assertFalse(user.is_staff)
self._auto_auth({'username': 'test', 'course_id': course_id})
self.assertEqual(CourseEnrollment.objects.count(), 1) enrollment = CourseEnrollment.objects.get(course_id=course_key) self.assertEqual(enrollment.user.username, "test")
self._auto_auth({'username': 'test', 'course_id': course_id})
self._auto_auth({'username': 'test', 'course_id': course_id})
self.assertEqual(CourseEnrollment.objects.count(), 1) enrollment = CourseEnrollment.objects.get(course_id=course_key) self.assertEqual(enrollment.user.username, "test")
response = self._auto_auth({ 'username': 'test', 'course_id': course_id, 'redirect': True, 'staff': 'true', }, status_code=302)
self.assertEqual(CourseEnrollment.objects.count(), 1) enrollment = CourseEnrollment.objects.get(course_id=course_key) self.assertEqual(enrollment.user.username, "test")
if settings.ROOT_URLCONF == 'lms.urls': url_pattern = '/info' else: url_pattern = '/course/{}'.format(unicode(course_key))
response = self._auto_auth({ 'username': 'test', 'redirect': True, 'staff': 'true', }, status_code=302)
if settings.ROOT_URLCONF == 'lms.urls': url_pattern = '/dashboard' else: url_pattern = '/home'
url_pattern = '/u/test#about_me' response = self._auto_auth({ 'username': 'test', 'redirect_to': url_pattern, 'staff': 'true', }, status_code=302)
for cookie in ['csrftoken', 'sessionid']:
super(AutoAuthDisabledTestCase, self).setUp() self.url = '/auto_auth' self.client = Client()
self.course = CourseFactory()
CreditProvider.objects.create( provider_id=self.PROVIDER_ID, display_name=self.PROVIDER_NAME, provider_status_url=self.PROVIDER_STATUS_URL, enable_integration=True, )
credit_api.set_credit_requirements(
self.enrollment = CourseEnrollmentFactory( user=self.user,
response = self._load_dashboard() self.assertNotContains(response, "credit-eligibility-msg") self.assertNotContains(response, "purchase-credit-btn")
self._make_eligible()
response = self._load_dashboard() self.assertContains(response, "credit-eligibility-msg") self.assertContains(response, "purchase-credit-btn")
eligibility = CreditEligibility.objects.get(username=self.USERNAME) eligibility.deadline = datetime.datetime.now(pytz.UTC) + datetime.timedelta(days=29) eligibility.save()
self._make_eligible() self._purchase_credit()
self._make_eligible() self._purchase_credit() self._initiate_request()
response = self._load_dashboard() self.assertContains(response, "credit-request-pending-msg")
self._make_eligible() self._purchase_credit() request_uuid = self._initiate_request() self._set_request_status(request_uuid, "approved")
response = self._load_dashboard() self.assertContains(response, "credit-request-approved-msg")
self._make_eligible() self._purchase_credit() request_uuid = self._initiate_request() self._set_request_status(request_uuid, "rejected")
response = self._load_dashboard() self.assertContains(response, "credit-request-rejected-msg")
self._make_eligible() self._purchase_credit() CourseEnrollmentAttribute.objects.all().delete()
response = self._load_dashboard() self.assertContains(response, "credit-error-msg")
self._make_eligible()
with patch('student.views.get_credit_provider_display_names') as mock_method: mock_method.return_value = providers_list response = self._load_dashboard()
self.user = UserFactory.build(username='test', email='test@edx.org') self.user.set_password('test_password') self.user.save()
RegistrationFactory(user=self.user)
UserProfileFactory(user=self.user)
self.client = Client() cache.clear()
try: self.url = reverse('login_post') except NoReverseMatch: self.url = reverse('login')
self.user.is_active = False self.user.save()
self.user.is_active = False self.user.save()
cookie = self.client.cookies[settings.EDXMKTG_USER_INFO_COOKIE_NAME] user_info = json.loads(cookie.value)
self.assertEqual(user_info["version"], settings.EDXMKTG_USER_INFO_COOKIE_VERSION)
self.assertEqual(user_info["username"], self.user.username) self.assertEqual(user_info["email"], self.user.email)
for url in user_info["header_urls"].values(): self.assertIn("http://testserver/", url)
self.assertIn(settings.EDXMKTG_LOGGED_IN_COOKIE_NAME, self.client.cookies) self.assertIn(settings.EDXMKTG_USER_INFO_COOKIE_NAME, self.client.cookies)
logout_url = reverse('logout') response = self.client.post(logout_url)
for cookie_name in [settings.EDXMKTG_LOGGED_IN_COOKIE_NAME, settings.EDXMKTG_USER_INFO_COOKIE_NAME]: cookie = self.client.cookies[cookie_name] self.assertIn("01-Jan-1970", cookie.get('expires'))
response, _ = self._login_response('test@edx.org', 'test_password') self._assert_response(response, success=True)
self.user = User.objects.get(pk=self.user.pk)
response = client2.post(self.url, creds) self._assert_response(response, success=True)
url = reverse('dashboard')
self.assertEqual(response.status_code, 302)
self.assertFalse(hasattr(user, 'profile'))
user = User.objects.get(pk=user.pk)
self.assertTrue(hasattr(user, 'profile'))
response = client2.post(self.url, creds) self._assert_response(response, success=True)
url = reverse('dashboard')
self.assertEqual(response.status_code, 302)
self.user = User.objects.get(pk=self.user.pk)
response = client2.post(self.url, creds) self._assert_response(response, success=True)
if bypass_activation_email: self.assertFalse(mock_send_mail.called) else: self.assertTrue(mock_send_mail.called)
del params["username"] assert_username_error("Username must be minimum of two characters long")
for username in ["", "a"]: params["username"] = username assert_username_error("Username must be minimum of two characters long")
params["username"] = "this_username_has_31_characters" assert_username_error("Username cannot be more than 30 characters long")
params["username"] = "invalid username" assert_username_error("Usernames must contain only letters, numbers, underscores (_), and hyphens (-).")
del params["email"] assert_email_error("A properly formatted e-mail is required")
for email in ["", "a"]: params["email"] = email assert_email_error("A properly formatted e-mail is required")
params["email"] = "this_email_address_has_76_characters_in_it_so_it_is_unacceptable@example.com" assert_email_error("Email cannot be more than 75 characters long")
params["email"] = "not_an_email_address" assert_email_error("A properly formatted e-mail is required")
del params["password"] assert_password_error("A valid password is required")
for password in ["", "a"]: params["password"] = password assert_password_error("A valid password is required")
params["username"] = params["password"] = "test_username_and_password" assert_password_error("Username and password fields cannot match")
del params["name"] assert_name_error("Your legal name must be a minimum of two characters long")
for name in ["", "a"]: params["name"] = name assert_name_error("Your legal name must be a minimum of two characters long")
del params["honor_code"] assert_honor_code_error("To enroll, you must follow the honor code.")
for honor_code in ["", "false", "not_boolean"]: params["honor_code"] = honor_code assert_honor_code_error("To enroll, you must follow the honor code.")
params["honor_code"] = "tRUe" self.assert_success(params)
del params["honor_code"] params["username"] = "another_test_username" params["email"] = "another_test_email@example.com" self.assert_success(params)
del params["terms_of_service"] assert_terms_of_service_error("You must accept the terms of service.")
for terms_of_service in ["", "false", "not_boolean"]: params["terms_of_service"] = terms_of_service assert_terms_of_service_error("You must accept the terms of service.")
params["terms_of_service"] = "tRUe" self.assert_success(params)
assert_extra_field_error()
params[field] = "" assert_extra_field_error()
if min_length > 1: params[field] = "a" assert_extra_field_error()
THIRD_PARTY_AUTH_BACKENDS = ["google-oauth2", "facebook"] THIRD_PARTY_AUTH_PROVIDERS = ["Google", "Facebook"]
params = [('course_id', self.course_id)] response = self.client.get(self.url, params)
expected_url = _third_party_login_url( backend_name, "login", redirect_url=_finish_auth_url(params), ) self.assertContains(response, expected_url)
expected_url = _third_party_login_url( backend_name, "login", redirect_url=self.courseware_url ) self.assertContains(response, expected_url)
response = self.client.get(self.url, params)
post_login_handler = _finish_auth_url(params) js_success_var = 'var nextUrl = "{}";'.format(post_login_handler) self.assertContains(response, js_success_var)
response = self.client.get(self.url, params)
post_login_handler = _finish_auth_url(params) js_success_var = 'var nextUrl = "{}";'.format(post_login_handler) self.assertContains(response, js_success_var)
self._check_linkedin_visibility(False)
self._check_linkedin_visibility(True)
self._check_linkedin_visibility(False)
for url_name, url_path in header_urls.iteritems(): header_urls[url_name] = request.build_absolute_uri(url_path)
CACHE_TOOLBOX_DEFAULT_TIMEOUT = getattr( settings, 'CACHE_TOOLBOX_DEFAULT_TIMEOUT', 60 * 60 * 24 * 3, )
try: return getattr(self, descriptor.cache_name) except AttributeError: pass
try: return getattr(self, '_%s_cache' % related_name) except AttributeError: pass
return
session_user_id = SafeSessionMiddleware.get_user_id_from_session(request)
raise Exception
super(CacheBackedAuthenticationMiddleware, self).process_request(request)
instance = model(pk=pk, **data)
instance._state.adding = False
instance._state.db = using or DEFAULT_DB_ALIAS
cache.delete(key)
if field.primary_key: continue
file = getattr(instance, field.attname) data[field.attname] = file.name
return '%s.%s:%d' % ( model._meta.app_label, model._meta.model_name, getattr(instance_or_pk, 'pk', instance_or_pk), )
pass
from django.utils.translation import get_language
return view_func(request, *args, **kwargs)
if RateLimitConfiguration.current().enabled: return func(*args, **kwargs) else: msg = "Rate limiting is disabled because `RateLimitConfiguration` is not enabled." LOGGER.info(msg) return
if not issubclass(clz, APIView): msg = ( u"{clz} is not a Django Rest Framework APIView subclass." ).format(clz=clz) LOGGER.warning(msg) return clz
if hasattr(clz, 'check_throttles'): clz.check_throttles = _check_throttles_decorator(clz.check_throttles)
resolve('/')
URLCONF_MODULES = None
URLCONF_MODULES = ['myapp.url']
URLCONF_MODULES = ['myapp.url', 'another_app.urls']
TestCase._enter_atomics = enter_atomics_wrapper(TestCase._enter_atomics) TestCase._rollback_atomics = rollback_atomics_wrapper(TestCase._rollback_atomics)
key = cleaned_string(key) key_prefix = cleaned_string(key_prefix) version = cleaned_string(version)
combined = ":".join([key_prefix, version, key])
if len(combined) > 250: combined = fasthash(combined)
return combined
format = ugettext("LONG_DATE_FORMAT") if format == "LONG_DATE_FORMAT": format = DEFAULT_LONG_DATE_FORMAT
format = ugettext("DATE_TIME_FORMAT") if format == "DATE_TIME_FORMAT": format = DEFAULT_DATE_TIME_FORMAT
raise ValueError("strftime format ends with raw %")
part = dtime.strftime(code)
pgettext = real_pgettext
milestones_api.add_course_milestone(course_key, 'requires', milestone)
milestones_api.add_course_milestone(prerequisite_course_key, 'fulfills', milestone)
if prerequisite_course_keys: for prerequisite_course_key_string in prerequisite_course_keys: prerequisite_course_key = CourseKey.from_string(prerequisite_course_key_string) add_prerequisite_course(course_key, prerequisite_course_key)
if required_courses: pre_requisite_courses[course_key] = {'courses': required_courses}
seed_milestone_relationship_types() course_milestones = milestones_api.get_course_milestones(course_key=course_key, relationship="fulfills")
try: milestone_paths = get_course_milestones_fulfillment_paths( unicode(course.id), serialize_user(user) ) except InvalidMilestoneRelationshipTypeException: return required_content
raise
log.exception("Error in django view.") return render_to_response(template_path, context)
raise
client_args={"disable_ssl_certificate_validation": True}
zendesk_tags = list(tags.values()) + ["LMS"]
white_label_org = microsite.get_value('course_org_filter') if white_label_org: zendesk_tags = zendesk_tags + ["whitelabel_{org}".format(org=white_label_org)]
log.warning('Unable to find group named %s for Zendesk ticket with ID %s.', group_name, ticket_id)
user_id = context.get('user_id') course_title = context.get('course_title')
from __future__ import unicode_literals
from __future__ import unicode_literals
from functools import wraps import random
ENABLED = True
if self.read_committed is True: if connection.vendor == 'mysql': cursor = connection.cursor() cursor.execute("SET TRANSACTION ISOLATION LEVEL READ COMMITTED")
try: connection.commit() except DatabaseError: try: connection.rollback() except Error: connection.close() raise
try: connection.rollback() except Error: connection.close()
if connection.commit_on_success_block_level == 0: if connection.features.autocommits_when_autocommit_is_off: connection.autocommit = True else: connection.set_autocommit(True)
else: return CommitOnSuccessManager(using, read_committed)
if not self.ALLOW_NESTED and connection.in_atomic_block: raise transaction.TransactionManagementError('Cannot be inside an atomic block.')
if self.read_committed is True: if connection.vendor == 'mysql': cursor = connection.cursor() cursor.execute("SET TRANSACTION ISOLATION LEVEL READ COMMITTED")
else: return OuterAtomic(using, savepoint, read_committed)
stored_file_name = file_storage.save(stored_file_name, uploaded_file)
USER_SETTINGS_CHANGED_EVENT_NAME = u'edx.user.settings.changed'
return {}
if isinstance(value, Country): if value.code: return value.code else: return None return value
if hasattr(instance, '_changed_fields'): del instance._changed_fields
max_value_length = settings.TRACK_MAX_EVENT / 4
module = descriptor
settings.ALLOWED_HOSTS = [request.META['HTTP_HOST']] self.assertEqual(safe_get_host(request), request.META['HTTP_HOST'])
settings.ALLOWED_HOSTS = ["the_valid_website.com"] with self.assertRaises(SuspiciousOperation): safe_get_host(request)
UNICODE_CHAR_CODES = (range(30) + [127] + [129, 500, 2 ** 8 - 1, 2 ** 8 + 1, 2 ** 16 - 1])
self.assertEqual(safe_key(1, 'prefix', 'version'), 'prefix:version:1')
self.assertEqual(safe_key('test', 5, 'version'), '5:version:test')
self.assertEqual(safe_key('test', 'prefix', 5), 'prefix:5:test')
for length in [248, 249, 250, 251, 252]:
key = 'a' * length
key = safe_key(key, '', '')
self.assertTrue(self._is_valid_key(key), msg="Failed for key length {0}".format(length))
key = safe_key('a' * 300, 'prefix', 'version') self.assertTrue(self._is_valid_key(key))
key = safe_key('key', 'a' * 300, 'version') self.assertTrue(self._is_valid_key(key))
key = safe_key('key', 'prefix', 'a' * 300) self.assertTrue(self._is_valid_key(key))
key = unichr(unicode_char)
key = safe_key(key, '', '')
self.assertTrue(self._is_valid_key(key), msg="Failed for unicode character {0}".format(unicode_char))
prefix = unichr(unicode_char)
key = safe_key('test', prefix, '')
self.assertTrue(self._is_valid_key(key), msg="Failed for unicode character {0}".format(unicode_char))
version = unichr(unicode_char)
key = safe_key('test', '', version)
self.assertTrue(self._is_valid_key(key), msg="Failed for unicode character {0}".format(unicode_char))
if len(key) > 250: return False
for char in key: if ord(char) < 33 or ord(char) == 127: return False
verify_file_presence(False)
verify_file_presence(True)
self.assertEqual(expected, dtime.strftime(fmt.encode('utf8')).decode('utf8'))
response = organizations_helpers.get_organization_by_short_name('non_existing') self.assertIsNone(response)
RateLimitConfiguration.objects.create(enabled=True)
request = mock.Mock() with self.assertRaises(Throttled): self.view.check_throttles(request)
RateLimitConfiguration.objects.create(enabled=False)
request = mock.Mock() self.view.check_throttles(request)
self.assertFalse(zendesk_mock_class.return_value.mock_calls) self.assertFalse(datadog_mock.mock_calls)
self.assertFalse(zendesk_mock_class.mock_calls) self.assertFalse(datadog_mock.mock_calls)
PYTHON_LIB_ZIP = "python_lib.zip"
COURSE_REGEX = re.compile(r'^(.*?/courses/)(?!v[0-9]+/[^/]+){}'.format(settings.COURSE_ID_PATTERN))
#pylint: skip-file from __future__ import unicode_literals
from __future__ import unicode_literals
content = None try: content = self.load_asset_from_location(loc) except (ItemNotFoundError, NotFoundError): return HttpResponseNotFound()
safe_course_key = loc.course_key if safe_course_key.run is None: safe_course_key = safe_course_key.replace(run='only')
is_from_cdn = StaticContentServer.is_cdn_request(request) newrelic.agent.add_custom_parameter('contentserver.from_cdn', is_from_cdn)
locked = self.is_content_locked(content) newrelic.agent.add_custom_parameter('contentserver.locked', locked)
if not self.is_user_authorized(request, content, loc): return HttpResponseForbidden('Unauthorized')
last_modified_at_str = content.last_modified_at.strftime(HTTP_DATE_FORMAT) if 'HTTP_IF_MODIFIED_SINCE' in request.META: if_modified_since = request.META['HTTP_IF_MODIFIED_SINCE'] if if_modified_since == last_modified_at_str: return HttpResponseNotModified()
log.exception( u"%s in Range header: %s for content: %s", exception.message, header_value, unicode(loc) )
log.warning(u"Unknown unit in Range header: %s for content: %s", header_value, unicode(loc))
log.warning( u"More than 1 ranges in Range header: %s for content: %s", header_value, unicode(loc) )
if response is None: response = HttpResponse(content.stream_data()) response['Content-Length'] = content.length
response['Accept-Ranges'] = 'bytes' response['Content-Type'] = content.content_type
return True
content = get_cached_content(location) if content is None: try: content = AssetManager.find(location, as_stream=True) except (ItemNotFoundError, NotFoundError): raise
for byte_range_string in byte_ranges_string.split(','): byte_range_string = byte_range_string.strip()
if hasattr(settings, 'DEPRECATED_ADVANCED_COMPONENT_TYPES'): xblock_types.extend( xblock_type for xblock_type in settings.DEPRECATED_ADVANCED_COMPONENT_TYPES if xblock_type not in xblock_types )
from __future__ import unicode_literals
from __future__ import unicode_literals
XBlockDisableConfig.objects.create( disabled_blocks='', enabled=True )
wrapped_func = wrapped_func.__func__
UserSocialAuth._meta.app_label = "default" Nonce._meta.app_label = "default" Association._meta.app_label = "default" Code._meta.app_label = "default"
__BACKUP_ATTRIBUTE_NAME = '__monkey_patch'
Options.FORWARD_PROPERTIES = {'fields', 'many_to_many', 'concrete_fields', 'local_concrete_fields', '_forward_fields_map'}
if timeout_in_seconds: utc_now = datetime.utcnow()
last_touch = request.session.get(LAST_TOUCH_KEYNAME)
if last_touch: time_since_last_activity = utc_now - last_touch
if time_since_last_activity > timedelta(seconds=timeout_in_seconds): del request.session[LAST_TOUCH_KEYNAME] auth.logout(request) return
import lettuce.django
xf.XMODULE_FACTORY_LOCK.enable()
success = False num_attempts = 0 while (not success) and num_attempts < MAX_VALID_BROWSER_ATTEMPTS:
if not success: raise IOError("Could not acquire valid {driver} browser session.".format(driver=browser_driver))
'transcript': 'http://video.google.com/timedtext?lang=en&v=OEoXaMPEzfM',
scenario.steps = [] return
return False
if method in self.URL_HANDLERS: handlers_list = self.URL_HANDLERS[method] else: self.log_error("Unrecognized method '{method}'".format(method=method)) return
if self._match_pattern(handlers_list): return else: self.send_response(404, content="404 Not Found")
notes = deepcopy(notes[start:end])
elif self._is_correct_lti_request(): params = {k: v for k, v in self.post_dict.items() if k != 'oauth_signature'}
response = requests.post(url, data=data, headers=headers, verify=False)
if self.post_dict.get('roles'): role = '<h5>Role: {}</h5>'.format(self.post_dict['roles']) else: role = ''
return urllib.unquote(urllib.unquote(response_str))
'Content-Type': content_type,
logging.basicConfig(level=logging.DEBUG, format="%(levelname)s %(message)s")
DEFAULT_DELAY_SEC = 0.5
time.sleep(self.server.config.get('time_to_response', self.DEFAULT_DELAY_SEC))
callback = self.get_params['callback']
if comment_id in self.server.config.get('comments', {}): comment = self.server.config['comments'][comment_id] self.send_json_response(comment)
self.server.config['test_reset'] = 'This is a reset config test'
response = requests.delete(reset_config_url) self.assertEqual(response.status_code, 200)
self.assertEqual(self.server.config, {})
response = requests.get(self._get_url("api/v1/search")) self.assertEqual(response.status_code, 400)
response = requests.get(self._get_url("api/v1/annotations")) self.assertEqual(response.status_code, 400)
response = requests.get(self._get_url("api/v1/annotations"), params={"user": "dummy-user-id"})
response = requests.get(self._get_url("api/v1/annotations"), params={ "user": "dummy-user-id", "page": 2, "page_size": 3 })
self.test_cleanup()
patcher = mock.patch('terrain.stubs.xqueue.post') self.post = patcher.start() self.addCleanup(patcher.stop)
patcher = mock.patch('terrain.stubs.xqueue.Timer') timer = patcher.start() timer.side_effect = FakeTimer self.addCleanup(patcher.stop)
expected_body = json.dumps({'correct': True, 'score': 1, 'msg': '<div></div>'}) self._check_grade_response(callback_url, expected_header, expected_body)
response_content = {'test_response': 'test_content'} self.server.config['default'] = response_content
self._check_grade_response(callback_url, expected_header, json.dumps(response_content))
response_content = {'test_response': 'test_content'} self.server.config['This is only a test.'] = response_content
self._check_grade_response(callback_url, expected_header, json.dumps(response_content))
self.server.config['test_1'] = {'response': True} self.server.config['test_2'] = {'response': False}
self.assertFalse(self.post.called) self.assertTrue(logger.error.called)
self.assertEqual(resp.status_code, 200)
return grade_request['xqueue_header']
expected_callback_dict = { 'xqueue_header': expected_header, 'xqueue_body': expected_body, }
self.post.assert_called_with(callback_url, data=expected_callback_dict)
post_params = {key: json.dumps(val)} response = requests.put(self.url, data=post_params) self.assertEqual(response.status_code, 200)
for key, val in params.iteritems(): self.assertEqual(self.server.config.get(key), val)
response = requests.put(self.url, data={'test_unicode': u'\u2603 the snowman'}) self.assertEqual(response.status_code, 400)
response = requests.get(self.url, params={"test_param": 2}) self.assertEqual(response.status_code, 200)
response = requests.get(self.url) self.assertEqual(response.status_code, 400)
response = requests.get(self.url + "?test_param=") self.assertEqual(response.status_code, 400)
response = requests.post(self.url, data={"test_param": 2}) self.assertEqual(response.status_code, 200)
response = requests.post(self.url) self.assertEqual(response.status_code, 400)
response = requests.post(self.url, data={"test_param": None}) self.assertEqual(response.status_code, 400)
missing = [] for key in required_keys: if params.get(key) is None: missing.append(key)
else: return func(self, *args, **kwargs)
try: post_dict = urlparse.parse_qs(contents, keep_blank_values=True) return { key: list_val[0] for key, list_val in post_dict.items() }
return { key: value[0] if len(value) == 1 else value for key, value in urlparse.parse_qs(query).items() }
try: key = unicode(key, 'utf-8') value = unicode(value, 'utf-8') except UnicodeDecodeError: self.log_message("Could not decode request params as UTF-8")
else: self.send_response(200)
HANDLER_CLASS = StubHttpRequestHandler
self.config = dict()
server_thread = threading.Thread(target=self.serve_forever) server_thread.daemon = True server_thread.start()
LOGGER.debug('Starting service on port {0}'.format(self.port))
HTTPServer.shutdown(self)
self.socket.close()
if self._is_grade_request():
error_msg = "XQueue received invalid grade request" self._send_immediate_response(False, message=error_msg)
error_msg = "XQueue could not decode grade request" self._send_immediate_response(False, message=error_msg)
self._send_immediate_response(True)
delayed_grade_func = lambda: self._send_grade_response( callback_url, xqueue_header, self.post_dict['xqueue_body'] )
else: self._send_immediate_response(False, message="Invalid request URL")
response_str = json.dumps( {'return_code': 0 if success else 1, 'content': message} )
grade_response = None
else: self.log_error( "Multiple response patterns matched '{0}'".format(xqueue_body_json), ) return
if grade_response is None: grade_response = self.server.config.get( 'default', copy.deepcopy(self.DEFAULT_GRADE_RESPONSE) )
if isinstance(grade_response, dict) and 'msg' in grade_response: grade_response['msg'] = "<div>{0}</div>".format(grade_response['msg'])
if url is not None:
grader_payload = xqueue_body.get('grader_payload')
import lettuce.django
re.compile(r'^Schedule & Details Settings \|'): [ "jquery", "js/base", "js/models/course", "js/models/settings/course_details", "js/views/settings/main"],
re.compile(r'^Advanced Settings \|'): [ "jquery", "js/base", "js/models/course", "js/models/settings/advanced", "js/views/settings/advanced", "codemirror"],
re.compile(r'^Course Outline \|'): [ "js/base", "js/models/course", "js/models/location", "js/models/section"],
re.compile(r'^Pages \|'): [ 'js/models/explicit_url', 'coffee/src/views/tabs', 'xmodule', 'coffee/src/main', 'xblock/cms.runtime.v1' ],
world.wait(1) continue
world.wait(1) continue
if result['requireType'] == 'require': world.wait(1) continue
if dependencies[0] != "jquery": dependencies.insert(0, "jquery")
world.wait(1) continue
if text: wait_for(lambda _: css_text(css_selector, index=index))
if partial_text: wait_for(lambda _: css_html(css_selector, index=index), timeout=8)
if value: wait_for(lambda _: css_value(css_selector, index=index))
if is_css_present(css_selector): return retry_on_exception(lambda: css_find(css_selector, wait_time=timeout)[index].text) else: return ""
if is_css_present(css_selector): return retry_on_exception(lambda: css_find(css_selector)[index].value) else: return ""
world.wait_for_js_to_load()
world.browser.execute_script("jQuery.fx.off = true;")
if len(User.objects.filter(username=uname)) > 0: return
user = User.objects.get(username=username) world.scenario_dict['USER'] = user
if is_staff: user.is_staff = True user.save() CourseEnrollment.enroll(user, course_key)
registration = world.RegistrationFactory(user=user) registration.register(user) registration.activate() CourseEnrollment.enroll(user, course_key)
languages.sort() return languages
self.assertAcceptEquals( 'rel;q=1.0, rel;q=0.5', self.process_request(accept='rel-ter;q=1.0, rel;q=0.5') )
self.assertAcceptEquals( 'rel-ter;q=1.0, rel-ter;q=0.5', self.process_request(accept='rel-ter;q=1.0, rel;q=0.5') )
DarkLangConfig( released_languages=('es-419, en'), changed_by=self.user, enabled=True ).save()
DarkLangConfig( released_languages=('es, en'), changed_by=self.user, enabled=True ).save()
DarkLangConfig( released_languages=('es-419, es, es-es'), changed_by=self.user, enabled=True ).save() self.assertAcceptEquals( expected, self.process_request(accept=accept_header) )
self.assertSessionLangEquals( 'rel', self.process_request(preview_lang='rel') )
DARK_LANGUAGE_KEY = 'dark-lang'
from __future__ import unicode_literals
from django.db import migrations, models
from __future__ import unicode_literals
CHINESE_LANGUAGE_CODE_MAP = {
if LANGUAGE_SESSION_KEY in request.session: del request.session[LANGUAGE_SESSION_KEY]
delete_user_preference(request.user, DARK_LANGUAGE_KEY) user_pref = get_user_preference(request.user, LANGUAGE_KEY) if user_pref: request.session[LANGUAGE_SESSION_KEY] = user_pref
preview_lang = request.GET.get('preview-lang', None) if not preview_lang and auth_user: preview_lang = get_user_preference(request.user, DARK_LANGUAGE_KEY)
if not preview_lang: return
request.session[LANGUAGE_SESSION_KEY] = preview_lang
if auth_user: set_user_preference(request.user, DARK_LANGUAGE_KEY, preview_lang)
course_id = CourseKeyField(max_length=255, db_index=True, verbose_name=_("Course"))
mode_slug = models.CharField(max_length=100, verbose_name=_("Mode"))
mode_display_name = models.CharField(max_length=255, verbose_name=_("Display Name"))
currency = models.CharField(default="usd", max_length=8)
expiration_datetime_is_explicit = models.BooleanField(default=False)
expiration_date = models.DateField(default=None, null=True, blank=True)
suggested_prices = models.CommaSeparatedIntegerField(max_length=255, blank=True, default='')
description = models.TextField(null=True, blank=True)
bulk_sku = models.CharField( max_length=255, null=True, blank=True,
VERIFIED_MODES = [VERIFIED, PROFESSIONAL]
NON_VERIFIED_MODES = [HONOR, AUDIT, NO_ID_PROFESSIONAL_MODE]
CREDIT_MODES = [CREDIT_MODE]
UPSELL_TO_VERIFIED_MODES = [HONOR, AUDIT]
DEFAULT_SHOPPINGCART_MODE_SLUG = HONOR DEFAULT_SHOPPINGCART_MODE = Mode(HONOR, _('Honor'), 0, '', 'usd', None, None, None, None)
if new_datetime is not None: self.expiration_datetime_is_explicit = True self._expiration_datetime = new_datetime
missing_courses = set(course_id_list) - set(modes_by_course.keys()) for course_id in missing_courses: modes_by_course[course_id] = [cls.DEFAULT_MODE]
if not include_expired: found_course_modes = found_course_modes.filter( Q(_expiration_datetime__isnull=True) | Q(_expiration_datetime__gte=now) )
return professional_mode if professional_mode else verified_mode
if cls.has_professional_mode(modes_dict): return False
if cls.is_white_label(course_id, modes_dict=modes_dict): return False
return cls.AUDIT in modes_dict or cls.HONOR in modes_dict
course_id = CourseKeyField(max_length=255, db_index=True)
mode_slug = models.CharField(max_length=100)
mode_display_name = models.CharField(max_length=255)
min_price = models.IntegerField(default=0)
suggested_prices = models.CommaSeparatedIntegerField(max_length=255, blank=True, default='')
currency = models.CharField(default="usd", max_length=8)
expiration_date = models.DateField(default=None, null=True, blank=True)
url(r'^choose/{}/$'.format(settings.COURSE_ID_PATTERN), views.ChooseModeView.as_view(), name='course_modes_choose'),
embargo_redirect = embargo_api.redirect_if_blocked( course_key, user=request.user, ip_address=get_ip(request), url=request.path ) if embargo_redirect: return redirect(embargo_redirect)
if is_active and (enrollment_mode in CourseMode.VERIFIED_MODES + [CourseMode.NO_ID_PROFESSIONAL_MODE]): return redirect(reverse('dashboard'))
return redirect(reverse('dashboard'))
amount_value = decimal.Decimal(amount).quantize(decimal.Decimal('.01'), rounding=decimal.ROUND_DOWN)
if amount_value < mode_info.min_price: error_msg = _("No selected price or selected price is too low.") return self.get(request, course_id, error=error_msg)
for parameter, default in PARAMETERS.iteritems(): PARAMETERS[parameter] = request.GET.get(parameter, default)
course_key = CourseKey.from_string(course_id) CourseMode.objects.get_or_create(course_id=course_key, **PARAMETERS)
return HttpResponse("Mode '{mode_slug}' created for '{course}'.".format( mode_slug=PARAMETERS['mode_slug'], course=course_id ))
[(CourseMode.DEFAULT_SHOPPINGCART_MODE_SLUG, CourseMode.DEFAULT_SHOPPINGCART_MODE_SLUG)]
if self.cleaned_data.get("_expiration_datetime"): return self.cleaned_data.get("_expiration_datetime").replace(tzinfo=UTC)
if verification_deadline is not None and mode_slug not in CourseMode.VERIFIED_MODES: raise forms.ValidationError("Verification deadline can be set only for verified modes.")
if verification_deadline is not None: if upgrade_deadline is not None and verification_deadline < upgrade_deadline: raise forms.ValidationError("Verification deadline must be after the upgrade deadline.")
if course_key is not None and mode_slug in CourseMode.VERIFIED_MODES: verification_models.VerificationDeadline.set_deadline(course_key, verification_deadline)
expiration_datetime_custom.short_description = "Upgrade Deadline"
from __future__ import unicode_literals
from __future__ import unicode_literals from datetime import timedelta
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
response = self.client.post(reverse('admin:course_modes_coursemode_add'), data=data) self.assertRedirects(response, reverse('admin:course_modes_coursemode_changelist'))
course_mode = CourseMode.objects.get(pk=1) self.assertEqual(course_mode.expiration_datetime.replace(tzinfo=None), expiration.replace(tzinfo=None))
VerificationDeadline.set_deadline(self.course.id, self.VERIFICATION_DEADLINE)
deadline = self.UPGRADE_DEADLINE if mode == "verified" else None form = self._admin_form(mode, upgrade_deadline=deadline)
VerificationDeadline.set_deadline(self.course.id, self.VERIFICATION_DEADLINE)
form = self._admin_form(course_mode)
new_deadline = (self.VERIFICATION_DEADLINE + timedelta(days=1)).replace(microsecond=0) self._set_form_verification_deadline(form, new_deadline) form.save()
updated_deadline = VerificationDeadline.deadline_for_course(self.course.id) self.assertEqual(updated_deadline, new_deadline)
VerificationDeadline.set_deadline(self.course.id, self.VERIFICATION_DEADLINE)
form = self._admin_form("verified", upgrade_deadline=self.UPGRADE_DEADLINE)
self._set_form_verification_deadline(form, None) form.save()
self.assertIs(VerificationDeadline.deadline_for_course(self.course.id), None)
form = self._admin_form(course_mode) self._set_form_verification_deadline(form, self.VERIFICATION_DEADLINE) self._assert_form_has_error(form, "Verification deadline can be set only for verified modes.")
class CourseModeFactory(DjangoModelFactory): class Meta(object): model = CourseMode
for mode in ('audit', 'honor', 'verified'): CourseModeFactory.create(mode_slug=mode, course_id=self.course.id)
if enrollment_mode is not None: CourseEnrollmentFactory( is_active=is_active, mode=enrollment_mode, course_id=self.course.id, user=self.user )
url = reverse('course_modes_choose', args=[unicode(self.course.id)]) response = self.client.get(url)
if redirect: self.assertRedirects(response, reverse('dashboard')) else: self.assertEquals(response.status_code, 200)
CourseModeFactory.create(mode_slug=CourseMode.NO_ID_PROFESSIONAL_MODE, course_id=self.course.id, min_price=100)
CourseEnrollmentFactory( is_active=False, mode=CourseMode.NO_ID_PROFESSIONAL_MODE, course_id=self.course.id, user=self.user )
for mode in ('audit', 'honor', 'verified'): CourseModeFactory.create(mode_slug=mode, course_id=self.course.id)
url = reverse('course_modes_choose', args=[unicode(self.course.id)]) response = self.client.get(url)
for mode in ('audit', 'honor'): CourseModeFactory.create(mode_slug=mode, course_id=self.course.id)
CourseEnrollmentFactory( is_active=True, course_id=self.course.id, user=self.user )
response = self.client.get( reverse('course_modes_choose', args=[unicode(self.course.id)]), follow=False, )
for mode in available_modes: CourseModeFactory.create(mode_slug=mode, course_id=self.course.id)
url = reverse('course_modes_choose', args=[unicode(self.course.id)]) response = self.client.get(url)
CourseModeFactory.create(mode_slug=mode, course_id=self.course.id, min_price=1)
choose_track_url = reverse('course_modes_choose', args=[unicode(self.course.id)]) response = self.client.get(choose_track_url)
start_flow_url = reverse('verify_student_start_flow', args=[unicode(self.course.id)]) self.assertRedirects(response, start_flow_url)
CourseEnrollmentFactory( user=self.user, is_active=True, mode=mode, course_id=unicode(self.course.id), )
response = self.client.get(choose_track_url) self.assertRedirects(response, reverse('dashboard'))
choose_track_url = reverse('course_modes_choose', args=[unicode(self.course.id)]) response = self.client.post(choose_track_url, self.POST_PARAMS_FOR_COURSE_MODE[course_mode])
CourseModeFactory.create(mode_slug='honor', course_id=self.course.id) CourseModeFactory.create(mode_slug='verified', course_id=self.course.id, min_price=1)
choose_track_url = reverse('course_modes_choose', args=[unicode(self.course.id)]) self.client.post(choose_track_url, self.POST_PARAMS_FOR_COURSE_MODE['verified'])
self.assertIn('donation_for_course', self.client.session) self.assertIn(unicode(self.course.id), self.client.session['donation_for_course'])
for mode in (CourseMode.DEFAULT_MODE_SLUG, 'verified'): CourseModeFactory.create(mode_slug=mode, course_id=self.course.id)
params = { 'enrollment_action': 'enroll', 'course_id': unicode(self.course.id) } self.client.post(reverse('change_enrollment'), params)
choose_track_url = reverse('course_modes_choose', args=[unicode(self.course.id)]) self.client.post(choose_track_url, self.POST_PARAMS_FOR_COURSE_MODE[CourseMode.DEFAULT_MODE_SLUG])
mode, is_active = CourseEnrollment.enrollment_mode_for_user(self.user, self.course.id) self.assertEqual(mode, CourseMode.DEFAULT_MODE_SLUG) self.assertEqual(is_active, True)
for mode in ('honor', 'verified'): CourseModeFactory.create(mode_slug=mode, course_id=self.course.id)
choose_track_url = reverse('course_modes_choose', args=[unicode(self.course.id)]) response = self.client.post(choose_track_url, self.POST_PARAMS_FOR_COURSE_MODE['unsupported'])
url = reverse('create_mode', args=[unicode(self.course.id)]) response = self.client.get(url)
base_url = reverse('create_mode', args=[unicode(self.course.id)]) self.client.get(base_url)
url = reverse('create_mode', args=[unicode(self.course.id)]) self.client.get(url, parameters)
for mode in ["honor", "verified"]: CourseModeFactory.create(mode_slug=mode, course_id=self.course.id)
url = reverse('course_modes_choose', args=[unicode(self.course.id)]) response = self.client.get(url)
self.assertNotContains(response, "How it Works") self.assertNotContains(response, "Find courses") self.assertNotContains(response, "Schools & Partners")
redirect_url = reverse('dashboard') + '?course_closed=1%2F1%2F15%2C+12%3A00+AM' self.assertRedirects(response, redirect_url)
self.url = reverse('course_modes_choose', args=[unicode(self.course.id)])
modes = CourseMode.modes_for_course(self.course_key) self.assertEqual([CourseMode.DEFAULT_MODE], modes)
self.assertEqual(0, CourseMode.min_course_price_for_currency(self.course_key, 'usd'))
self.create_mode('professional', 'Professional Education Verified Certificate', 10)
honor, _ = self.create_mode('honor', 'Honor') self.assertFalse(CourseMode.has_payment_options(self.course_key))
verified, _ = self.create_mode('verified', 'Verified', min_price=5) self.assertTrue(CourseMode.has_payment_options(self.course_key))
verified.delete() self.assertFalse(CourseMode.has_payment_options(self.course_key))
honor.suggested_prices = '5, 10, 15' honor.save() self.assertTrue(CourseMode.has_payment_options(self.course_key))
self.create_mode('no-id-professional', 'no-id-professional', min_price=5) self.assertTrue(CourseMode.has_payment_options(self.course_key))
for mode_slug, min_price in modes_and_prices: self.create_mode(mode_slug, mode_slug.capitalize(), min_price=min_price)
self.assertEqual(CourseMode.can_auto_enroll(self.course_key), can_auto_enroll)
self.assertEqual(CourseMode.auto_enroll_mode(self.course_key, modes), result)
CourseModeFactory.create( course_id=self.course_key, mode_display_name="Honor No Expiration", mode_slug="honor_no_expiration", expiration_datetime=None )
CourseModeFactory.create( course_id=self.course_key, mode_display_name="Honor Not Expired", mode_slug="honor_not_expired", expiration_datetime=future )
CourseModeFactory.create( course_id=self.course_key, mode_display_name="Verified Expired", mode_slug="verified_expired", expiration_datetime=past )
self.assertEqual(len(all_modes[other_course_key]), 1) self.assertEqual(all_modes[other_course_key][0], CourseMode.DEFAULT_MODE)
self.assertFalse(CourseMode.is_professional_mode(None))
if is_verified: self.assertTrue(CourseMode.is_verified_slug(mode_slug)) else: self.assertFalse(CourseMode.is_verified_slug(mode_slug))
for mode in available_modes: CourseModeFactory.create( course_id=self.course_key, mode_display_name=mode, mode_slug=mode, )
selectable_modes = CourseMode.modes_for_course_dict(self.course_key) self.assertItemsEqual(selectable_modes.keys(), expected_selectable_modes)
all_modes = CourseMode.modes_for_course_dict(self.course_key, only_selectable=False) self.assertItemsEqual(all_modes.keys(), available_modes)
return "SELECT MAX(id) FROM {table_name} GROUP BY {key_fields}".format( key_fields=', '.join(key_fields_escaped),
cache_timeout = 600
verbose_name=_("Changed by"),
patcher = patch('config_models.models.cache', Mock(get=Mock(return_value=None))) patcher.start() self.addCleanup(patcher.stop)
with transaction.atomic(): return wrapped_func(*args, **kwargs)
return self.model.current()
serializer.save(changed_by=self.request.user)
def has_delete_permission(self, request, obj=None): return False
def get_readonly_fields(self, request, obj=None):
queryset = self.model.objects.current_set()
if course_message: msg = u"{} <br /> {}".format(msg, course_message.message)
pass
cache.clear() self.course_key = CourseLocator(org='TestOrg', course='TestCourse', run='TestRun')
self.assertEqual(get_site_status_msg(None), None) self.assertEqual(get_site_status_msg(self.course_key), None)
from __future__ import unicode_literals
if not GlobalStatusMessage.current().enabled: return None
if not username: username = request.user.username if username != request.user.username and not has_api_key_permissions: return Response(status=status.HTTP_404_NOT_FOUND)
user = User.objects.get(username=username)
response = api.add_enrollment(username, unicode(course_id), mode=mode, is_active=is_active)
manage.py ... enroll_user_in_course -e test@example.com -c edX/Open_DemoX/edx_demo_course
pass
user_enroll = get_enrollment(self.username, self.course_id) self.assertTrue(user_enroll['is_active'])
log.exception(u"Error occurred while retrieving course enrollment details from the cache")
log.exception(u"Error occurred while caching course enrollment details for course %s", course_id) raise errors.CourseEnrollmentError(u"An unexpected error occurred while retrieving course enrollment details.")
include_expired = not is_active if is_active is not None else False
api_path = getattr(settings, "ENROLLMENT_DATA_API", DEFAULT_DATA_API)
deleted = [] valid = [] for enrollment in enrollments: if enrollment.get("course_details") is not None: valid.append(enrollment) else: deleted.append(enrollment)
self.data = data
([], 'honor'),
(['honor', 'verified', 'audit'], 'honor'),
(['professional'], 'professional'), (['no-id-professional'], 'no-id-professional')
fake_data_api.add_course(self.COURSE_ID, course_modes=course_modes) api.add_enrollment(self.USERNAME, self.COURSE_ID)
fake_data_api.add_course(self.COURSE_ID, course_modes=['professional']) api.add_enrollment(self.USERNAME, self.COURSE_ID, mode='verified')
([], 'honor'),
(['honor', 'verified', 'audit'], 'honor'),
(['professional'], 'professional'), (['no-id-professional'], 'no-id-professional')
fake_data_api.add_course(self.COURSE_ID, course_modes=['honor']) api.update_enrollment(self.USERNAME, self.COURSE_ID, mode='honor', is_active=False)
([]),
api.add_enrollment(self.USERNAME, self.COURSE_ID, mode='audit')
fake_data_api.add_course(self.COURSE_ID, course_modes=['honor', 'verified', 'audit'])
details = api.get_course_enrollment_details(self.COURSE_ID)
fake_data_api.reset() cached_details = api.get_course_enrollment_details(self.COURSE_ID)
self.assertEqual(len(details['course_modes']), 3) self.assertEqual(details, cached_details)
([], 'honor'),
(['honor', 'verified', 'audit'], 'honor'),
self._create_course_modes(course_modes) enrollment = data.create_course_enrollment( self.user.username, unicode(self.course.id), enrollment_mode, True )
self.assertEqual(course_mode, enrollment['mode']) self.assertEqual(is_active, enrollment['is_active'])
CourseEnrollment.enroll(self.user, self.course.id, mode="honor")
self.assertFalse(enrollment['is_active'])
self.assertFalse(CourseEnrollment.is_enrolled(self.user, self.course.id))
([]),
(['honor', 'verified', 'audit']),
([], []),
(['honor', 'verified', 'audit'], ['1', '2', '3']),
created_courses = [] for course_number in course_numbers: created_courses.append(CourseFactory.create(number=course_number))
created_enrollments.append(data.create_course_enrollment( self.user.username, unicode(course.id), 'honor', True ))
results = data.get_course_enrollments(self.user.username) self.assertEqual(results, created_enrollments)
([], 'honor'),
(['honor', 'verified', 'audit'], 'verified'),
result = data.get_course_enrollment(self.user.username, unicode(self.course.id)) self.assertIsNone(result)
([], 'credit'),
(['honor', 'verified', 'audit', 'credit'], 'credit'),
self.assertTrue(mock_audit_log.called)
mock_audit_log.reset_mock()
self.course = CourseFactory.create(emit_signals=True)
([], CourseMode.DEFAULT_MODE_SLUG),
([CourseMode.VERIFIED, CourseMode.AUDIT], CourseMode.DEFAULT_MODE_SLUG),
for mode_slug in course_modes: CourseModeFactory.create( course_id=self.course.id, mode_slug=mode_slug, mode_display_name=mode_slug, )
self.assert_enrollment_status()
CourseModeFactory.create( course_id=self.course.id, mode_slug='professional', mode_display_name='Professional Education', )
resp = self.assert_enrollment_status(expected_status=status.HTTP_400_BAD_REQUEST)
self.client.logout()
self.assert_enrollment_status(expected_status=status.HTTP_401_UNAUTHORIZED)
self.client.logout()
self.user = UserFactory.create( username="inactive", email="inactive@example.com", password=self.PASSWORD, is_active=True )
self.client.login(username="inactive", password=self.PASSWORD)
self.user.is_active = False self.user.save()
self.assert_enrollment_status()
self.client.logout() response = self.client.get(url, **{'HTTP_X_EDX_API_KEY': self.API_KEY}) self.assertEqual(response.status_code, status.HTTP_200_OK)
__ = CourseOverview.get_from_id(course.id)
resp = self.client.get(reverse('courseenrollments')) self.assertEqual(resp.status_code, status.HTTP_200_OK)
CourseModeFactory.create( course_id=self.course.id, mode_slug='professional', mode_display_name='professional', )
self.assert_enrollment_status(as_server=True, mode='professional')
CourseModeFactory.create( course_id=self.course.id, mode_slug=CourseMode.HONOR, mode_display_name=CourseMode.HONOR, )
CourseModeFactory.create( course_id=self.course.id, mode_slug=CourseMode.VERIFIED, mode_display_name=CourseMode.VERIFIED, expiration_datetime='1970-01-01 05:00:00' )
self.assertEqual(len(v_data['course_modes']), 2)
self.assertEqual(len(h_data['course_modes']), 1) self.assertEqual(h_data['course_modes'][0]['slug'], CourseMode.HONOR)
for mode in [CourseMode.DEFAULT_MODE_SLUG, CourseMode.VERIFIED]: CourseModeFactory.create( course_id=self.course.id, mode_slug=mode, mode_display_name=mode, )
self.assert_enrollment_status(as_server=True)
self.assert_enrollment_status(as_server=True)
self.assert_enrollment_status(as_server=True)
for mode in [CourseMode.DEFAULT_MODE_SLUG, CourseMode.VERIFIED]: CourseModeFactory.create( course_id=self.course.id, mode_slug=mode, mode_display_name=mode, )
self.assert_enrollment_status(as_server=True, mode=CourseMode.VERIFIED)
for mode in configured_modes: CourseModeFactory.create( course_id=self.course.id, mode_slug=mode, mode_display_name=mode, )
self.assert_enrollment_status(as_server=True, mode=selected_mode)
self.assert_enrollment_status( as_server=True, mode=None, is_active='foo', expected_status=status.HTTP_400_BAD_REQUEST )
self.assert_enrollment_activation(False, selected_mode)
self.assert_enrollment_activation(False, selected_mode)
expected_status = ( status.HTTP_200_OK if CourseMode.DEFAULT_MODE_SLUG in configured_modes else status.HTTP_400_BAD_REQUEST ) self.assert_enrollment_status( as_server=True, is_active=False, expected_status=expected_status, )
self.assert_enrollment_status(as_server=True, mode=CourseMode.VERIFIED)
self.assert_enrollment_activation(False, CourseMode.VERIFIED)
for mode in [CourseMode.DEFAULT_MODE_SLUG, CourseMode.VERIFIED]: CourseModeFactory.create( course_id=self.course.id, mode_slug=mode, mode_display_name=mode, )
self.assert_enrollment_status()
response = self.assert_enrollment_status( as_server=True, mode=new_mode, is_active=new_is_active, expected_status=expected_status, )
self.assertEqual(is_active, new_is_active) self.assertEqual(course_mode, new_mode)
__ = CourseOverview.get_from_id(self.course.id)
self.assertEqual(response.status_code, 403)
resp_data = json.loads(response.content) user_message_url = get_absolute_url(user_message_path) self.assertEqual(resp_data['user_message_url'], user_message_url)
self.assertEqual(self._get_enrollments(), [])
with restrict_course(self.course.id) as redirect_path: self.assert_access_denied(redirect_path)
cache.clear()
self.user.profile.country = restricted_country.country self.user.profile.save()
unrestricted_country, __ = self._setup_embargo()
self.user.profile.country = unrestricted_country.country self.user.profile.save() self.assert_enrollment_status()
self.assertEqual(len(self._get_enrollments()), 1)
self.assertEqual(resp.status_code, 200)
valid_assocs = [a for a in associations if a.getExpiresIn() > 0] if valid_assocs: valid_assocs.sort(lambda a: a.getExpiresIn(), reverse=True) assoc = valid_assocs.sort[0]
return 0
return 0
response = None
response = external_auth.views.redirect_with_get('root', request.GET)
response = redirect(reverse('cas-login'))
response = external_auth.views.redirect_with_get('root', request.GET)
request.session['ExternalAuthMap'] = eamap
username = re.sub(r'\s', '', _flatten_to_ascii(eamap.external_name), flags=re.UNICODE)
context['ask_for_fullname'] = eamap.external_name.strip() == ''
try: validate_email(eamap.external_email) context['ask_for_email'] = False except ValidationError: context['ask_for_email'] = True
cert = request._req.subprocess_env.get(certkey, '')
if not settings.FEATURES['AUTH_USE_CERTIFICATES']: return HttpResponseForbidden()
return student.views.index(request)
return redirect_with_get('signin_user', request.GET)
if ( settings.FEATURES.get('AUTH_USE_SHIB') and course.enrollment_domain and course.enrollment_domain.startswith(SHIBBOLETH_DOMAIN_PREFIX) ): return redirect_with_get('shib-login', request.GET)
return redirect_with_get('signin_user', request.GET)
return redirect_with_get('register_user', request.GET)
if ( settings.FEATURES.get('AUTH_USE_SHIB') and course.enrollment_domain and course.enrollment_domain.startswith(SHIBBOLETH_DOMAIN_PREFIX) ): return redirect_with_get('shib-login', request.GET)
return redirect_with_get('register_user', request.GET)
sreg_response = sreg.SRegResponse.extractResponse(sreg_request, sreg_data) sreg_response.toMessage(response.fields)
pass
ax_response.toMessage(response.fields)
add_openid_simple_registration(request, response, data) add_openid_attribute_exchange(request, response, data)
webresponse = server.encodeResponse(response) http_response = HttpResponse(webresponse.body) http_response.status_code = webresponse.code
for k, v in webresponse.headers.iteritems(): http_response[k] = v
return True
if (not hasattr(openid_request, 'trust_root') or not openid_request.trust_root): log.error('no trust_root') return False
trust_root = TrustRoot.parse(openid_request.trust_root) if not trust_root: log.error('invalid trust_root') return False
if (not hasattr(openid_request, 'return_to') or not openid_request.return_to): log.error('empty return_to') return False
if not trust_root.validateURL(openid_request.return_to): log.error('invalid return_to') return False
if not any(r for r in trusted_roots if fnmatch.fnmatch(trust_root, r)): log.error('non-trusted root') return False
endpoint = get_xrds_url('login', request) if not endpoint: return default_render_failure(request, "Invalid OpenID request")
store = DjangoOpenIDStore() server = Server(store, endpoint)
if not validate_trust_root(openid_request): return default_render_failure(request, "Invalid OpenID trust root")
if openid_request.mode == 'checkid_immediate': return provider_respond(server, openid_request, openid_request.answer(False), {})
if 'openid_error' in request.session: error = True del request.session['openid_error']
else: return provider_respond(server, openid_request, server.handleRequest(openid_request), {})
if not validate_trust_root(openid_request): return default_render_failure(request, "Invalid OpenID trust root")
if user is not None and user.is_active: if 'openid_error' in request.session: del request.session['openid_error']
url = endpoint + urlquote(user.username) response = openid_request.answer(True, None, url)
results = { 'nickname': user.username, 'email': user.email, 'fullname': user.profile.name, }
return provider_respond(server, openid_request, response, results)
response = render_to_response('provider_login.html', { 'error': error, 'return_to': return_to })
response['X-XRDS-Location'] = get_xrds_url('xrds', request) return response
response['X-XRDS-Location'] = get_xrds_url('identity', request) return response
response['X-XRDS-Location'] = get_xrds_url('xrds', request) return response
from __future__ import unicode_literals
self.assertIn(SESSION_KEY, self.client.session)
response = self.client.get(reverse('signup'), follow=True) self.assertEqual(response.status_code, 404)
response = self.client.get(reverse('signin_user')) self.assertEqual(200, response.status_code) self.assertTrue('login-and-registration-container' in response.content)
dec_mock(request) self.assertTrue(self.mock.called) self.assertEqual(0, len(ExternalAuthMap.objects.all()))
self.mock.reset_mock() request = self._create_ssl_request(self.MOCK_URL) request.user = UserFactory() dec_mock(request) self.assertTrue(self.mock.called)
self.assertIn(SESSION_KEY, self.client.session)
IDP = 'https://idp.stanford.edu/' REMOTE_USER = 'test_user@stanford.edu'
self.assertEquals(len(audit_log_calls), 0)
self.assertEquals(len(audit_log_calls), 0)
response = client.get(path='/shib-login/', data={}, follow=False, **identity)
client = DjangoTestClient() response1 = client.get(path='/shib-login/', data={}, follow=False, **identity) postvars = {'email': u'post_email@stanford.edu',
request2 = self.request_factory.post('/create_account', data=postvars) request2.session = client.session request2.user = AnonymousUser()
if mail: self.assertEqual(user.email, mail) self.assertEqual(list(User.objects.filter(email=postvars['email'])), [])
with self.store.branch_setting(ModuleStoreEnum.Branch.draft_preferred, course.id): course.enrollment_domain = domain self.store.update_item(course, self.test_user_id)
for course in [shib_course, open_enroll_course]: for student in [shib_student, other_ext_student, int_student]: request = self.request_factory.post('/change_enrollment')
data = parse_qs(body) response = self.client.post(url, data)
data = {} if headers and 'Accept' in headers: data['CONTENT_TYPE'] = headers['Accept'] response = self.client.get(url, data)
provider_url = reverse('openid-provider-xrds') factory = RequestFactory() request = factory.request() abs_provider_url = request.build_absolute_uri(location=provider_url)
with self.settings(OPENID_SSO_SERVER_URL=abs_provider_url):
provider_url = reverse('openid-provider-login') factory = RequestFactory() request = factory.request() abs_provider_url = request.build_absolute_uri(location=provider_url)
for key in kwargs: args["openid." + key] = kwargs[key]
for _ in xrange(30): self._send_bad_redirection_login()
self.assertEquals(response.status_code, 302) cache.clear()
return self.client.post(url, post_args)
return self.client.post(url, post_args)
user.profile.name = u'Jan ĄĘ'
self.attempt_login(200) user.is_active = False
self.client.post(url, post_args)
provider_url = reverse('openid-provider-xrds') factory = RequestFactory() request = factory.request() abs_provider_url = request.build_absolute_uri(location=provider_url)
try: value = result.get(timeout=4.0) success = True except TimeoutError: value = None success = False
response = self.client.get(self.ping_url)
self.assertEqual(response.status_code, 200)
result_dict = json.loads(response.content)
self.assertTrue(result_dict['success'])
self.assertEqual(result_dict['value'], "pong")
self.assertIsInstance(result_dict['task_id'], unicode) self.assertIsInstance(result_dict['time'], float) self.assertTrue(result_dict['time'] > 0.0)
import signals import exceptions
options = {'statsd': True}
dog_stats_api.start(**options)
full_url = "http://{site_name}".format(site_name=settings.SITE_NAME) parsed_url = urlparse(full_url)
return RequestFactory( SERVER_NAME=parsed_url.hostname, SERVER_PORT=parsed_url.port or 80, ).get("/")
course_id = CourseKeyField(max_length=255, db_index=True, unique=True)
embargoed = models.BooleanField(default=False)
return u"Course '{}' is {}Embargoed".format(self.course_id.to_deprecated_string(), not_em)
embargoed_countries = models.TextField( blank=True, help_text="A comma-separated list of country codes that fall under U.S. embargo restrictions" )
return ( cls.is_restricted_course(unicode(course_id)) and cls._get_restricted_courses_from_cache().get(unicode(course_id))["disable_access_check"] )
cache_key = cls.MESSAGE_URL_CACHE_KEY.format( access_point=access_point, course_key=course_key ) url = cache.get(cache_key)
if url is None: url = cls._get_message_url_path_from_db(course_key, access_point) cache.set(cache_key, url)
if not cls.is_restricted_course(course_key): return default_path
if country not in cls.ALL_COUNTRIES: return True
rules_for_course = CountryAccessRule.objects.select_related('country').filter( restricted_course__course_key=course_id )
if not whitelist_countries: whitelist_countries = cls.ALL_COUNTRIES
return list(whitelist_countries - blacklist_countries)
("restricted_course", "country")
RestrictedCourse.invalidate_cache_for_course(instance.course_key) CountryAccessRule.invalidate_cache_for_course(instance.course_key)
pass
CountryAccessRule.invalidate_cache_for_course(restricted_course.course_key)
post_save.connect(invalidate_country_rule_cache, sender=CountryAccessRule) post_save.connect(invalidate_country_rule_cache, sender=RestrictedCourse) post_delete.connect(invalidate_country_rule_cache, sender=CountryAccessRule) post_delete.connect(invalidate_country_rule_cache, sender=RestrictedCourse)
if settings.FEATURES.get('USE_CUSTOM_THEME') and message_key in messages.CUSTOM_THEME_OVERRIDES: message_dict = messages.CUSTOM_THEME_OVERRIDES
elif access_point == self.ENROLLMENT_ACCESS_POINT: message_dict = messages.ENROLL_MESSAGES elif access_point == self.COURSEWARE_ACCESS_POINT: message_dict = messages.COURSEWARE_MESSAGES
ipaddr.IPNetwork(address)
cache.clear()
CountryAccessRule.objects.all().delete()
country, __ = Country.objects.get_or_create(country='IR')
restricted_course, __ = RestrictedCourse.objects.get_or_create(course_key=course_key) restricted_course.enroll_msg_key = 'default' restricted_course.access_msg_key = 'default' restricted_course.disable_access_check = disable_access_check restricted_course.save()
CountryAccessRule.objects.get_or_create( restricted_course=restricted_course, country=country, rule_type='blacklist' )
mock_ip.return_value = 'IR'
redirect_url = reverse( 'embargo_blocked_message', kwargs={ 'access_point': access_point, 'message_key': 'default' } ) yield redirect_url
from __future__ import unicode_literals
from __future__ import unicode_literals
if not settings.FEATURES.get('EMBARGO'): return True
if user is not None and has_course_author_access(user, course_key): return True
user_country_from_ip = _country_code_from_ip(ip_address)
user_country_from_profile = _get_user_country_from_profile(user)
'description',
'template',
CUSTOM_THEME_OVERRIDES = { 'embargo': BlockedMessage( description='Embargo', template='static_templates/theme-embargo.html' ) }
cache.clear()
for whitelist_country in whitelist: CountryAccessRule.objects.create( rule_type=CountryAccessRule.WHITELIST_RULE, restricted_course=self.restricted_course, country=Country.objects.get(country=whitelist_country) )
if profile_country is not None: self.user.profile.country = profile_country self.user.profile.save()
with self._mock_geoip(ip_country): result = embargo_api.check_course_access(self.course.id, user=self.user, ip_address='0.0.0.0')
self.assertEqual(result, allow_access)
result = embargo_api.check_course_access(self.course.id, ip_address='0.0.0.0') self.assertTrue(result)
result = embargo_api.check_course_access(self.course.id, ip_address='0.0.0.0') self.assertFalse(result)
unrestricted_course = CourseFactory.create() with self.assertNumQueries(1): embargo_api.check_course_access(unrestricted_course.id, user=self.user, ip_address='0.0.0.0')
with self.assertNumQueries(0): embargo_api.check_course_access(unrestricted_course.id, user=self.user, ip_address='0.0.0.0')
result = embargo_api.check_course_access(self.course.id, user=self.user, ip_address='FE80::0202:B3FF:FE1E:8329') self.assertTrue(result)
result = embargo_api.check_course_access(self.course.id, user=self.user, ip_address='0.0.0.0') self.assertTrue(result)
with self.assertNumQueries(3): embargo_api.check_course_access(self.course.id, user=self.user, ip_address='0.0.0.0')
CountryAccessRule.objects.create( rule_type=CountryAccessRule.BLACKLIST_RULE, restricted_course=self.restricted_course, country=Country.objects.get(country='US') )
with self._mock_geoip('US'): result = embargo_api.check_course_access(self.course.id, user=self.user, ip_address='0.0.0.0')
self.assertFalse(result, msg="User should not have access because the user isn't staff.")
staff_role.add_users(self.user)
with self._mock_geoip('US'): result = embargo_api.check_course_access(self.course.id, user=self.user, ip_address='0.0.0.0')
url_path = embargo_api.message_url_path(self.course.id, access_point) self.assertEqual(url_path, expected_url_path)
with self.assertNumQueries(2): embargo_api.message_url_path(self.course.id, "enrollment")
with self.assertNumQueries(0): embargo_api.message_url_path(self.course.id, "enrollment")
url_path = embargo_api.message_url_path(self.course.id, access_point)
self.assertEqual(url_path, '/embargo/blocked-message/courseware/default/')
self._restrict_course(self.course.id) embargo_api.message_url_path(self.course.id, 'courseware')
RestrictedCourse.objects.get(course_key=self.course.id).delete()
message_cache_key = ( 'embargo.message_url_path.courseware.{course_key}' ).format(course_key=self.course.id) cache.delete(message_cache_key)
url_path = embargo_api.message_url_path(self.course.id, 'courseware') self.assertEqual(url_path, '/embargo/blocked-message/courseware/default/')
from config_models.models import cache from embargo.models import IPFilter from embargo.forms import RestrictedCourseForm, IPFilterForm
form = RestrictedCourseForm(data={'course_key': 'not/valid'}) self._assert_course_field_error(form)
self.assertFalse(form.is_valid())
cache.clear()
self._load_page(access_point, 'default')
self.assertFalse(EmbargoedCourse.is_embargoed(course_id))
cauth = EmbargoedCourse(course_id=course_id, embargoed=True) cauth.save()
self.assertTrue(EmbargoedCourse.is_embargoed(course_id)) self.assertEquals( unicode(cauth), u"Course '{course_id}' is Embargoed".format(course_id=course_id) )
good_states = ['AZ', 'FR'] blocked_states = ['US', 'AQ'] currently_blocked = EmbargoedState.current().embargoed_countries_list
cauth = EmbargoedState(embargoed_countries='US, AQ') cauth.save() currently_blocked = EmbargoedState.current().embargoed_countries_list
blocked_states.append('IM') cauth.embargoed_countries = 'US, AQ, IM' cauth.save() currently_blocked = EmbargoedState.current().embargoed_countries_list
with self.assertNumQueries(1): RestrictedCourse.is_restricted_course(course_id) RestrictedCourse.is_disabled_access_check(course_id)
with self.assertNumQueries(0): RestrictedCourse.is_restricted_course(course_id) RestrictedCourse.is_disabled_access_check(course_id)
with self.assertNumQueries(0): RestrictedCourse.is_restricted_course(new_course_id) RestrictedCourse.is_disabled_access_check(new_course_id)
abc = RestrictedCourse.objects.get(course_key=new_course_id) abc.delete() with self.assertNumQueries(1): RestrictedCourse.is_restricted_course(new_course_id)
with self.assertNumQueries(0): RestrictedCourse.is_restricted_course(new_course_id)
with self.assertNumQueries(1): CountryAccessRule.check_country_access(course_id, 'NZ')
course.delete() with self.assertNumQueries(1): CountryAccessRule.check_country_access(course_id, 'NZ')
us_rule.delete() self._assert_history([('AU', 'blacklist')])
au_rule.delete() self._assert_history([])
CountryAccessRule.objects.create( restricted_course=self.restricted_course, country=self.countries['US'], rule_type=CountryAccessRule.WHITELIST_RULE )
self.restricted_course.delete() self._assert_history_deleted()
self.restricted_course.enroll_msg_key = 'embargo' self.restricted_course.access_msg_key = 'embargo' self.restricted_course.save()
self._assert_history([], enroll_msg='embargo', access_msg='embargo')
self.assertEqual(record.course_key, self.course_key)
snapshot = json.loads(record.snapshot) self.assertEqual(snapshot['enroll_msg'], enroll_msg) self.assertEqual(snapshot['access_msg'], access_msg)
for (country, rule_type) in country_rules: self.assertIn( { 'country': country, 'rule_type': rule_type }, snapshot['country_rules'] )
self.assertEqual(len(snapshot['country_rules']), len(country_rules))
django_cache.clear() config_cache.clear()
RestrictedCourse.objects.create(course_key=self.course.id)
response = self.client.get(self.courseware_url) self.assertEqual(response.status_code, 200)
self.client.logout()
IPFilter.objects.create( blacklist=", ".join(blacklist), whitelist=", ".join(whitelist), enabled=is_enabled )
response = self.client.get( "/", HTTP_X_FORWARDED_FOR=request_ip, REMOTE_ADDR=request_ip )
IPFilter.objects.create( blacklist="192.168.10.20", enabled=True )
IPFilter.objects.create( whitelist="192.168.10.20", enabled=True )
self.assertEqual(response.status_code, 200)
self.user.is_staff = True
ip_address = "192.168.10.20" IPFilter.objects.create( blacklist=ip_address, enabled=True )
with restrict_course(self.course.id): response = self.client.get( url, HTTP_X_FORWARDED_FOR=ip_address, REMOTE_ADDR=ip_address ) self.assertEqual(response.status_code, 200)
re.compile(r'^/embargo/blocked-message/'),
re.compile(r'^/admin/'),
re.compile(r'^/api/course_structure/v[\d+]/courses/{}/$'.format(settings.COURSE_ID_PATTERN)),
if not settings.FEATURES.get('EMBARGO'): raise MiddlewareNotUsed()
for pattern in self.ALLOW_URL_PATTERNS: if pattern.match(request.path) is not None: return None
ip_blacklist_url = reverse( 'embargo_blocked_message', kwargs={ 'access_point': 'courseware', 'message_key': 'embargo' } ) return redirect(ip_blacklist_url)
return None
return self.country_access_rules(request.user, ip_address, request.path)
created_time = models.DateTimeField(auto_now_add=True)
updated_time = models.DateTimeField(auto_now=True)
course_key = CourseKeyField(max_length=255, db_index=True)
action = models.CharField(max_length=100, db_index=True)
state = models.CharField(max_length=50)
objects = CourseActionStateManager()
MAX_MESSAGE_LENGTH = 1000
should_display = models.BooleanField(default=False)
message = models.CharField(max_length=MAX_MESSAGE_LENGTH)
source_course_key = CourseKeyField(max_length=255, db_index=True)
display_name = models.CharField(max_length=255, default="", blank=True)
objects = CourseRerunUIStateManager()
from __future__ import unicode_literals
self.initiate_rerun()
CourseRerunState.objects.succeeded(course_key=self.course_key) self.expected_rerun_state.update({ 'state': CourseRerunUIStateManager.State.SUCCEEDED, }) rerun = self.verify_rerun_state()
self.dismiss_ui_and_verify(rerun)
self.initiate_rerun()
exception = Exception("failure in rerunning") try: raise exception except: CourseRerunState.objects.failed(course_key=self.course_key)
self.dismiss_ui_and_verify(rerun)
COURSE_ACTION_STATES = (CourseRerunState, )
for CourseState in self.course_actions_displayable_states + self.courses_with_state3_non_displayable: action_class.objects.update_state( CourseState.course_key, CourseState.state, should_display=CourseState.should_display, allow_not_found=True )
if user: state_object.updated_user = user
if kwargs: for key, value in kwargs.iteritems(): setattr(state_object, key, value)
actual_url = staticfiles_storage.url(path_overrides[module])
self.assertEqual(map(str.strip, result.splitlines()), self.OVERRIDES_JS)
css_include = compressed_css('style-main-v1') self.assertIn(u'lms-main-v1.css', css_include)
css_include = compressed_css('style-main-v1', raw=True) self.assertIn(u'lms-main-v1.css?raw', css_include)
with self.settings(PIPELINE_ENABLED=True): js_include = compressed_js('base_application') self.assertIn(u'lms-base-application.js', js_include)
with self.settings(PIPELINE_ENABLED=False): js_include = compressed_js('base_application') self.assertIn(u'/static/js/src/logger.js', js_include)
"error": "invalid_client", "error_description": "{} is not a public client".format(client_id),
self.request.social_strategy.clean_partial_pipeline() raise OAuthValidationError( { "error": "invalid_grant", "error_description": "access_token is not valid", } )
self.request.backend = social_utils.load_backend(self.request.social_strategy, self.BACKEND, redirect_uri)
self.data = { "access_token": self.access_token, "client_id": self.client_id, }
assert self.match_social_auth(social_auth) return social_auth.uid
details = pipeline_kwargs.get('details')
)
return getattr(settings, 'SOCIAL_AUTH_OAUTH_SECRETS', {}).get(self.backend_name, '')
return social_auth.uid[len(self.idp_slug) + 1:]
return getattr(settings, 'SOCIAL_AUTH_SAML_SP_PUBLIC_CERT', '')
return getattr(settings, 'SOCIAL_AUTH_SAML_SP_PRIVATE_KEY', '')
icon_class = None icon_image = None secondary = False
accepts_logins = False
return social_auth.uid[len(self.lti_consumer_key) + 1:]
django_settings.FIELDS_STORED_IN_SESSION = _FIELDS_STORED_IN_SESSION
django_settings.MIDDLEWARE_CLASSES += _MIDDLEWARE_CLASSES
django_settings.SOCIAL_AUTH_LOGIN_ERROR_URL = '/'
django_settings.SOCIAL_AUTH_LOGIN_REDIRECT_URL = _SOCIAL_AUTH_LOGIN_REDIRECT_URL
django_settings.SOCIAL_AUTH_STRATEGY = 'third_party_auth.strategy.ConfigurationModelStrategy'
django_settings.SOCIAL_AUTH_PROTECTED_USER_FIELDS = ['email']
django_settings.SOCIAL_AUTH_RAISE_EXCEPTIONS = False
self.strategy.clean_partial_pipeline()
validated_lti_params = self.get_validated_lti_params(self.strategy)
self.strategy.session_setdefault('auth_entry', 'login')
from django.conf import settings
return redirect(request.GET.get('next', 'dashboard'))
return super(ConfigurationModelStrategy, self).setting(name, default, backend)
from __future__ import unicode_literals
from __future__ import unicode_literals
AUTH_ENTRY_LOGIN = 'login' AUTH_ENTRY_REGISTER = 'register' AUTH_ENTRY_ACCOUNT_SETTINGS = 'account_settings'
AUTH_ENTRY_LOGIN_API = 'login_api' AUTH_ENTRY_REGISTER_API = 'register_api'
if should_force_account_creation(): return dispatch_to_register() return dispatch_to_login()
return dispatch_to_register()
return redirect_to_custom_form(strategy.request, auth_entry, kwargs)
return association_response
OAuth2AuthenticationAllowInactiveUser, SessionAuthenticationAllowInactiveUser,
if not request.user.is_superuser and not ApiKeyHeaderPermission().has_permission(request, self): return Response(status=status.HTTP_403_FORBIDDEN)
self.provider = Registry.get(provider_id) if not self.provider: raise Http404
uid = self.provider.get_social_auth_uid('uid') if uid is not 'uid': query_set = query_set.filter(uid__startswith=uid[:-3])
return False
LINKED_USERS = (ALICE_USERNAME, STAFF_USERNAME, ADMIN_USERNAME) PASSWORD = "edx"
"remote_id": 'remote_' + username,
user = UserFactory.create(is_staff=True, is_superuser=True) user.save() self.client.login(username=user.username, password='test')
providers = OAuth2ProviderConfig.objects.all() pcount = len(providers)
provider1 = self.configure_dummy_provider( enabled=True, icon_class='', icon_image=SimpleUploadedFile('icon.svg', '<svg><rect width="50" height="100"/></svg>'), )
providers = OAuth2ProviderConfig.objects.all() self.assertEquals(len(providers), 1) self.assertEquals(providers[pcount].id, provider1.id)
post_data = models.model_to_dict(provider1) del post_data['icon_image']
post_data['name'] = 'Another name'
response = self.client.post(update_url, post_data) self.assertEquals(response.status_code, 302)
providers = OAuth2ProviderConfig.objects.all() self.assertEquals(len(providers), pcount + 2) self.assertEquals(providers[pcount].id, provider1.id) provider2 = providers[pcount + 1]
self.assertEquals(provider2.icon_image, provider1.icon_image) self.assertEquals(provider2.name, post_data['name'])
for char in pipeline.make_random_password(length=100000): self.assertIn(char, pipeline._PASSWORD_CHARSET)
self.assertEqual(google_provider.id, google_state.provider.id) self.assertEqual(self.user, google_state.user) self.assertEqual(user_social_auth_google.id, google_state.association_id)
google_provider = self.configure_google_provider(enabled=True) linkedin_provider = self.configure_linkedin_provider(enabled=True) self.assertEqual(len(provider.Registry.enabled()), 2)
self.assertEqual(google_provider.id, google_state.provider.id) self.assertEqual(self.user, google_state.user)
from third_party_auth.tasks import SAML_XML_NS XMLDSIG_XML_NS = 'http://www.w3.org/2000/09/xmldsig#'
('saml_key', 'MIICsDCCAhmgAw'), ('saml_key_alt', 'MIICWDCCAcGgAw'),
patcher = patch( 'social.backends.twitter.TwitterOAuth.unauthorized_token', create=True, return_value="unauth_token" ) patcher.start() self.addCleanup(patcher.stop)
user = User.objects.get(email=EMAIL) self.assertEqual(user.username, EDX_USER_ID)
httpretty.enable()
uid_patch = patch('onelogin.saml2.utils.OneLogin_Saml2_Utils.generate_unique_id', return_value='TESTID') uid_patch.start() self.addCleanup(uid_patch.stop)
self.assertTrue(provider_redirect_url.startswith(TESTSHIB_SSO_URL)) return self.client.post( self.complete_url, content_type='application/x-www-form-urlencoded', data=self.read_data_file('testshib_response.txt'), )
self.assertEqual(provider_redirect_url, self.url_prefix + self.complete_url) return self.client.get(provider_redirect_url)
PROVIDER_NAME = "override" PROVIDER_BACKEND = "override" PROVIDER_ID = "override" USER_EMAIL = "override" USER_NAME = "override" USER_USERNAME = "override"
self.client.logout() self._test_return_login(user_is_activated=False)
self.client.logout() self._test_return_login()
provider = None
self.assertFalse(payload.get('success')) self.assertIn('There was an error receiving your login information', payload.get('value'))
self.assertTrue(payload.get('success'))
self.assertFalse(payload.get('success')) self.assertIn('incorrect', payload.get('value'))
self.assertIn(self.provider.name, response.content)
self.assertEqual(auth_settings._SOCIAL_AUTH_LOGIN_REDIRECT_URL, response.get('Location'))
self.assertIn(self.provider.name, response.content)
self.client.get( pipeline.get_login_url(self.provider.provider_id, pipeline.AUTH_ENTRY_LOGIN))
self.assert_account_settings_context_looks_correct(account_settings_context(request), request.user, linked=False) self.assert_social_auth_does_not_exist_for_user(request.user, strategy)
self.assert_logged_in_cookie_redirect(actions.do_complete(
self.set_logged_in_cookies(request)
self.assert_redirect_to_dashboard_looks_correct(actions.do_complete(
self.assert_social_auth_exists_for_user(request.user, strategy) self.assert_account_settings_context_looks_correct(account_settings_context(request), request.user, linked=True)
self.set_logged_in_cookies(request)
self.client.get( pipeline.get_login_url(self.provider.provider_id, pipeline.AUTH_ENTRY_LOGIN))
self.assert_account_settings_context_looks_correct(account_settings_context(request), user, linked=True) self.assert_social_auth_exists_for_user(request.user, strategy)
self.assert_redirect_to_dashboard_looks_correct(actions.do_disconnect( request.backend, request.user, None, redirect_field_name=auth.REDIRECT_FIELD_NAME))
self.assert_account_settings_context_looks_correct(account_settings_context(request), user, linked=False) self.assert_social_auth_does_not_exist_for_user(user, strategy)
actions.do_complete(backend, social_views._do_login, user=unlinked_user)
self.assert_login_response_before_pipeline_looks_correct(self.client.get('/login'))
self.assert_redirect_to_provider_looks_correct(self.client.get( pipeline.get_login_url(self.provider.provider_id, pipeline.AUTH_ENTRY_LOGIN)))
self.assert_redirect_to_login_looks_correct(actions.do_complete(request.backend, social_views._do_login))
self.assert_login_response_in_pipeline_looks_correct(student_views.signin_user(strategy.request))
self.assert_json_success_response_looks_correct(student_views.login_user(strategy.request))
self.assert_logged_in_cookie_redirect(actions.do_complete(
self.set_logged_in_cookies(request)
request, strategy = self.get_request_and_strategy( auth_entry=pipeline.AUTH_ENTRY_REGISTER, redirect_uri='social:complete') strategy.request.backend.auth_complete = mock.MagicMock(return_value=self.fake_auth_complete(strategy))
self.assert_register_response_before_pipeline_looks_correct(self.client.get('/register'))
self.assert_redirect_to_provider_looks_correct(self.client.get( pipeline.get_login_url(self.provider.provider_id, pipeline.AUTH_ENTRY_LOGIN)))
self.assert_redirect_to_register_looks_correct(actions.do_complete(request.backend, social_views._do_login))
self.assert_register_response_in_pipeline_looks_correct( student_views.register_user(strategy.request), pipeline.get(request)['kwargs'])
with self.assertRaises(auth_models.User.DoesNotExist): self.get_user_by_email(strategy, email)
self.assert_social_auth_does_not_exist_for_user(created_user, strategy)
self.assert_logged_in_cookie_redirect(actions.do_complete(
self.assert_redirect_to_register_looks_correct(actions.do_complete(backend, social_views._do_login))
student_views.create_account(strategy.request) self.assert_json_failure_response_is_username_collision(student_views.create_account(strategy.request))
TOKEN_RESPONSE_DATA = None
USER_RESPONSE_DATA = None
response = self.client.get(complete_url) self.assertEqual(response.status_code, 302) self.assertEqual(response['Location'], 'http://example.none/misc/final-destination')
self.client.defaults['SERVER_NAME'] = 'example.none' self.url_prefix = 'http://example.none'
settings.apply_settings(self.settings) self.assertEqual([], provider.Registry.enabled())
settings.apply_settings(self.settings) self.assertFalse(self.settings.SOCIAL_AUTH_RAISE_EXCEPTIONS)
UID_FIELD = "id"
UID_FIELD = "email"
redirect_uri = super(ExceptionMiddleware, self).get_redirect_uri(request, exception)
auth_entry = request.session.get(pipeline.AUTH_ENTRY_KEY)
if auth_entry and auth_entry in pipeline.AUTH_DISPATCH_URLS: redirect_uri = pipeline.AUTH_DISPATCH_URLS[auth_entry]
url = path
if rest.endswith('?raw'): return original
base_url = AssetBaseUrlConfig.get_base_url() excluded_exts = AssetExcludedExtensionsConfig.get_excluded_extensions() url = StaticContent.get_canonicalized_asset_path(course_id, rest, base_url, excluded_exts)
else: course_path = "/".join((static_asset_path or data_directory, rest))
assert_equals('"/static/data_dir/file.png"', replace_static_urls(STATIC_SOURCE, DATA_DIRECTORY))
assert_equals( '"' + mock_static_content.get_canonicalized_asset_path.return_value + '"', replace_static_urls(STATIC_SOURCE, DATA_DIRECTORY, course_id=COURSE_KEY) )
unlock_content = cls.create_image(prefix, (32, 32), 'blue', '{}_unlock.png')
lock_content = cls.create_image(prefix, (32, 32), 'green', '{}_lock.png', locked=True)
contentstore().generate_thumbnail(unlock_content, dimensions=(16, 16)) contentstore().generate_thumbnail(lock_content, dimensions=(16, 16))
cls.create_image(prefix, (1, 1), 'red', 'special/{}_unlock.png')
cls.create_image(prefix, (1, 1), 'yellow', 'special/{}_lock.png', locked=True)
from __future__ import unicode_literals
from __future__ import unicode_literals
decorator `django.utils.decorators.decorator_from_middleware(middleware_class)`
log.debug(u"Referrer hostname is `None`, so it is not on the whitelist.")
from __future__ import unicode_literals
request = args[0] request.META['CROSS_DOMAIN_CSRF_COOKIE_USED'] = True
return ensure_csrf_cookie(func)(*args, **kwargs)
if not is_cross_domain_request_allowed(request): log.debug("Could not set cross-domain CSRF cookie.") return response
LANGUAGE_KEY = 'pref-lang'
Language = namedtuple('Language', 'code name')
released_languages = [ Language(tuple[0], tuple[1]) for tuple in settings.LANGUAGES if tuple[0] in released_language_codes ]
self.middleware.process_request(self.request) self.assertNotIn(LANGUAGE_SESSION_KEY, self.request.session)
set_user_preference(self.user, LANGUAGE_KEY, 'eo') self.middleware.process_request(self.request) self.assertEquals(self.request.session[LANGUAGE_SESSION_KEY], 'eo')
self.request.session[LANGUAGE_SESSION_KEY] = 'en' set_user_preference(self.user, LANGUAGE_KEY, 'eo') self.middleware.process_request(self.request)
for browser_lang in lang_headers: if browser_lang in system_released_languages: if request.session.get(LANGUAGE_SESSION_KEY, None) is None: request.session[LANGUAGE_SESSION_KEY] = unicode(browser_lang) break
domain = domain.split(':')[0] microsites = cls.objects.filter(site__domain__iexact=domain)
original = Microsite.objects.get(id=instance.id) _make_archive_copy(original)
history = HistoricalRecords()
history = HistoricalRecords()
microsite = Microsite.get_microsite_for_domain(domain)
try: microsite = Microsite.objects.get(key='default') except Microsite.DoesNotExist: pass
self._set_microsite_config_from_obj(microsite.site.domain, domain, microsite)
config = microsite.values return config.get(val_name, default)
return set(MicrositeOrganizationMapping.objects.all().values_list('organization', flat=True))
organizations = microsite_object.get_organizations()
if not organizations: raise Exception( 'Configuration error. Microsite {key} does not have any ORGs mapped to it!'.format( key=microsite_object.key ) )
config['course_org_filter'] = organizations[0] self.current_request_configuration.data = config
template_obj = MicrositeTemplate.get_template_for_microsite( microsite_get_value('site_domain'), uri )
if 'default' in settings.MICROSITE_CONFIGURATION: self._set_microsite_config('default', subdomain, domain) return
for value in settings.MICROSITE_CONFIGURATION.itervalues(): org_filter = value.get('course_org_filter', None) if org_filter == org: return value.get(val_name, default) return default
for microsite in settings.MICROSITE_CONFIGURATION.itervalues(): org_filter = microsite.get('course_org_filter') if org_filter: org_filter_set.add(org_filter)
from __future__ import unicode_literals
from __future__ import unicode_literals
self.assertEqual(microsite.get_backend(None, BaseMicrositeBackend), None)
with self.assertRaises(TypeError): microsite.get_backend('microsite_configuration.microsite.get_backend', BaseMicrositeBackend)
with self.assertRaises(ValueError): microsite.get_backend('microsite_configuration.microsite.invalid_method', BaseMicrositeBackend)
self.assertIsInstance( microsite.get_backend( 'microsite_configuration.backends.database.DatabaseMicrositeBackend', BaseMicrositeBackend ), DatabaseMicrositeBackend )
settings.DEFAULT_TEMPLATE_ENGINE['DIRS'] = [ path for path in settings.DEFAULT_TEMPLATE_ENGINE['DIRS'] if path != settings.MICROSITE_ROOT_DIR ]
settings.DEFAULT_TEMPLATE_ENGINE['DIRS'] = [ path for path in settings.DEFAULT_TEMPLATE_ENGINE['DIRS'] if path != settings.MICROSITE_ROOT_DIR ]
microsite.set_by_domain('unknown') self.assertIsNone(microsite.get_value('platform_name'))
Microsite.objects.all().delete() microsite.clear() microsite.set_by_domain('unknown') self.assertIsNone(microsite.get_value('platform_name'))
microsite.clear() with patch('django.conf.settings.MICROSITE_CONFIGURATION', False): self.assertEqual( microsite.get_all_orgs(), set() )
microsite.set_by_domain('unknown') self.assertEqual(microsite.get_value('university'), 'default_university')
if microsite.has_override_value('SESSION_COOKIE_DOMAIN'):
def _set_cookie_wrapper(key, value='', max_age=None, expires=None, path='/', domain=None, secure=None, httponly=False):
if key == settings.SESSION_COOKIE_NAME: domain = microsite.get_value('SESSION_COOKIE_DOMAIN', domain)
return response.set_cookie_wrapped_func( key, value, max_age=max_age, expires=expires, path=path, domain=domain, secure=secure, httponly=httponly )
response.set_cookie_wrapped_func = response.set_cookie response.set_cookie = _set_cookie_wrapper
return None
_strip_value(value, lookup)
if not self.blank and value is self.Empty: raise ValidationError(self.error_messages['blank']) else: return super(OpaqueKeyField, self).validate(value, model_instance)
from track.backends.django import TrackingLog
if values: engine = values['ENGINE'] options = values.get('OPTIONS', {}) backends[name] = _instantiate_backend_from_name(engine, options)
extra = kwargs.get('extra', {})
extra['w'] = extra.get('w', 0)
extra['tz_aware'] = extra.get('tz_aware', True)
msg = 'Error inserting to MongoDB event tracker backend' log.exception(msg)
self.assertEqual(str(results[0].time), '2013-01-01 17:01:00+00:00')
from __future__ import unicode_literals
context_fields_to_remove = set(CONTEXT_FIELDS_TO_INCLUDE) context_fields_to_remove.add('client_id') for field in context_fields_to_remove: if field in context: del context[field]
def __call__(self, event): context = event.get('context', {}) course_id = context.get('course_id')
shim.remove_shim_context(event)
full_event = dict(event, **task_info)
self.mock_tracker.reset_mock() try: views.server_track(request, str(sentinel.event_type), '{}')
self.mock_tracker.reset_mock() try: views.server_track(request, str(sentinel.event_type), '{}')
self.assert_no_events_emitted() try: response = segmentio.segmentio_event(request) self.assertEquals(response.status_code, 200)
del input_payload['current_time']
del expected_event['event']['currentTime']
full_segment_event = request.json
segment_properties = full_segment_event.get('properties', {})
segment_context = full_segment_event.get('context')
context = {}
segment_event_name = segment_properties['name'] disallowed_substring_names = [ a.lower() for a in getattr(settings, 'TRACKING_SEGMENTIO_DISALLOWED_SUBSTRING_NAMES', []) ]
context['client'] = dict(segment_context) context['agent'] = segment_context.get('userAgent', '')
for field in ('traits', 'integrations', 'userAgent'): if field in context['client']: del context['client'][field]
context.update(app_context)
super(TestTrackerInstantiation, self).setUp() self.get_backend = tracker._instantiate_backend_from_name
tracker._initialize_backends_from_django_settings()
self.assertEqual(context[context_key], 'test latin1 Ó é ñ'.decode('utf8'))
obj = UTC.localize(obj)
obj = obj.astimezone(UTC)
for prefix in sorted(self._prefix_registry, reverse=True): if key.startswith(prefix): return self._prefix_registry[prefix]
if self.name == "edx.video.seeked": self['name'] = "edx.video.position.changed"
'HTTP_REFERER': 'referer', 'HTTP_ACCEPT_LANGUAGE': 'accept_language',
if re.match(pattern, path): return False
context[context_key] = request.META.get(header_name, '').decode('latin1')
self.assertIsNotNone(get_template_request_context())
self.assertIsNone(get_template_request_context())
link_map = settings.MKTG_URL_LINK_MAP enable_mktg_site = microsite.get_value( 'ENABLE_MKTG_SITE', settings.FEATURES.get('ENABLE_MKTG_SITE', False) )
if name == 'ROOT': return settings.MKTG_URLS.get('ROOT') return settings.MKTG_URLS.get('ROOT') + settings.MKTG_URLS.get(name)
if link_map[name] is not None: return reverse(link_map[name])
template_name = microsite.get_template_path(template_name)
request_context = get_template_request_context() if request_context: for item in request_context: context_dictionary.update(item) for item in context_instance: context_dictionary.update(item) if context: context_dictionary.update(context)
KEY_CSRF_TOKENS = ('csrf_token', 'csrf') for key in KEY_CSRF_TOKENS: if key in context_dictionary: context_dictionary[key] = unicode(context_dictionary[key])
template = lookup_template(namespace, template_name) return template.render_unicode(**context_dictionary)
self._collection.clear() self._uri_cache.clear()
namespace_dirs = {namespace: list(look.directories) for namespace, look in LOOKUP.items()}
LOOKUP.clear()
for namespace, directories in namespace_dirs.items(): for directory in directories: add_lookup(namespace, directory)
self.base_loader = base_loader
template = Template(filename=file_path, module_directory=self.module_directory, input_encoding='utf-8', output_encoding='utf-8', default_filters=['decode.utf8'], encoding_errors='replace', uri=template_name) return template, None
return self.base_loader.load_template_source(template_name, template_dirs)
context_dictionary = {}
for processor in get_template_context_processors(): context.update(processor(request))
assign_default_role(instance.course_id, instance.user)
db_table = 'django_comment_client_role'
return self.name + " for " + (self.course_id.to_deprecated_string() if self.course_id else "all courses")
db_table = 'django_comment_client_permission'
from django.test import TestCase
self.staff_user = User.objects.create_user( "patty", "patty@fake.edx.org", ) self.staff_user.is_staff = True
from __future__ import unicode_literals
community_ta_role.inherit_permissions(moderator_role)
DATABASES = { 'default': { 'ENGINE': 'django.db.backends.sqlite3', 'ATOMIC_REQUESTS': True, },
STATICFILES_STORAGE = 'openedx.core.lib.django_require.staticstorage.OptimizedCachedRequireJsStorage'
TEST_ROOT = REPO_ROOT / "test_root" LOG_DIR = (TEST_ROOT / "log").abspath()
STATIC_ROOT = (TEST_ROOT / "staticfiles" / "cms").abspath()
os.environ['REQUIRE_BUILD_PROFILE_OPTIMIZE'] = 'none'
SERVICE_VARIANT = os.environ.get('SERVICE_VARIANT', None)
CONFIG_ROOT = path(os.environ.get('CONFIG_ROOT', ENV_ROOT))
CONFIG_PREFIX = SERVICE_VARIANT + "." if SERVICE_VARIANT else ""
BROKER_POOL_LIMIT = 0 BROKER_CONNECTION_TIMEOUT = 1
CELERY_RESULT_BACKEND = 'djcelery.backends.cache:CacheBackend'
BROKER_HEARTBEAT = 10.0 BROKER_HEARTBEAT_CHECKRATE = 2
CELERYD_PREFETCH_MULTIPLIER = 1
with open(CONFIG_ROOT / CONFIG_PREFIX + "env.json") as env_file: ENV_TOKENS = json.load(env_file)
DEFAULT_COURSE_ABOUT_IMAGE_URL = ENV_TOKENS.get('DEFAULT_COURSE_ABOUT_IMAGE_URL', DEFAULT_COURSE_ABOUT_IMAGE_URL)
GITHUB_REPO_ROOT = ENV_TOKENS.get('GITHUB_REPO_ROOT', GITHUB_REPO_ROOT)
SOCIAL_SHARING_SETTINGS = ENV_TOKENS.get('SOCIAL_SHARING_SETTINGS', SOCIAL_SHARING_SETTINGS)
EDXMKTG_LOGGED_IN_COOKIE_NAME = ENV_TOKENS.get('EDXMKTG_LOGGED_IN_COOKIE_NAME', EDXMKTG_LOGGED_IN_COOKIE_NAME) EDXMKTG_USER_INFO_COOKIE_NAME = ENV_TOKENS.get('EDXMKTG_USER_INFO_COOKIE_NAME', EDXMKTG_USER_INFO_COOKIE_NAME)
CSRF_COOKIE_SECURE = ENV_TOKENS.get('CSRF_COOKIE_SECURE', False)
THEME_NAME = ENV_TOKENS.get('THEME_NAME', None) COMPREHENSIVE_THEME_DIR = path(ENV_TOKENS.get('COMPREHENSIVE_THEME_DIR', COMPREHENSIVE_THEME_DIR))
GIT_REPO_EXPORT_DIR = ENV_TOKENS.get('GIT_REPO_EXPORT_DIR', '/edx/var/edxapp/export_course_repos')
LANGUAGES = ENV_TOKENS.get('LANGUAGES', LANGUAGES) LANGUAGE_CODE = ENV_TOKENS.get('LANGUAGE_CODE', LANGUAGE_CODE) USE_I18N = ENV_TOKENS.get('USE_I18N', USE_I18N)
for app in ENV_TOKENS.get('ADDL_INSTALLED_APPS', []): INSTALLED_APPS += (app,)
if "TRACKING_IGNORE_URL_PATTERNS" in ENV_TOKENS: TRACKING_IGNORE_URL_PATTERNS = ENV_TOKENS.get("TRACKING_IGNORE_URL_PATTERNS")
with open(CONFIG_ROOT / CONFIG_PREFIX + "auth.json") as auth_file: AUTH_TOKENS = json.load(auth_file)
CMS_SEGMENT_KEY = AUTH_TOKENS.get('SEGMENT_KEY')
AWS_QUERYSTRING_AUTH = AUTH_TOKENS.get('AWS_QUERYSTRING_AUTH', True)
DATADOG = AUTH_TOKENS.get("DATADOG", {}) DATADOG.update(ENV_TOKENS.get("DATADOG", {}))
VIDEO_CDN_URL = ENV_TOKENS.get('VIDEO_CDN_URL', {})
SEARCH_ENGINE = "search.elastic.ElasticSearchEngine"
OAUTH_OIDC_ISSUER = ENV_TOKENS['OAUTH_OIDC_ISSUER']
PARTNER_SUPPORT_EMAIL = ENV_TOKENS.get('PARTNER_SUPPORT_EMAIL', PARTNER_SUPPORT_EMAIL)
AFFILIATE_COOKIE_NAME = ENV_TOKENS.get('AFFILIATE_COOKIE_NAME', AFFILIATE_COOKIE_NAME)
REST_FRAMEWORK,
SECRET_KEY = 'dev key'
'ENABLE_DISCUSSION_SERVICE': True, 'ENABLE_TEXTBOOK': True, 'ENABLE_STUDENT_NOTES': True,
'STUDIO_REQUEST_EMAIL': '',
'CMS_SEGMENT_KEY': None,
'ENABLE_SERVICE_STATUS': False,
'AUTOPLAY_VIDEOS': False,
'ENABLE_CREATOR_GROUP': False,
'ENFORCE_PASSWORD_POLICY': False,
'ENABLE_MAX_FAILED_LOGIN_ATTEMPTS': False,
'EDITABLE_SHORT_DESCRIPTION': True,
'SQUELCH_PII_IN_LOGS': False,
'EMBARGO': False,
'USE_MICROSITES': False,
'ALLOW_UNICODE_COURSE_ID': False,
'PREVENT_CONCURRENT_LOGINS': False,
'ADVANCED_SECURITY': False,
'ENABLE_VIDEO_UPLOAD_PIPELINE': False,
'ENABLE_EDXNOTES': False,
'ENABLE_CONTENT_LIBRARIES': True,
'MILESTONES_APP': False,
'ENABLE_PREREQUISITE_COURSES': False,
'ENTRANCE_EXAMS': False,
'LICENSING': False,
'ENABLE_COURSEWARE_INDEX': False,
'ENABLE_LIBRARY_INDEX': False,
'ALLOW_COURSE_RERUNS': True,
'CERTIFICATES_HTML_VIEW': False,
'ENABLE_TEAMS': True,
'ENABLE_VIDEO_BUMPER': False,
'SHOW_BUMPER_PERIODICITY': 7 * 24 * 3600,
'ENABLE_CREDIT_ELIGIBILITY': ENABLE_CREDIT_ELIGIBILITY,
'ALLOW_HIDING_DISCUSSION_TAB': False,
'ENABLE_SPECIAL_EXAMS': False,
'SHOW_LANGUAGE_SELECTOR': False,
'CUSTOM_COURSE_URLS': False
GEOIP_PATH = REPO_ROOT / "common/static/data/geoip/GeoIP.dat" GEOIPV6_PATH = REPO_ROOT / "common/static/data/geoip/GeoIPv6.dat"
'debug': False
AUTHENTICATION_BACKENDS = ( 'ratelimitbackend.backends.RateLimitModelBackend', )
from lms.envs.common import ( COURSE_KEY_PATTERN, COURSE_ID_PATTERN, USAGE_KEY_PATTERN, ASSET_KEY_PATTERN )
CSRF_COOKIE_AGE = 60 * 60 * 24 * 7 * 52 CSRF_COOKIE_SECURE = False
simplefilter('ignore')
'openedx.core.djangoapps.safe_sessions.middleware.SafeSessionMiddleware',
'cache_toolbox.middleware.CacheBackedAuthenticationMiddleware', 'django.contrib.auth.middleware.SessionAuthenticationMiddleware',
'lang_pref.middleware.LanguagePreferenceMiddleware',
'dark_lang.middleware.DarkLangMiddleware',
'django.middleware.locale.LocaleMiddleware',
'edxmako.middleware.MakoMiddleware',
'ratelimitbackend.middleware.RateLimitMiddleware',
'session_inactivity_timeout.middleware.SessionInactivityTimeout',
'django.middleware.clickjacking.XFrameOptionsMiddleware',
X_FRAME_OPTIONS = 'ALLOW'
P3P_HEADER = 'CP="Open EdX does not have a P3P policy."'
from xmodule.modulestore.inheritance import InheritanceMixin from xmodule.modulestore import prefer_xmodules from xmodule.x_module import XModuleMixin
XBLOCK_MIXINS = ( LmsBlockMixin, InheritanceMixin, XModuleMixin, EditInfoMixin, AuthoringMixin, )
XBLOCK_FIELD_DATA_WRAPPERS = ()
MODULESTORE_FIELD_OVERRIDE_PROVIDERS = ()
'python_bin': None, 'user': 'sandbox',
'limits': { 'CPU': 1, },
DEBUG = False SESSION_COOKIE_SECURE = False SESSION_SAVE_EVERY_REQUEST = False SESSION_SERIALIZER = 'django.contrib.sessions.serializers.PickleSerializer'
SITE_ID = 1 SITE_NAME = "localhost:8001" HTTPS = 'on' ROOT_URLCONF = 'cms.urls'
EDX_PLATFORM_REVISION = dealer.git.Backend(path=REPO_ROOT).revision
EDX_PLATFORM_REVISION = 'unknown'
STATIC_URL = '/static/' + EDX_PLATFORM_REVISION + "/" STATIC_ROOT = ENV_ROOT / "staticfiles" / EDX_PLATFORM_REVISION
]
MESSAGE_STORAGE = 'django.contrib.messages.storage.session.SessionStorage'
PIPELINE_CSS_COMPRESSOR = None PIPELINE_JS_COMPRESSOR = None
"spec", "spec_helpers",
"xmodule_js", "common_static",
REQUIRE_BASE_URL = "./"
REQUIRE_JS = "js/vendor/requiresjs/require.js"
REQUIRE_STANDALONE_MODULES = {}
REQUIRE_DEBUG = False
REQUIRE_EXCLUDE = ("build.txt",)
'API': 'https://www.youtube.com/iframe_api',
'METADATA_URL': 'https://www.googleapis.com/youtube/v3/videos',
'openedx.core.djangoapps.common_views',
'simple_history',
'config_models',
'service_status',
'django_nose',
'contentstore', 'contentserver', 'course_creators', 'external_auth',
'track', 'eventtracking.django.apps.EventTrackingConfig',
'datadog',
'edxmako', 'pipeline', 'static_replace', 'require',
'openedx.core.djangoapps.theming',
'openedx.core.djangoapps.site_configuration',
'django_comment_common',
'django.contrib.admin',
'course_modes',
'dark_lang',
'reverification',
'openedx.core.djangoapps.user_api', 'django_openid_auth',
'monitoring',
'course_action_state',
'openedx.core.djangoapps.credit',
'edx_proctoring',
'openedx.core.djangoapps.bookmarks',
'openedx.core.djangoapps.programs',
'openedx.core.djangoapps.self_paced',
'provider', 'provider.oauth2', 'edx_oauth2_provider',
'oauth2_provider',
'lms.djangoapps.verify_student',
'microsite_configuration',
'milestones',
'statici18n',
'cms.lib.xblock.tagging',
TRACKING_IGNORE_URL_PATTERNS = [r'^/event', r'^/login', r'^/heartbeat']
'submissions', 'openassessment', 'openassessment.assessment', 'openassessment.fileupload', 'openassessment.workflow', 'openassessment.xblock',
'edxval',
'organizations',
try: imp.find_module(app_name) except ImportError: try: __import__(app_name) except ImportError: continue INSTALLED_APPS += (app_name,)
ADVANCED_SECURITY_CONFIG = {}
MAX_ASSET_UPLOAD_FILE_SIZE_URL = ""
ADVANCED_PROBLEM_TYPES = [ { 'component': 'openassessment', 'boilerplate_name': None, }, ]
SEARCH_ENGINE = None ELASTIC_FIELD_MAPPINGS = { "start_date": { "type": "date" } }
DEPRECATED_ADVANCED_COMPONENT_TYPES = []
CREDIT_TASK_DEFAULT_RETRY_DELAY = 30
CREDIT_TASK_MAX_RETRIES = 5
CREDIT_PROVIDER_TIMESTAMP_EXPIRATION = 15 * 60
OAUTH_OIDC_ISSUER = 'https://www.example.com/oauth2'
OAUTH_ID_TOKEN_EXPIRATION = 5 * 60
PARTNER_SUPPORT_EMAIL = ''
AFFILIATE_COOKIE_NAME = 'affiliate_id'
from yaml import Loader, SafeLoader
SERVICE_VARIANT = os.environ.get('SERVICE_VARIANT', None)
CONFIG_ROOT = path(os.environ.get('CONFIG_ROOT', ENV_ROOT))
CONFIG_PREFIX = SERVICE_VARIANT + "." if SERVICE_VARIANT else ""
BROKER_POOL_LIMIT = 0 BROKER_CONNECTION_TIMEOUT = 1
CELERY_RESULT_BACKEND = 'djcelery.backends.cache:CacheBackend'
BROKER_HEARTBEAT = 10.0 BROKER_HEARTBEAT_CHECKRATE = 2
CELERYD_PREFETCH_MULTIPLIER = 1
if 'FEATURES' in ENV_TOKENS: del ENV_TOKENS['FEATURES']
STATIC_URL = STATIC_URL_BASE.encode('ascii') if not STATIC_URL.endswith("/"): STATIC_URL += "/" STATIC_URL += EDX_PLATFORM_REVISION + "/"
for app in ADDL_INSTALLED_APPS: INSTALLED_APPS += (app,)
for database_name in DATABASES: DATABASES[database_name]['ATOMIC_REQUESTS'] = False
from lms.envs.test import ( WIKI_ENABLED, PLATFORM_NAME, SITE_NAME, DEFAULT_FILE_STORAGE, MEDIA_ROOT, MEDIA_URL, )
MONGO_PORT_NUM = int(os.environ.get('EDXAPP_TEST_MONGO_PORT', '27017')) MONGO_HOST = os.environ.get('EDXAPP_TEST_MONGO_HOST', 'localhost')
TEST_RUNNER = 'openedx.core.djangolib.nose.NoseTestSuiteRunner'
STATIC_ROOT = TEST_ROOT / "staticfiles"
FEATURES['ENABLE_EXPORT_GIT'] = True GIT_REPO_EXPORT_DIR = TEST_ROOT / "export_course_repos"
STATICFILES_STORAGE = 'pipeline.storage.NonPackagingPipelineStorage' STATIC_URL = "/static/"
'ADDITIONAL_OPTIONS': { 'trashcan': { 'bucket': 'trash_fs' } }
MIGRATION_MODULES = NoOpMigrationModules()
filterwarnings('ignore', message='No request passed to the backend, unable to rate-limit')
simplefilter('ignore')
LETTUCE_SERVER_PORT = 8003 XQUEUE_PORT = 8040 YOUTUBE_PORT = 8031 LTI_PORT = 8765 VIDEO_SOURCE_PORT = 8777
PASSWORD_HASHERS = ( 'django.contrib.auth.hashers.SHA1PasswordHasher', 'django.contrib.auth.hashers.MD5PasswordHasher', )
CMS_SEGMENT_KEY = None
FEATURES['EMBARGO'] = True
FEATURES['ENABLE_DISCUSSION_SERVICE'] = False
PARENTAL_CONSENT_AGE_LIMIT = 13
FEATURES['ENABLE_CONTENT_LIBRARIES'] = True
FEATURES['MILESTONES_APP'] = True
FEATURES['ENTRANCE_EXAMS'] = True ENTRANCE_EXAM_MIN_SCORE_PCT = 50
FEATURES['ENABLE_COURSEWARE_INDEX'] = True FEATURES['ENABLE_LIBRARY_INDEX'] = True SEARCH_ENGINE = "search.tests.mock_search_engine.MockSearchEngine"
FEATURES['ENABLE_TEAMS'] = True
SECRET_KEY = '85920908f28904ed733fe576320db18cabd7b6cd'
INSTALLED_APPS += ('openedx.core.djangoapps.api_admin',)
OAUTH2_PROVIDER_APPLICATION_MODEL = 'oauth2_provider.Application'
DEBUG = True
REQUIRE_DEBUG = False
STATICFILES_STORAGE = 'pipeline.storage.PipelineCachedStorage'
INSTALLED_APPS += ('django_extensions',)
DEBUG = True
FEATURES['AUTOMATIC_AUTH_FOR_TESTING'] = True
FEATURES['MILESTONES_APP'] = True
FEATURES['ENABLE_PREREQUISITE_COURSES'] = True
FEATURES['ENABLE_EDXNOTES'] = True
FEATURES['ENABLE_TEAMS'] = True
FEATURES['LICENSING'] = True
PARTNER_SUPPORT_EMAIL = 'partner-support@example.com'
DEPRECATED_BLOCK_TYPES = ['poll', 'survey']
MOCK_SEARCH_BACKING_FILE = ( TEST_ROOT / "index_file.dat" ).abspath()
SECRET_KEY = "very_secret_bok_choy_key"
try:
DEBUG_TOOLBAR_MONGO_STACKTRACES = True
del DEFAULT_FILE_STORAGE MEDIA_ROOT = "/edx/var/edxapp/uploads"
for pkg_name in ['track.contexts', 'track.middleware', 'dd.dogapi']: logging.getLogger(pkg_name).setLevel(logging.CRITICAL)
PIPELINE_ENABLED = False STATICFILES_STORAGE = 'openedx.core.storage.DevelopmentStorage'
CELERY_ALWAYS_EAGER = True
DEBUG_TOOLBAR_MONGO_STACKTRACES = False
XBLOCK_SETTINGS = { "VideoDescriptor": { "licensing_enabled": True } }
REQUIRE_DEBUG = DEBUG
if os.path.isfile(join(dirname(abspath(__file__)), 'private.py')):
MODULESTORE = convert_module_store_setting_if_needed(MODULESTORE)
SECRET_KEY = '85920908f28904ed733fe576320db18cabd7b6cd'
DEBUG = True
import logging logging.basicConfig(filename=TEST_ROOT / "log" / "cms_acceptance.log", level=logging.ERROR)
logging.getLogger().setLevel(logging.ERROR)
'ADDITIONAL_OPTIONS': { 'trashcan': { 'bucket': 'trash_fs' } }
FEATURES['AUTOMATIC_AUTH_FOR_TESTING'] = True
USE_I18N = True
LETTUCE_SELENIUM_CLIENT = os.environ.get('LETTUCE_SELENIUM_CLIENT', 'local')
try:
import uuid SECRET_KEY = uuid.uuid4().hex
from lms.envs.dev import (WIKI_ENABLED)
'origin': 'git@github.com:MITx/content-mit-6002x.git',
CACHE_TIMEOUT = 0
SECRET_KEY = '85920908f28904ed733fe576320db18cabd7b6cd'
CELERY_ALWAYS_EAGER = True
DEBUG_TOOLBAR_MONGO_STACKTRACES = False
FEATURES['ENABLE_SERVICE_STATUS'] = True
import os CMS_SEGMENT_KEY = os.environ.get('SEGMENT_KEY')
try:
from .aws import * import os from django.core.exceptions import ImproperlyConfigured
if db != 'read_replica': DATABASES[db].update(get_db_overrides(db))
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'proj.settings')
APP.config_from_object('django.conf:settings') APP.autodiscover_tasks(lambda: settings.INSTALLED_APPS)
from ratelimitbackend import admin
url(r'^edge/(?P<org>[^/]+)/(?P<course>[^/]+)/course/(?P<coursename>[^/]+)$', 'contentstore.views.landing', name='landing'),
url(r'^api/user/', include('openedx.core.djangoapps.user_api.urls')),
url(r'^lang_pref/session_language', 'lang_pref.views.update_session_language', name='session_language'),
urlpatterns += patterns( '',
urlpatterns += patterns( 'contentstore.views',
'packages': ('openassessment',),
if settings.FEATURES.get('AUTOMATIC_AUTH_FOR_TESTING'): urlpatterns += ( url(r'^auto_auth$', 'student.views.auto_auth'), )
url(r'^programs/id_token/$', ProgramsIdTokenView.as_view(), name='programs_id_token'), url(r'^program/', ProgramAuthoringView.as_view(), name='programs'),
handler404 = 'contentstore.views.render_404' handler500 = 'contentstore.views.render_500'
urlpatterns += ( url(r'^404$', handler404), url(r'^500$', handler500), )
from .celery import APP as CELERY_APP
if settings.COMPREHENSIVE_THEME_DIR: enable_comprehensive_theme(settings.COMPREHENSIVE_THEME_DIR)
if settings.THEME_NAME == "": settings.THEME_NAME = None return
theme_root = settings.ENV_ROOT / "themes" / settings.THEME_NAME
settings.STATICFILES_DIRS.append( (u'themes/{}'.format(settings.THEME_NAME), theme_root / 'static') )
if isinstance(authored_data, CmsFieldData):
from __future__ import unicode_literals
context = { 'reorderable_items': set(), 'read_only': True } problem_html = get_preview_fragment(request, self.problem, context).content
self.assertNotRegexpMatches(problem_html, r"data-block-type=[\"\']acid_aside[\"\']")
video_html = get_preview_fragment(request, self.video, context).content self.assertNotRegexpMatches(video_html, "<select")
from __future__ import unicode_literals
def __init__(self, course_descriptor): self.graders = [ CourseGradingModel.jsonize_grader(i, grader) for i, grader in enumerate(course_descriptor.raw_grader)
index = int(grader.get('id', len(descriptor.raw_grader))) grader = CourseGradingModel.parse_grader(grader)
if graceperiodjson is not None: if 'grace_period' in graceperiodjson: graceperiodjson = graceperiodjson['grace_period']
if minimum_grade_credit is not None: minimum_grade_credit = minimum_grade_credit
descriptor.raw_grader = descriptor.raw_grader
filtered_list = list(cls.FILTERED_LIST)
if not settings.FEATURES.get('ENABLE_EXPORT_GIT'): filtered_list.append('giturl')
if not settings.FEATURES.get('ENABLE_EDXNOTES'): filtered_list.append('edxnotes')
if not settings.FEATURES.get('ENABLE_VIDEO_UPLOAD_PIPELINE'): filtered_list.append('video_upload_pipeline')
if not settings.FEATURES.get('ENABLE_TEAMS'): filtered_list.append('teams_configuration')
if not settings.FEATURES.get('CUSTOM_COURSES_EDX'): filtered_list.append('enable_ccx') filtered_list.append('ccx_connector')
if not filter_tabs: filtered_list.remove("tabs")
key_values = {}
if did_validate: updated_data = cls.update_from_dict(key_values, descriptor, user, save=False)
update_creator_state = Signal(providing_args=["caller", "user", "state"])
send_admin_notification = Signal(providing_args=["user"])
send_user_notification = Signal(providing_args=["user", "state"])
if instance.state == CourseCreator.DENIED or granted_state_change: send_user_notification.send( sender=sender, user=instance.user, state=instance.state )
if instance.state == CourseCreator.PENDING: send_admin_notification.send( sender=sender, user=instance.user )
return user[0].state
obj.admin = request.user obj.save()
message_template = 'emails/course_creator_revoked.txt'
from __future__ import unicode_literals
self.assertFalse(auth.user_has_role(self.user, CourseCreatorRole()))
for _ in xrange(30): response = self.client.post('/admin/login/', post_params) self.assertEquals(response.status_code, 200)
self.assertEquals(response.status_code, 403)
add_user_with_status_granted(self.admin, self.user) self.assertEqual('unrequested', get_course_creator_status(self.user))
self.assertFalse(auth.user_has_role(self.user, CourseCreatorRole()))
add_user_with_status_unrequested(self.user) self.assertEqual('granted', get_course_creator_status(self.user))
user_requested_access(self.user) self.assertEqual('granted', get_course_creator_status(self.user))
add_user_with_status_unrequested(self.admin) self.assertIsNone(get_course_creator_status(self.admin))
add_user_with_status_granted(self.admin, self.admin) self.assertIsNone(get_course_creator_status(self.admin))
return
return
_timed_exams = modulestore().get_items( course_key, qualifiers={ 'category': 'sequential', }, settings={ 'is_time_limited': True, } )
timed_exams = [ timed_exam for timed_exam in _timed_exams if is_item_in_course_tree(timed_exam) ]
if timed_exam.is_proctored_exam and not timed_exam.is_practice_exam: try: update_review_policy( exam_id=exam_id, set_by_user_id=timed_exam.edited_by, review_policy=timed_exam.exam_review_rules ) except ProctoredExamReviewPolicyNotFoundException:
remove_review_policy(exam_id=exam_id)
exams = get_all_exams_for_course(course_key)
msg = 'Disabling timed exam {exam_id}'.format(exam_id=exam['id']) log.info(msg) update_exam( exam_id=exam['id'], is_proctored=False, is_active=False, )
super(GitExportError, self).__init__(unicode(message))
on_course_publish(course_key)
from .tasks import update_search_index
from .tasks import update_library_index
admin = User.objects.get(username=username, email=email)
for user in get_users_with_role(CourseStaffRole.ROLE): add_user_with_status_unrequested(user)
try: course_key = CourseKey.from_string(args[0]) except InvalidKeyError: try: course_key = SlashSeparatedCourseKey.from_deprecated_string(args[0]) except InvalidKeyError: raise CommandError(unicode(GitExportError.BAD_COURSE))
split_modulestore = modulestore()._get_modulestore_by_type(ModuleStoreEnum.Type.split) active_version_collection = split_modulestore.db_connection.course_index structure_collection = split_modulestore.db_connection.structures
CourseInstructorRole(dest_course_id).add_users( *CourseInstructorRole(source_course_id).users_with_role() ) CourseStaffRole(dest_course_id).add_users( *CourseStaffRole(source_course_id).users_with_role() )
assets_deleted = content_store.remove_redundant_content_for_courses() success = True
searcher = SearchEngine.get_search_engine(index_name)
if setup_option or query_yes_no(self.CONFIRMATION_PROMPT, default="no"): course_keys = [course.id for course in modulestore().get_courses()] else: return
course_keys = map(self._parse_course_key, args)
help = "Create a course in one of {}".format([ModuleStoreEnum.Type.mongo, ModuleStoreEnum.Type.split]) args = "modulestore user org course run"
self.assertTrue(self.store.has_item(course.id.make_usage_key('html', 'multi_parent_html')))
call_command('delete_orphans', unicode(published_branch), '--commit')
published_branch = course.id.for_branch( ModuleStoreEnum.BranchName.published )
self.assertOrphanCount(course.id, 1) self.assertOrphanCount(published_branch, 1)
self.store.delete_item( orphan.location, self.user.id, skip_auto_publish=True )
self.assertOrphanCount(course.id, 0) self.assertOrphanCount(published_branch, 1) self.assertIn(orphan, self.store.get_items(published_branch))
self.module_store = modulestore()._get_modulestore_by_type(ModuleStoreEnum.Type.mongo)
self.course = CourseFactory.create( org=org, number=course_number, run=course_run )
self.assertTrue(self.store.has_changes(self.store.get_item(self.course.location)))
versions = get_course_versions(unicode(self.course.id)) draft_version = versions['draft-branch'] published_version = versions['published-branch']
self.assertNotEqual(draft_version, published_version)
call_command('force_publish', unicode(self.course.id), '--commit')
self.assertFalse(self.store.has_changes(self.store.get_item(self.course.location)))
versions = get_course_versions(unicode(self.course.id)) new_draft_version = versions['draft-branch'] new_published_version = versions['published-branch']
self.assertEqual(draft_version, new_draft_version) self.assertNotEqual(published_version, new_published_version)
self.assertEqual(new_draft_version, new_published_version)
self.good_dir = self.create_course_xml(self.content_dir, self.base_course_key)
self.course_dir = self.create_course_xml(self.content_dir, self.truncated_key)
call_command('import', self.content_dir, self.good_dir) store = modulestore() self.assertIsNotNone(store.get_course(self.base_course_key))
call_command('import', self.content_dir, self.course_dir) self.assertIsNotNone(store.get_course(self.truncated_key))
modulestore().mappings = {}
self.assertEqual(unicode(course.location.course_key), unicode(course.children[0].course_key))
with self.assertRaisesRegexp(CommandError, unicode(GitExportError.URL_BAD)): call_command('git_export', 'foo/bar/baz', 'silly', stderr=StringIO.StringIO())
with self.assertRaisesRegexp(CommandError, unicode(GitExportError.BAD_COURSE)): call_command('git_export', 'foo/bar:baz', 'silly', stderr=StringIO.StringIO())
with self.assertRaisesRegexp(GitExportError, unicode(GitExportError.XML_EXPORT_FAIL)): git_export_utils.export_to_git( course_key, 'file://{0}'.format(self.bare_repo_dir))
with self.assertRaisesRegexp(GitExportError, unicode(GitExportError.CANNOT_PULL)): git_export_utils.export_to_git( course_key, 'https://user:blah@example.com/r.git')
course = self.store.get_course(course.id)
dangling_pointer = course.id.make_usage_key('chapter', 'DanglingPointer')
self.assertEqual(len(course.children), 2) self.assertIn(dangling_pointer, course.children)
course = self.store.get_course(course.id) self.assertEqual(len(course.children), 1) self.assertNotIn(dangling_pointer, course.children)
call_command( "migrate_to_split", str(self.course.id), str(self.user.id), )
self.temp_dir_1 = mkdtemp() self.temp_dir_2 = mkdtemp(dir="")
self.addCleanup(shutil.rmtree, self.temp_dir_1) self.addCleanup(shutil.rmtree, self.temp_dir_2)
errstring = "Invalid course_key: 'InvalidCourseID'." with self.assertRaisesRegexp(CommandError, errstring): call_command('export', "InvalidCourseID", self.temp_dir_1)
for output_dir in [self.temp_dir_1, self.temp_dir_2]: call_command('export', course_id, output_dir)
courses, failed_export_courses = export_courses_to_output_path(self.temp_dir) self.assertEqual(len(courses), 2) self.assertEqual(len(failed_export_courses), 0)
self.assertEqual(store, modulestore()._get_modulestore_for_courselike(new_key).get_modulestore_type())
from optparse import make_option from django.core.management.base import BaseCommand, CommandError from .prompt import query_yes_no
raise CommandError(e)
from __future__ import unicode_literals
log = logging.getLogger(__name__)
save_course_update_items(location, course_updates, course_update_items, user) if "status" in course_update_dict: del course_update_dict["status"] return course_update_dict
if 0 < passed_index <= len(course_update_items): course_update_item = course_update_items[passed_index - 1] course_update_item["status"] = CourseInfoModule.STATUS_DELETED course_update_items[passed_index - 1] = course_update_item
save_course_update_items(location, course_updates, course_update_items, user) return _get_visible_update(course_update_items)
return 0
modulestore().update_item(course_updates, user.id)
Push.alert( data=push_payload, channels={"$in": push_channels}, where={"deviceType": "android"}, )
source_subs_filedata = request.FILES['transcript-file'].read().decode('utf-8-sig') source_subs_filename = request.FILES['transcript-file'].name
if video_list: sub_attr = source_subs_name try: generate_subs_from_source({1: sub_attr}, source_subs_ext, source_subs_filedata, item)
copy_or_rename_transcript(video_name, sub_attr, item, user=request.user)
youtube_id = videos.get('youtube', None) if youtube_id: transcripts_presence['is_youtube_mode'] = True
subs = ''
if ( transcripts_presence['youtube_diff'] and transcripts_presence['youtube_local'] and
html5_id_to_remove = [x for x in videos['html5'] if x != html5_id] if html5_id_to_remove: remove_subs_from_store(html5_id_to_remove, item)
copy_or_rename_transcript(new_name, old_name, item, user=request.user)
error_response(response, "Can't find transcripts in storage for {}".format(old_name))
current_subs = data.get('current_subs') if current_subs is not None: for sub in current_subs: remove_subs_from_store(sub, item)
item = modulestore().get_item(usage_key)
if not has_course_author_access(request.user, item.location.course_key): raise PermissionDenied()
COMPONENT_TYPES = ['discussion', 'html', 'problem', 'video']
xblock_info = create_xblock_info(xblock, include_ancestor_info=is_unit_page)
index = 1 for child in subsection.get_children(): if child.location == unit.location: break index += 1
component_types = COMPONENT_TYPES[:]
if library: component_types = [component for component in component_types if component != 'discussion']
if library: return component_templates
usage_key = usage_key.replace(course_key=modulestore().fill_in_run(usage_key.course_key))
req = django_to_webob_request(request)
modulestore().update_item(descriptor, request.user.id)
if not settings.FEATURES.get(feature_name, False): return HttpResponseBadRequest() return view_func(request, *args, **kwargs)
if not has_course_author_access(request.user, course_key): return HttpResponse(status=403)
if request.method == 'GET': return _get_entrance_exam(request, course_key)
entrance_exam_minimum_score_pct = _get_default_entrance_exam_minimum_pct() if ee_min_score != '' and ee_min_score is not None: entrance_exam_minimum_score_pct = float(ee_min_score) return create_entrance_exam(request, course_key, entrance_exam_minimum_score_pct)
elif request.method == 'DELETE': return delete_entrance_exam(request, course_key)
else: return HttpResponse(status=405)
if entrance_exam_minimum_score_pct is None: entrance_exam_minimum_score_pct = _get_default_entrance_exam_minimum_pct()
course = modulestore().get_course(course_key) if course is None: return HttpResponse(status=400)
create_xblock( parent_locator=unicode(created_block.location), user=request.user, category='sequential', display_name=_('Entrance Exam - Subsection') ) add_entrance_exam_milestone(course.id, created_block)
remove_entrance_exam_graders(course_key, request.user)
CONTENT_RE = re.compile(r"(?P<start>\d{1,11})-(?P<stop>\d{1,11})/(?P<end>\d{1,11})")
try: data_root = path(settings.GITHUB_REPO_ROOT) subdir = base64.urlsafe_b64encode(repr(courselike_key)) course_dir = data_root / subdir filename = request.FILES['course-data'].name
session_status = request.session.setdefault("import_status", {}) courselike_string = unicode(courselike_key) + filename _save_request_status(request, courselike_string, 0)
if root_name == COURSE_ROOT: if courselike_module.entrance_exam_enabled: remove_entrance_exam_milestone_reference(request, courselike_key) log.info( "entrance exam milestone content reference for course %s has been removed", courselike_module.id )
try: matches = CONTENT_RE.search(request.META["HTTP_CONTENT_RANGE"]) content_range = matches.groupdict()
content_range = {'start': 0, 'stop': 1, 'end': 2}
try: log.info("Course import %s: Upload complete", courselike_key) _save_request_status(request, courselike_string, 1)
if session_status[courselike_string] != 4: _save_request_status(request, courselike_string, -abs(session_status[courselike_string]))
pass
requested_format = request.GET.get('_accept', request.META.get('HTTP_ACCEPT', 'text/html'))
return HttpResponse(status=406)
static_tab_loc = course_key.make_usage_key('static_tab', tab.url_slug) tab.locator = static_tab_loc
requested_tab_id_locators = request.json['tabs']
old_tab_list = course_item.tabs
non_displayed_tabs = set(old_tab_list) - set(new_tab_list) new_tab_list.extend(non_displayed_tabs)
course_item.tabs = new_tab_list modulestore().update_item(course_item, request.user.id)
tab_id_locator = request.json['tab_id_locator']
tab.is_hidden = request.json['is_hidden'] modulestore().update_item(course_item, request.user.id)
modulestore().update_item(course, ModuleStoreEnum.UserID.primitive_command)
if requested_sort == 'date_added': requested_sort = 'uploadDate' elif requested_sort == 'display_name': requested_sort = 'displayname' sort = [(requested_sort, sort_direction)]
thumbnail_location = asset.get('thumbnail_location', None) if thumbnail_location: thumbnail_location = course_key.make_asset_key( 'thumbnail', thumbnail_location[4])
try: modulestore().get_course(course_key) except ItemNotFoundError: logging.error("Could not find course: %s", course_key) return HttpResponseBadRequest()
upload_file = request.FILES['file'] filename = upload_file.name mime_type = upload_file.content_type size = get_file_size(upload_file)
(thumbnail_content, thumbnail_location) = contentstore().generate_thumbnail( content, tempfile_path=tempfile_path, )
del_cached_content(thumbnail_location) if thumbnail_content is not None: content.thumbnail_location = thumbnail_location
contentstore().save(content) del_cached_content(content.location)
try: content = contentstore().find(asset_key) except NotFoundError: raise AssetNotFoundException
contentstore('trashcan').save(content)
contentstore().delete(content.get_id()) del_cached_content(content.location)
'id': unicode(location)
def landing(request, org, course, coursename): return render_to_response('temp-course-landing.html', {})
def edge(request): return redirect('/')
return xblock.has_children
fields = {}
child_position = None if is_entrance_exams_enabled(): if category == 'chapter' and is_entrance_exam: fields['is_entrance_exam'] = is_entrance_exam
if '/' == asset_key_string[0]: asset_key_string = asset_key_string[1:] asset_key = AssetKey.from_string(asset_key_string) try: delete_asset(course_key, asset_key) except AssetNotFoundException: pass
certificate["version"] = CERTIFICATE_SCHEMA_VERSION if certificate.get("signatories") is None: certificate["signatories"] = [] certificate["editing"] = False return certificate
if certificate_data.get('course_title'): certificate_response["course_title"] = certificate_data['course_title']
return certificate
certificates = course.certificates.get('certificates', []) if only_active: certificates = [certificate for certificate in certificates if certificate.get('is_active', False)] return certificates
course.certificates['certificates'].pop(index) store.update_item(course, request.user.id) break
for certificate in certificates: certificate['is_active'] = is_active break
for certificate in certificates: is_active = certificate.get('is_active', False) break
if not GlobalStaff().has_user(request.user): raise PermissionDenied()
if not GlobalStaff().has_user(request.user): raise PermissionDenied()
for index, cert in enumerate(certificates_list): if certificate_id is not None: if int(cert['id']) == int(certificate_id): match_cert = cert
if auth.user_has_role(user, CourseInstructorRole(course_id)): return 'instructor' else: return 'staff'
KEY_EXPIRATION_IN_SECONDS = 86400
return _("{profile_name} URL").format(profile_name=profile)
course = get_course_and_check_access(course_key, user)
for video in videos: video["status"] = StatusDisplayStrings.get(video["status"])
NEVER = lambda x: False ALWAYS = lambda x: True
xblock.runtime.wrappers.append(partial( wrap_xblock, 'StudioRuntime', usage_id_serializer=unicode, request_token=request_token(request), ))
reorderable_items = set() if view_name == 'reorderable_container_child_preview': reorderable_items.add(xblock.location)
context = {
return modulestore().update_item(xblock, user.id)
with store.bulk_operations(xblock.location.course_key):
if publish == "discard_changes": store.revert_to_published(xblock.location, user.id) return JsonResponse({'id': unicode(xblock.location)})
xblock.children = children
xblock = _update_with_callback(xblock, user, old_metadata, old_content)
if publish == 'make_public': modulestore().publish(xblock.location, user.id)
return JsonResponse(result, encoder=EdxJSONEncoder)
if category not in ['html', 'problem', 'video']: return HttpResponseBadRequest( "Category '%s' not supported for Libraries" % category, content_type='text/plain' )
dest_usage_key = source_item.location.replace(name=uuid4().hex) category = dest_usage_key.block_type
if source_item.has_children and not children_handled: dest_module.children = dest_module.children or [] for child in source_item.children: dupe = _duplicate_item(dest_module.location, child, user=user)
if branch == ModuleStoreEnum.BranchName.published: revision = ModuleStoreEnum.RevisionOption.published_only store.delete_item(itemloc, user_id, revision=revision)
return store.create_item(user.id, usage_key.course_key, usage_key.block_type, block_id=usage_key.block_id)
if not isinstance(xblock.location, LibraryUsageLocator): modulestore().has_changes(modulestore().get_course(xblock.location.course_key, depth=None))
xblock_info = create_xblock_info( xblock, data=data, metadata=own_metadata(xblock), include_ancestor_info=include_ancestor_info ) if include_publishing_info: add_container_page_publishing_info(xblock, xblock_info) return xblock_info
has_changes = None if (is_xblock_unit or course_outline) and not is_library_block: has_changes = modulestore().has_changes(xblock)
graders = _filter_entrance_exam_grader(graders)
if course is None: course = modulestore().get_course(xblock.location.course_key)
xblock_actions = {'deletable': True, 'draggable': True, 'childAddable': True} explanatory_message = None
xblock_info.update(_get_gating_info(course, xblock))
if getattr(xblock, "in_entrance_exam", False): xblock_info["is_header_visible"] = False
return VisibilityState.needs_attention
reset_to_default = False try: reset_to_default = xblock.start.year < 1900 except ValueError: reset_to_default = True
return get_default_time_display(xblock.start) if xblock.start != DEFAULT_START_DATE else None
user_perms = get_user_permissions(request.user, course_key) if not user_perms & STUDIO_VIEW_USERS: raise PermissionDenied()
staff = set(CourseStaffRole(course_key).users_with_role()).union(instructors)
if is_library: role_hierarchy = (CourseInstructorRole, CourseStaffRole, LibraryUserRole) else: role_hierarchy = (CourseInstructorRole, CourseStaffRole)
if not ((requester_perms & STUDIO_EDIT_ROLES) or (user.id == request.user.id)): return permissions_error_response
auth.add_users(request.user, role, user) role_added = True
old_roles.add(role)
CourseEnrollment.enroll(user, course_key)
req = django_to_webob_request(request) try: resp = instance.handle(handler, req, suffix)
is_author_mode = True
partial( wrap_xblock, 'PreviewRuntime', display_name_only=display_name_only, usage_id_serializer=unicode, request_token=request_token(request) ),
partial(replace_static_urls, None, course_id=course_id), _studio_wrap_xblock,
wrappers.insert(0, wrap_with_license)
wrappers=wrappers, wrappers_asides=wrappers_asides, error_descriptor_class=ErrorDescriptor, get_user_role=lambda: get_user_role(request.user, course_id),
resp = self.client.ajax_post( first_update_url, payload, HTTP_X_HTTP_METHOD_OVERRIDE="PUT", REQUEST_METHOD="POST" )
refetched = self.client.get_json(first_update_url) self.assertHTMLEqual( content, json.loads(refetched.content)['content'], "get w/ provided id" )
content = '<ol/>' payload = get_response(content, 'January 11, 2013') self.assertHTMLEqual(content, payload['content'], "self closing ol")
self.assertContains( self.client.ajax_post(course_update_url, {'garbage': 1}), 'Failed to save', status_code=400 )
content = 'outside <strong>inside</strong> after' payload = get_response(content, 'June 22, 2000') self.assertHTMLEqual(content, payload['content'], "text outside tag")
content = '<garbage tag No closing brace to force <span>error</span>' payload = {'content': content, 'date': 'January 11, 2013'}
content = "<p><br><br></p>" payload = get_response(content, 'January 11, 2013') self.assertHTMLEqual(content, payload['content'])
self.assertContains(self.client.delete(course_update_url + '19'), "delete", status_code=400)
resp = self.client.get_json(course_update_url) payload = json.loads(resp.content) self.assertTrue(len(payload) == 1)
self.client.ajax_post(course_update_url)
self.assertEqual(resp.status_code, 200)
self.assertFalse(mock_push_update.called)
handouts_location = self.course.id.make_usage_key('course_info', 'handouts') course_handouts_url = reverse_usage_url('xblock_handler', handouts_location)
self.assertEqual(resp.status_code, 200)
libraries = [LibraryFactory.create() for _ in range(3)] lib_dict = dict([(lib.location.library_key, lib) for lib in libraries])
self.assertNotIn(extra_user.username, response.content)
response = self.client.get(manage_users_url) self.assertEqual(response.status_code, 200) self.assertIn(extra_user.username, response.content)
course_url = u'/course/{}'.format(unicode(self.course.id)) self.assertEqual(xblock_studio_url(self.course), course_url)
video = ItemFactory.create(parent_location=child_vertical.location, category="video", display_name="My Video") self.assertIsNone(xblock_studio_url(video))
library = LibraryFactory.create() expected_url = u'/library/{}'.format(unicode(library.location.library_key)) self.assertEqual(xblock_studio_url(library), expected_url)
child_vertical = ItemFactory.create(parent_location=vertical.location, category='vertical', display_name='Child Vertical') self.assertEqual(xblock_type_display_name(child_vertical), u'Vertical')
self.odd_course = CourseFactory.create( org='test.org_1-2', number='test-2.3_course', display_name='dotted.course.name-2', )
lib1 = LibraryFactory.create()
outline_response = self.client.get(link.get("href"), {}, HTTP_ACCEPT='text/html') self.assertEqual(outline_response.status_code, 200)
non_staff_client, _ = self.create_non_staff_authed_user_client() response = non_staff_client.delete(outline_url, {}, HTTP_ACCEPT='application/json') self.assertEqual(response.status_code, 403)
self.check_index_and_outline(course_staff_client)
self.assert_correct_json_response(json_response)
notification_url = reverse_course_url('course_notifications_handler', self.course.id, kwargs={ 'action_state_id': 1, })
self.assertEquals(resp.status_code, 400)
rerun_state = CourseRerunState.objects.update_state( course_key=self.course.id, new_state=state, allow_not_found=True ) CourseRerunState.objects.update_should_display( entry_id=rerun_state.id, user=UserFactory(), should_display=should_display )
user2 = UserFactory() add_instructor(rerun_course_key, self.user, user2)
rerun_state = CourseRerunState.objects.update_state( course_key=rerun_course_key, new_state=state, allow_not_found=True ) CourseRerunState.objects.update_should_display( entry_id=rerun_state.id, user=user2, should_display=should_display )
CourseRerunState.objects.get(id=rerun_state.id)
self.course.display_coursenumber = None updated_course = self.update_course(self.course, self.user.id)
self.assertEqual(updated_course.display_coursenumber, None)
course_outline_url = reverse_course_url('course_handler', updated_course.id) response = self.client.get_html(course_outline_url)
self.assertEqual(response.status_code, 200)
self.assertIn('display_course_number: ""', response.content)
self.assert_correct_json_response(json_response)
self.assertIsNone(course_outline_initial_state('no-such-locator', course_structure))
self.assertEqual(_get_release_date(response), 'Unscheduled') _assert_settings_link_present(response)
self.assertEqual(info['blocks'], [])
if delete_vertical: self.store.delete_item(vertical1.location, self.user.id) else: self.store.delete_item(problem1.location, self.user.id)
self.assertEqual( info['blocks'], [[reverse_usage_url('container_handler', vertical2.location), 'notes problem in vert2']] )
self.assertIn(self.SUCCESSFUL_RESPONSE, response.content) self.assertEqual(response.status_code, 200)
non_staff_client, _ = self.create_non_staff_authed_user_client() response = non_staff_client.get(index_url, {}, HTTP_ACCEPT='application/json') self.assertEqual(response.status_code, 403)
self.assertIn(self.SUCCESSFUL_RESPONSE, response.content) self.assertEqual(response.status_code, 200)
err = SearchIndexingError mock_index_dictionary.return_value = err
response = self.client.get(index_url, {}, HTTP_ACCEPT='application/json') self.assertEqual(response.status_code, 500)
response = perform_search( "unique", user=self.user, size=10, from_=0, course_id=unicode(self.course.id)) self.assertEqual(response['total'], 1)
reindex_course_and_check_access(self.course.id, self.user)
response = perform_search( "unique", user=self.user, size=10, from_=0, course_id=unicode(self.course.id)) self.assertEqual(response['total'], 1)
response = perform_search( "unique", user=self.user, size=10, from_=0, course_id=unicode(self.course.id)) self.assertEqual(response['total'], 1)
err = SearchIndexingError mock_index_dictionary.return_value = err
with self.assertRaises(SearchIndexingError): reindex_course_and_check_access(self.course.id, self.user)
response = perform_search( "unique", user=self.user, size=10, from_=0, course_id=unicode(self.course.id)) self.assertEqual(response['total'], 1)
err = SearchIndexingError mock_index_dictionary.return_value = err
with self.assertRaises(SearchIndexingError): reindex_course_and_check_access(self.course.id, self.user)
response = perform_search( "unique", user=self.user, size=10, from_=0, course_id=unicode(self.course.id)) self.assertEqual(response['total'], 1)
err = Exception mock_index_dictionary.return_value = err
with self.assertRaises(SearchIndexingError): reindex_course_and_check_access(self.course.id, self.user)
err = ItemNotFoundError mock_get_course.return_value = err
with self.assertRaises(SearchIndexingError): reindex_course_and_check_access(self.course.id, self.user)
user2 = UserFactory() with self.assertRaises(PermissionDenied): reindex_course_and_check_access(self.course.id, user2)
response = perform_search( "unique", user=self.user, size=10, from_=0, course_id=unicode(self.course.id)) self.assertEqual(response['total'], 1)
CoursewareSearchIndexer.do_course_reindex(modulestore(), self.course.id)
response = perform_search( "unique", user=self.user, size=10, from_=0, course_id=unicode(self.course.id)) self.assertEqual(response['total'], 1)
response = perform_search( "unique", user=self.user, size=10, from_=0, course_id=unicode(self.course.id)) self.assertEqual(response['total'], 1)
err = Exception mock_index_dictionary.return_value = err
with self.assertRaises(SearchIndexingError): CoursewareSearchIndexer.do_course_reindex(modulestore(), self.course.id)
response = perform_search( "unique", user=self.user, size=10, from_=0, course_id=unicode(self.course.id)) self.assertEqual(response['total'], 1)
err = Exception mock_index_dictionary.return_value = err
with self.assertRaises(SearchIndexingError): CoursewareSearchIndexer.do_course_reindex(modulestore(), self.course.id)
response = perform_search( "unique", user=self.user, size=10, from_=0, course_id=unicode(self.course.id)) self.assertEqual(response['total'], 1)
err = Exception mock_index_dictionary.return_value = err
with self.assertRaises(SearchIndexingError): CoursewareSearchIndexer.do_course_reindex(modulestore(), self.course.id)
err = ItemNotFoundError mock_get_course.return_value = err
with self.assertRaises(SearchIndexingError): CoursewareSearchIndexer.do_course_reindex(modulestore(), self.course.id)
test_container_html(draft_container)
self.store.publish(self.vertical.location, self.user.id) draft_container = self.store.get_item(draft_container.location) test_container_html(draft_container)
self.assertRaises( Http404, views.container_handler, request, usage_key_string='i4x://InvalidOrg/InvalidCourse/vertical/static/InvalidContent', )
response = views.container_handler( request=request, usage_key_string=unicode(self.vertical.location) ) self.assertEqual(response.status_code, 200)
self.course = modulestore().get_course(self.course.id)
self.assertEqual(len(paths[milestone_key]), 0)
update_entrance_exam(request, self.course.id, {})
super(TabsPageTests, self).setUp()
self.url = reverse_course_url('tabs_handler', self.course.id)
self.test_tab = ItemFactory.create( parent_location=self.course.location, category="static_tab", display_name="Static_1" ) self.reload_course()
with self.assertRaises(NotImplementedError): self.client.get(self.url)
with self.assertRaises(NotImplementedError): self.client.ajax_post( self.url, data={'invalid_request': None}, )
orig_tab_ids = [tab.tab_id for tab in self.course.tabs] tab_ids = list(orig_tab_ids) num_orig_tabs = len(orig_tab_ids)
self.assertTrue(num_orig_tabs >= 5)
tab_ids[num_orig_tabs - 1], tab_ids[num_orig_tabs - 2] = tab_ids[num_orig_tabs - 2], tab_ids[num_orig_tabs - 1]
removed_tab = tab_ids.pop(num_orig_tabs / 2) self.assertTrue(len(tab_ids) == num_orig_tabs - 1)
resp = self.client.ajax_post( self.url, data={'tabs': [{'tab_id': tab_id} for tab_id in tab_ids]}, ) self.assertEqual(resp.status_code, 204)
self.reload_course() new_tab_ids = [tab.tab_id for tab in self.course.tabs] self.assertEqual(new_tab_ids, tab_ids + [removed_tab]) self.assertNotEqual(new_tab_ids, orig_tab_ids)
tab_ids[0], tab_ids[1] = tab_ids[1], tab_ids[0]
resp = self.client.ajax_post( self.url, data={'tabs': [{'tab_id': tab_id} for tab_id in invalid_tab_ids]}, ) self.check_invalid_tab_id_response(resp)
old_tab = CourseTabList.get_tab_by_type(self.course.tabs, tab_type)
self.assertNotEqual(old_tab.is_hidden, new_is_hidden_setting)
self.reload_course() new_tab = CourseTabList.get_tab_by_type(self.course.tabs, tab_type) self.assertEqual(new_tab.is_hidden, new_is_hidden_setting)
self.assertEquals(course.tabs[2], {'type': 'discussion', 'name': 'Discussion'})
self.assertEqual(content.content_type, 'application/pdf')
relative_path = 'just_a_test.jpg' absolute_path = base_url + relative_path
self.assert_correct_filter_response(self.url, 'asset_type', 'OTHER')
output = assets._get_asset_json("my_file", content_type, upload_date, location, thumbnail_location, True)
json.dumps(assets._get_asset_json( "sample_static.txt", content_type, upload_date, asset_location, None, lock)), "application/json"
module_store = modulestore() course_items = import_course_from_xml( module_store, self.user.id, TEST_DATA_DIR, ['toy'], static_content_store=contentstore(), verbose=True ) course = course_items[0] verify_asset_locked_state(False)
resp_asset = post_asset_update(True, course) self.assertTrue(resp_asset['locked']) verify_asset_locked_state(True)
resp_asset = post_asset_update(False, course) self.assertFalse(resp_asset['locked']) verify_asset_locked_state(False)
self.asset_name = 'delete_test' self.asset = self.get_sample_asset(self.asset_name)
resp_status = self.client.get( reverse_course_url( 'import_status_handler', self.course.id, kwargs={'filename': os.path.split(self.bad_tar)[1]} ) )
__, nonstaff_user = self.create_non_staff_authed_user_client() auth.add_users(self.user, CourseStaffRole(self.course.id), nonstaff_user)
self.assertNotEqual(display_name_before_import, display_name_after_import)
self.assertFalse(CourseInstructorRole(self.course.id).has_user(nonstaff_user)) self.assertTrue(CourseStaffRole(self.course.id).has_user(nonstaff_user))
self.assertFalse(CourseInstructorRole(self.course.id).has_user(nonstaff_user)) self.assertTrue(CourseStaffRole(self.course.id).has_user(nonstaff_user))
with self.settings(DATA_DIR='/not/the/data/dir'): try_tar(self._edx_platform_tar())
extract_dir_relative = path.relpath(extract_dir, settings.DATA_DIR)
extract_dir_relative = path.relpath(extract_dir, settings.DATA_DIR)
extract_dir_relative = path.relpath(extract_dir, settings.DATA_DIR)
import_library_from_xml( self.store, 'test_user', self.export_dir, ['exported_source_library'], static_content_store=contentstore(), target_id=source_library2_key, load_error_modules=False, raise_on_failure=True, create_if_not_present=True, )
self.assertCoursesEqual(source_library1_key, source_library2_key)
if resp.context: self.assertEqual(resp.context['course'], self.course)
no_ids = [] self.reload_course() for textbook in self.course.pdf_textbooks: del textbook["id"] no_ids.append(textbook) self.assertEqual(no_ids, textbooks)
self.save_course() self.url_nonexist = self.get_details_url("1=20")
for video in self.previous_uploads: self.assertIn(video["edx_video_id"], response.content)
assert_bad({})
assert_bad({"files": [{"content_type": "video/mp4"}]})
assert_bad({"files": [{"file_name": "test.mp4"}]})
mock_key.side_effect = mock_key_instances + [Mock()]
response_file = response_obj["files"][i] self.assertEqual(response_file["file_name"], file_info["file_name"]) self.assertEqual(response_file["upload_url"], mock_key_instance.generate_url())
context = { 'reorderable_items': set(), 'read_only': True } html = get_preview_fragment(request, html, context).content
context = { 'reorderable_items': set(), 'read_only': True } html = get_preview_fragment(request, html, context).content
invalid_json = "{u'name': 'Test Name', []}"
user_partititons = self.course.user_partitions
user_partititons = self.course.user_partitions self.assertEqual(len(user_partititons), 1) self.assertEqual(len(user_partititons[0].groups), 3)
user_partititons = self.course.user_partitions
user_partititons = self.course.user_partitions self.assertEqual(len(user_partititons), 1) self.assertEqual(user_partititons[0].name, 'Name 1')
user_partititons = self.course.user_partitions self.assertEqual(len(user_partititons), 2) self.assertEqual(user_partititons[0].name, 'Name 0')
user_partititons = self.course.user_partitions self.assertEqual(len(user_partititons), 2) self.assertEqual(user_partititons[0].name, 'Name 0')
actual = GroupConfiguration.get_or_create_content_group(self.store, self.course)
self.assertEqual(actual, expected)
actual = GroupConfiguration.get_content_groups_usage_info(self.store, self.course) self.assertEqual(actual.keys(), [0])
self.verify_validation_update_usage_info(expected_result, mocked_message)
self.create_programs_config() self.mock_programs_api(data={'results': []})
self.mock_programs_api()
self.create_programs_config()
user, client_name = mock_get_id_token.call_args[0] self.assertEqual(user, self.user) self.assertEqual(client_name, "programs")
self.assertNotContains(resp, self.ext_user.email)
ext_user = User.objects.get(email=self.ext_user.email) self.assertFalse(auth.user_has_role(ext_user, CourseStaffRole(self.course.id)))
ext_user = User.objects.get(email=self.ext_user.email) self.assertFalse(auth.user_has_role(ext_user, CourseInstructorRole(self.course.id)))
ext_user = User.objects.get(email=self.ext_user.email) self.assertTrue(auth.user_has_role(ext_user, CourseInstructorRole(self.course.id)))
ext_user = User.objects.get(email=self.ext_user.email) self.assertTrue(auth.user_has_role(ext_user, CourseInstructorRole(self.course.id)))
user = User.objects.get(email=self.user.email) self.assertFalse(auth.user_has_role(user, CourseStaffRole(self.course.id)))
ext_user = User.objects.get(email=self.ext_user.email) self.assertTrue(auth.user_has_role(ext_user, CourseStaffRole(self.course.id)))
self.assert_not_enrolled()
self.course.enable_subsection_gating = True self.save_course()
self.chapter = ItemFactory.create( parent_location=self.course.location, category='chapter', display_name='untitled chapter' )
self.clear_subs_content()
self.assertEqual(json.loads(resp.content).get('status'), 'Transcripts are supported only for "video" modules.')
self.assertIn("ufeff", filedata) self.ufeff_srt_file.write(filedata) self.ufeff_srt_file.seek(0)
link = reverse('download_transcripts') resp = self.client.get(link, {'locator': 'BAD_LOCATOR'}) self.assertEqual(resp.status_code, 404)
resp = self.create_xblock(category='vertical') usage_key = self.response_usage_key(resp)
resp = self.client.get(reverse_usage_url('xblock_handler', usage_key)) self.assertEqual(resp.status_code, 200)
self.assertIn('wrapper-xblock-message', html) self.assertNotRegexpMatches(html, r'wrapper-xblock[^-]+')
self.assertIn('<header class="xblock-header xblock-header-vertical">', html) self.assertIn('<article class="xblock-render">', html)
child_vertical_usage_key = self._create_vertical(parent_usage_key=root_usage_key) resp = self.create_xblock(parent_usage_key=child_vertical_usage_key, category='problem', boilerplate='multiplechoice.yaml') self.assertEqual(resp.status_code, 200)
html, __ = self._get_container_preview(root_usage_key)
self.assertIn('level-element', html)
root_usage_key = self._create_vertical()
resp = self.create_xblock(category='static_tab', parent_usage_key=course.location) usage_key = self.response_usage_key(resp)
resp = self.client.delete(reverse_usage_url('xblock_handler', usage_key)) self.assertEqual(resp.status_code, 204)
display_name = 'Nicely created' resp = self.create_xblock(display_name=display_name, category='chapter')
course = self.get_item_from_modulestore(self.usage_key) self.assertIn(chap_usage_key, course.children)
resp = self.create_xblock(parent_usage_key=chap_usage_key, category='vertical') vert_usage_key = self.response_usage_key(resp)
resp = self.create_xblock(category='problem', boilerplate='nosuchboilerplate.yaml') self.assertEqual(resp.status_code, 200)
resp = self.create_xblock(category='static_tab') usage_key = self.response_usage_key(resp)
new_tab = self.get_item_from_modulestore(usage_key) self.assertEquals(new_tab.display_name, 'Empty')
self.assertTrue( self._check_equality(source_usage_key, usage_key, parent_usage_key, check_asides=check_asides), "Duplicated item differs from original" )
original_item = self.get_item_from_modulestore(source_usage_key) duplicated_item = self.get_item_from_modulestore(duplicate_usage_key)
duplicated_item.location = original_item.location duplicated_item.display_name = original_item.display_name duplicated_item.parent = original_item.parent
data = { 'parent_locator': unicode(parent_usage_key), 'duplicate_source_locator': unicode(source_usage_key) } if display_name is not None: data['display_name'] = display_name
resp = self.create_xblock(parent_usage_key=self.usage_key, category='chapter') self.chapter_usage_key = self.response_usage_key(resp)
resp = self.create_xblock(parent_usage_key=self.chapter_usage_key, category='sequential') self.seq_usage_key = self.response_usage_key(resp)
resp = self.create_xblock(parent_usage_key=self.seq_usage_key, category='problem', boilerplate='multiplechoice.yaml') self.problem_usage_key = self.response_usage_key(resp)
self.create_xblock(parent_usage_key=self.chapter_usage_key, category='sequential2')
verify_order(self.html_usage_key, self.seq_usage_key, 2) verify_order(self.seq_usage_key, self.chapter_usage_key, 0)
verify_order(self.html_usage_key, self.usage_key)
verify_name(self.html_usage_key, self.seq_usage_key, "Duplicate of 'Text'")
verify_name(self.seq_usage_key, self.chapter_usage_key, "Duplicate of sequential")
verify_name(self.seq_usage_key, self.chapter_usage_key, "customized name", display_name="customized name")
resp = self.create_xblock(parent_usage_key=self.usage_key, category='chapter') self.chapter_usage_key = self.response_usage_key(resp)
resp = self.create_xblock(parent_usage_key=self.chapter_usage_key, category='sequential') self.seq_usage_key = self.response_usage_key(resp)
resp = self.create_xblock(parent_usage_key=self.seq_usage_key, category='problem', boilerplate='multiplechoice.yaml') self.problem_usage_key = self.response_usage_key(resp)
display_name = 'chapter created' resp = self.create_xblock(display_name=display_name, category='chapter') chap_usage_key = self.response_usage_key(resp)
resp = self.create_xblock(parent_usage_key=chap_usage_key, category='sequential') self.seq_usage_key = self.response_usage_key(resp) self.seq_update_url = reverse_usage_url("xblock_handler", self.seq_usage_key)
resp = self.client.delete(reverse_usage_url("xblock_handler", chapter1_usage_key)) self.assertEqual(resp.status_code, 204)
course = self.get_item_from_modulestore(self.usage_key) self.assertNotIn(chapter1_usage_key, course.children) self.assertIn(chapter2_usage_key, course.children)
children = self.get_item_from_modulestore(self.seq_usage_key).children self.assertEqual(unit1_usage_key, children[1]) self.assertEqual(unit2_usage_key, children[2])
resp = self.client.ajax_post( self.seq2_update_url, data={'children': [unicode(unit_1_key), unicode(unit_2_key)]} ) self.assertEqual(resp.status_code, 200)
self.assertListEqual( self.get_item_from_modulestore(self.seq2_usage_key).children, [unit_1_key, unit_2_key], ) self.assertListEqual( self.get_item_from_modulestore(self.seq_usage_key).children,
self.assertListEqual( self.get_item_from_modulestore(self.seq2_usage_key).children, [] )
self.assertListEqual( self.get_item_from_modulestore(self.seq2_usage_key).children, [unit_1_key, unit_2_key] )
self.assertFalse(self._is_location_published(self.problem_usage_key))
self.client.ajax_post( self.problem_update_url, data={'publish': 'make_public'} )
draft = self.get_item_from_modulestore(self.problem_usage_key, verify_is_draft=True) self.assertNotEqual(draft.data, published.data)
unit_update_url = reverse_usage_url('xblock_handler', unit_usage_key) self.assertFalse(self._is_location_published(unit_usage_key)) self.assertFalse(self._is_location_published(html_usage_key))
data={'metadata': {'user_partition_id': str(partition_id)}}
split_test = self.get_item_from_modulestore(self.split_test_usage_key, verify_is_draft=True) self.assertEqual(partition_id, split_test.user_partition_id) return split_test
self.assertEqual(-1, split_test.user_partition_id) self.assertEqual(0, len(split_test.children))
split_test = self._update_partition_id(0)
split_test = self._update_partition_id(0) self.assertEqual(2, len(split_test.children)) initial_vertical_0_location = split_test.children[0] initial_vertical_1_location = split_test.children[1]
split_test = self._update_partition_id(0) self.assertEqual(2, len(split_test.children)) initial_group_id_to_child = split_test.group_id_to_child
split_test = self._update_partition_id(0) self.assertEqual(2, len(split_test.children)) self.assertEqual(initial_group_id_to_child, split_test.group_id_to_child)
split_test = self._update_partition_id(0) self.assertEqual(2, len(split_test.children)) initial_group_id_to_child = split_test.group_id_to_child
split_test = self._update_partition_id(-50) self.assertEqual(2, len(split_test.children)) self.assertEqual(initial_group_id_to_child, split_test.group_id_to_child)
split_test = self._update_partition_id(0)
split_test = self._assert_children(2) group_id_to_child = split_test.group_id_to_child.copy() self.assertEqual(2, len(group_id_to_child))
split_test.add_missing_groups(self.request) split_test = self._assert_children(3) self.assertEqual(group_id_to_child, split_test.group_id_to_child)
self.descriptor = self.modulestore.return_value.get_item.return_value
req_factory_method = getattr(self.request_factory, method.lower()) request = req_factory_method('/dummy-url') request.user = self.user
XBlockDisableConfig.objects.create( disabled_create_blocks='', enabled=True )
with check_mongo_calls(chapter_queries_1): self.client.get(outline_url, HTTP_ACCEPT='application/json')
self.assertEqual(xblock_info['is_header_visible'], False) self.assertEqual(xblock_info['display_name'], 'Subsection - Entrance Exam')
self.assertIsNone(xblock_info.get('is_header_visible', None))
self.validate_xblock_info_consistency(xblock_info, has_child_info=has_child_info, course_outline=course_outline)
self.validate_xblock_info_consistency(xblock_info, has_child_info=has_child_info)
self.validate_xblock_info_consistency(xblock_info, has_child_info=has_child_info)
self.validate_xblock_info_consistency(xblock_info, has_child_info=True, has_ancestor_info=True)
self.validate_xblock_info_consistency(xblock_info)
self.assertEqual(xblock_info['enable_proctored_exams'], True)
self.assertEqual(xblock_info['is_proctored_exam'], True) self.assertEqual(xblock_info['is_time_limited'], True) self.assertEqual(xblock_info['default_time_limit_minutes'], 100)
return modulestore().get_item(child.location)
self._verify_visibility_state(xblock_info, VisibilityState.ready)
xblock_info = self._get_xblock_info(chapter.location) self._verify_visibility_state(xblock_info, VisibilityState.ready) self.assertFalse(course.self_paced)
course.self_paced = True self.store.update_item(course, self.user.id) self.assertTrue(course.self_paced)
xblock_info = self._get_xblock_info(chapter.location) self._verify_visibility_state(xblock_info, VisibilityState.live)
drag_handle_html = '<span data-tooltip="Drag to reorder" class="drag-handle action"></span>' self.assertIn(drag_handle_html, html)
add_button_html = '<div class="add-xblock-component new-component-item adding"></div>' if can_add: self.assertIn(add_button_html, html) else: self.assertNotIn(add_button_html, html)
{ u'description': 'Test description', u'version': CERTIFICATE_SCHEMA_VERSION },
{},
result = self.client.get_html(self._url()) self.assertNotIn('Test certificate', result.content)
from edxmako.shortcuts import render_to_response from mako.exceptions import TopLevelLookupException from django.http import HttpResponseNotFound
if not course_key_string or not action_state_id: return HttpResponseBadRequest()
return _dismiss_notification(request, action_state_id)
return HttpResponseBadRequest()
remove_all_instructors(action_state.course_key)
action_state.delete()
if isinstance(course.id, CCXLocator): return False
raise AccessListFallback
courses_list[course_key] = course
return [lib for lib in modulestore().get_libraries() if has_studio_read_access(user, lib.location.library_key)]
courses, in_process_course_actions = _accessible_courses_summary_list(request)
courses, in_process_course_actions = _accessible_courses_summary_list(request)
start = request.json.get('start', CourseFields.start.default) run = request.json.get('run')
fields.update({ 'language': getattr(settings, 'DEFAULT_COURSE_LANGUAGE', 'en'), 'cert_html_view_enabled': True, })
new_course = modulestore().create_course( org, number, run, user.id, fields=fields, )
add_instructor(new_course.id, user, user)
initialize_permissions(new_course.id, user) return new_course
if not has_studio_write_access(request.user, source_course_key): raise PermissionDenied()
store = modulestore() with store.default_store('split'): destination_course_key = store.make_course_key(org, number, run)
if store.has_course(destination_course_key, ignore_case=True): raise DuplicateCourseError(source_course_key, destination_course_key)
add_instructor(destination_course_key, request.user, request.user)
CourseRerunState.objects.initiated(source_course_key, destination_course_key, request.user, fields['display_name'])
fields['advertised_start'] = None
json_fields = json.dumps(fields, cls=EdxJSONEncoder) rerun_course.delay(unicode(source_course_key), unicode(destination_course_key), request.user.id, json_fields)
return JsonResponse({ 'url': reverse_url('course_handler'), 'destination_course_key': unicode(destination_course_key) })
if not has_studio_write_access(request.user, usage_key.course_key): raise PermissionDenied()
courses = [course for course in courses if course.id != course_key] if courses: courses = _remove_in_process_courses(courses, in_process_course_actions) settings_context.update({'possible_pre_requisite_courses': courses})
credit_requirements = get_credit_requirements(course_key) paired_requirements = {} for requirement in credit_requirements: namespace = requirement.pop("namespace") paired_requirements.setdefault(namespace, []).append(requirement)
show_min_grade_warning = False if course_module.minimum_grade_credit > 0 else True settings_context.update( { 'is_credit_course': True, 'credit_requirements': paired_requirements, 'show_min_grade_warning': show_min_grade_warning, } )
encoder=CourseSettingsEncoder
elif not entrance_exam_enabled and course_entrance_exam_present: delete_entrance_exam(request, course_key)
return JsonResponse( CourseDetails.update_from_json(course_key, request.json, request.user), encoder=CourseSettingsEncoder )
encoder=CourseSettingsEncoder
if 'minimum_grade_credit' in request.json: update_credit_course_requirements.delay(unicode(course_key))
for tab_type in CourseTabPluginManager.get_tab_types(): if not tab_type.is_dynamic and tab_type.is_default: tab_enabled = tab_type.is_enabled(course_module, user=request.user) update_tab(course_tabs, tab_type, tab_enabled)
if course_tabs != course_module.tabs: course_module.tabs = course_tabs
is_valid, errors, updated_data = CourseMetadata.validate_and_update_from_json( course_module, request.json, user=request.user, )
_refresh_course_tabs(request, course_module)
modulestore().update_item(course_module, request.user.id)
except (TypeError, ValueError, InvalidTabsException) as err: return HttpResponseBadRequest( django.utils.html.escape(err.message), content_type="text/plain" )
tid = random.choice(string.digits) + tid
tid = tid + random.choice(string.ascii_lowercase)
try: new_configuration = GroupConfiguration(request.body, course).get_user_partition() except GroupConfigurationsValidationError as err: return JsonResponse({"error": err.message}, status=400)
try: new_configuration = GroupConfiguration(request.body, course, group_configuration_id).get_user_partition() except GroupConfigurationsValidationError as err: return JsonResponse({"error": err.message}, status=400)
add_user_with_status_unrequested(user) course_creator_status = get_course_creator_status(user)
if library_key_string: return _display_library(library_key_string, request)
add_instructor(new_lib.location.library_key, request.user, request.user)
return redirect_with_get('login', request.GET, False)
next_url = request.GET.get('next') if next_url: return redirect(next_url) else: return redirect('/course/')
return redirect(reverse('cas-login'))
CONTENT_GROUP_CONFIGURATION_DESCRIPTION = 'The groups in this configuration can be mapped to cohort groups in the LMS.'
self.assertFalse(user(email).is_active)
resp = self.client.get(reverse('activate', kwargs={'key': activation_key})) return resp
self.assertTrue(user(email).is_active)
cache.clear()
self.assertEqual(resp.status_code, 400)
self.assertEqual(resp.status_code, 200)
resp = self._login(self.email, self.pw) data = parse_json(resp) self.assertFalse(data['success'])
self.login(self.email, self.pw)
self.login(self.email, self.pw)
self.login(self.email, self.pw)
self.client.logout() resp = self._activate_user(self.email) self.assertEqual(resp.status_code, 200)
expected = 'You can now <a href="' + reverse('login') + '">login</a>.' self.assertIn(expected, resp.content)
simple_auth_pages = ( '/home/', )
self.test_create_account()
self.client = AjaxEnabledTestClient()
print 'Not logged in' for page in auth_pages: print "Checking '{0}'".format(page) self.check_page_get(page, expected=302)
self.login(self.email, self.pw)
resp = self.client.get_html('/home/') self.assertEqual(resp.status_code, 302)
course_url = '/home/' resp = self.client.get_html(course_url) self.assertEquals(resp.status_code, 200)
time.sleep(2)
self.assertRedirects(resp, settings.LOGIN_REDIRECT_URL + '?next=/home/')
self.course.discussion_blackouts = [[]] self.assertTrue(self.course.forum_posts_allowed)
self.user.is_staff = True
course = CourseFactory.create() course.display_coursenumber = escaping_content
response = self.client.get('/home') self.assertEqual(response.status_code, 200) self.assert_no_xss(response, escaping_content)
courses_list, __ = _accessible_courses_list(self.request) self.assertEqual(len(courses_list), 1)
courses_list_by_groups, __ = _accessible_courses_list_from_groups(self.request) self.assertEqual(len(courses_list_by_groups), 1)
self.assertEqual(courses_list, courses_list_by_groups)
course_location = self.store.make_course_key('Org1', 'Course1', 'Run1') course = self._create_course_with_access_groups(course_location, self.user)
ccx_course_key = CCXLocator.from_course_locator(course.id, '1') self._add_role_access_to_user(self.user, ccx_course_key)
courses_list, __ = _accessible_courses_list_from_groups(self.request) self.assertEqual(len(courses_list), 1) self.assertNotIn( ccx_course_key, [course.id for course in courses_list] )
instructor_courses = UserBasedRole(self.user, CourseInstructorRole.ROLE).courses_with_role() staff_courses = UserBasedRole(self.user, CourseStaffRole.ROLE).courses_with_role() all_courses = (instructor_courses | staff_courses)
self.assertIn( ccx_course_key, [access.course_id for access in all_courses] )
courses_list, __ = _accessible_courses_list(self.request) self.assertEqual(courses_list, [])
courses_list_by_groups, __ = _accessible_courses_list_from_groups(self.request) self.assertEqual(courses_list_by_groups, [])
GlobalStaff().add_users(self.user) self.assertTrue(GlobalStaff().has_user(self.user))
courses_list_by_staff, __ = get_courses_accessible_to_user(self.request) self.assertEqual(len(courses_list_by_staff), TOTAL_COURSES_COUNT)
self.assertTrue(all(isinstance(course, CourseSummary) for course in courses_list_by_staff))
with check_mongo_calls(mongo_calls): _accessible_courses_summary_list(self.request)
courses_list, __ = _accessible_courses_list(self.request) self.assertEqual(courses_list, [])
courses_list_by_groups, __ = _accessible_courses_list_from_groups(self.request) self.assertEqual(courses_list_by_groups, []) self.assertEqual(courses_list, courses_list_by_groups)
courses_list, __ = _accessible_courses_list(self.request) self.assertEqual(len(courses_list), 1)
self.assertTrue(all(isinstance(course, CourseSummary) for course in courses_summary_list)) self.assertEqual(len(courses_summary_list), 1)
courses_list_by_groups, __ = _accessible_courses_list_from_groups(self.request) self.assertEqual(len(courses_list_by_groups), 1)
self.assertEqual(courses_list, courses_list_by_groups)
delete_course_and_groups(course_key, self.user.id)
courses_list, __ = _accessible_courses_list(self.request)
courses_summary_list, __ = _accessible_courses_summary_list(self.request)
courses_list_by_groups, __ = _accessible_courses_list_from_groups(self.request)
self.assertEqual( [len(courses_list), len(courses_list_by_groups), len(courses_summary_list)], [0, 0, 0] )
user_course_ids = random.sample(range(TOTAL_COURSES_COUNT), USER_COURSES_COUNT)
with Timer() as iteration_over_courses_time_1: courses_list, __ = _accessible_courses_list(self.request) self.assertEqual(len(courses_list), USER_COURSES_COUNT)
with Timer() as iteration_over_courses_time_2: courses_list, __ = _accessible_courses_list(self.request) self.assertEqual(len(courses_list), USER_COURSES_COUNT)
with Timer() as iteration_over_groups_time_1: courses_list, __ = _accessible_courses_list_from_groups(self.request) self.assertEqual(len(courses_list), USER_COURSES_COUNT)
with Timer() as iteration_over_groups_time_2: courses_list, __ = _accessible_courses_list_from_groups(self.request) self.assertEqual(len(courses_list), USER_COURSES_COUNT)
self.assertGreaterEqual(iteration_over_courses_time_1.elapsed, iteration_over_groups_time_1.elapsed) self.assertGreaterEqual(iteration_over_courses_time_2.elapsed, iteration_over_groups_time_2.elapsed)
with check_mongo_calls(courses_list_from_group_calls): _accessible_courses_list_from_groups(self.request)
role.add_users(self.user)
self.assertEqual(len(courses_list), 2) self.assertTrue(all(isinstance(course, CourseSummary) for course in courses_list))
for course in courses_in_progress: CourseRerunState.objects.initiated( sourse_course_key, destination_course_key=course.id, user=self.user, display_name="test course" )
repo_dir = os.path.abspath(git_export_utils.GIT_REPO_EXPORT_DIR) os.mkdir(repo_dir) self.addCleanup(shutil.rmtree, repo_dir)
self.assertTrue(self.store.has_item(course.id.make_usage_key('html', "multi_parent_html")))
course = self.create_course_with_orphans(module_store)
self.assertIn(orphan_vertical.location, self.store.get_orphans(course.id))
self.assertIn(multi_parent_html.location, orphan_vertical.children) self.assertIn(multi_parent_html.location, vertical1.children)
self.assertIn(orphan_chapter.location, self.store.get_orphans(course.id))
vertical1_parent = self.store.get_parent_location(vertical1.location) self.assertEqual(unicode(vertical1_parent), unicode(chapter1.location))
vertical1.children.append(html.location) self.store.update_item(vertical1, self.user.id)
html_parent = self.store.get_parent_location(html.location) self.assertEquals(unicode(html_parent), unicode(vertical1.location))
self.assertEqual(course.display_name, u"Φυσικά το όνομα Unicode")
print "static_asset_path = {0}".format(course.static_asset_path) self.assertEqual(course.static_asset_path, 'test_import_course')
all_assets, count = content_store.get_all_content_for_course(course.id) self.assertEqual(len(all_assets), 0) self.assertEqual(count, 0)
with check_exact_number_of_calls(store, 'refresh_cached_metadata_inheritance_tree', 28):
with check_exact_number_of_calls(store, '_get_cached_metadata_inheritance_tree', 1):
__, __, course = self.load_test_import_course(target_id=course_id, module_store=module_store)
__, __, re_course = self.load_test_import_course(target_id=course.id, module_store=module_store)
transcripts_utils.download_youtube_subs(good_youtube_sub, self.course, settings)
raise SkipTest
transcripts_utils.download_youtube_subs(good_youtube_sub, self.course, settings)
transcripts_utils.generate_subs_from_source(youtube_subs, 'SRT', srt_filedata, self.course)
self.html_unit = ItemFactory.create( parent_location=self.vertical.location, category="html", display_name="Html Content", modulestore=store, publish_item=False, )
added_to_index = self.reindex_course(store) self.assertEqual(added_to_index, 3) response = self.search() self.assertEqual(response["total"], 3)
self.publish_item(store, self.vertical.location) self.reindex_course(store) response = self.search() self.assertEqual(response["total"], 4)
self.publish_item(store, self.vertical.location) self.reindex_course(store) response = self.search() self.assertEqual(response["total"], 4)
self.publish_item(store, self.vertical.location) self.reindex_course(store) response = self.search() self.assertEqual(response["total"], 5)
self.reindex_course(store) response = self.search() self.assertEqual(response["total"], 1)
self.publish_item(store, self.vertical.location) self.reindex_course(store) response = self.search() self.assertEqual(response["total"], 4)
self.delete_item(store, self.html_unit.location) self.reindex_course(store) response = self.search() self.assertEqual(response["total"], 4)
self.publish_item(store, self.vertical.location) self.reindex_course(store) response = self.search() self.assertEqual(response["total"], 3)
self.publish_item(store, self.vertical.location) self.reindex_course(store) response = self.search() self.assertEqual(response["total"], 4)
self.publish_item(store, self.vertical.location) self.reindex_course(store) response = self.search() self.assertEqual(response["total"], 4)
self.publish_item(store, self.vertical.location) self.reindex_course(store) response = self.search() self.assertEqual(response["total"], 4)
sequential2 = ItemFactory.create( parent_location=self.chapter.location, category='sequential', display_name='Section 2', modulestore=store, publish_item=True, start=datetime(2015, 3, 1, tzinfo=UTC), )
new_indexed_count = self.index_recent_changes(store, before_time) self.assertEqual(new_indexed_count, 5)
indexed_count = self.reindex_course(store) self.assertEqual(indexed_count, 7)
CoursewareSearchIndexer.do_course_reindex(store, course.id)
course = store.get_course(course.id, depth=1)
chapter_to_delete = course.get_children()[0] self.delete_item(store, chapter_to_delete.location)
CoursewareSearchIndexer.do_course_reindex(store, course.id) deleted_count = 1 + load_factor + (load_factor ** 2) + (load_factor ** 3) self.assert_search_count(course_size - deleted_count)
print "Failed with load_factor of {}".format(load_factor)
cls.html_unit = ItemFactory.create( parent_location=cls.vertical.location, category="html", display_name="Html Content", publish_item=False, )
response = searcher.search( doc_type=CoursewareSearchIndexer.DOCUMENT_TYPE, field_dictionary={"course": unicode(self.course.id)} ) self.assertEqual(response["total"], 3)
response = searcher.search(field_dictionary={"library": library_search_key}) self.assertEqual(response["total"], 2)
WORKS_WITH_STORES = (ModuleStoreEnum.Type.split, )
data = "Some data" ItemFactory.create( parent_location=self.library.location, category="html", display_name="Html Content 3", data=data, modulestore=store, publish_item=False, )
self.delete_item(store, self.html_unit1.location) self.reindex_library(store) response = self.search() self.assertEqual(response["total"], 1)
with translation.override("fr"):
with wrap_ugettext_with_xyz(french_translation): self.assertEqual(i18n_service.ugettext(self.test_language), 'XYZ dummy language')
self.assertEqual(i18n_service.ugettext(self.test_language), 'dummy language')
self.user = User.objects.create_user(self.uname, self.email, self.password)
mongo_course1_id = self.import_and_populate_course()
mongo_course2_id = mongo_course1_id
course = CourseFactory.create( org=org, number=course_number, run=course_run, display_name=display_name, default_store=ModuleStoreEnum.Type.split )
self.assertEqual(result.get(), "succeeded") rerun_state = CourseRerunState.objects.find_first(course_key=split_rerun_id) self.assertEqual(rerun_state.state, CourseRerunUIStateManager.State.SUCCEEDED)
self._bind_module(lc_block) self.assertEqual(len(lc_block.children), num_to_create) self.assertEqual(len(lc_block.get_child_descriptors()), num_expected)
self._bind_module(lc_block) chosen_child = get_child_of_lc_block(lc_block) chosen_child_defn_id = chosen_child.definition_locator.definition_id lc_block.save()
lc_block = self._refresh_children(lc_block) check()
with modulestore().default_store(ModuleStoreEnum.Type.split): course = CourseFactory.create()
with modulestore().default_store(ModuleStoreEnum.Type.split): course = CourseFactory.create()
lc_block = self._add_library_content_block(course, self.lib_key) lc_block = self._refresh_children(lc_block) course_block = modulestore().get_item(lc_block.children[0])
with modulestore().default_store(ModuleStoreEnum.Type.split): course = CourseFactory.create()
lc_block = self._add_library_content_block(course, self.lib_key) lc_block = self._refresh_children(lc_block) self.assertEqual(len(lc_block.children), 1)
with modulestore().default_store(ModuleStoreEnum.Type.split): course = CourseFactory.create()
lc_block = self._add_library_content_block(course, self.lib_key) lc_block = self._refresh_children(lc_block) self.assertEqual(len(lc_block.children), 1)
with modulestore().default_store(ModuleStoreEnum.Type.split): course = CourseFactory.create()
lc_block = self._add_library_content_block(course, self.lib_key) lc_block = self._refresh_children(lc_block) self.assertEqual(len(lc_block.children), 2)
with modulestore().default_store(ModuleStoreEnum.Type.split): course = CourseFactory.create()
lc_block = self._add_library_content_block(course, self.lib_key) lc_block = self._refresh_children(lc_block) self.assertEqual(len(lc_block.children), 0)
self.client.logout()
library2_key = self._create_library(library="lib2") self._login_as_non_staff_user()
access_role(library2_key).add_users(self.non_staff_user)
lib_key_pacific = self._create_library(org="PacificX", library="libP") lib_key_atlantic = self._create_library(org="AtlanticX", library="libA")
self._login_as_non_staff_user()
org_access_role(lib_key_pacific.org).add_users(self.non_staff_user)
block = self._add_simple_content_block()
self._login_as_non_staff_user() self.assertFalse(self._can_access_library(self.library))
if use_org_level_role: OrgLibraryUserRole(self.lib_key.org).add_users(self.non_staff_user) else: LibraryUserRole(self.lib_key).add_users(self.non_staff_user)
block = self._add_simple_content_block() with modulestore().default_store(ModuleStoreEnum.Type.split): course = CourseFactory.create()
if library_role: library_role(self.lib_key).add_users(self.non_staff_user) if course_role: course_role(course.location.course_key).add_users(self.non_staff_user)
self._add_simple_content_block() with modulestore().default_store(ModuleStoreEnum.Type.split): course = CourseFactory.create()
if library_role: library_role(self.lib_key).add_users(self.non_staff_user) if course_role: course_role(course.location.course_key).add_users(self.non_staff_user)
self.problem = ItemFactory.create( category="problem", parent_location=self.library.location,
self.library = modulestore().get_library(self.lib_key)
with modulestore().default_store(ModuleStoreEnum.Type.split): self.course = CourseFactory.create()
for field_name in ["display_name", "weight"]: self.problem_in_course.fields[field_name].delete_from(self.problem_in_course)
modulestore().update_item(self.problem_in_course, self.user.id) self.problem_in_course = modulestore().get_item(self.problem_in_course.location)
self.library = store.get_library(self.lib_key)
self.lc_block = store.get_item(self.lc_block.location) self.problem_in_course = store.get_item(self.problem_in_course.location)
self.assertEqual(len(self.library.children), 2)
with modulestore().default_store(ModuleStoreEnum.Type.mongo): self.course = CourseFactory.create()
self.lc_block = self._add_library_content_block(self.course, self.lib_key)
self.assertEquals(self.get_about_page_link(), "//localhost:8000/courses/mitX/101/test/about")
link = utils.get_lms_link_for_item(location, True) self.assertEquals( link, "//preview.localhost/courses/mitX/101/test/jump_to/i4x://mitX/101/vertical/contacting_us" )
location = course_key.make_usage_key('course', 'test') link = utils.get_lms_link_for_item(location) self.assertEquals(link, "//localhost:8000/courses/mitX/101/test/jump_to/i4x://mitX/101/course/test")
vertical.start = self.future modulestore().update_item(vertical, self.dummy_user)
staff_lock = self._create_xblock_with_start_date( name + "_locked", start_date, publish, visible_to_staff_only=True ) self.assertFalse(utils.is_currently_visible_to_students(staff_lock))
self.sequential.children = [self.vertical.location] self.sequential = self.store.update_item(self.sequential, ModuleStoreEnum.UserID.test)
self.set_group_access(self.vertical, {1: []}) self.set_group_access(self.html, {2: None})
self.set_group_access(self.vertical, {1: []}) self.set_group_access(self.problem, {2: [3, 4]})
self._set_group_access({0: [1]}) expected[0]["groups"][1]["selected"] = True self.assertEqual(self._get_partition_info(), expected)
self._set_group_access({0: [3]})
partitions = self._get_partition_info() self.assertEqual(len(partitions), 1) self.assertEqual(partitions[0]["scheme"], "cohort")
partitions = self._get_partition_info() self.assertEqual(len(partitions), 1) self.assertEqual(partitions[0]["scheme"], "verification")
course = self.store.get_course(self.store.make_course_key( 'test_org', 'import_draft_order', 'import_draft_order' )) self.assertIsNotNone(course)
assets, count = content_store.get_all_content_for_course(course.id) self.assertEqual(count, 2)
for asset in assets: self.assertEquals(asset['displayname'], expected_displayname)
root_dir = path(mkdtemp_clean()) print 'Exporting to tempdir = {0}'.format(root_dir) export_course_to_xml(self.store, content_store, course.id, root_dir, 'test_export')
self.assertEqual(len(exported_static_files), 1) self.assertTrue(filesystem.exists(expected_displayname)) self.assertEqual(exported_static_files[0], expected_displayname)
shutil.rmtree(root_dir)
effort = self.store.get_item(course_key.make_usage_key('about', 'end_date')) self.assertEqual(effort.data, 'TBD')
all_assets, __ = content_store.get_all_content_for_course(course.id) self.assertGreater(len(all_assets), 0)
all_thumbnails = content_store.get_all_content_thumbnails_for_course(course.id) self.assertGreater(len(all_thumbnails), 0)
with filesystem.open('updates.html', 'r') as course_policy: on_disk = course_policy.read() self.assertEqual(course_updates.data, on_disk)
root_dir = path(mkdtemp_clean()) print 'Exporting to tempdir = {0}'.format(root_dir) export_course_to_xml(self.store, content_store, course.id, root_dir, 'test_export')
with filesystem.open('updates.html', 'r') as grading_policy: on_disk = grading_policy.read() self.assertEqual(on_disk, course_updates.data)
html_module_location = course_key.make_usage_key('html', 'nonportable_link') html_module = self.store.get_item(html_module_location) self.assertIn('/jump_to_id/nonportable_link', html_module.data)
export_course_to_xml(self.store, content_store, course_id, root_dir, 'test_export')
self.verify_content_existence(self.store, root_dir, course_id, 'tabs', 'static_tab', '.html')
self.verify_content_existence(self.store, root_dir, course_id, 'about', 'about', '.html')
filesystem = OSFS(root_dir / 'test_export/policies/2012_Fall') self.assertTrue(filesystem.exists('grading_policy.json'))
with filesystem.open('grading_policy.json', 'r') as grading_policy: on_disk = loads(grading_policy.read()) self.assertEqual(on_disk, course.grading_policy)
self.assertTrue(filesystem.exists('policy.json'))
self.store.delete_course(course_id, self.user.id)
self.check_import(root_dir, content_store, course_id)
new_course_id = self.store.make_course_key('anotherX', 'anotherToy', 'Someday') self.check_import(root_dir, content_store, new_course_id) self.assertCoursesEqual(course_id, new_course_id)
import_course_from_xml( self.store, self.user.id, root_dir, ['test_export'], static_content_store=content_store, target_id=course_id, )
self.check_populated_course(course_id)
verticals = self.store.get_items(course_id, qualifiers={'category': 'vertical'})
export_course_to_xml(self.store, content_store, course_id, root_dir, 'test_export')
export_course_to_xml(self.store, content_store, course_id, root_dir, 'test_export')
word_cloud = ItemFactory.create(parent_location=parent.location, category="word_cloud", display_name="untitled") del word_cloud.data self.assertEquals(word_cloud.data, '')
root_dir = path(mkdtemp_clean()) export_course_to_xml(self.store, content_store, course_id, root_dir, 'test_roundtrip')
import_course_from_xml(self.store, self.user.id, root_dir) imported_word_cloud = self.store.get_item(course_id.make_usage_key('word_cloud', 'untitled'))
self.assertEquals(imported_word_cloud.data, '')
root_dir = path(mkdtemp_clean()) export_course_to_xml(self.store, content_store, course_id, root_dir, 'test_roundtrip')
import_course_from_xml(self.store, self.user.id, root_dir, create_if_not_present=True)
open_assessment = ItemFactory.create( parent_location=vertical.location, category="openassessment", display_name="untitled", ) draft_open_assessment = self.store.convert_to_draft( open_assessment.location, self.user.id )
self.assertFalse(hasattr(draft_open_assessment, "xml_attributes"))
root_dir = path(mkdtemp_clean()) export_course_to_xml( self.store, content_store, course_id, root_dir, 'test_no_xml_attributes' )
resp = self.client.get_html(get_url('container_handler', self.vert_loc)) self.assertEqual(resp.status_code, 200)
self.assertNotIn(malicious_code, resp.content)
self.check_components_on_page( ADVANCED_COMPONENT_TYPES, ['Word cloud', 'Annotation', 'Text Annotation', 'Video Annotation', 'Image Annotation', 'split_test'], )
assets, count = content_store.get_all_content_for_course(self.course.id) self.assertEqual(count, 1) display_name = assets[0]['displayname'] self.assertEqual(display_name, invalid_displayname)
self.assertTrue(filesystem.exists(exported_asset_name)) self.assertEqual(len(exported_static_files), 1)
shutil.rmtree(root_dir)
assets, count = content_store.get_all_content_for_course(self.course.id) self.assertEqual(count, 2)
for asset in assets: self.assertEquals(asset['displayname'], asset_displayname)
filesystem = OSFS(root_dir / 'test_export/static') exported_static_files = filesystem.listdir() self.assertTrue(filesystem.exists(asset_displayname)) self.assertEqual(len(exported_static_files), 1)
shutil.rmtree(root_dir)
usage_key = self.course.id.make_usage_key('vertical', None)
problem = self.store.get_item(problem.location)
self.store.publish(problem.location, self.user.id)
problem = self.store.get_item(problem.location)
self.store.convert_to_draft(problem.location, self.user.id) problem = self.store.get_item(problem.location)
problem.save() self.assertIn('graceperiod', own_metadata(problem)) self.assertEqual(problem.graceperiod, new_graceperiod)
problem = self.store.get_item(problem.location)
self.store.publish(problem.location, self.user.id)
self.store.convert_to_draft(problem.location, self.user.id) problem = self.store.get_item(problem.location)
num_drafts = self._get_draft_counts(self.course) self.assertEqual(num_drafts, 0)
self.store.convert_to_draft(self.problem.location, self.user.id)
draft_problem = self.store.get_item(self.problem.location) self.assertTrue(getattr(draft_problem, 'is_draft', False))
course = self.store.get_course(self.course.id, depth=None)
num_drafts = self._get_draft_counts(course) self.assertEqual(num_drafts, 1)
self.assertGreater(len(items[0].question), 0)
resp = self.client.get_json( get_url('xblock_view_handler', self.vert_loc, kwargs={'view_name': 'container_preview'}) ) self.assertEqual(resp.status_code, 200)
chapter = self.store.get_item(self.chapter_loc) self.assertIn(self.seq_loc, chapter.children)
self.assertNotIn(self.seq_loc, chapter.children)
content = contentstore().find(asset_key, throw_on_not_found=False) self.assertIsNone(content)
content = contentstore('trashcan').find(asset_key, throw_on_not_found=False) self.assertIsNotNone(content)
restore_asset_from_trashcan(unicode(asset_key))
content = contentstore('trashcan').find(asset_key, throw_on_not_found=False) self.assertIsNotNone(content)
all_assets, __ = contentstore('trashcan').get_all_content_for_course(self.course.id) self.assertGreater(len(all_assets), 0)
empty_asset_trashcan([self.course.id])
all_assets, count = contentstore('trashcan').get_all_content_for_course(self.course.id) self.assertEqual(len(all_assets), 0) self.assertEqual(count, 0)
with self.assertRaises(InvalidVersionError): self.store.convert_to_draft(self.chapter_loc, self.user.id)
with self.store.default_store(ModuleStoreEnum.Type.split): resp = self.client.get_html('/c4x/InvalidOrg/InvalidCourse/asset/invalid.png') self.assertEqual(resp.status_code, 404)
self.store.delete_course(self.course.id, self.user.id)
items = self.store.get_items(self.course.id) self.assertEqual(len(items), 0)
assets, count = contentstore().get_all_content_for_course(self.course.id) self.assertEqual(len(assets), 0) self.assertEqual(count, 0)
resp = self.client.get(get_url('xblock_handler', handouts.location))
self.assertIn(self.seq_loc, course.system.module_data)
self.assertNotIn(self.vert_loc, course.system.module_data)
self.assertTrue(CourseEnrollment.is_enrolled(self.user, course_key)) return test_course_data
with self.assertRaises(ItemNotFoundError): are_permissions_roles_seeded(course_id)
course_id = _get_course_id(self.store, test_course_data) delete_course_and_groups(course_id, self.user.id) with self.assertRaises(ItemNotFoundError): are_permissions_roles_seeded(course_id)
self.assertTrue(are_permissions_roles_seeded(second_course_id))
self.assertTrue(CourseEnrollment.is_enrolled(self.user, course_id)) self.assertTrue(self.user.roles.filter(name="Student", course_id=course_id))
self.assertTrue(CourseEnrollment.is_enrolled(self.user, course_id)) self.assertTrue(self.user.roles.filter(name="Student", course_id=course_id))
instructor_role = CourseInstructorRole(course_id)
delete_course_and_groups(course_id, self.user.id)
self.user = User.objects.get_by_natural_key(self.user.natural_key()[0])
pass
self.assertEqual(initially_enrolled, CourseEnrollment.is_enrolled(self.user, course_id))
resp = self.client.get_html( get_url(handler, course_key, 'course_key_string') ) self.assertEqual(resp.status_code, 200)
delete_item(category='html', name='test_html')
delete_item(category='vertical', name='test_vertical')
delete_item(category='sequential', name='test_sequence')
delete_item(category='chapter', name='chapter_2')
self.assertGreater(len(modules), 10)
course_module = self.store.get_course(target_id)
self.assertTrue(did_load_item)
discussion_item = self.store.create_item(self.user.id, course.id, 'discussion', 'new_component')
fetched = self.store.get_item(discussion_item.location)
refetched = self.store.get_item(discussion_item.location)
self.assertEqual(fetched.discussion_id, discussion_item.discussion_id) self.assertEqual(fetched.discussion_id, refetched.discussion_id)
self.assertNotEqual(discussion_item.discussion_id, '$$GUID$$')
for vertical in verticals: self.assertEqual(course.xqa_key, vertical.xqa_key) self.assertEqual(course.start, vertical.start)
parent = verticals[0] new_block = self.store.create_child( self.user.id, parent.location, 'html', 'new_component' )
new_block = self.store.get_item(new_block.location)
self.assertEqual(parent.graceperiod, new_block.graceperiod) self.assertEqual(parent.start, new_block.start) self.assertEqual(course.start, new_block.start)
new_block.graceperiod = timedelta(1) self.store.update_item(new_block, self.user.id)
new_block = self.store.get_item(new_block.location)
courses = import_course_from_xml( self.store, self.user.id, TEST_DATA_DIR, ['conditional_and_poll'], static_content_store=content_store, create_if_not_present=True )
self.assertEqual(course.course_image, 'images_course_image.jpg')
asset_key = course.id.make_asset_key('asset', course.course_image) content_store.find(asset_key)
rerun_course_data = {'source_course_key': unicode(source_course_key)} if not destination_course_data: destination_course_data = self.destination_course_data rerun_course_data.update(destination_course_data) destination_course_key = _get_course_id(self.store, destination_course_data)
course_url = get_url('course_handler', destination_course_key, 'course_key_string') response = self.client.ajax_post(course_url, rerun_course_data)
self.assertTrue(CourseEnrollment.is_enrolled(self.user, destination_course_key))
self.assertInCourseListing(source_course_key) self.assertInCourseListing(destination_course_key)
source_videos = list(get_videos_for_course(source_course.id)) target_videos = list(get_videos_for_course(destination_course_key)) self.assertEqual(1, len(source_videos)) self.assertEqual(source_videos, target_videos)
self.assertFalse(CourseEnrollment.is_enrolled(self.user, non_existent_course_key))
self.assertInCourseListing(existent_course_key)
self.assertInUnsucceededCourseActions(destination_course_key)
with self.assertRaises(CourseActionStateItemNotFoundError): CourseRerunState.objects.find_first(course_key=destination_course_key)
self.assertInCourseListing(existent_course_key)
self.assertEquals(source_course.wiki_slug, source_wiki_slug)
self.assertEquals(destination_course.wiki_slug, destination_wiki_slug)
self._test_page("/logout", 302)
test_course = self.store.get_course(test_course.id.version_agnostic()) self.assertIn(test_chapter.location, test_course.children)
class DraftReorderTestCase(ModuleStoreTestCase):
exam_review_policy = get_review_policy_by_exam_id(exam['id']) self.assertEqual(exam_review_policy['review_policy'], sequence.exam_review_rules)
self.assertEqual(exam['hide_after_due'], sequence.hide_after_due)
sequence.default_time_limit_minutes += sequence.default_time_limit_minutes self.store.update_item(sequence, self.user.id)
listen_for_course_publish(self, self.course.id)
self._verify_exam_data(sequence, expected_active)
listen_for_course_publish(self, self.course.id)
exams = get_all_exams_for_course(unicode(self.course.id)) self.assertEqual(len(exams), 1)
exams = get_all_exams_for_course(unicode(self.course.id)) self.assertEqual(len(exams), expected_count)
self.user = UserFactory(is_staff=True) self.client = AjaxEnabledTestClient() self.client.login(username=self.user.username, password='test')
self.course_key = self.store.make_course_key('Org_1', 'Course_1', 'Run_1') self._create_course_with_given_location(self.course_key)
self.assertTrue(CourseEnrollment.is_enrolled(self.user, self.course_key))
self.assertTrue(self.user.roles.filter(name="Student", course_id=self.course_key))
self.assertTrue(CourseEnrollment.is_enrolled(self.user, self.course_key))
self.assertTrue(self.user.roles.filter(name="Student", course_id=self.course_key))
self.assertTrue(CourseEnrollment.is_enrolled(self.user, self.course_key)) self.assertTrue(self.user.roles.filter(name="Student", course_id=self.course_key))
delete_course_and_groups(self.course_key, self.user.id) resp = self._create_course_with_given_location(self.course_key) self.assertEqual(resp.status_code, 200)
self.assertTrue(CourseEnrollment.is_enrolled(self.user, self.course_key))
self.assertTrue(self.user.roles.filter(name="Student", course_id=self.course_key))
self.assertTrue(CourseEnrollment.is_enrolled(self.user, self.course_key)) delete_course_and_groups(self.course_key, self.user.id)
new_course_key = self.course_key.replace(course=self.course_key.course.upper()) resp = self._create_course_with_given_location(new_course_key) self.assertEqual(resp.status_code, 200)
self.assertTrue( self.user.roles.filter(name="Student", course_id=new_course_key) )
private_vertical = self.store.create_item(self.user.id, course_id, 'vertical', self.PRIVATE_VERTICAL) self.assertFalse(self.store.has_published_version(private_vertical))
content_store.set_attr(self.LOCKED_ASSET_KEY, 'locked', True)
vertical = get_and_verify_publish_state('vertical', self.TEST_VERTICAL, True) for child in vertical.get_children(): verify_item_publish_state(child, True)
self.assertTrue(getattr(vertical, "is_draft", False))
sequential = get_and_verify_publish_state('sequential', self.SEQUENTIAL, True) self.assertFalse(getattr(sequential, "is_draft", False))
private_vertical = get_and_verify_publish_state('vertical', self.PRIVATE_VERTICAL, False)
public_vertical = get_and_verify_publish_state('vertical', self.PUBLISHED_VERTICAL, True)
draft_html = self.store.get_item(course_id.make_usage_key('html', self.DRAFT_HTML)) self.assertTrue(getattr(draft_html, 'is_draft', False))
draft_video = self.store.get_item(course_id.make_usage_key('video', self.DRAFT_VIDEO)) self.assertTrue(getattr(draft_video, 'is_draft', False))
for vert in [vertical, private_vertical, public_vertical]: self.assertIn(vert.location, sequential.children)
self.assertIn(draft_html.location, public_vertical.children)
self.assertIn(draft_video.location, public_vertical.children)
course = self.store.get_course(course_id) self.assertGreater(len(course.textbooks), 0)
self.assertAssetsEqual(self.LOCKED_ASSET_KEY, self.LOCKED_ASSET_KEY.course_key, course_id)
html_module = self.store.get_item(course_id.make_usage_key('html', 'nonportable')) self.assertIn('/static/foo.jpg', html_module.data)
self.assertEqual( self.store.has_published_version(course1_item), self.store.has_published_version(course2_item) )
self.assertEqual(own_metadata(course1_item), own_metadata(course2_item))
self.assertEqual([], course_detail_json['pre_requisite_courses'])
resp = self.client.get_json(url) course_detail_json = json.loads(resp.content) self.assertEqual(pre_requisite_course_keys, course_detail_json['pre_requisite_courses'])
self.assertEquals(course.entrance_exam_minimum_score_pct, .5)
altered_grader = CourseGradingModel.fetch(self.course.id) self.assertDictEqual(test_grader.grade_cutoffs, altered_grader.grade_cutoffs, "Noop update")
altered_grader = CourseGradingModel.fetch(self.course.id) self.assertEqual(test_grader.grace_period, altered_grader.grace_period, "Noop update")
descriptor = modulestore().get_item(self.course.location) section_grader_type = CourseGradingModel.get_section_grader_type(self.course.location)
self.assertGreater(len(sections), 0, "No sections found")
self.assertFalse(is_valid) self.assertEqual(len(errors), 3) self.assertFalse(test_model)
fresh = modulestore().get_course(self.course.id) test_model = CourseMetadata.fetch(fresh)
self.assertNotIn(self.notes_tab, self.course.tabs)
course = modulestore().get_course(self.course.id) self.assertNotIn("notes", course.advanced_modules)
if world.is_css_present('div#login_error'): assert_false(world.css_visible('div#login_error'))
files = files_string.split(",") upload_css = 'a.upload-button' world.css_click(upload_css)
assert world.is_css_not_present(ASSET_NAMES_CSS)
_write_test_file(file_name, "This is an arbitrary file for testing uploads")
with open(os.path.abspath(path), 'w') as cur_file: cur_file.write(text)
world.browser.driver.get(url) assert_equal(world.css_text('body'), expected_text)
try: ranges.last.value = 'Failure' except InvalidElementStateException:
ranges = world.css_find(range_css) assert_equal(len(ranges), 2) assert_not_equal(ranges.last.value, 'Failure')
ele.value = grace_period
world.wait_for( lambda _: world.css_has_value(grace_period_css, grace_period) )
assert_true(world.css_contains_text(problem_css, category))
if world.is_css_present('{}.is-shown'.format(saving_mini_css)): world.css_find('{}.is-hiding'.format(saving_mini_css))
label = world.css_html(".level-element>header>div>div>span.xblock-display-name") assert_equal(display_name, label)
_verify_page_names('First', 'Empty')
draggables = world.css_find(css_class + ' .drag-handle') source = draggables.first target = draggables.last
world.visit('/') signin_css = 'a.action-signin' assert world.is_css_present(signin_css)
world.visit('/') assert_in(uname, world.css_text('span.account-username', timeout=10))
world.visit('/') course_link_css = 'a.course-link' world.css_click(course_link_css) course_title_css = 'span.course-title' assert_true(world.is_css_present(course_title_css))
if key is not None:
attach_file(filename, sub_path) modal_css = 'div.wrapper-modal-window-assetupload' button_css = '{} .action-upload'.format(modal_css) world.css_click(button_css)
world.wait_for_ajax_complete()
assert world.is_css_not_present(modal_css, wait_time=10)
roles = (CourseStaffRole, CourseInstructorRole)
index = world.get_setting_entry_index(DISPLAY_NAME) world.set_field_value(index, '3.4') verify_modified_display_name()
world.click_course_content() outline_css = 'li.nav-course-courseware-outline a' world.css_click(outline_css)
DELAY = 0.5
video_url = world.browser.url
_step.given('I have uploaded subtitles "{}"'.format(sub_id))
world.visit(video_url)
_step.given('I edit the component') world.wait_for_ajax_complete() _step.given('I save changes')
DELAY = 0.5
world.trigger_event(SELECTORS['url_inputs'], event='input', index=index)
DEPRECATED_SETTINGS = ["CSS Class for Course Reruns", "Hide Progress Tab", "XQA Key"]
assert_policy_entries( [ADVANCED_MODULES_KEY, DISPLAY_NAME_KEY, "Show Calculator"], ["[]", DISPLAY_NAME_VALUE, "false"])
key = world.css_value(KEY_CSS, index=i) if key == expected_key: return i
verify_date_or_time(COURSE_START_DATE_CSS, '12/20/2013') verify_date_or_time(COURSE_START_TIME_CSS, DUMMY_TIME)
verify_date_or_time(COURSE_START_TIME_CSS, DUMMY_TIME)
e._element.send_keys(Keys.ENTER)
assert_true(world.css_has_value(css, date_or_time))
verify_date_or_time(COURSE_END_TIME_CSS, DEFAULT_TIME) verify_date_or_time(ENROLLMENT_START_TIME_CSS, DEFAULT_TIME) verify_date_or_time(ENROLLMENT_END_TIME_CSS, DUMMY_TIME)
assert_equal('Paragraph', dropdowns[0].text) assert_equal('Font Family', dropdowns[1].text)
world.css_click(".mce-i-none")
assert_false(world.css_has_class('.CodeMirror', 'is-inactive')) assert_true(world.is_css_not_present('.tiny-mce')) type_in_codemirror(0, text)
world.css_click(button_class) perform_action_in_plugin(action)
buttons = world.css_find('div.mce-widget>button')
world.wait_for_visible('.mce-window')
action()
world.css_click('.mce-primary')
module_count_before = len(world.browser.find_by_css(module_css))
world.disable_jquery_animations() world.css_click(component_button_css)
tab2_css = 'div.ui-tabs-panel#tab2' world.wait_for_visible(tab2_css)
buttons = world.css_find('div.new-component-{} button'.format(category))
matched_buttons = [btn for btn in buttons if btn.text == component_type]
assert_equal(len(matched_buttons), 1) return matched_buttons[0]
world.retry_on_exception( _click_advanced, ignored_exceptions=AssertionError, )
link = world.retry_on_exception( lambda: _find_matching_button(category, component_type), ignored_exceptions=AssertionError )
world.retry_on_exception(lambda: link.click())
settings_button = world.browser.find_by_css('.settings-button') if len(settings_button) > 0: world.css_click('.settings-button')
reload_the_page(step) edit_component_and_select_settings()
reload_the_page(step)
CourseInstructorRole(course_key).add_users(new_instructor) auth.add_users(requesting_user, CourseStaffRole(course_key), new_instructor)
seed_permissions_roles(course_key)
CourseEnrollment.enroll(user_who_created_course, course_key)
assign_default_role(course_key, user_who_created_course)
try: remove_all_instructors(course_key) except Exception as err: log.error("Error in deleting course groups for {0}: {1}".format(course_key, err))
about_base = marketing_urls.get('ROOT', None)
about_base = re.sub(r"^https?://", "", about_base)
except ItemNotFoundError: return False
if published.visible_to_staff_only: return False
if 'detached' not in published._class_tags and published.start is not None: return datetime.now(UTC) > published.start
return True
if xblock.category == 'chapter': return xblock
if not parent_location: return xblock
if xblock.fields['visible_to_staff_only'].is_set_on(xblock): return xblock
if xblock.category == 'chapter': return None
if not parent_location: return None
if p.active and p.groups and (schemes is None or p.scheme.name in schemes):
groups = [] for g in p.groups:
partitions.append({ "id": p.id, "name": p.name, "scheme": p.scheme.name, "groups": groups, })
for p in user_partitions: has_selected = any(g["selected"] for g in p["groups"]) has_selected_groups = has_selected_groups or has_selected
from edxval.api import copy_course_videos
source_course_key = CourseKey.from_string(source_course_key_string) destination_course_key = CourseKey.from_string(destination_course_key_string) fields = deserialize_fields(fields) if fields else None
store = modulestore() with store.default_store('split'): store.clone_course(source_course_key, destination_course_key, user_id, fields=fields)
initialize_permissions(destination_course_key, User.objects.get(id=user_id))
CourseRerunState.objects.succeeded(course_key=destination_course_key)
copy_course_videos(source_course_key, destination_course_key)
CourseRerunState.objects.failed(course_key=destination_course_key) logging.exception(u'Course Rerun Error') return "duplicate course"
CourseRerunState.objects.failed(course_key=destination_course_key) logging.exception(u'Course Rerun Error')
modulestore().delete_course(destination_course_key, user_id)
pass
time_isoformat.split('+')[0], "%Y-%m-%dT%H:%M:%S.%f"
indexed_count = { "count": 0 }
items_index = []
if not item_index_dictionary and not item.has_children: return
cls.supplemental_index_information(modulestore, structure)
for item in structure.get_children(): prepare_item_index(item, groups_usage_info=groups_usage_info) searcher.index(cls.DOCUMENT_TYPE, items_index) cls.remove_deleted_items(searcher, structure_key, indexed_items)
log.exception( "Indexing error encountered, courseware index may be out of date %s - %r", structure_key, err ) error_list.append(_('General indexing error occurred'))
FROM_ABOUT_INFO = from_about_dictionary FROM_COURSE_PROPERTY = from_course_property FROM_COURSE_MODE = from_course_mode
about_dictionary = { item.location.name: item.data for item in modulestore.get_items(course.id, qualifiers={"category": "about"}) }
try: section_content = about_information.get_value(**about_context)
try: searcher.index(cls.DISCOVERY_DOCUMENT_TYPE, [course_info])
CONFIG_FILE = open(settings.REPO_ROOT / "docs" / "cms_config.ini") CONFIG = ConfigParser.ConfigParser() CONFIG.readfp(CONFIG_FILE)
from safe_lxml import defuse_xml_libs defuse_xml_libs()
import contracts contracts.disable_all()
from django.core.wsgi import get_wsgi_application application = get_wsgi_application()
call_command('flush', verbosity=0, interactive=False, load_initial_data=False)
cls.clear_caches()
cls.clear_caches()
for cache in settings.CACHES: caches[cache].clear()
sites.models.SITE_CACHE.clear()
def default(self, noDefaultEncodingObj): return noDefaultEncodingObj.value.replace("<script>", "sample-encoder-was-here")
for gen in xrange(3): gc.collect(gen) scanner.dump_all_objects( format_str.format("gc-gen-{}".format(gen)) )
if not hasattr(request, '_xblock_token'): request._xblock_token = uuid.uuid1().get_hex()
class_name = getattr(block, 'unmixed_class', block.__class__).__name__
css_classes.append('xmodule_display')
css_classes.append('xmodule_edit')
template_context['js_init_parameters'] = json.dumps(frag.json_init_args).replace("/", r"\/")
template_context['js_init_parameters'] = json.dumps(frag.json_init_args).replace("/", r"\/")
cursor.execute(query, [module_id.to_deprecated_string()])
edit_link = "//" + settings.CMS_BASE + '/container/' + unicode(block.location)
return wrap_fragment( frag, render_to_string( "edit_unit_link.html", {'frag_content': frag.content, 'edit_link': edit_link} ) )
filepath = filename
giturl = "" data_dir = ""
content = html_parsed[0].tail
return list(reversed(course_updates.items))
get_parents=None, get_children=get_children, filter_func=filter_func,
filter_func = filter_func or (lambda __: True)
stack = deque([_Node(start_node, get_children)])
visited = set()
current = stack[-1]
if current.node in visited or not filter_func(current.node): stack.pop() continue
try: next_child = current.children.next()
yield current.node visited.add(current.node) stack.pop()
stack.append(_Node(next_child, get_children))
filter_func = filter_func or (lambda __: True)
stack = deque([start_node])
while stack:
current_node = stack.pop()
if get_parents and current_node != start_node: parents = get_parents(current_node)
if not all(parent in yield_results for parent in parents): continue
elif not yield_descendants_of_unyielded and not any(yield_results[parent] for parent in parents): continue
if current_node not in yield_results:
unvisited_children = list(get_children(current_node))
unvisited_children = list( child for child in get_children(current_node) if child not in yield_results )
unvisited_children.reverse() stack.extend(unvisited_children)
should_yield_node = filter_func(current_node) if should_yield_node: yield current_node
yield_results[current_node] = should_yield_node
full_name = UserProfile.objects.get(user=user).name
full_name = UserProfile.objects.get(user=user).name
if local_loglevel not in LOG_LEVELS: local_loglevel = 'INFO'
service_variant = ''
url = settings.STATIC_URL + settings.DEFAULT_COURSE_ABOUT_IMAGE_URL
print public_key_str print private_key_str
try: mod = import_module(app + '.startup') except ImportError: continue
if hasattr(mod, 'run'): mod.run()
(plaintext, err_from_stderr) = process.communicate( input=html_message.encode('utf-8') )
COURSE_TAB_NAMESPACE = 'openedx.course_tab'
self.prefix = os.path.join(self.RESOURCE_PREFIX, module)
try: try: return super(PutAsCreateMixin, self).update(request, *args, **kwargs) except Http404: return super(PutAsCreateMixin, self).create(request, *args, **kwargs)
except ValidationError as err: return Response(err.messages, status=status.HTTP_400_BAD_REQUEST)
self.check_permissions(clone_request(self.request, 'POST'))
raise
object_results = map(ordered_objects, search_queryset_pks) paged_results.object_list = object_results
if not user or user.is_anonymous(): return None
return (user, None)
raise
if not hasattr(cls, "_plugins"): plugins = {}
exc = drf_exceptions.AuthenticationFailed({u'error_code': -1}) self.assertEqual(exc.detail, u"{u'error_code': -1}")
self.user.is_active = False self.user.save()
self.assertNotIn('error_code', json.loads(response.content))
self.assertNotIn('error_code', json.loads(response.content))
if field.source is None: field.bind(self.field_name, self)
GATING_NAMESPACE_QUALIFIER = '.gating'
log.warning("Multiple gating milestones found for prereq UsageKey %s", prereq_content_key)
return [ m['content_id'] for m in find_gating_milestones( course.id, None, 'requires', {'id': user.id} ) ]
self.chapter1 = ItemFactory.create( parent_location=self.course.location, category='chapter', display_name='untitled chapter 1' )
STRING_PAYLOAD = 'string_payload'
ROOT_EXTRA_FIELDS = 'root_extra_fields'
CONTEXT_EXTRA_FIELDS = 'context_extra_fields'
PAYLOAD_EXTRA_FIELDS = 'payload_extra_fields'
return { cls.STRING_PAYLOAD, cls.ROOT_EXTRA_FIELDS, cls.CONTEXT_EXTRA_FIELDS, }
if EventMatchTolerates.STRING_PAYLOAD in tolerate: expected = parse_event_payload(expected) actual = parse_event_payload(actual)
self._assert_num_requests(1)
get_edx_api_data(program_config, self.user, 'programs', cache_key=cache_key) get_edx_api_data(program_config, self.user, 'programs', resource_id=resource_id, cache_key=cache_key)
actual_collection = get_edx_api_data(program_config, self.user, 'programs', cache_key=cache_key) self.assertEqual(actual_collection, expected_collection)
self._assert_num_requests(2)
test_uuid = uuid.UUID(token, version=1) self.assertEqual(token, test_uuid.hex)
tip = resolved(joinpath(base, dirname(info.name))) return _is_bad_path(info.linkname, base=tip)
if not base.startswith(resolved(settings.DATA_DIR)): raise SuspiciousOperation("Attempted to import course outside of data dir")
from logging import getLogger
timeout_in_seconds = 60 * 60 * 24 self._cache.set( self._encode_root_cache_key(block_structure.root_block_usage_key), zp_data_to_cache, timeout=timeout_in_seconds, )
block_relations, transformer_data, block_data_map = zunpickle(zp_data_from_cache) block_structure = BlockStructureModulestoreData(root_block_usage_key) block_structure._block_relations = block_relations block_structure._transformer_data = transformer_data block_structure._block_data_map = block_data_map
if xblock.location in blocks_visited: return
blocks_visited.add(xblock.location)
for child in xblock.get_children():
TRANSFORMER_VERSION_KEY = '_version'
self.parents = []
self.children = []
self.root_block_usage_key = root_block_usage_key
self._block_relations = defaultdict(_BlockRelations)
self._add_block(self._block_relations, root_block_usage_key)
pruned_block_relations = defaultdict(_BlockRelations) old_block_relations = self._block_relations
for block_key in self.post_order_traversal(): if block_key in old_block_relations: self._add_block(pruned_block_relations, block_key)
for child in old_block_relations[block_key].children: if child in pruned_block_relations: self._add_to_relations(pruned_block_relations, block_key, child)
self._block_relations = pruned_block_relations
self.xblock_fields = {}
self.transformer_data = defaultdict(dict)
self._block_data_map = defaultdict(_BlockData)
self._transformer_data = defaultdict(dict)
for child in children: self._block_relations[child].parents.remove(usage_key)
for parent in parents: self._block_relations[parent].children.remove(usage_key)
self._block_relations.pop(usage_key, None) self._block_data_map.pop(usage_key, None)
if keep_descendants: for child in children: for parent in parents: self._add_relation(parent, child)
self._xblock_map = {}
self._requested_xblock_fields = set()
from collections import namedtuple from copy import deepcopy import ddt import itertools from nose.plugins.attrib import attr from unittest import TestCase
for parent, children in enumerate(children_map): self.assertSetEqual(set(block_structure.get_children(parent)), set(children))
for child, parents in enumerate(self.get_parents_map(children_map)): self.assertSetEqual(set(block_structure.get_parents(child)), set(parents))
for node in range(len(children_map)): self.assertIn(node, block_structure) self.assertNotIn(len(children_map) + 1, block_structure)
block_structure = BlockStructureModulestoreData(root_block_usage_key=0)
block_structure = BlockStructureModulestoreData(root_block_usage_key=0) for block in blocks: block_structure._add_xblock(block.location, block)
fields = ["field1", "field2", "field3"] block_structure.request_xblock_fields(*fields)
for block in blocks: for field in fields: self.assertIsNone(block_structure.get_xblock_field(block.location, field))
block_structure._collect_requested_xblock_fields()
for block in blocks: for field in fields: self.assertEquals( block_structure.get_xblock_field(block.location, field), block.field_map.get(field), )
for child in children_map[block_to_remove]: for parent in parents_map[block_to_remove]: removed_children_map[parent].append(child)
for child in children_map[block_to_remove]: if pruned_parents_map[child]: continue for block in traverse_post_order(child, get_children=lambda block: pruned_children_map[block]): missing_blocks.append(block) pruned_children_map[block] = []
([], []),
([TestTransformer1()], []),
([TestTransformer1(), TestTransformer2()], []),
([UnregisteredTestTransformer3()], [UnregisteredTestTransformer3.name()]),
([TestTransformer1(), UnregisteredTestTransformer3()], [UnregisteredTestTransformer3.name()]),
self.map = {} self.set_call_count = 0 self.timeout_from_last_call = 0
return cls.__name__
SIMPLE_CHILDREN_MAP = [[1, 2], [3, 4], [], [], []]
LINEAR_CHILDREN_MAP = [[1], [2], [3], []]
DAG_CHILDREN_MAP = [[1, 2], [3], [3, 4], [5, 6], [], [], []]
block_structure = block_structure_cls(root_block_usage_key=0)
for parent, children in enumerate(children_map): for child in children:
self.assertEquals( block_key in block_structure, block_key not in missing_blocks, 'Expected presence in block_structure for block_key {} to match absence in missing_blocks.'.format( unicode(block_key) ), )
if block_key not in missing_blocks: self.assertEquals( set(block_structure.get_children(block_key)), set(children), )
discussion_id_map_json = CompressedTextField(verbose_name='Discussion ID Map JSON', blank=True, null=True)
result[discussion_id] = UsageKey.from_string(result[discussion_id]).map_into_course(self.course_id)
cur_block = unordered_structure[block]
cs = CourseStructure.objects.get(course_id=self.course.id) self.assertEqual(cs.structure_json, structure_json)
course_id = self.course.id self.assertRaises(ValueError, update_course_structure, course_id)
from .tasks import update_course_structure
try: structure = CourseStructure.objects.get(course_id=course_key) structure.discussion_id_map_json = None structure.save() except CourseStructure.DoesNotExist: pass
update_course_structure.apply_async([unicode(course_key)], countdown=0)
from __future__ import unicode_literals
return dict( super(GradingPolicySerializer, self).to_representation( defaultdict(lambda: None, obj) ) )
if obj.get("parent") is None: data["parent"] = None
data["children"] = obj["children"]
tasks.update_course_structure.delay(unicode(course_key)) raise CourseStructureNotAvailableError
SignalHandler.course_published.connect(listen_for_course_publish)
SignalHandler.course_published.disconnect(listen_for_course_publish)
blocks_stack.extend(children)
from .models import CourseStructure
if not isinstance(course_key, basestring): raise ValueError('course_key must be a string. {} is not acceptable.'.format(type(course_key)))
VERSION = 4
version = IntegerField()
start = DateTimeField(null=True) end = DateTimeField(null=True) advertised_start = TextField(null=True) announcement = DateTimeField(null=True)
course_image_url = TextField() social_sharing_url = TextField(null=True) end_of_course_survey_url = TextField(null=True)
certificates_display_behavior = TextField(null=True) certificates_show_before_end = BooleanField(default=False) cert_html_view_enabled = BooleanField(default=False) has_any_active_web_certificate = BooleanField(default=False) cert_name_short = TextField() cert_name_long = TextField()
lowest_passing_grade = DecimalField(max_digits=5, decimal_places=2, null=True)
days_early_for_beta = FloatField(null=True) mobile_available = BooleanField(default=False) visible_to_staff_only = BooleanField(default=False)
enrollment_start = DateTimeField(null=True) enrollment_end = DateTimeField(null=True) enrollment_domain = TextField(null=True) invitation_only = BooleanField(default=False) max_student_enrollments_allowed = IntegerField(null=True)
catalog_visibility = TextField(null=True) short_description = TextField(null=True) course_video_url = TextField(null=True) effort = TextField(null=True) self_paced = BooleanField(default=False)
course_overview.delete() course_overview = None
if course_overview and not hasattr(course_overview, 'image_set'): CourseOverviewImageSet.create_for_course(course_overview)
course_overviews = CourseOverview.objects.all()
course_overviews = course_overviews.filter(org__iexact=org)
for tab in tabs: if tab.tab_id == "discussion" and django_comment_client.utils.is_discussion_enabled(self.id): return True return False
raw_image_url = self.course_image_url
urls = { 'raw': raw_image_url, 'small': raw_image_url, 'large': raw_image_url, }
if not url: return url
if netloc: return url
config = CourseOverviewImageConfig.current() if not config.enabled: return
if not course: course = modulestore().get_course(course_overview.id)
small_width = models.IntegerField(default=375) small_height = models.IntegerField(default=200)
large_width = models.IntegerField(default=750) large_height = models.IntegerField(default=400)
course_about_accessor = lambda object, field_name: CourseDetails.fetch_about_attribute(object.id, field_name)
for course_overview in [course_overview_cache_miss, course_overview_cache_hit]: course_overview_tabs = course_overview.tabs.all() course_resp_tabs = {tab.tab_id for tab in course_overview_tabs} self.assertEqual(self.COURSE_OVERVIEW_TABS, course_resp_tabs)
course = CourseFactory.create(default_store=modulestore_type, run="TestRun", **course_kwargs) self.check_course_overview_against_course(course)
course = CourseFactory.create(mobile_available=True, default_store=modulestore_type) course_overview_1 = CourseOverview.get_from_id(course.id) self.assertTrue(course_overview_1.mobile_available)
course.mobile_available = False with self.store.branch_setting(ModuleStoreEnum.Branch.draft_preferred): self.store.update_item(course, ModuleStoreEnum.UserID.test)
course_overview_2 = CourseOverview.get_from_id(course.id) self.assertFalse(course_overview_2.mobile_available)
with self.assertRaises(CourseOverview.DoesNotExist): self.store.delete_course(course.id, ModuleStoreEnum.UserID.test) CourseOverview.get_from_id(course.id)
course = CourseFactory.create(default_store=modulestore_type, emit_signals=True)
with check_mongo_calls(0): CourseOverview.get_from_id(course.id)
with self.assertRaises(IOError): CourseOverview.load_from_module_store(self.store.make_course_key('Non', 'Existent', 'Course'))
with check_mongo_calls_range(max_finds=max_mongo_calls, min_finds=min_mongo_calls): _course_overview_2 = CourseOverview.get_from_id(course.id)
with mock.patch( 'openedx.core.djangoapps.content.course_overviews.models.CourseOverview.objects.get' ) as mock_getter:
for _ in range(2): self.assertIsInstance(CourseOverview.get_from_id(course.id), CourseOverview)
overview_v10 = CourseOverview.get_from_id(course.id) self.assertEqual(overview_v10.version, 10)
overview_v10.version = 9 overview_v10.save()
updated_overview = CourseOverview.get_from_id(course.id) self.assertEqual(updated_overview.version, 10)
updated_overview.version = 11 updated_overview.save()
unmodified_overview = CourseOverview.get_from_id(course.id) self.assertEqual(unmodified_overview.version, 11)
self.assertEqual( {c.id for c in CourseOverview.get_all_courses(org='TEST_ORG_1')}, {c.id for c in org_courses[1]}, )
fallback_url = settings.STATIC_URL + settings.DEFAULT_COURSE_ABOUT_IMAGE_URL course_overview = self._assert_image_urls_all_default(modulestore_type, course_image, fallback_url)
self.assertTrue(hasattr(course_overview, 'image_set'))
self.set_config(enabled=False)
fake_course_image = 'sample_image.png' course_overview = self._assert_image_urls_all_default(modulestore_type, fake_course_image)
self.assertFalse(hasattr(course_overview, 'image_set'))
self.assertTrue(hasattr(course_overview_before, 'image_set'))
course_overview_before.image_set.small_url = broken_small_url course_overview_before.image_set.large_url = broken_large_url course_overview_before.image_set.save()
self.set_config(False)
course_overview_after = CourseOverview.get_from_id(course.id)
self.assertTrue(hasattr(course_overview_after, 'image_set')) image_set = course_overview_after.image_set self.assertEqual(image_set.small_url, broken_small_url) self.assertEqual(image_set.large_url, broken_large_url)
expected_url = course_image_url(course) self.assertEqual( course_overview_after.image_urls, { 'raw': expected_url, 'small': expected_url, 'large': expected_url } )
AssetBaseUrlConfig.objects.create(enabled=True, base_url='fakecdn.edx.org') expected_cdn_url = "//fakecdn.edx.org" + expected_path_start
AssetBaseUrlConfig.objects.create(enabled=True, base_url='fakecdn.edx.org') expected_cdn_url = "//fakecdn.edx.org"
fake_course_image = 'sample_image.png' patched_create_thumbnail.side_effect = Exception("Kaboom!")
course_overview = self._assert_image_urls_all_default(modulestore_type, fake_course_image)
patched_create_thumbnail.assert_called()
self.assertTrue(hasattr(course_overview, 'image_set')) self.assertEqual(course_overview.image_set.small_url, '') self.assertEqual(course_overview.image_set.large_url, '')
with mock.patch('openedx.core.lib.courses.create_course_image_thumbnail') as patched_create_thumbnail: course_overview = CourseOverview.get_from_id(course_overview.id) patched_create_thumbnail.assert_not_called()
course_image_asset_key = StaticContent.compute_location(course.id, course.course_image) course_image_content = StaticContent(course_image_asset_key, image_name, 'image/jpeg', image_buff) contentstore().save(course_image_content)
if create_after_overview: self.set_config(enabled=False)
course_overview = CourseOverview.get_from_id(course.id)
if create_after_overview: self.assertFalse(hasattr(course_overview, 'image_set')) self.set_config(enabled=True) course_overview = CourseOverview.get_from_id(course.id)
course_image_asset_key = StaticContent.compute_location(course.id, course.course_image) course_image_content = StaticContent(course_image_asset_key, image_name, 'image/png', image_buff) contentstore().save(course_image_content)
config = CourseOverviewImageConfig.current() course_overview = CourseOverview.get_from_id(course.id) image_urls = course_overview.image_urls
self.assertTrue(image_url.endswith('src_course_image-png-{}x{}.jpg'.format(*target)))
src_x, src_y = src_dimensions target_x, target_y = target image_x, image_y = image.size
self.set_config(False) course = CourseFactory.create()
overview = CourseOverview.get_from_id(course.id) self.assertFalse(hasattr(overview, 'image_set'))
CourseOverviewImageSet.objects.create(course_overview=overview)
self.set_config(True) CourseOverviewImageSet.create_for_course(overview) self.assertTrue(hasattr(overview, 'image_set'))
self.assertEqual( course_overview.image_urls, { 'raw': expected_url, 'small': expected_url, 'large': expected_url, } ) return course_overview
from cms.djangoapps.contentstore.courseware_index import CourseAboutSearchIndexer CourseAboutSearchIndexer.remove_deleted_items(course_key)
self._assert_courses_not_in_overview(self.course_key_1, self.course_key_2) self.command.handle(all=True)
self._assert_courses_in_overview(self.course_key_1, self.course_key_2)
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
]
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
self.problem_section = ItemFactory.create(parent_location=chapter.location, category='sequential', metadata={'graded': True, 'format': 'Homework'}, display_name=self.TEST_SECTION_NAME)
problem_vertical = ItemFactory.create( parent_location=self.problem_section.location, category='vertical', display_name='Problem Unit' )
values = ','.join(values)
values = set(values.split(',')) if values else set()
def __unicode__(self): return u'SystemUser'
from . import signals
from openedx.core.djangoapps.programs.models import ProgramsApiConfig
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
ROUTING_KEY = getattr(settings, 'CREDENTIALS_GENERATION_ROUTING_KEY', None)
if not config.is_certification_enabled: LOGGER.warning( 'Task award_program_certificates cannot be executed when program certification is disabled in API config', ) raise self.retry(countdown=countdown, max_retries=config.max_retries)
return
existing_program_ids = get_awarded_certificate_programs(student)
raise self.retry(exc=exc, countdown=countdown, max_retries=config.max_retries)
LOGGER.exception('Failed to award certificate for program %s to user %s', program_id, username) retry = True
LOGGER.info('Retrying task to award failed certificates to user %s', username) raise self.retry(countdown=countdown, max_retries=config.max_retries)
self.create_programs_config(enable_certification=False)
GeneratedCertificateFactory( user=self.bob, course_id=self.alternate_course_id, mode=MODES.verified, status=failing_status, )
self.assertEqual(len(httpretty.httpretty.latest_requests), 1)
utils.get_programs(self.user)
utils.get_programs(self.user)
self.assertEqual(len(httpretty.httpretty.latest_requests), 1)
for _ in range(2): utils.get_programs(staff_user)
self.assertEqual(len(httpretty.httpretty.latest_requests), 3)
enrollments = self._create_enrollments(solo_course_id, shared_course_id) meter = utils.ProgramProgressMeter(self.user, enrollments)
meter = utils.ProgramProgressMeter(self.user, []) self._assert_progress(meter)
cache_key = programs_config.CACHE_KEY if programs_config.is_cache_enabled and not user.is_staff else None return get_edx_api_data(programs_config, user, 'programs', resource_id=program_id, cache_key=cache_key)
self.course_ids = [unicode(e.course_id) for e in enrollments]
authentication_classes = []
email_label = _(u"Email")
email_placeholder = _(u"username@domain.com")
email_instructions = _("The email address you used to register with {platform_name}").format( platform_name=settings.PLATFORM_NAME )
password_label = _(u"Password")
from student.views import login_user return shim_student_view(login_user, check_logged_in=True)(request)
authentication_classes = []
self.field_handlers = {} for field_name in self.DEFAULT_FIELDS + self.EXTRA_FIELDS: handler = getattr(self, "_add_{field_name}_field".format(field_name=field_name)) self.field_handlers[field_name] = handler
for field_name in self.DEFAULT_FIELDS: self.field_handlers[field_name](form_desc, required=True)
custom_form = get_registration_extension_form()
for field_name in self.EXTRA_FIELDS: if self._is_field_visible(field_name): self.field_handlers[field_name]( form_desc, required=self._is_field_required(field_name) )
email_label = _(u"Email")
email_placeholder = _(u"username@domain.com")
name_label = _(u"Full name")
name_placeholder = _(u"Jane Doe")
name_instructions = _(u"Your legal name, used for any certificates you earn.")
username_label = _(u"Public username")
u"The name that will identify you in your courses - " u"{bold_start}(cannot be changed later){bold_end}"
username_placeholder = _(u"JaneDoe")
password_label = _(u"Password")
education_level_label = _(u"Highest level of education completed")
gender_label = _(u"Gender")
yob_label = _(u"Year of birth")
mailing_address_label = _(u"Mailing address")
goals_label = _(u"Tell us why you're interested in {platform_name}").format( platform_name=settings.PLATFORM_NAME )
city_label = _(u"City")
state_label = _(u"State/Province/Region")
company_label = _(u"Company")
title_label = _(u"Title")
first_name_label = _(u"First Name")
last_name_label = _(u"Last Name")
country_label = _(u"Country") error_msg = _(u"Please select your Country.")
if self._is_field_visible("terms_of_service"): terms_text = _(u"Honor Code")
else: terms_text = _(u"Terms of Service and Honor Code")
label = _(u"I agree to the {platform_name} {terms_of_service}.").format( platform_name=get_themed_value("PLATFORM_NAME", settings.PLATFORM_NAME), terms_of_service=terms_link )
error_msg = _(u"You must agree to the {platform_name} {terms_of_service}.").format( platform_name=get_themed_value("PLATFORM_NAME", settings.PLATFORM_NAME), terms_of_service=terms_link )
terms_text = _(u"Terms of Service") terms_link = u"<a href=\"{url}\">{terms_text}</a>".format( url=marketing_link("TOS"), terms_text=terms_text )
label = _(u"I agree to the {platform_name} {terms_of_service}.").format( platform_name=get_themed_value("PLATFORM_NAME", settings.PLATFORM_NAME), terms_of_service=terms_link )
error_msg = _(u"You must agree to the {platform_name} {terms_of_service}.").format( platform_name=get_themed_value("PLATFORM_NAME", settings.PLATFORM_NAME), terms_of_service=terms_link )
field_overrides = current_provider.get_register_form_data( running_pipeline.get('kwargs') )
form_desc.override_field_properties( "password", default="", field_type="hidden", required=False, label="", instructions="", restrictions={} )
authentication_classes = []
email_label = _(u"Email")
email_placeholder = _(u"username@domain.com")
email_instructions = _(u"The email address you used to register with {platform_name}").format( platform_name=settings.PLATFORM_NAME )
email_opt_in = request.data['email_opt_in'].lower() == 'true' update_email_opt_in(request.user, org, email_opt_in) return HttpResponse(status=status.HTTP_200_OK)
NAME_MIN_LENGTH = 2 NAME_MAX_LENGTH = 255
USERNAME_MIN_LENGTH = 2 USERNAME_MAX_LENGTH = 30
EMAIL_MIN_LENGTH = 3 EMAIL_MAX_LENGTH = 254
PASSWORD_MIN_LENGTH = 2 PASSWORD_MAX_LENGTH = 75
ALL_USERS_VISIBILITY = 'all_users'
PRIVATE_VISIBILITY = 'private'
self.configuration = kwargs.pop('configuration', None) if not self.configuration: self.configuration = settings.ACCOUNT_VISIBILITY_CONFIGURATION
self.custom_fields = kwargs.pop('custom_fields', [])
read_only_fields = () explicit_read_only_fields = ("profile_image", "requires_parental_consent")
profile_privacy = UserPreference.get_value(user, ACCOUNT_VISIBILITY_PREF_KEY) return profile_privacy if profile_privacy else configuration.get('default_visibility')
urls = _get_default_profile_image_urls()
visible_fields = _visible_fields
changing_email = False if "email" in update: changing_email = True new_email = update["email"] del update["email"]
old_name = None if "name" in update: old_name = existing_user_profile.name
read_only_fields = set(update.keys()).intersection( AccountUserSerializer.get_read_only_fields() + AccountLegacyProfileSerializer.get_read_only_fields() )
field_errors = {}
if field_errors: raise AccountValidationError(field_errors)
if "language_proficiencies" in update: old_language_proficiencies = legacy_profile_serializer.data["language_proficiencies"]
if 'account_privacy' in update: update_user_preferences( requesting_user, {'account_privacy': update["account_privacy"]}, existing_user )
_validate_username(username) _validate_password(password, username) _validate_email(email)
user = User(username=username, email=email, is_active=False) user.set_password(password)
registration = Registration() registration.register(user)
UserProfile(user=user).save()
return registration.activation_key
registration.activate()
form = PasswordResetFormNoActive({'email': email})
if form.is_valid(): form.save( from_email=theming_helpers.get_value('default_from_email', settings.DEFAULT_FROM_EMAIL), domain_override=orig_host, use_https=is_secure ) else: raise UserNotFound
self.user.profile.year_of_birth = 1980 self.user.profile.profile_image_uploaded_at = TEST_PROFILE_IMAGE_UPLOAD_DT self.user.profile.save()
account_settings = get_account_settings(self.default_request, self.different_user.username) self.assertNotIn("email", account_settings)
naughty_update = { "username": "not_allowed", "gender": "undecided", "email": "not an email address" }
account_settings = get_account_settings(self.default_request) self.assertEqual("Mickey Mouse", account_settings["name"])
pending_change = PendingEmailChange.objects.filter(user=self.user) self.assertEqual(0, len(pending_change))
create_account(self.USERNAME, self.PASSWORD, self.EMAIL)
user = User.objects.get(username=self.USERNAME) request = RequestFactory().get("/api/user/v1/accounts/") request.user = user account_settings = get_account_settings(request)
self.assertIsNotNone(account_settings['date_joined']) del account_settings['date_joined']
u'{user}@example.com'.format( user=(u'e' * (EMAIL_MAX_LENGTH - 11)) )
activation_key = create_account(self.USERNAME, self.PASSWORD, self.EMAIL) user = User.objects.get(username=self.USERNAME)
activate_account(activation_key) account = get_account_settings(request) self.assertTrue(account['is_active'])
create_account(self.USERNAME, self.USERNAME, self.EMAIL)
activation_key = create_account(self.USERNAME, self.PASSWORD, self.EMAIL) activate_account(activation_key)
request_password_change(self.EMAIL, self.ORIG_HOST, self.IS_SECURE)
self.assertEqual(len(mail.outbox), 1)
email_body = mail.outbox[0].body result = re.search(r'(?P<url>https?://[^\s]+)', email_body) self.assertIsNot(result, None)
self.assertEqual(len(mail.outbox), 0)
create_account(self.USERNAME, self.PASSWORD, self.EMAIL)
self.assertEqual(len(mail.outbox), 1)
TEST_PROFILE_IMAGE_BACKEND = deepcopy(settings.PROFILE_IMAGE_BACKEND) TEST_PROFILE_IMAGE_BACKEND['options']['base_url'] = '/profile-images/'
response = client.patch(self.url, data=json.dumps(json_data), content_type=content_type) self.assertEqual(expected_status, response.status_code) return response
set_user_preference(self.user, ACCOUNT_VISIBILITY_PREF_KEY, preference_visibility) self.create_mock_profile(self.user) response = self.send_get(client)
response = self.send_get(client, query_parameters='view=shared') verify_fields_visible_to_all_users(response)
self.assertEqual(False, data["accomplishments_shared"])
self.user.is_active = False self.user.save() verify_get_own_information(9)
legacy_profile = UserProfile.objects.get(id=self.user.id) legacy_profile.year_of_birth = 2000 legacy_profile.save()
response = self.send_patch(client, {field: ""}) self.assertEqual("", response.data[field])
response = self.send_get(client) self.assertEqual("m", response.data["gender"])
self.assertIsNone(response.data[field_name])
response = self.send_patch(self.client, {field_name: ""}) self.assertIsNone(response.data[field_name])
get_response = self.send_get(self.client) self.assertEqual(new_name, get_response.data["name"])
self.assertEqual(old_email, response.data["email"]) self.assertEqual("change my email", response.data["goals"])
response = self.send_get(client, query_parameters='view=shared') self._verify_private_account_response( response, requires_parental_consent=True, account_privacy=PRIVATE_VISIBILITY )
mock_email_change.side_effect = [ValueError, "mock value error thrown"] self.client.login(username=self.user.username, password=self.test_password) old_email = self.user.email
OUTPUT_FIELD_NAMES = [ "email", "full_name", "course_id", "is_opted_in_for_email", "preference_set_datetime" ]
QUERY_INTERVAL = 1000
DEFAULT_DATETIME_STR = datetime.datetime(year=2014, month=12, day=1).isoformat(' ')
courses = self._get_courses_for_org(org_list) only_courses = options.get("courses")
org_list = list(set(org_list) | set(course.org for course in courses))
if not courses: raise CommandError( u"No courses found for orgs: {orgs}".format( orgs=", ".join(org_list) ) )
LOGGER.info( u"Retrieving data for courses: {courses}".format( courses=", ".join([unicode(course) for course in courses]) ) )
with open(file_path, "w") as file_handle: with self._log_execution_time(): self._write_email_opt_in_prefs(file_handle, org_list, courses)
LOGGER.info(u"Output file: {file_path}".format(file_path=file_path))
LOGGER.info(u"Retrieved {num_rows} records.".format(num_rows=row_count))
db_alias = ( 'read_replica' if 'read_replica' in settings.DATABASES else 'default' ) return connections[db_alias].cursor()
self._assert_output(output)
self._assert_output(output, (self.user, self.courses[0].id, True))
self._create_courses_and_enrollments( (self.TEST_ORG, True), ("other_org", True) )
self._set_opt_in_pref(self.user, "other_org", False)
output = self._run_command(self.TEST_ORG) self._assert_output( output, (self.user, self.courses[0].id, True), expect_pref_datetime=False )
self._create_courses_and_enrollments( (self.TEST_ORG, True), ("org_alias", True) )
self._set_opt_in_pref(self.user, self.TEST_ORG, True) self._set_opt_in_pref(self.user, "org_alias", False)
self._create_courses_and_enrollments((self.TEST_ORG, True)) self._set_opt_in_pref(self.user, self.TEST_ORG, opt_in_pref)
CourseEnrollment.unenroll(self.user, self.courses[0].id, skip_refund=True)
output = self._run_command(self.TEST_ORG) self._assert_output(output, (self.user, self.courses[0].id, opt_in_pref))
self._create_courses_and_enrollments( (self.TEST_ORG, True), (self.TEST_ORG, True), (self.TEST_ORG, True), ("org_alias", True) )
self._set_opt_in_pref(self.user, "org_alias", False)
CourseEnrollment.unenroll(self.user, self.courses[3].id, skip_refund=True)
with self.assertRaisesRegexp(CommandError, "^No courses found for orgs:"): self._run_command("other_org")
self._create_courses_and_enrollments( (self.TEST_ORG, True), (self.TEST_ORG, True), (self.TEST_ORG, True), )
only_courses = [self.courses[0].id, self.courses[1].id] self._run_command(self.TEST_ORG, only_courses=only_courses)
output = self._run_command(self.TEST_ORG, query_interval=4)
output_emails = [row["email"] for row in output] for email in output_emails: self.assertIn(email, output_emails)
self._create_courses_and_enrollments( ("MyOrg", True), ("myorg", True) )
self._set_opt_in_pref(self.user, "MyOrg", True) self._set_opt_in_pref(self.user, "myorg", False)
temp_dir_path = tempfile.mkdtemp() self.addCleanup(shutil.rmtree, temp_dir_path)
if other_names is None: other_names = []
if query_interval is not None: command.QUERY_INTERVAL = query_interval
command.handle(output_path, *org_list, courses=only_courses)
return rows
if include_default_option: field_dict["options"].append({ "value": "", "name": "--", "default": True })
field_dict.update(self._field_overrides.get(name, {}))
if "field_type" in kwargs: kwargs["type"] = kwargs["field_type"]
if "default" in kwargs: kwargs["defaultValue"] = kwargs["default"]
request.POST = request.POST.copy()
if "enrollment_action" in request.POST: del request.POST["enrollment_action"] if "course_id" in request.POST: del request.POST["course_id"]
response = view_func(request)
else: response.status_code = 403 response.content = msg
elif response.status_code != 200 or not success: if response.status_code == 200: response.status_code = 400 response.content = msg
else: response.content = msg
return response
from __future__ import unicode_literals
course_tag_api.set_course_tag(user, course_key, partition_key, group.id)
if value is None: return Response(status=status.HTTP_404_NOT_FOUND)
(27, True, u"True"),
(32, False, u"False"),
(14, True, u"True"),
(13, True, u"False"),
(12, True, u"False")
course = CourseFactory.create() create_account(self.USERNAME, self.PASSWORD, self.EMAIL)
course = CourseFactory.create() create_account(self.USERNAME, self.PASSWORD, self.EMAIL)
(27, True, False, u"False"),
(32, False, True, u"True"),
(13, True, False, u"False"),
(12, True, False, u"False")
course = CourseFactory.create() create_account(self.USERNAME, self.PASSWORD, self.EMAIL)
set_user_preference(self.user, "dict_pref", {"int_key": 10}) set_user_preference(self.user, "string_pref", "value")
self.send_delete(self.client) self.send_get(self.client, expected_status=404)
self.send_delete(self.client, expected_status=404)
if request.user.is_staff: return True user = get_object_or_404(User, username__iexact=url_username) if field_name in visible_fields(user.profile, user): return True raise Http404()
COURSE_SCOPE = 'course'
tag = course_tag_api.get_course_tag(self.user, self.course_id, self.test_key) self.assertIsNone(tag)
self.user = UserFactory.create()
group1_id = RandomUserPartitionScheme.get_group_for_user(self.MOCK_COURSE_ID, self.user, self.user_partition)
for __ in range(10): group2_id = RandomUserPartitionScheme.get_group_for_user( self.MOCK_COURSE_ID, self.user, self.user_partition ) self.assertEqual(group1_id, group2_id)
group = RandomUserPartitionScheme.get_group_for_user( self.MOCK_COURSE_ID, self.user, self.user_partition, assign=False )
group = RandomUserPartitionScheme.get_group_for_user(self.MOCK_COURSE_ID, self.user, self.user_partition)
with self.assertRaisesRegexp(UserPartitionError, "Cannot assign user to an empty user partition"): RandomUserPartitionScheme.get_group_for_user(self.MOCK_COURSE_ID, self.user, empty_partition)
old_group = RandomUserPartitionScheme.get_group_for_user(self.MOCK_COURSE_ID, self.user, self.user_partition) self.assertIn(old_group.id, [0, 1])
new_group = RandomUserPartitionScheme.get_group_for_user(self.MOCK_COURSE_ID, self.user, user_partition) self.assertIn(new_group.id, [3, 4])
new_group_2 = RandomUserPartitionScheme.get_group_for_user(self.MOCK_COURSE_ID, self.user, user_partition) self.assertEqual(new_group, new_group_2)
old_group = RandomUserPartitionScheme.get_group_for_user(self.MOCK_COURSE_ID, self.user, self.user_partition) self.assertIn(old_group.id, [0, 1])
new_group = RandomUserPartitionScheme.get_group_for_user(self.MOCK_COURSE_ID, self.user, user_partition) self.assertEqual(old_group.id, new_group.id)
try: intercepted_function(raise_error=FakeInputException) except FakeOutputException as ex: self.assertEqual(ex.message, expected_log_msg)
mock_logger.exception.assert_called_once_with(expected_log_msg)
self.assertNotIn("enrollment_action", self.captured_request.POST) self.assertNotIn("course_id", self.captured_request.POST)
self.assertEqual(self.captured_request.POST.get("course_id"), "edX/DemoX/Fall")
class UserPreferenceFactory(DjangoModelFactory): class Meta(object): model = UserPreference
response = self.client.get(self.url, content_type="application/json") self.assertHttpOK(response)
UserFactory.create(username=self.USERNAME, email=self.EMAIL, password=self.PASSWORD)
response = self.client.post(self.url, { "email": self.EMAIL, "password": self.PASSWORD, }) self.assertHttpOK(response)
response = self.client.get(reverse("dashboard")) self.assertHttpOK(response)
UserFactory.create(username=self.USERNAME, email=self.EMAIL, password=self.PASSWORD)
data = { "email": self.EMAIL, "password": self.PASSWORD, }
self.assertEqual( self.client.session.get_expire_at_browser_close(), expire_at_browser_close )
UserFactory.create(username=self.USERNAME, email=self.EMAIL, password=self.PASSWORD)
response = self.client.post(self.url, { "email": self.EMAIL, "password": "invalid" }) self.assertHttpForbidden(response)
response = self.client.post(self.url, { "email": "invalid@example.com", "password": self.PASSWORD, }) self.assertHttpForbidden(response)
UserFactory.create(username=self.USERNAME, email=self.EMAIL, password=self.PASSWORD)
response = self.client.post(self.url, { "email": self.EMAIL, }) self.assertHttpBadRequest(response)
response = self.client.post(self.url, { "password": self.PASSWORD, }) self.assertHttpBadRequest(response)
response = self.client.post(self.url, {})
response = self.client.get(self.url, content_type="application/json") self.assertHttpOK(response)
},
self._assert_reg_field( no_extra_fields_setting, { "name": "password", "type": "hidden", "required": False, } )
response = self.client.get(reverse("dashboard")) self.assertHttpOK(response)
user = User.objects.get(username=self.USERNAME) request = RequestFactory().get('/url') request.user = user account_settings = get_account_settings(request)
response = self.client.get(reverse("dashboard")) self.assertHttpOK(response)
data = { "email": self.EMAIL, "name": self.NAME, "username": self.USERNAME, "password": self.PASSWORD, }
data.update(invalid_fields)
response = self.client.post(self.url, data) self.assertHttpBadRequest(response)
response = self.client.post(self.url, data) self.assertHttpBadRequest(response)
with override_settings(REGISTRATION_EXTRA_FIELDS=extra_fields_setting): response = self.client.get(self.url) self.assertHttpOK(response)
form_desc = json.loads(response.content)
actual_field = None for field in form_desc["fields"]: if field["name"] == expected_field["name"]: actual_field = field break
with override_settings(REGISTRATION_EXTRA_FIELDS={"country": "required"}): response = self.client.get(self.url) self.assertHttpOK(response)
original_modified = tag.modified tag.value = "barfoo" tag.save() self.assertEquals(tag.value, "barfoo") self.assertNotEqual(original_modified, tag.modified)
set_user_preference(user, key, value) pref = UserPreference.get_value(user, key) self.assertEqual(pref, value)
pref = UserPreference.get_value(user, 'testkey_none') self.assertIsNone(pref)
self.assertEquals(self.middleware.process_request(self.request), None)
self.assertEquals( self.middleware.process_response(self.request, self.response), self.response ) exit_context.assert_called_with(UserTagsEventContextMiddleware.CONTEXT_NAME) exit_context.reset_mock()
get_tracker.side_effect = Exception self.assertEquals( self.middleware.process_response(self.request, self.response), self.response )
from __future__ import unicode_literals
if not self.theme_location: return False
func = with_comprehensive_theme(EDX_THEME_DIR)(func)
from django.template.base import ( TemplateSyntaxError, Library, token_kwargs, TemplateDoesNotExist ) from django.template.loader_tags import IncludeNode
path = path[len(self.storage.prefix):]
from __future__ import unicode_literals
site_configuration = SiteConfigurationFactory.create( site=self.site, )
site_configuration_history = SiteConfigurationHistory.objects.filter( site=site_configuration.site, ).all()
self.assertEqual(len(site_configuration_history), 1)
site_configuration = SiteConfigurationFactory.create( site=self.site, )
site_configuration_history = SiteConfigurationHistory.objects.filter( site=site_configuration.site, ).all()
self.assertEqual(len(site_configuration_history), 2)
site_configuration = SiteConfigurationFactory.create( site=self.site, )
site_configuration_history = SiteConfigurationHistory.objects.filter( site=site_configuration.site, ).all()
self.assertEqual(len(site_configuration_history), 1)
site_configuration = SiteConfigurationFactory.create( site=self.site, )
self.assertEqual(len(site_configuration_history), 1)
from __future__ import unicode_literals
from __future__ import unicode_literals
get_user_credentials(self.user)
get_user_credentials(self.user)
self.assertEqual(len(httpretty.httpretty.latest_requests), 1)
for _ in range(2): get_user_credentials(staff_user)
self.assertEqual(len(httpretty.httpretty.latest_requests), 3)
self.create_credentials_config() self.create_programs_config()
self.mock_programs_api() self.mock_credentials_api(self.user, reset_url=False)
self.assertEqual(len(actual), 2) self.assertEqual(actual, expected)
self.create_credentials_config() self.create_programs_config()
self.mock_programs_api() self.mock_credentials_api(self.user, reset_url=False) actual = get_programs_credentials(self.user, category='xseries') expected = self.expected_credentials_display_data()
self.assertEqual(len(actual), 2) self.assertEqual(actual, expected)
self.create_credentials_config() self.create_programs_config()
self.mock_programs_api() self.mock_credentials_api(self.user, reset_url=False) actual = get_programs_credentials(self.user, category='dummy_category') expected = self.expected_credentials_display_data()
self.mock_programs_api() self.mock_credentials_api(self.user, reset_url=False) actual = get_programs_credentials(self.user) expected = self.expected_credentials_display_data()
use_cache = credential_configuration.is_cache_enabled and not user.is_staff cache_key = credential_configuration.CACHE_KEY + '.' + user.username if use_cache else None
self.org = org self.course_id = course_id self.run = run self.language = None
course_details.license = getattr(course_descriptor, "license", "all-rights-reserved")
except ValueError: pass
for attribute in ABOUT_ATTRIBUTES: if attribute in jsondict: cls.update_about_item(descriptor, attribute, jsondict[attribute], user.id)
return CourseDetails.fetch(course_key)
GRADES_UPDATED = Signal(providing_args=["username", "grade_summary", "course_key", "deadline"])
from openedx.core.djangoapps.ccxcon import tasks tasks.update_ccxcon.delay(unicode(course_key))
from __future__ import unicode_literals
course_instructors = list_with_level(course, 'instructor') course_instructors_ids = [anonymous_id_for_user(user, course_key) for user in course_instructors] course_details = CourseDetails.fetch(course_key)
add_course_url = urlparse.urljoin(course.ccx_connector, CCXCON_COURSEXS_URL) resp = oauth_ccxcon.post( url=add_course_url, json=payload, headers=headers, timeout=CCXCON_REQUEST_TIMEOUT )
with cls.store.bulk_operations(course.id, emit_signals=False):
self.assertEqual(k_args, tuple()) self.assertEqual( k_kwargs.get('url'), urlparse.urljoin(self.course.ccx_connector, ccxconapi.CCXCON_COURSEXS_URL) )
mock_response.status_code = 200 mock_post.return_value = mock_response
self.assertEqual(k_args, tuple()) self.assertEqual( k_kwargs.get('url'), urlparse.urljoin(self.course.ccx_connector, ccxconapi.CCXCON_COURSEXS_URL) )
if cur_retry < 5: update_ccxcon.apply_async( kwargs={'course_id': course_id, 'cur_retry': cur_retry + 1},
xblock_class = XBlock.load_class(block_type, select=settings.XBLOCK_SELECT_FUNCTION) content = xblock_class.open_local_resource(uri)
from __future__ import unicode_literals
from __future__ import unicode_literals
requirements = CreditRequirement.objects.filter(course__course_key=course_key, active=True)
history = HistoricalRecords()
deadline = models.DateTimeField( default=default_deadline_for_credit_eligibility, help_text=ugettext_lazy("Deadline for purchasing and requesting credit.") )
status_by_req = defaultdict(lambda: False) for status in CreditRequirementStatus.get_statuses(requirements, username): status_by_req[status.requirement.id] = status.status
unique_together = ('username', 'course', 'provider') get_latest_by = 'created'
icrv_blocks = get_course_blocks(course_key, VERIFICATION_BLOCK_CATEGORY)
_set_verification_partitions(course_key, icrv_blocks)
used_ids = set(p.id for p in course.user_partitions) return generate_int_id(used_ids=used_ids)
partitions += _other_partitions(verified_partitions, partitions, course_key) course.set_user_partitions_for_scheme(partitions, scheme) modulestore().update_item(course, ModuleStoreEnum.UserID.system)
lookup_value = '[^/.]+'
provider_ids = self.request.GET.get('provider_ids', None)
provider = generics.get_object_or_404(CreditProvider, provider_id=provider_id)
course_key = request.data.get('course_key') try: course_key = CourseKey.from_string(course_key) except InvalidKeyError: raise InvalidCourseKey(course_key)
username = request.data.get('username') if not username: raise ValidationError({'detail': 'A username must be specified.'})
if not CreditEligibility.is_user_eligible_for_credit(course_key, username): raise UserNotEligibleException(course_key, username)
authentication_classes = () permission_classes = ()
serializer = CreditProviderCallbackSerializer(data=data, provider=provider) serializer.is_valid(raise_exception=True)
@method_decorator(csrf_exempt) def dispatch(self, request, *args, **kwargs): return super(CreditCourseViewSet, self).dispatch(request, *args, **kwargs)
course_key = self.kwargs.get(self.lookup_field) if course_key is not None: self.kwargs[self.lookup_field] = CourseKey.from_string(course_key)
from openedx.core.djangoapps.credit import api, tasks
from openedx.core.djangoapps.credit import api
status = 'satisfied' reason = {'final_grade': grade_summary['percent']}
status = 'failed' reason = { 'current_date': now, 'deadline': deadline }
status = 'failed' reason = { 'final_grade': grade_summary['percent'], 'minimum_grade': min_grade }
if not date_time: msg = '[{}] is not a valid timestamp'.format(value) log.warning(msg) raise serializers.ValidationError(msg)
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from openedx.core.djangoapps.credit.api.eligibility import ( is_credit_course, )
from openedx.core.djangoapps.credit.api.eligibility import ( is_credit_course, get_credit_requirement_status, )
return None
from openedx.core.djangoapps.credit.api.eligibility import ( is_credit_course, set_credit_requirement_status as api_set_credit_requirement_status )
if not is_credit_course(course_key): return
try: user = User.objects.get(id=user_id) except ObjectDoesNotExist: return None
from openedx.core.djangoapps.credit.api.eligibility import ( is_credit_course, remove_credit_requirement_status as api_remove_credit_requirement_status )
if not is_credit_course(course_key): return
log_msg = ( 'remove_credit_requirement_status was called with ' 'user_id={user_id}, course_key_or_id={course_key_or_id} ' 'req_namespace={req_namespace}, req_name={req_name}, '.format( user_id=user_id, course_key_or_id=course_key_or_id, req_namespace=req_namespace, req_name=req_name ) ) log.info(log_msg)
try: user = User.objects.get(id=user_id) except ObjectDoesNotExist: return None
is_verified, has_skipped, has_completed = _get_user_statuses(user, course_key, checkpoint)
cache_values = cache.get_many([ enrollment_cache_key, has_skipped_cache_key, verification_status_cache_key ])
is_verified = cache_values.get(enrollment_cache_key) if is_verified is None: is_verified = CourseEnrollment.is_enrolled_as_verified(user, course_key) cache.set(enrollment_cache_key, is_verified)
has_skipped = cache_values.get(has_skipped_cache_key) if has_skipped is None: has_skipped = SkippedReverification.check_user_skipped_reverification_exists(user, course_key) cache.set(has_skipped_cache_key, has_skipped)
verification_statuses = cache_values.get(verification_status_cache_key) if verification_statuses is None: verification_statuses = VerificationStatus.get_all_checkpoints(user.id, course_key) cache.set(verification_status_cache_key, verification_statuses)
checkpoint = verification_statuses.get(checkpoint) has_completed_check = bool(checkpoint)
notification_msg = MIMEMultipart('related') msg_alternative = MIMEMultipart('alternative') notification_msg.attach(msg_alternative) subject = _(u'Course Credit Eligibility')
email_body_plain = render_to_string('credit_notifications/credit_eligibility_email.txt', context) msg_alternative.attach(SafeMIMEText(email_body_plain, _subtype='plain', _charset='utf-8'))
if logo_image: notification_msg.attach(logo_image)
from_address = theming_helpers.get_value('default_from_email', settings.DEFAULT_FROM_EMAIL) to_address = user.email
msg = EmailMessage(subject, None, from_address, [to_address]) msg.attach(notification_msg) msg.send()
html_with_inline_css = pynliner.fromString('<style>' + css_content + '</style>' + html_without_css) return html_with_inline_css
providers_string = _("{first_provider} and {second_provider}").format( first_provider=providers[0], second_provider=providers[1] )
providers_string = _("{first_providers}, and {last_provider}").format( first_providers=u", ".join(providers[:-1]), last_provider=providers[-1] )
credit_request, created = CreditRequest.objects.get_or_create( course=credit_course, provider=credit_provider, username=username, )
user = User.objects.select_related('profile').get(username=username)
try: final_grade = CreditRequirementStatus.objects.get( username=username, requirement__namespace="grade", requirement__name="grade", requirement__course__course_key=course_key, status="satisfied" ).reason["final_grade"]
if len(unicode(final_grade)) > 7: final_grade = u'{:.5f}'.format(final_grade) else: final_grade = unicode(final_grade)
course_enrollment = CourseEnrollment.get_enrollment(user, course_key) enrollment_date = course_enrollment.created if course_enrollment else ""
completion_date = get_last_exam_completion_date(course_key, username)
parameters["signature"] = signature(parameters, shared_secret_key)
reqs = CreditRequirement.get_course_requirements(course_key)
req_to_update = next(( req for req in reqs if req.namespace == req_namespace and req.name == req_name ), None)
CreditRequirementStatus.add_or_update_requirement_status( username, req_to_update, status=status, reason=reason )
if status == "satisfied" and not eligible_before_update: is_eligible, eligibility_record_created = CreditEligibility.update_eligibility(reqs, username, course_key) if eligibility_record_created and is_eligible: try: send_credit_notifications(username, course_key)
req_to_remove = CreditRequirement.get_course_requirements(course_key, namespace=req_namespace, name=req_name)
CreditRequirementStatus.remove_requirement_status( username, req_to_remove )
if req["namespace"] == old_req.namespace and req["name"] == old_req.name: found_flag = True break
api.set_credit_requirements(self.course_key, requirements[1:])
visible_reqs = api.get_credit_requirements(self.course_key) self.assertEqual(len(visible_reqs), 1) self.assertEqual(visible_reqs[0]["namespace"], "grade")
credit_course = self.add_credit_course() CreditEligibility.objects.create( course=credit_course, username="staff", deadline=datetime.datetime.now(pytz.UTC) - datetime.timedelta(days=1) )
is_eligible = api.is_user_eligible_for_credit("staff", credit_course.course_key) self.assertFalse(is_eligible)
eligibilities = api.get_eligibilities_for_user("staff") self.assertEqual(eligibilities, [])
credit_course = self.add_credit_course() credit_course.enabled = False credit_course.save()
is_eligible = api.is_user_eligible_for_credit("staff", credit_course.course_key) self.assertFalse(is_eligible)
eligibilities = api.get_eligibilities_for_user("staff") self.assertEqual(eligibilities, [])
self.assert_grade_requirement_status(None, 0)
api.set_credit_requirement_status(username, self.course_key, "grade", "grade") self.assert_grade_requirement_status('satisfied', 0)
api.set_credit_requirement_status(username, self.course_key, "grade", "grade", status="failed") self.assert_grade_requirement_status('failed', 0)
self.assertEqual(req_status[1]["status"], None) self.assertEqual(req_status[1]["order"], 1)
self.add_credit_course()
api.remove_credit_requirement_status("bob", self.course_key, "grade", "grade")
req_status = api.get_credit_requirement_status(self.course_key, "bob", namespace="grade", name="grade") self.assertEqual(len(req_status), 0)
self.add_credit_course() CourseFactory.create(org='edX', number='DemoX', display_name='Demo_Course')
with self.assertNumQueries(12): api.set_credit_requirement_status( user.username, self.course_key, requirements[0]["namespace"], requirements[0]["name"] )
self.assertFalse(api.is_user_eligible_for_credit("bob", self.course_key))
with self.assertNumQueries(20): api.set_credit_requirement_status( "bob", self.course_key, requirements[1]["namespace"], requirements[1]["name"] )
self.assertTrue(api.is_user_eligible_for_credit("bob", self.course_key))
self.assertEqual(len(mail.outbox), 1) self.assertEqual( mail.outbox[0].subject, 'You are eligible for credit from Hogwarts School of Witchcraft and Wizardry' )
email_image = email_payload_first[1]
text_content_first = email_payload_first[0]._payload[0]._payload self.assertIn( 'credit from Hogwarts School of Witchcraft and Wizardry for', text_content_first )
self.assertEqual(len(mail.outbox), 2)
self.add_credit_course()
api.set_credit_requirement_status("bob", self.course_key, "grade", "grade")
req_status = api.get_credit_requirement_status(self.course_key, "bob", namespace="grade", name="grade") self.assertEqual(req_status, [])
self.assertEqual(len(mail.outbox), 1)
self.assertEqual(mail.outbox[0].subject, expected_subject)
self._configure_credit()
provider = CreditProvider.objects.get() provider.active = False provider.save()
result = api.get_credit_providers(['fake_provider_id']) self.assertEqual(result, [])
request = api.create_credit_request(self.course_key, self.PROVIDER_ID, self.USER_INFO['username'])
self.assertIn('request_uuid', parameters) self.assertEqual(len(parameters['request_uuid']), 32)
self.assertIn('timestamp', parameters) parsed_date = from_timestamp(parameters['timestamp']) self.assertLess(parsed_date, datetime.datetime.now(pytz.UTC))
request = api.create_credit_request(self.course_key, self.PROVIDER_ID, self.USER_INFO['username'])
self._assert_credit_status("pending")
api.update_credit_request_status(request["parameters"]["request_uuid"], self.PROVIDER_ID, status) self._assert_credit_status(status)
uuid = request["parameters"]["request_uuid"] with self.assertNumQueries(3): api.update_credit_request_status(uuid, self.PROVIDER_ID, "approved")
first_request = api.create_credit_request(self.course_key, self.PROVIDER_ID, self.USER_INFO["username"])
self.assertEqual( first_request["parameters"]["request_uuid"], second_request["parameters"]["request_uuid"] )
self.assertEqual(second_request["parameters"]["user_full_name"], "Bobby")
request = api.create_credit_request(self.course_key, self.PROVIDER_ID, self.USER_INFO["username"])
api.update_credit_request_status(request["parameters"]["request_uuid"], self.PROVIDER_ID, status)
with self.assertRaises(RequestAlreadyCompleted): api.create_credit_request(self.course_key, self.PROVIDER_ID, self.USER_INFO['username'])
first_request = api.create_credit_request(self.course_key, self.PROVIDER_ID, self.USER_INFO["username"])
other_course_key = CourseKey.from_string("edX/other/2015") self._configure_credit(course_key=other_course_key) second_request = api.create_credit_request(other_course_key, self.PROVIDER_ID, self.USER_INFO["username"])
self.assertEqual(first_request["parameters"]["course_num"], self.course_key.course) self.assertEqual(second_request["parameters"]["course_num"], other_course_key.course)
self.user.profile.mailing_address = None self.user.profile.save()
request = api.create_credit_request(self.course_key, self.PROVIDER_ID, self.USER_INFO["username"]) self.assertEqual(request["parameters"]["user_mailing_address"], "")
query = "UPDATE auth_userprofile SET country = NULL WHERE id = %s" connection.cursor().execute(query, [str(self.user.profile.id)])
request = api.create_credit_request(self.course_key, self.PROVIDER_ID, self.USER_INFO["username"]) self.assertEqual(request["parameters"]["user_country"], "")
grade_status = CreditRequirementStatus.objects.get( username=self.USER_INFO['username'], requirement__namespace="grade", requirement__name="grade" ) grade_status.reason = {} grade_status.save()
with self.assertRaises(CreditRequestNotFound): api.update_credit_request_status("invalid_uuid", self.PROVIDER_ID, "approved")
response_providers = get_credit_provider_display_names(self.course_key) self.assertListEqual(self.PROVIDERS_LIST, response_providers)
response_providers = get_credit_provider_display_names(self.course_key) self.assertListEqual(self.PROVIDERS_LIST, response_providers)
self.assertEqual(len(httpretty.httpretty.latest_requests), 1)
import datetime import json
self.course = CourseFactory.create() self.checkpoint_location = u'i4x://{org}/{course}/edx-reverification-block/first_uuid'.format( org=self.course.id.org, course=self.course.id.course )
user = self.create_user_and_enroll(enrollment_type) if verification_status: self.add_verification_status(user, verification_status)
user = self.create_user_and_enroll('verified')
with self.assertNumQueries(0): self._assert_group_assignment(user, VerificationPartitionScheme.ALLOW)
user = self.create_user_and_enroll('verified') self.add_verification_status(user, VerificationStatus.APPROVED_STATUS) with self.assertNumQueries(4): self._assert_group_assignment(user, VerificationPartitionScheme.ALLOW)
with self.assertNumQueries(0): self._assert_group_assignment(user, VerificationPartitionScheme.ALLOW)
user = self.create_user_and_enroll('verified') self.add_verification_status(user, VerificationStatus.DENIED_STATUS)
with self.assertNumQueries(4): self._assert_group_assignment(user, VerificationPartitionScheme.ALLOW)
with self.assertNumQueries(0): self._assert_group_assignment(user, VerificationPartitionScheme.ALLOW)
user = self.create_user_and_enroll('honor') with self.assertNumQueries(3): self._assert_group_assignment(user, VerificationPartitionScheme.ALLOW)
with self.assertNumQueries(0): self._assert_group_assignment(user, VerificationPartitionScheme.ALLOW)
with self.assertNumQueries(3): self._assert_group_assignment(user, VerificationPartitionScheme.DENY)
with self.assertNumQueries(0): self._assert_group_assignment(user, VerificationPartitionScheme.DENY)
if self.list_path: self.path = reverse(self.list_path)
self.user = UserFactory(password=self.password, is_staff=True) self.client.login(username=self.user.username, password=self.password)
response = self.client.get(self.path) self.assertEqual(response.status_code, 403)
user.is_staff = True
response = client.get('/')
response = client.post(self.path, data=json.dumps(data), content_type=JSON, HTTP_X_CSRFTOKEN=csrf_token) self.assertEqual(response.status_code, 201)
response = self.client.get(self.path, **headers) self.assertEqual(response.status_code, 403)
user.is_staff = True
self.assertDictEqual(json.loads(response.content), data)
course_key = CourseKey.from_string(course_id) self.assertTrue(CreditCourse.objects.filter(course_key=course_key, enabled=enabled).exists())
self.assertDictEqual(json.loads(response.content), self._serialize_credit_course(cc1))
self.assertListEqual(json.loads(response.content), expected)
self.assertDictEqual(json.loads(response.content), data)
credit_course = CreditCourse.objects.get(course_key=credit_course.course_key) self.assertTrue(credit_course.enabled)
self.provider.enable_integration = True self.provider.save()
requirement = CreditRequirement.objects.create( course=course, namespace='grade', name='grade', )
CreditRequirementStatus.objects.create( username=username, requirement=requirement, status='satisfied', reason={'final_grade': final_grade} )
request = CreditRequest.objects.get(username=username, course__course_key=course_key) self.assertEqual(request.status, 'pending')
content = json.loads(response.content) parameters = content['parameters']
self.provider.enable_integration = True self.provider.save()
with override_settings(CREDIT_PROVIDER_SECRET_KEYS={}): response = self.post_credit_request(self.user.username, self.eligibility.course.course_key) self.assertEqual(response.status_code, 400)
self.client.logout()
response = self._credit_provider_callback(request_uuid, "approved", sig="invalid") self.assertEqual(response.status_code, 403)
self._assert_request_status(request_uuid, "pending")
self._credit_provider_callback(request_uuid, 'approved') self._assert_request_status(request_uuid, "approved")
self._credit_provider_callback(request_uuid, 'approved') self._assert_request_status(request_uuid, "approved")
CreditProvider.objects.create(provider_id=other_provider_id, enable_integration=True)
request_uuid = self._create_credit_request_and_get_uuid()
response = self._credit_provider_callback( request_uuid, 'approved', provider_id=other_provider_id, secret_key=other_provider_secret_key, keys={other_provider_id: other_provider_secret_key} )
self.assertEqual(response.status_code, 404)
self._assert_request_status(request_uuid, 'pending')
with override_settings(CREDIT_PROVIDER_SECRET_KEYS={}): response = self._credit_provider_callback(request_uuid, 'approved', keys={}) self.assertEqual(response.status_code, 403)
key = signature.get_shared_secret_key("asu") sig = signature.signature({}, key) self.assertEqual(sig, "7d70a26b834d9881cc14466eceac8d39188fc5ef5ffad9ab281a8327c2c0d093")
key = signature.get_shared_secret_key("asu") self.assertIs(key, None)
MODULESTORE = TEST_DATA_SPLIT_MODULESTORE
SignalHandler.pre_publish.disconnect(receiver=on_pre_publish) self.addCleanup(SignalHandler.pre_publish.connect, receiver=on_pre_publish)
self.assertEqual(len(partition.groups), 2) self.assertItemsEqual( [g.id for g in partition.groups], [ VerificationPartitionScheme.ALLOW, VerificationPartitionScheme.DENY, ] )
self.store.delete_item( self.icrv.location, ModuleStoreEnum.UserID.test, revision=ModuleStoreEnum.RevisionOption.published_only ) self._update_partitions()
other_icrv = ItemFactory.create(parent=self.verticals[3], category='edx-reverification-block') self._update_partitions()
icrv_location = self.icrv.location self.store.delete_item( self.icrv.location, ModuleStoreEnum.UserID.test, revision=ModuleStoreEnum.RevisionOption.published_only ) self._update_partitions()
self.store.delete_item( self.icrv.location, ModuleStoreEnum.UserID.test, revision=ModuleStoreEnum.RevisionOption.published_only )
with check_mongo_calls_range(max_finds=4, max_sends=2): self._update_partitions(reload_items=False)
with check_mongo_calls_range(max_finds=5, max_sends=3): self._update_partitions(reload_items=False)
ItemFactory.create(parent=self.verticals[3], category='edx-reverification-block') with check_mongo_calls_range(max_finds=6, max_sends=3): self._update_partitions(reload_items=False)
if reload_items:
self.assertEqual(self.course.user_partitions, [])
from __future__ import unicode_literals
self.assertFalse([ requirement for requirement in requirements if requirement['namespace'] == 'proctored_exam' ])
self.assertFalse([ requirement for requirement in requirements if requirement['namespace'] == 'proctored_exam' ])
create_exam( course_id=unicode(self.course.id), content_id='foo3', exam_name='A Proctored Exam', time_limit_mins=10, is_proctored=True, is_active=True, is_practice_exam=True )
self.assertFalse([ requirement for requirement in requirements if requirement['namespace'] == 'proctored_exam' ])
requirements = get_credit_requirements(self.course.id, namespace="reverification") self.assertEqual(len(requirements), 1)
with self.store.branch_setting(ModuleStoreEnum.Branch.draft_preferred, self.course.id): self.store.delete_item(self.subsection.location, ModuleStoreEnum.UserID.test)
on_course_publish(self.course.id) requirements = get_credit_requirements(self.course.id, namespace="reverification") self.assertEqual(len(requirements), 0)
start = datetime.now(UTC) self.add_icrv_xblock(related_assessment_name="Midterm A", start_date=start)
start = datetime.now(UTC) first_block = self.add_icrv_xblock(related_assessment_name="Midterm Start Date")
self.assertEqual(requirements[2]["name"], first_block.get_credit_requirement_name()) self.assertEqual(requirements[3]["name"], second_block.get_credit_requirement_name())
start = datetime.now(UTC) self.add_icrv_xblock(related_assessment_name="Midterm A", start_date=start)
CreditCourse.objects.create( course_key=self.course.id, enabled=True, )
CreditProvider.objects.create( provider_id="ASU", enable_integration=True, provider_url="https://credit.example.com/request", )
set_credit_requirements(self.course.id, requirements)
self.service.set_credit_requirement_status( self.user.id, self.course.id, 'grade', 'grade' )
self.service.set_credit_requirement_status( self.user.id, self.course.id, 'grade', 'grade' )
credit_state = self.service.get_credit_state(self.user.id, self.course.id) self.assertEqual(credit_state['credit_requirement_status'][0]['status'], "satisfied")
self.service.remove_credit_requirement_status( self.user.id, self.course.id, 'grade', 'grade' )
credit_state = self.service.get_credit_state(self.user.id, self.course.id) self.assertEqual(credit_state['credit_requirement_status'][0]['status'], None)
retval = self.service.set_credit_requirement_status( self.user.id, self.course.id, 'grade', 'grade' ) self.assertIsNone(retval)
retval = self.service.remove_credit_requirement_status( 0, self.course.id, 'grade', 'grade' ) self.assertIsNone(retval)
self.service.remove_credit_requirement_status( self.user.id, no_credit_course.id, 'grade', 'grade' )
credit_state = self.service.get_credit_state(self.user.id, self.course.id) self.assertNotIn('course_name', credit_state)
self.service.set_credit_requirement_status( self.user.id, no_credit_course.id, 'grade', 'grade' )
retval = self.service.set_credit_requirement_status( 0, self.course.id, 'grade', 'grade' ) self.assertIsNone(retval)
self.service.set_credit_requirement_status( self.user.id, unicode(self.course.id), 'grade', 'grade' )
return [ block for block in modulestore().get_items( course_key, qualifiers={"category": category}, revision=ModuleStoreEnum.RevisionOption.published_only, ) if _is_in_course_tree(block) ]
CREDIT_REQUIREMENT_XBLOCK_CATEGORIES = [ "edx-reverification-block", ]
@task(default_retry_delay=settings.CREDIT_TASK_DEFAULT_RETRY_DELAY, max_retries=settings.CREDIT_TASK_MAX_RETRIES)
sorted_block_requirements = sorted( block_requirements, key=lambda x: (x['start_date'] is None, x['start_date'], x['display_name']) )
from edx_proctoring.api import get_all_exams_for_course
course_id = CourseKeyField( max_length=255, db_index=True, help_text="Which course is this group associated with?", )
with outer_atomic(read_committed=True):
if created: return
@receiver(pre_delete, sender=CohortMembership)
always_cohort_inline_discussions = models.BooleanField(default=True)
global _local_random
ans = set()
course_cohort_settings = get_course_cohort_settings(course_key) if not course_cohort_settings.is_cohorted: return request_cache.data.setdefault(cache_key, None)
membership = CohortMembership.objects.create( user=user, course_user_group=get_random_cohort(course_key) ) return request_cache.data.setdefault(cache_key, membership.course_user_group)
if created: manual_cohorts = CourseUserGroup.objects.filter( course_id=course.id, group_type=CourseUserGroup.COHORT ).exclude(name__in=course.auto_cohort_groups) for cohort in manual_cohorts: CourseCohort.create(course_user_group=cohort)
migrate_cohort_settings(course)
return JsonResponse({"error": unicode(err)}, 400)
return JsonResponse({"error": "Cohort name must be specified."}, 400)
return JsonResponse({"error": "Assignment type must be specified."}, 400)
return JsonResponse( {"error": "If group_id is specified, user_partition_id must also be specified."}, 400 )
existing_group_id, _ = cohorts.get_group_info_for_cohort(cohort) if existing_group_id is not None: unlink_cohort_partition_group(cohort)
course_key = SlashSeparatedCourseKey.from_deprecated_string(course_key_string)
cohort = cohorts.get_cohort_by_id(course_key, int(cohort_id))
return HttpResponseBadRequest('Requested page must be numeric')
course_key = SlashSeparatedCourseKey.from_deprecated_string(course_key_string) get_course_with_access(request.user, 'staff', course_key)
course_key = SlashSeparatedCourseKey.from_deprecated_string(course_key_string) get_course_with_access(request.user, 'staff', course_key)
course_key = SlashSeparatedCourseKey.from_deprecated_string(course_key_string) get_course_with_access(request.user, 'staff', course_key)
course_wide_entries = discussion_category_map.pop('entries')
config_course_cohorts(self.course1, is_cohorted=True, auto_cohorts=["Course1AutoGroup1", "Course1AutoGroup2"])
membership1 = CohortMembership( course_id=course_1_auto_cohort_1.course_id, user=self.user1, course_user_group=course_1_auto_cohort_1 ) membership1.save() membership2 = CohortMembership( course_id=course_1_auto_cohort_1.course_id, user=self.user2, course_user_group=course_1_auto_cohort_1 ) membership2.save()
call_command('post_cohort_membership_fix')
call_command('post_cohort_membership_fix', commit='commit')
self.assertEqual(self.user1.course_groups.count(), 1) self.assertEqual(CohortMembership.objects.filter(user=self.user1).count(), 1)
course_group.users.remove(user) user.course_groups.remove(course_group)
from __future__ import unicode_literals
import json
discussion_topics = { "Topic B": {"id": "Topic B"}, }
ItemFactory.create( parent_location=self.course.location, category="discussion", discussion_id="Topic_A", discussion_category="Chapter", discussion_target="Discussion", start=now )
self.verify_lists_expected_cohorts([])
config_course_cohorts(self.course, is_cohorted=True)
self.verify_lists_expected_cohorts([])
users = [UserFactory() for _ in range(3)] self._enroll_users(users, self.course.id)
for user in users: get_cohort(user, self.course.id)
config_course_cohorts_legacy(self.course, [], cohorted=True, auto_cohort_groups=["AutoGroup"])
with self.assertRaises(CourseUserGroup.DoesNotExist): get_cohort_by_name(self.course.id, "AutoGroup")
cohort_name = 'I AM A RANDOM COHORT' data = {'name': cohort_name, 'assignment_type': CourseCohort.RANDOM} response_dict = self.put_handler(self.course, data=data)
cohort_name = 'I AM A RANDOM COHORT' data = {'name': cohort_name, 'assignment_type': CourseCohort.RANDOM} response_dict = self.put_handler(self.course, data=data)
self.assertEqual(self.cohort1.name, response_dict.get("name"))
self.create_cohorted_discussions()
always_cohort_inline_discussions = True
modulestore().update_item(course, ModuleStoreEnum.UserID.test)
modulestore().update_item(course, ModuleStoreEnum.UserID.test)
import ddt from mock import call, patch from nose.plugins.attrib import attr import before_after
cohort.name = "NewName" cohort.save() self.assertFalse(mock_tracker.called)
CourseUserGroup.objects.create( name="TestOtherGroupType", course_id=self.course_key, group_type="dummy" ) self.assertFalse(mock_tracker.called)
cohort_list[0].users.add(*user_list) assert_events("added", user_list, cohort_list[:1]) mock_tracker.reset_mock()
cohort_list[0].users.remove(*user_list) assert_events("removed", user_list, cohort_list[:1]) mock_tracker.reset_mock()
cohort_list[0].users.add(*user_list) cohort_list[0].users.clear() assert_events("removed", user_list, cohort_list[:1]) mock_tracker.reset_mock()
non_cohort.users.add(*user_list) non_cohort.users.clear() self.assertFalse(mock_tracker.emit.called)
user_list[0].course_groups.add(*cohort_list) assert_events("added", user_list[:1], cohort_list) mock_tracker.reset_mock()
user_list[0].course_groups.remove(*cohort_list) assert_events("removed", user_list[:1], cohort_list) mock_tracker.reset_mock()
user_list[0].course_groups.add(*cohort_list) user_list[0].course_groups.clear() assert_events("removed", user_list[:1], cohort_list) mock_tracker.reset_mock()
user_list[0].course_groups.add(non_cohort) user_list[0].course_groups.clear() self.assertFalse(mock_tracker.emit.called)
fake_key = SlashSeparatedCourseKey('a', 'b', 'c') self.assertRaises(Http404, lambda: cohorts.is_course_cohorted(fake_key))
config_course_cohorts(course, is_cohorted=True)
config_course_cohorts( course, is_cohorted=True, auto_cohorts=["AutoGroup"] )
self.assertIsNone(cohorts.get_cohort(user, course.id, assign=False))
self.assertEquals(cohorts.get_cohort(user, course.id).name, "AutoGroup")
config_course_cohorts( course, is_cohorted=True, auto_cohorts=["AutoGroup"] )
config_course_cohorts( course, is_cohorted=True, auto_cohorts=["AutoGroup"] )
config_course_cohorts_legacy( course, discussions=[], cohorted=True, auto_cohort_groups=["OtherGroup"] )
config_course_cohorts( course, is_cohorted=True, auto_cohorts=[] )
config_course_cohorts_legacy( course, discussions=[], cohorted=True, auto_cohort_groups=["AutoGroup"] )
CohortFactory(course_id=course.id, name="ManualCohort") CohortFactory(course_id=course.id, name="ManualCohort2")
self.assertEqual(first_cohort.users.get(), course_user)
add_user_to_cohort(first_cohort, self.student.username) self.assert_student_in_group(None)
add_user_to_cohort(second_cohort, self.student.username) self.assert_student_in_group(self.groups[1])
remove_user_from_cohort(second_cohort, self.student.username) self.assert_student_in_group(None)
add_user_to_cohort(test_cohort, self.student.username) self.assert_student_in_group(None)
link_cohort_to_partition_group( test_cohort, self.user_partition.id, self.groups[0].id, ) self.assert_student_in_group(self.groups[0])
unlink_cohort_partition_group(test_cohort) link_cohort_to_partition_group( test_cohort, self.user_partition.id, self.groups[1].id, ) self.assert_student_in_group(self.groups[1])
unlink_cohort_partition_group( test_cohort, ) self.assert_student_in_group(None)
self.assert_student_in_group(None)
cohort = get_course_cohorts(self.course)[0]
link_cohort_to_partition_group( cohort, self.user_partition.id, self.groups[0].id, )
self.assert_student_in_group(self.groups[0])
link_cohort_to_partition_group( test_cohort, self.user_partition.id, self.groups[0].id, ) add_user_to_cohort(test_cohort, self.student.username) self.assert_student_in_group(self.groups[0])
new_groups = [Group(10, 'New Group 10'), Group(20, 'New Group 20'), Group(30, 'New Group 30')] new_user_partition = UserPartition(
self.assert_student_in_group(new_groups[0], new_user_partition)
new_user_partition = UserPartition(
new_user_partition = UserPartition(
self._verify_masquerade_for_all_groups()
return None
return None
DEFAULT_USER_MESSAGE = ugettext_noop(u'An error has occurred. Please try again.')
response.data["current_page"] = self.page.number
response.data["start"] = (self.page.number - 1) * self.get_page_size(self.request)
tasks.update_xblocks_cache.apply_async([unicode(course_key)], countdown=0)
try: fields = kwargs['context'].pop('fields', DEFAULT_FIELDS) or DEFAULT_FIELDS except KeyError: fields = DEFAULT_FIELDS super(BookmarkSerializer, self).__init__(*args, **kwargs)
required_fields = set(fields) all_fields = set(self.fields.keys()) for field_name in all_fields - required_fields: self.fields.pop(field_name)
from __future__ import unicode_literals
if Bookmark.objects.filter(user=user, course_key=course_key).count() >= settings.MAX_BOOKMARKS_PER_COURSE: return False
with self.assertNumQueries(1): bookmarks = api.get_bookmarks(user=self.user, course_key=course.id, serialized=False) self.assertEqual(len(bookmarks), count)
self.assert_bookmark_data_is_valid(bookmarks[-1], bookmarks_data[0], check_optional_fields=check_all_fields) self.assert_bookmark_data_is_valid(bookmarks[0], bookmarks_data[-1], check_optional_fields=check_all_fields)
response = self.send_get( client=self.client, url=reverse('bookmarks'), query_parameters='course_id=invalid' ) bookmarks_data = response.data['results']
self.other_sequential_2.children.append(self.other_vertical_1.location)
self.assertEqual(bookmark, bookmark2) self.assertEqual(bookmark.xblock_cache, bookmark2.xblock_cache) self.assert_bookmark_model_is_valid(bookmark2, bookmark_data)
block = modulestore().get_course(course.id, depth=None) for __ in range(depth - 1): children = block.get_children() block = children[-1]
usage_key = UsageKey.from_string('i4x://edX/apis/html/interactive') usage_key.replace(course_key=self.course.id) self.assertEqual(Bookmark.get_path(usage_key), [])
self.other_sequential_1.children = []
bookmark_service = BookmarksService(self.other_user) with self.assertNumQueries(1): self.assertFalse(bookmark_service.is_bookmarked(usage_key=self.sequential_1.location))
with self.assertNumQueries(0): self.assertFalse( self.bookmark_service.set_bookmarked(usage_key=UsageKey.from_string("i4x://ed/ed/ed/interactive")) )
blocks_stack.extend(children)
if not isinstance(course_id, basestring): raise ValueError('course_id must be a string. {} is not acceptable.'.format(type(course_id)))
managed = False
for field in ('client_type', 'client_secret', 'client_id', 'authorization_grant_type'): form.fields.pop(field)
Application.objects.filter(user=self.request.user).delete() return super(ApiRequestStatusView, self).form_valid(form)
if not username: return redirect(reverse('api_admin:catalog-search')) return redirect(reverse('api_admin:catalog-list', kwargs={'username': username}))
kwargs.setdefault('label_suffix', '') super(ApiAccessRequestForm, self).__init__(*args, **kwargs)
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
final_attrs['value'] = force_text(value)
self.assertEqual(len([r for r in httpretty.httpretty.latest_requests if r.method == 'POST']), 0)
self.assertEqual(len([r for r in httpretty.httpretty.latest_requests if r.method == 'PATCH']), 0)
from smtplib import SMTPException
mock_model_log_exception.assert_called_once_with( 'Error sending API user notification email for request [%s].', self.api_access_request.id ) self.assertIsNotNone(self.api_access_request.id)
mock_model_log_exception.assert_called_once_with( 'Error sending API user notification email for request [%s].', self.api_access_request.id ) self.assertEqual(self.api_access_request.status, ApiAccessRequest.APPROVED)
full_name = UserProfile.objects.get(user=user).name
uploaded_file = request.FILES['file']
with closing(uploaded_file):
try: validate_uploaded_image(uploaded_file) except ImageValidationError as error: return Response( {"developer_message": error.message, "user_message": error.user_message}, status=status.HTTP_400_BAD_REQUEST, )
profile_image_names = get_profile_image_names(username) create_profile_images(uploaded_file, profile_image_names)
set_has_profile_image(username, True, _make_upload_dt())
return Response(status=status.HTTP_204_NO_CONTENT)
set_has_profile_image(username, False)
profile_image_names = get_profile_image_names(username) remove_profile_images(profile_image_names)
return Response(status=status.HTTP_204_NO_CONTENT)
if exif is None: image.save(string_io, format='JPEG') else: image.save(string_io, format='JPEG', exif=exif)
width, height = image_obj.size self.assertEqual(width, height) actual_sizes[width] = name
_view_name = None client_class = PatchedClient
self.reset_tracker()
profile = self.user.profile.__class__.objects.get(user=self.user) self.assertEqual(profile.has_profile_image, has_profile_image)
with make_image_file() as image_file: response = self.client.post(self.url, {'file': image_file}, format='multipart') self.check_response(response, 204)
self.reset_tracker()
self.reset_tracker() different_client = APIClient() different_client.login(username=different_user.username, password=TEST_PASSWORD) response = different_client.delete(self.url) self.check_response(response, 404)
patched_client_login = Client.login
self.assertIsNone(getattr(self.request, 'session', None)) self.assertIsNone(getattr(self.request, 'safe_cookie_verified_user_id', None))
self.assert_response(safe_cookie_data) self.assert_user_in_session()
self.assertEquals(self.request.COOKIES[settings.SESSION_COOKIE_NAME], session_id)
self.assertIsNotNone(self.request.session)
self.assertEquals(self.request.safe_cookie_verified_user_id, self.user.id)
self.assert_response_with_delete_cookie()
safe_cookie_data_1 = SafeCookieData.create(session_id, user_id) self.assertTrue(safe_cookie_data_1.verify(user_id))
serialized_value = unicode(safe_cookie_data_1)
safe_cookie_data_2 = SafeCookieData.parse(serialized_value) self.assertTrue(safe_cookie_data_2.verify(user_id))
self.assert_cookie_data_equal(safe_cookie_data_1, safe_cookie_data_2)
self.assertEqual( self.safe_cookie_data._compute_digest(self.user_id), self.safe_cookie_data._compute_digest(self.user_id), )
log.debug( "SafeCookieData received empty user_id '%s' for session_id '%s'.", user_id, session_id, )
return self._on_user_authentication_failed(request)
return process_request_response
if is_request_from_mobile_app(request): return HttpResponse(status=401)
log_func = log.debug if request.user.id is None else log.warning log_func(
safe_cookie_data = SafeCookieData.create( cookies[settings.SESSION_COOKIE_NAME].value, user_id, )
cookies[settings.SESSION_COOKIE_NAME] = unicode(safe_cookie_data)
if settings.ROOT_URLCONF != 'lms.urls': raise unittest.SkipTest('Test only valid in lms')
for resource_id, resource in self.test_recommendations.iteritems(): for xblock_name in self.XBLOCK_NAMES: result = self.call_event('add_resource', resource, xblock_name)
for resource, xblock_name in itertools.product(self.test_recommendations.values(), self.XBLOCK_NAMES): self.call_event('add_resource', resource, xblock_name)
self.check_event_response_by_key( 'add_resource', self.test_recommendations[self.resource_id], 'id', self.resource_id )
self.check_event_response_by_key('flag_resource', resource, 'reason', '')
self.check_event_response_by_key('flag_resource', resource, 'reason', 'reason 0')
for xblock_name in self.XBLOCK_NAMES: self.check_event_response_by_key('flag_resource', resource, 'reason', 'reason 0', xblock_name)
resp = json.loads(self.call_event('export_resources', {}).content)
self.check_event_response_by_key('handle_vote', resource, 'newVotes', test_case['new_votes'])
self.check_event_response_by_key('handle_vote', resource, 'newVotes', test_case['new_votes'])
resource['event'] = test_case['event_second'] self.check_event_response_by_key('handle_vote', resource, 'newVotes', test_case['new_votes'])
resource['id'] = self.resource_id_second self.check_event_response_by_key('handle_vote', resource, 'newVotes', test_case['new_votes'])
self.check_event_response_by_key( 'handle_vote', resource, 'newVotes', test_case['new_votes'], self.XBLOCK_NAMES[1] )
self.check_event_response_by_key('handle_vote', resource, 'newVotes', test_case['new_votes'])
self.check_event_response_by_key('endorse_resource', resource, test_case['key'], test_case['val'])
self.check_event_response_by_http_status('remove_resource', resource, test_case['status'])
resource = {"id": self.resource_id, 'reason': ''} self.check_event_response_by_http_status(test_case['handler'], resource, test_case['status'])
self.attempt_upload_file_and_verify_result(test_case, 'upload_screenshot')
self.attempt_upload_file_and_verify_result(test_case, 'upload_screenshot')
self.login(email, password)
if settings.ROOT_URLCONF != 'lms.urls': raise unittest.SkipTest('Test only valid in lms') super(XBlockTestCase, cls).setUpClass()
html_response.debug = {'url': url, 'section': section, 'block_urlname': block_urlname} return html_response
test_configuration = [ { "urlname": "two_done_block_test_case_0", #"olx": self.olx_scenarios[0],
self.assertEqual(resp.data, {"state": desired_state})
self.check_response('done_0', 'done-unmarked') self.check_response('done_1', 'done-unmarked')
self.toggle_button('done_0', {}, False) self.toggle_button('done_1', {}, True)
self.check_response('done_0', 'done-unmarked') self.check_response('done_1', 'done-marked')
if settings.ROOT_URLCONF != 'lms.urls': raise unittest.SkipTest('Test only valid in lms')
from random import choice characters = 'abcdefghijklmnopqrstuvwxyz0123456789!@#$%^&*(-_=+)' SECRET_KEY = ''.join([choice(characters) for i in range(50)])
PROFILE_IMAGE_MAX_BYTES = 1000 PROFILE_IMAGE_MIN_BYTES = 1000
import django try: django.setup()
#templates_path.append('source/_templates')
#html_static_path.append('source/_static')
if on_rtd: os.environ['DJANGO_SETTINGS_MODULE'] = 'lms' else: os.environ['DJANGO_SETTINGS_MODULE'] = 'lms'
#templates_path.append('source/_templates')
#html_static_path.append('source/_static')
#)
if on_rtd: os.environ['DJANGO_SETTINGS_MODULE'] = 'lms' else: os.environ['DJANGO_SETTINGS_MODULE'] = 'lms'
exclude_patterns = ['build', 'links.rst']
#needs_sphinx = '1.0'
templates_path = add_base(['_templates'])
source_suffix = '.rst'
#source_encoding = 'utf-8-sig'
master_doc = 'index'
project = u'edX' copyright = u'2013, EdX Doc Team'
version = '0.1' release = '0.1'
#today = '' #today_fmt = '%B %d, %Y'
exclude_patterns = []
#default_role = None
#add_function_parentheses = True
#add_module_names = True
#show_authors = False
pygments_style = 'sphinx'
#modindex_common_prefix = []
#keep_warnings = False
html_theme = 'default'
#html_theme_options = {}
#html_theme_path = []
#html_title = None
#html_short_title = None
#html_logo = None
#html_favicon = None
#html_static_path = add_base(['_static'])
#html_last_updated_fmt = '%b %d, %Y'
#html_use_smartypants = True
#html_sidebars = {}
#html_additional_pages = {}
#html_domain_indices = True
#html_use_index = True
#html_split_index = False
#html_show_sourcelink = True
#html_show_sphinx = True
#html_show_copyright = True
#html_use_opensearch = ''
#html_file_suffix = None
htmlhelp_basename = 'edxdoc'
#'papersize': 'letterpaper',
#'pointsize': '10pt',
#'preamble': '',
latex_documents = [ ( 'index', 'getting_started.tex', u'edX Studio Documentation', u'EdX Doc Team', 'manual', ), ]
#latex_logo = None
#latex_use_parts = False
#latex_show_pagerefs = False
#latex_show_urls = False
#latex_appendices = []
#latex_domain_indices = True
man_pages = [ ('index', 'getting_started', u'getting_started Documentation', [u'EdX Doc Team'], 1) ]
#man_show_urls = False
#texinfo_appendices = []
#texinfo_domain_indices = True
#texinfo_show_urls = 'footnote'
#texinfo_no_detailmenu = False
epub_title = u'getting_started' epub_author = u'EdX Doc Team' epub_publisher = u'EdX Doc Team' epub_copyright = u'2013, EdX Doc Team'
#epub_language = ''
#epub_scheme = ''
#epub_identifier = ''
#epub_uid = ''
#epub_cover = ()
#epub_guide = ()
#epub_pre_files = []
#epub_post_files = []
#epub_exclude_files = []
#epub_tocdepth = 3
#epub_tocdup = True
#epub_fix_images = False
#epub_max_image_width = 0
#epub_show_urls = 'inline'
#epub_use_index = True
intersphinx_mapping = {'http://docs.python.org/': None}
sys = 'cms' if sys == 'studio' else sys return cmd("python manage.py", sys, "--settings={}".format(settings), *args)
self.log_dir.makedirs_p() self.har_dir.makedirs_p() self.report_dir.makedirs_p()
self.load_data()
CourseFixture('foobar_org', '1117', 'seed_forum', 'seed_foo').install() print 'Forums permissions/roles data has been seeded'
pass
substring = [ "--with-xunitmp --xunitmp-file={}".format(self.xunit_report), "--processes={}".format(self.num_processes), "--no-color --process-timeout=1200" ]
bokchoy_utils.clear_mongo() self.cache.flush_all()
self.load_data()
self.load_courses()
msg = colorize('green', "Confirming servers are running...") print msg bokchoy_utils.start_servers(self.default_store, self.coveragerc)
if not self.test_spec: test_spec = self.test_dir else: test_spec = self.test_dir / self.test_spec
if self.serversonly: return ""
self.should_fetch_course = False
self.should_fetch_course = kwargs.get('should_fetch_course') self.imports_dir = path('test_root/courses/')
with self: if self.cmd: passed = self.run_test() if not passed: self.failed_suites.append(self)
self.dbs[db].remove()
for db_alias in self.dbs.keys(): sh("cp {db_cache} {db}".format(db_cache=self.db_caches[db_alias], db=self.dbs[db_alias]))
for db_alias in self.dbs.keys(): sh("cp {db} {db_cache}".format(db_cache=self.db_caches[db_alias], db=self.dbs[db_alias]))
if self.failed_only: opts += "--failed"
env_fail_fast_set = ( 'TESTS_FAIL_FAST' in os.environ and os.environ['TEST_FAIL_FAST'] )
self.processes = 0
default_test_id = ( "{system}/djangoapps/*" " common/djangoapps/*" " openedx/core/djangoapps/*" " openedx/tests/*" " openedx/core/lib/*" )
sh('find {dir} -type f -delete'.format(dir=directory))
reports_dir = Env.REPORT_DIR.makedirs_p() clean_dir(reports_dir)
output = os.popen('mongo --eval "print(\'running\')"').read() return output and "running" in output
return Env.BOK_CHOY_CACHE.set('test', 'test')
REPO_ROOT = path(__file__).abspath().parent.parent.parent
REPORT_DIR = REPO_ROOT / 'reports' METRICS_DIR = REPORT_DIR / 'metrics'
PYTHON_COVERAGERC = REPO_ROOT / ".coveragerc"
BOK_CHOY_STUB_DIR = REPO_ROOT / "common" / "djangoapps" / "terrain"
VIDEO_SOURCE_DIR = REPO_ROOT / "test_root" / "data" / "video"
BOK_CHOY_MONGO_DATABASE = "test" BOK_CHOY_CACHE = memcache.Client(['0.0.0.0:11211'], debug=0)
TEST_DIR = REPO_ROOT / ".testids"
I18N_REPORT_DIR = REPORT_DIR / 'i18n'
SERVICE_VARIANT = os.environ.get('SERVICE_VARIANT', None)
try: with open(env_path) as env_file: return json.load(env_file)
child_pids = p1_group.get_children(recursive=True)
if tasks.environment.dry_run: for cmd in cmd_list: tasks.environment.info(cmd) return
except Exception as err: print("Error running process {}".format(err), file=sys.stderr)
child_pids = p1_group.get_children(recursive=True)
proc.wait()
python_suite = suites.PythonTestSuite('Python Tests', **opts) js_suite = suites.JsTestSuite('JS Tests', mode='run', with_coverage=True)
all_unittests_suite = suites.TestSuite('All Tests', subsuites=[js_suite, python_suite]) all_unittests_suite.run()
sh("coverage combine --rcfile={}".format(rcfile))
err_msg = colorize( 'red', "No coverage info found. Run `paver test` before running " "`paver coverage`.\n" ) sys.stderr.write(err_msg) return
sh("coverage xml --rcfile={}".format(rcfile)) sh("coverage html --rcfile={}".format(rcfile)) call_task('diff_coverage', options=dict(options))
xml_reports = []
sh( "diff-cover {xml_report_str} --compare-branch={compare_branch} " "--html-report {diff_html_path}".format( xml_report_str=xml_report_str, compare_branch=compare_branch, diff_html_path=diff_html_path, ) )
PRIVATE_REQS = 'requirements/private.txt' if os.path.exists(PRIVATE_REQS): PYTHON_REQ_FILES.append(PRIVATE_REQS)
if os.path.isfile(path_item): with open(path_item, "rb") as file_handle: hasher.update(file_handle.read())
new_hash = compute_fingerprint(paths) if new_hash != old_hash: install_func()
PACKAGES_TO_UNINSTALL = [
for _ in range(3): uninstalled = False frozen = sh("pip freeze", capture=True)
sh("pip uninstall --disable-pip-version-check -y {}".format(package_name)) uninstalled = True
print "Couldn't uninstall unwanted Python packages!" return
with open(state_file_path, "w") as state_file: state_file.write(expected_version)
files_to_fingerprint = list(PYTHON_REQ_FILES)
files_to_fingerprint.append(sysconfig.get_python_lib())
src_dir = os.path.join(sys.prefix, "src") if os.path.isdir(src_dir): files_to_fingerprint.append(src_dir)
this_file = __file__ if this_file.endswith(".pyc"):
report_dir = (Env.REPORT_DIR / system).makedirs_p()
Env.METRICS_DIR.makedirs_p()
report_dir = (Env.REPORT_DIR / system).makedirs_p()
violations_count_str = "Number of pylint violations: " + str(num_violations) print violations_count_str
with open(Env.METRICS_DIR / "pylint", "w") as f: f.write(violations_count_str)
if num_violations > violations_limit > -1: raise BuildFailure("Failed. Too many pylint violations. " "The limit is {violations_limit}.".format(violations_limit=violations_limit))
pylint_pattern = re.compile(r".(\d+):\ \[(\D\d+.+\]).")
if len(violation_list_for_line) == 4: num_violations_report += 1
Env.METRICS_DIR.makedirs_p()
violations_count_str = "Number of pep8 violations: {count}".format(count=count) print violations_count_str print violations_list
with open(Env.METRICS_DIR / "pep8", "w") as f: f.write(violations_count_str + '\n\n') f.write(violations_list)
if count: failure_string = "Too many pep8 violations. " + violations_count_str failure_string += "\n\nViolations:\n{violations_list}".format(violations_list=violations_list) raise BuildFailure(failure_string)
Env.METRICS_DIR.makedirs_p() _prepare_report_dir(complexity_report_dir)
_write_metric(num_violations, (Env.METRICS_DIR / "jshint"))
if num_violations > violations_limit > -1: raise BuildFailure( "JSHint Failed. Too many violations ({count}).\nThe limit is {violations_limit}.".format( count=num_violations, violations_limit=violations_limit ) )
_write_metric(metrics_str, metrics_report) sh("cat {metrics_report}".format(metrics_report=metrics_report), ignore_error=True)
violations_count_str = "Number of {safecommit_script} violations: {num_violations}\n".format( safecommit_script=safecommit_script, num_violations=num_violations )
metrics_report = (Env.METRICS_DIR / "safecommit") _write_metric(violations_count_str, metrics_report) sh("cat {metrics_report}".format(metrics_report=metrics_report), ignore_error=True)
raise BuildFailure(file_not_found_message)
regex = r'\d+.\d+'
regex = r'^\d+'
except (AttributeError, ValueError): return None
except (AttributeError, ValueError): violations['total'] = None return violations
dquality_dir = (Env.REPORT_DIR / "diff_quality").makedirs_p()
diff_quality_percentage_pass = True
(count, violations_list) = _get_pep8_violations()
print _pep8_output(count, violations_list)
with open(dquality_dir / "diff_quality_pep8.html", "w") as f: f.write(_pep8_output(count, violations_list, is_html=True))
compare_branch = getattr(options, 'compare_branch', None) compare_branch_string = u'' if compare_branch: compare_branch_string = u'--compare-branch={0}'.format(compare_branch)
diff_threshold = int(getattr(options, 'percentage', -1)) percentage_string = u'' if diff_threshold > -1: percentage_string = u'--fail-under={0}'.format(diff_threshold)
if not run_diff_quality( violations_type="pylint", prefix=pythonpath_prefix, reports=pylint_reports, percentage_string=percentage_string, branch_string=compare_branch_string, dquality_dir=dquality_dir ): diff_quality_percentage_pass = False
if not run_diff_quality( violations_type="jshint", prefix=pythonpath_prefix, reports=jshint_reports, percentage_string=percentage_string, branch_string=compare_branch_string, dquality_dir=dquality_dir ): diff_quality_percentage_pass = False
if not diff_quality_percentage_pass: raise BuildFailure("Diff-quality failure(s).")
NPM_VENDOR_DIRECTORY = path("common/static/common/js/vendor")
restart_django_servers()
import sass
if tasks.environment.dry_run: tasks.environment.info("install npm_assets") return
NPM_VENDOR_DIRECTORY.mkdir_p()
for library in NPM_INSTALLED_LIBRARIES: sh('/bin/cp -rf node_modules/{library} {vendor_dir}'.format( library=library, vendor_dir=NPM_VENDOR_DIRECTORY, ))
if tasks.environment.dry_run: return
try: while True: observer.join(2) except KeyboardInterrupt: observer.stop() print("\nStopped asset watcher.")
using_firefox = (os.environ.get('SELENIUM_BROWSER', 'firefox') == 'firefox') validate_firefox = getattr(options, 'validate_firefox_version', using_firefox)
if settings == DEFAULT_SETTINGS: args.append('--skip-collect') call_task('pavelib.assets.update_assets', args=args)
args = [ 'lms', 'studio', '--settings={}'.format(asset_settings), '--skip-collect' ] call_task('pavelib.assets.update_assets', args=args)
if settings != DEFAULT_SETTINGS: collect_assets(['lms'], asset_settings_lms) collect_assets(['studio'], asset_settings_cms)
call_task('pavelib.assets.watch_assets', options={'background': True})
sh("NO_EDXAPP_SUDO=1 EDX_PLATFORM_SETTINGS_OVERRIDE={settings} /edx/bin/edxapp-migrate-{system} --traceback --pythonpath=. {fake}".format( settings=settings, system=system, fake=fake))
os.environ.clear() os.environ.update(_orig_environ)
self.assertEqual(sysex.exception.args, (1,))
self.assertEqual(sysex.exception.args, (1,))
call_task('pavelib.quality.run_safelint', options={"thresholds": '{"total": 5}'})
call_task('pavelib.quality.run_safelint', options={"thresholds": '{"rules": {"javascript-escape": 5}}'})
check_firefox_version()
self._mock_paver_needs = patch.object(pavelib.quality.run_jshint, 'needs').start() self._mock_paver_needs.return_value = 0
patcher = patch('pavelib.quality.sh') self._mock_paver_sh = patcher.start()
self.addCleanup(patcher.stop) self.addCleanup(self._mock_paver_needs.stop)
suite = BokChoyTestSuite('', default_store='invalid') name = 'tests' self.assertEqual( suite.cmd, self._expected_command(name=name, store='invalid') )
mock_sh = patch('pavelib.utils.test.suites.bokchoy_suite.sh') self._mock_sh = mock_sh.start()
self.addCleanup(mock_sh.stop)
tasks.environment = MockEnvironment()
os.environ['NO_PREREQ_INSTALL'] = 'true'
with open(self.f.name, 'w') as f: f.write("foo/hello/test.py:304:15: E203 whitespace before ':'")
self._mock_paver_needs = patch.object(pavelib.quality.run_quality, 'needs').start() self._mock_paver_needs.return_value = 0
self.f = tempfile.NamedTemporaryFile(delete=False) self.f.close()
self.addCleanup(self._mock_paver_needs.stop) self.addCleanup(os.remove, self.f.name)
self.assertEqual(_mock_pep8_violations.call_count, 1) self.assertEqual(self._mock_paver_sh.call_count, 2)
self.assertEqual(self._mock_paver_sh.call_count, 1)
paver.easy.sh("exit 1")
paver.easy.sh("exit 1")
self._mock_paver_needs = patch.object(pavelib.js_test.test_js, 'needs').start() self._mock_paver_needs.return_value = 0
self.addCleanup(self._mock_paver_needs.stop)
sh("i18n_tool generate")
for system in ['lms', 'cms']: sh(django_cmd(system, DEFAULT_SETTINGS, 'compilejsi18n'))
from safe_lxml import defuse_xml_libs defuse_xml_libs()
if not enable_contracts and not edx_args.contracts: contracts.disable_all()
django_args.append('--help')
line_start_indexes = [0] index = 0 while True: index = string.find('\n', index) if index < 0: break index += 1 line_start_indexes.append(index) return line_start_indexes
return self._string
current_line_number = 0 for line_break_index in self._line_start_indexes: if line_break_index <= index: current_line_number += 1 else: break return current_line_number
line_number = self.index_to_line_number(index) return self.line_number_to_start_index(line_number)
line_number = self.index_to_line_number(index) return self.line_number_to_end_index(line_number)
return self._line_start_indexes[line_number - 1]
start_index = self._line_start_indexes[line_number - 1] if len(self._line_start_indexes) == line_number: line = self._string[start_index:] else: end_index = self._line_start_indexes[line_number] line = self._string[start_index:end_index - 1] return line
return len(self._line_start_indexes)
self.rule = rule self.full_path = '' self.is_disabled = False
return (0, 0, self.rule.rule_id)
return ''
self.full_path = full_path self._mark_disabled(string_lines.get_string())
print("{}: {}".format(self.full_path, self.rule.rule_id), file=out)
return (self.start_line, self.start_column, self.rule.rule_id)
return self.lines[0]
self.total_violations = 0 self.totals_by_rule = dict.fromkeys( [rule.rule_id for rule in Rules.__members__.values()], 0 )
self.total_violations += 1 self.totals_by_rule[violation.rule.rule_id] += 1
self.full_path = full_path self.directory = os.path.dirname(full_path) self.is_file = os.path.isfile(full_path) self.violations = []
string_lines = StringLines(file_string) for violation in self.violations: violation.prepare_results(self.full_path, string_lines) if line_comment_delim is not None: self._filter_commented_code(line_comment_delim)
self.violations = [v for v in self.violations if not self._is_commented(v, line_comment_delim)]
start_match = quote_regex.search(template, start_index, end_index) if start_match is None: return None else: return start_match.start()
with open(file_full_path, 'r') as input_file: file_contents = input_file.read() return file_contents.decode(encoding='utf-8')
super(UnderscoreTemplateLinter, self).__init__() self._skip_underscore_dirs = SKIP_DIRS + ('test',)
self._check_underscore_expressions(underscore_template, results) results.prepare_results(underscore_template)
expressions = self._find_unescaped_expressions(underscore_template) for expression in expressions: if not self._is_safe_unescaped_expression(expression): results.violations.append(ExpressionRuleViolation( Rules.underscore_not_escaped, expression ))
super(JavaScriptLinter, self).__init__() self._skip_javascript_dirs = SKIP_DIRS + ('i18n', 'static/coffee') self._skip_coffeescript_dirs = SKIP_DIRS self.underscore_linter = UnderscoreTemplateLinter()
if match is not None: return True
super(BaseVisitor, self).__init__() self.file_contents = file_contents self.lines = StringLines(self.file_contents) self.results = results
if node.name != '__repr__': self.generic_visit(node)
super(HtmlStringVisitor, self).__init__(file_contents, results) self.skip_wrapped_html = skip_wrapped_html self.unsafe_html_string_nodes = [] self.over_escaped_entity_string_nodes = [] self.has_text_or_html_call = False
super(ContainsFormatVisitor, self).__init__(file_contents, results) self.contains_format_call = False
super(FormatInterpolateVisitor, self).__init__(file_contents, results) self.interpolates_text_or_html = False self.format_caller_node = None
if node.attr == 'display_name_with_default_escaped': self.results.violations.append(ExpressionRuleViolation( Rules.python_deprecated_display_name, self.node_to_expression(node) )) self.generic_visit(node)
super(PythonLinter, self).__init__() self._skip_python_dirs = SKIP_DIRS + ('tests', 'test/acceptance')
super(MakoTemplateLinter, self).__init__() self.javascript_linter = JavaScriptLinter() self.python_linter = PythonLinter()
parse_string = self._find_string_wrapping_expression(mako_template, expression) if parse_string is None: results.violations.append(ExpressionRuleViolation( Rules.mako_js_missing_quotes, expression ))
lines = StringLines(data['string']) self.assertEqual(lines.line_number_to_line(data['line_number']), data['line'])
patcher = mock.patch.object(linter_class, '_is_valid_directory', return_value=True) patch_start = patcher.start() self.addCleanup(patcher.stop) return patch_start
linter = MakoTemplateLinter() self.assertEqual(linter._is_valid_directory(data['directory']), data['expected'])
'result': {'start_index': 2, 'end_index': 16, 'quote_length': 3}
'result': {'start_index': 3, 'end_index': 11, 'quote_length': 1}
'rule': None
linter = JavaScriptLinter() results = FileResults('') linter.check_javascript_file_is_safe(data['template'], results) self._validate_data_rules(data, results)
linter = JavaScriptLinter() results = FileResults('') linter.check_javascript_file_is_safe(data['template'], results) self._validate_data_rules(data, results)
linter = JavaScriptLinter() results = FileResults('') linter.check_javascript_file_is_safe(data['template'], results) self._validate_data_rules(data, results)
linter = JavaScriptLinter() results = FileResults('') linter.check_javascript_file_is_safe(data['template'], results) self._validate_data_rules(data, results)
linter = JavaScriptLinter() results = FileResults('') linter.check_javascript_file_is_safe(data['template'], results) self._validate_data_rules(data, results)
linter = JavaScriptLinter() results = FileResults('') linter.check_javascript_file_is_safe(data['template'], results) self._validate_data_rules(data, results)
linter = JavaScriptLinter() results = FileResults('') linter.check_javascript_file_is_safe(data['template'], results) self._validate_data_rules(data, results)
linter = JavaScriptLinter() results = FileResults('') linter.check_javascript_file_is_safe(data['template'], results) self._validate_data_rules(data, results)
linter = PythonLinter() results = FileResults('') linter.check_python_file_is_safe(data['template'], results) self._validate_data_rules(data, results)
'rule': Rules.python_wrap_html
'rule': None
'rule': Rules.python_interpolate_html
'rule': Rules.python_interpolate_html
return template.strip().format(hotfix_hash=hotfix_hash)
today = date.today() TUESDAY = 2 days_until_tuesday = (TUESDAY - today.isoweekday()) % 7 return today + timedelta(days=days_until_tuesday)
return set(JIRA_RE.findall(text))
return tuple(Commit.iter_items( repo, "{start}..{end}".format(start=start_ref, end=end_ref), first_parent=True, no_merges=True, ))
return self.construct_scalar(node)
microsite.enable_microsites(log)
field_dictionary, filter_dictionary, _ = LmsSearchFilterGenerator.generate_field_filters() self.assertTrue('start_date' in filter_dictionary) self.assertEqual(0, len(field_dictionary['course']))
_, _, exclude_dictionary = LmsSearchFilterGenerator.generate_field_filters(user=self.user) self.assertNotIn('org', exclude_dictionary)
if user not in self._user_enrollments: self._user_enrollments[user] = CourseEnrollment.enrollments_for_user(user) return self._user_enrollments[user]
if self._course_key is None: self._course_key = SlashSeparatedCourseKey.from_deprecated_string(self._results_fields["course"]) return self._course_key
if self._usage_key is None: self._usage_key = self.get_course_key().make_usage_key_from_deprecated_string(self._results_fields["id"]) return self._usage_key
if self._module_store is None: self._module_store = modulestore() return self._module_store
block = self.store.get_item(block_location) block.group_access = access_dict self.store.update_item(block, 1)
self.assertEqual(message.text, expected_message) self.assertEqual(message.type, expected_message_type)
validation = self.store.get_item(self.video_location).validate() self.assertEqual(len(validation.messages), 0)
self.assertTrue(self.open_assessment.has_score)
pass
block = self.store.get_item(block_location) self.assertEqual(block.merged_group_access, expected_dict)
self.attributes["commentable_id"] = self.attributes["id"] self.retrieved = True return self
return self.thread.context
def __init__(self, collection, page, num_pages, thread_count=0, corrected_text=None): self.collection = collection self.page = page self.num_pages = num_pages self.thread_count = thread_count self.corrected_text = corrected_text
return CourseEmailTemplate.get_template(name=self.template_name)
try: return CourseEmailTemplate.objects.get(name=name) except CourseEmailTemplate.DoesNotExist: log.exception("Attempting to fetch a non-existent course email template") raise
return CourseEmailTemplate._render(self.plain_template, plaintext, context)
try: record = cls.objects.get(course_id=course_id) return record.email_enabled except cls.DoesNotExist: return False
readonly_fields = ('sender',)
list_display = ('user', 'course_id')
return True
template = self.cleaned_data["html_template"] self._validate_template(template) return template
template = self.cleaned_data["plain_template"] self._validate_template(template) return template
CourseEmailTemplate = apps.get_model("bulk_email", "CourseEmailTemplate") if not CourseEmailTemplate.objects.exists(): call_command("loaddata", "course_email_template.json")
self.emails_sent += new_subtask_status.succeeded return update_subtask_status(entry_id, current_task_id, new_subtask_status)
self.instructor = InstructorFactory(course_key=self.course.id) self.staff = [ StaffFactory(course_key=self.course.id) for __ in xrange(STAFF_COUNT) ]
self.students = [UserFactory() for _ in xrange(STUDENT_COUNT)] for student in self.students: CourseEnrollmentFactory.create(user=student, course_id=self.course.id)
self.client.login(username=user.username, password="test")
self.test_send_to_all()
CourseStaffRole(self.course.id).add_users(self.instructor) self.test_send_to_all()
CourseEnrollment.enroll(self.instructor, self.course.id) self.test_send_to_all()
def test_get_missing_template(self): with self.assertRaises(CourseEmailTemplate.DoesNotExist): CourseEmailTemplate.get_template()
context = self._get_sample_plain_context() context['course_image_url'] = "/location/of/course/image/url" return context
pass
task_args = [entry_id, {}] return task_class.apply(task_args, task_id=task_id).get()
bogus_task_id = "this-is-bogus" update_subtask_status(entry_id, bogus_task_id, new_subtask_status)
return [self.create_student('robot%d' % i) for i in xrange(num_students)]
pass
subtask_id = initial_subtask_status.task_id new_subtask = send_course_email.subtask( ( entry_id, email_id, to_list, global_email_context, initial_subtask_status.to_dict(), ), task_id=subtask_id, routing_key=routing_key, ) return new_subtask
return from_addr_format.format( course_title=course_title_no_quotes, course_name=course_name, from_email=theming_helpers.get_value( 'bulk_email_default_from_email', settings.BULK_EMAIL_DEFAULT_FROM_EMAIL ) )
return str(s).replace('<', '&lt;').replace('>', '&gt;')
allowable_chars = string.ascii_letters + string.digits username = '' for _index in range(30): username = username + random.SystemRandom().choice(allowable_chars) return username
return self.lti_consumer.consumer_key == client_key
return self.lti_consumer.consumer_secret
raise NotImplementedError
raise NotImplementedError
raise NotImplementedError
raise NotImplementedError
raise NotImplementedError
raise NotImplementedError
raise NotImplementedError
raise NotImplementedError
raise NotImplementedError
raise NotImplementedError
raise NotImplementedError
raise NotImplementedError
raise NotImplementedError
raise NotImplementedError
raise NotImplementedError
raise NotImplementedError
raise NotImplementedError
raise NotImplementedError
raise NotImplementedError
raise NotImplementedError
return LtiConsumer( consumer_name='Consumer Name', consumer_key='Consumer Key', consumer_secret='Consumer Secret' )
key = self.lti_consumer.consumer_key self.assertTrue(SignatureValidator(self.lti_consumer).check_client_key(key))
self.assertFalse(SignatureValidator(self.lti_consumer).check_client_key(key))
nonce = '0123456789012345678901234567890123456789012345678901234567890123' self.assertTrue(SignatureValidator(self.lti_consumer).check_nonce(nonce))
self.assertFalse(SignatureValidator(self.lti_consumer).check_nonce(nonce))
key = self.lti_consumer.consumer_key secret = SignatureValidator(self.lti_consumer).get_client_secret(key, None) self.assertEqual(secret, self.lti_consumer.consumer_secret)
request = build_launch_request() views.lti_launch(request, unicode(COURSE_KEY), unicode(USAGE_KEY)) render.assert_called_with(request, USAGE_KEY)
request = build_launch_request() del request.POST[missing_param] return views.lti_launch(request, None, None)
self.setup_course() self.setup_user(admin=False, enroll=False, login=True) self.verify_response()
response = MagicMock() response.status_code = status response.content = xml.format(major_code=major_code).encode('ascii', 'ignore') return response
assignment = GradedAssignment( user=self.user, course_key=self.course.id, usage_key=desc.location, outcome_service=outcome_service, lis_result_sourcedid=result_id, version_number=0 ) assignment.save() return assignment
edx_user = User(username=self.edx_user_id) edx_user.save() lti_user = LtiUser( lti_consumer=self.lti_consumer, lti_user_id=self.lti_user_id, edx_user=edx_user ) lti_user.save() return lti_user
mock = MagicMock(return_value=return_value) new_patch = patch(function_name, new=mock) new_patch.start() self.addCleanup(new_patch.stop) return mock
cls = args[0] if cls.search_is_enabled(): return f(*args, **kwargs)
languages = dict(settings.ALL_LANGUAGES) try: return languages[self.course_team.language] except KeyError: return self.course_team.language
search_engine = cls.engine() serialized_course_team = CourseTeamIndexer(course_team).data() search_engine.index(cls.DOCUMENT_TYPE_NAME, [serialized_course_team])
cls.engine().remove(cls.DOCUMENT_TYPE_NAME, [course_team.team_id])
try: return SearchEngine.get_search_engine(index=cls.INDEX_NAME) except ConnectionError as err: logging.error('Error connecting to elasticsearch: %s', err) raise ElasticSearchConnectionError
return settings.FEATURES.get(cls.ENABLE_SEARCH_KEY, False)
try: CourseTeamIndexer.index(kwargs['instance']) except ElasticSearchConnectionError: pass
comment = kwargs['post'] handle_activity(kwargs['user'], comment, long(comment.thread.user_id))
if original_author_id is not None and user.id != original_author_id: return if getattr(post, "context", "course") == TEAM_DISCUSSION_CONTEXT: CourseTeamMembership.update_last_activity(user, post.commentable_id)
if not CourseEnrollment.is_enrolled(user, self.course_id): raise NotEnrolledInCourseForTeam if CourseTeamMembership.user_in_team_for_course(user, self.course_id): raise AlreadyOnTeamInCourse return CourseTeamMembership.objects.create( user=user, team=self )
self.team_size = CourseTeamMembership.objects.filter(team=self).count() self.save()
super(CourseTeamMembership, self).delete(*args, **kwargs) self.team.reset_team_size()
page_size = TOPICS_PER_PAGE
page_size = TEAM_MEMBERSHIPS_PER_PAGE
return has_team_api_access(request.user, obj.course_id)
return CourseTeam.objects.all()
return sorted(course_module.teams_topics, key=lambda t: t['name'].lower())
try: return CourseTeam.objects.get(team_id=team_id) except CourseTeam.DoesNotExist: raise Http404
try: return CourseTeamMembership.objects.get(user__username=username, team=team) except CourseTeamMembership.DoesNotExist: raise Http404
try: result = CourseTeam.objects.get(team_id=team_id) except ObjectDoesNotExist: raise CommandError(u"Argument {0} is not a course_team team_id".format(team_id)) return result
with self.assertRaisesRegexp(CommandError, ".* requires one or more arguments.*"): call_command('reindex_course_team')
with self.assertRaisesRegexp(CommandError, ".*ENABLE_TEAMS must be enabled.*"): call_command('reindex_course_team', self.team1.team_id)
call_command('reindex_course_team', self.team1.team_id) mock_index.assert_called_once_with(self.team1) mock_index.reset_mock()
call_command('reindex_course_team', self.team1.team_id, self.team2.team_id) mock_index.assert_any_call(self.team1) mock_index.assert_any_call(self.team2) mock_index.reset_mock()
return unicode(obj)
if data and data not in self.COUNTRY_CODES: raise serializers.ValidationError( u"{code} is not a valid country code".format(code=data) ) return data
def __init__(self, *args, **kwargs): super(CourseTeamSerializerWithoutMembership, self).__init__(*args, **kwargs) del self.fields['membership']
class Meta(object): list_serializer_class = BulkTeamCountTopicListSerializer
return {'username': username, 'team_id': team}
return self.build_membership_data_raw(self.users[username].username, team.team_id)
return self.make_call(reverse('teams_list'), expected_status, 'post', data, **kwargs)
return self.make_call(reverse('teams_detail', args=[team_id]), expected_status, 'get', data, **kwargs)
return self.make_call(reverse('teams_detail', args=[team_id]), expected_status, 'delete', **kwargs)
return self.make_call( reverse('teams_detail', args=[team_id]), expected_status, 'patch', json.dumps(data) if data else None, 'application/merge-patch+json', **kwargs )
return self.make_call(reverse('topics_list'), expected_status, 'get', data, **kwargs)
return self.make_call( reverse('topics_detail', kwargs={'topic_id': topic_id, 'course_id': str(course_id)}), expected_status, 'get', data, **kwargs )
return self.make_call(reverse('team_membership_list'), expected_status, 'get', data, **kwargs)
return self.make_call(reverse('team_membership_list'), expected_status, 'post', data, **kwargs)
return self.make_call( reverse('team_membership_detail', args=[team_id, username]), expected_status, 'get', data, **kwargs )
url = reverse('team_membership_detail', args=[team_id, username]) + '?admin=true' return self.make_call(url, expected_status, 'delete', **kwargs)
for field in ['id', 'name', 'course_id', 'topic_id', 'date_created', 'description']: self.assertIn(field, team)
CourseTeamIndexer.engine().destroy() for team in self.test_team_name_id_map.values(): CourseTeamIndexer.index(team)
return expected_prefix + '-' + team['discussion_topic_id']
self.assertIn('id', team) self.assertIn('discussion_topic_id', team) self.assertEqual(team['id'], self._expected_team_id(team, expected_prefix))
self.delete_team(self.wind_team.team_id, 204, user='staff')
result = first.copy() result.update(second) return result
self.course.teams_configuration['topics'] = [] self.assert_serializer_output([], num_teams_per_topic=0, num_queries=0)
topics = self.setup_topics(teams_per_topic=0) self.assert_serializer_output(topics, num_teams_per_topic=0, num_queries=1)
teams_per_topic = 10 topics = self.setup_topics(teams_per_topic=teams_per_topic) self.assert_serializer_output(topics, num_teams_per_topic=teams_per_topic, num_queries=1)
teams_per_topic = 10 topics = self.setup_topics(num_topics=self.NUM_TOPICS, teams_per_topic=teams_per_topic) self.assert_serializer_output(topics, num_teams_per_topic=teams_per_topic, num_queries=1)
result = first.copy() result.update(second) return result
if user is None: user = self.user return Mock( user_id=user.id, commentable_id=self.DISCUSSION_TOPIC_ID, context=context, **{'thread.user_id': self.user.id} )
with self.assert_last_activity_updated(should_update): user = getattr(self, user) signal.send(sender=None, user=user, post=self.mock_comment())
return [profile.strip() for profile in cls.current().video_profiles.split(",") if profile]
latest_version_config = cls.objects.filter(platform=platform, enabled=True).first() if latest_version_config: return latest_version_config.version
self.login() response = self.api_response(expected_response_code=302) self.assertTrue(self.username in response['location'])
self.verify_pdf_certificate()
self.verify_pdf_certificate()
path = self._last_visited_module_path(request, course) path_ids = [unicode(module.location) for module in path] return Response({ "last_visited_module_id": path_ids[0], "last_visited_module_path": path_ids, })
return self._get_course_info(request, course)
xblock.group_access = {partition_id: group_ids} self.store.update_item(xblock, self.user.id)
return ( usage_key.block_type in self.block_types or usage_key.block_type in BLOCK_TYPES_WITH_CHILDREN )
return ['platform', 'version', 'expire_at', 'enabled', 'created_at', 'updated_at']
self._add_prerequisite_course() self.init_course_access() self._verify_unfulfilled_milestone_response()
self._add_entrance_exam() self.init_course_access() self._verify_unfulfilled_milestone_response()
self._add_entrance_exam() self._pass_entrance_exam() self.init_course_access() self.api_response()
request = get_request_for_user(self.user) answer_entrance_exam_problem(self.course, request, self.problem_1)
self.client.login(username=self.username, password=self.password)
self.client.logout()
CourseEnrollment.enroll(self.user, course_id or self.course.id)
CourseEnrollment.unenroll(self.user, course_id or self.course.id)
self.login() self.enroll(course_id)
url = self.reverse_url(reverse_args, **kwargs) response = self.url_method(url, **kwargs) if expected_response_code is not None: self.assertEqual(response.status_code, expected_response_code) return response
return self.client.get(url)
def test_no_auth(self): self.logout() self.api_response(expected_response_code=401)
self.assertEqual(response.status_code, 200)
self.assertEqual(response.status_code, 404) if error_type: self.assertEqual(response.data, error_type.to_json())
self.login_and_enroll(course_id)
return view_course_access(depth=depth, access_action='load_mobile', check_for_milestones=True)
return view_auth_classes(is_user)
if is_request_from_mobile_app(request): return MobilePlatform.get_instance(user_agent)
return AppVersionConfig.last_supported_date(platform_name, platform_version) or self.NO_LAST_SUPPORTED_DATE
self.def_ms = modulestore() self.msg = u'' self.datatable = [] super(SysadminDashboardView, self).__init__(**kwargs)
return self.def_ms.get_courses()
csv_file.seek(0) csv_data = csv_file.read() csv_file.seek(0) csv_file.truncate() return csv_data
for row in data: writer.writerow(row) csv_data = read_and_flush() yield csv_data
with self.assertRaisesRegexp(CommandError, regex): call_command('git_add_course', *args, stderr=StringIO.StringIO())
MESSAGE = None def __init__(self, message=None): if message is None: message = self.message super(GitImportError, self).__init__(message)
MESSAGE = _( 'Non usable git url provided. Expecting something like:' ' git@github.com:mitocw/edx4edx_lite.git' )
MESSAGE = _('Unable to get git log')
MESSAGE = _('git clone or pull failed!')
MESSAGE = _('Unable to run import command.')
MESSAGE = _('The underlying module store does not support import.')
GlobalStaff().add_users(self.user) self.client.login(username=self.user.username, password='foo')
for path in glob.glob(path): shutil.rmtree(path)
os.mkdir(path) self.addCleanup(shutil.rmtree, path)
self.user.is_staff = True self.user.save() self.client.login(username=self.user.username, password='foo')
return ['sub']
return data['user'].is_staff
return ['name', 'locale']
user = data['user'] profile = UserProfile.objects.get(user=user) return profile.name
return self.find_courses(data['user'], CourseInstructorRole.ROLE, data.get('values'))
return self.find_courses(data['user'], CourseStaffRole.ROLE, data.get('values'))
self._test_view('sitemap_xml', 'application/xml')
self._test_view('about', 'text/html')
super(SupportViewIndexTests, self).setUp() SupportStaffRole().add_users(self.user)
super(SupportViewCertificatesTests, self).setUp() SupportStaffRole().add_users(self.user)
self.client.logout() self._verify_response(403)
self._verify_response(200)
self.create_programs_config(enable_certification=False) self._verify_response(400)
course = get_course_overview_with_access(user, 'staff', course_id) return bool(has_access(user, 'staff', course))
has_access.return_value = True response = views.all_problem_grade_distribution(self.request, 'test/test/test') self.assertEqual(json.dumps(self.simple_data), response.content)
has_access.return_value = True response = views.all_sequential_open_distrib(self.request, 'test/test/test') self.assertEqual(json.dumps(self.simple_data), response.content)
has_access.return_value = True response = views.section_problem_grade_distrib(self.request, 'test/test/test', '1') self.assertEqual(json.dumps(self.simple_data), response.content)
mailchimp = MailSnake(api_key) result = mailchimp.ping() log.debug(result) return mailchimp
objects = UserProfile.objects course_key = CourseKey.from_string(course_id) students = objects.filter(user__courseenrollment__course_id=course_key, user__courseenrollment__is_active=True) return students
return get_members(mailchimp, list_id, 'subscribed')
return get_members(mailchimp, list_id, 'unsubscribed')
return get_members(mailchimp, list_id, 'cleaned')
batch_unsubscribe = mailchimp.listBatchUnsubscribe result = batch_unsubscribe(id=list_id, emails=emails, send_goodbye=False, delete_member=False) log.debug(result)
if len(name) > 10: name = name[:10] return name.replace(' ', '_').strip()
return ''
return ''
return ''
return None
return u'%b %d, %Y'
return ''
return ''
return render_to_string('courseware/date_summary.html', self.get_context())
if self.date is not None: return datetime.now(pytz.UTC) <= self.date return False
css_class = 'start-date' title = ugettext_lazy('Course Starts') @property def date(self): return self.course.start
return SoftwareSecurePhotoVerification.user_status(self.user)[0]
deadline = self.date return deadline is not None and deadline <= datetime.now(pytz.UTC)
parent = block.get_parent() while parent: yield parent parent = parent.get_parent()
disabled = ()
prev = _OVERRIDES_DISABLED.disabled _OVERRIDES_DISABLED.disabled += (True,) yield _OVERRIDES_DISABLED.disabled = prev
return bool(_OVERRIDES_DISABLED.disabled)
raise NotImplementedError
return False
if not overrides_disabled(): for provider in self.providers: value = provider.get(block, name, NOTSET) if value is not NOTSET: return value return NOTSET
items = list(items) return (items[i:i + chunk_size] for i in xrange(0, len(items), chunk_size))
queryset = cls.objects.filter( course_id=course_id, module_type='problem', grade__isnull=False ) if "read_replica" in settings.DATABASES: return queryset.using("read_replica") else: return queryset
return StudentModule.objects.get(pk=self.student_module_id)
class Meta(object): app_label = "courseware" unique_together = (('student', 'field_name'),) student = models.ForeignKey(User, db_index=True)
pass
md5 = hashlib.md5() for data in resource: md5.update(repr(data)) return md5.hexdigest()
score_bucket = "incorrect" if grade > 0 and grade < max_grade: score_bucket = "partial" elif grade == max_grade: score_bucket = "correct" return score_bucket
return u"grades.MaxScores.{}___{}".format(self.cache_prefix, unicode(location))
return remote_key.split(u"___", 1)[1]
return len(self._max_scores_cache)
return len(self._max_scores_updates)
loc_str = unicode(location) if self._max_scores_cache.get(loc_str) != max_score: self._max_scores_updates[loc_str] = max_score
loc_str = unicode(location) max_score = self._max_scores_updates.get(loc_str) if max_score is None: max_score = self._max_scores_cache.get(loc_str) return max_score
descriptor_filter = partial(descriptor_affects_grading, course.block_types_affecting_grading) return FieldDataCache.cache_for_descriptor_descendents( course.id, user, course, depth=None, descriptor_filter=descriptor_filter )
progress = _progress_summary(student, request, course, field_data_cache, scores_client) if progress: return progress.chapters else: return None
request = RequestFactory().get('/') request.user = student return request
with modulestore().bulk_operations(course_key): course = modulestore().get_course(course_key, depth=depth) if course: return course else: raise Http404("Course not found.")
for directory in dirs: filepath = path(directory) / filename if filesystem.exists(filepath): return filepath raise ResourceNotFoundError(u"Could not find {0}".format(filename))
blocks = _get_course_date_summary_blocks(course, user) return '\n'.join( b.render() for b in blocks )
if block.date is None: return datetime.max.replace(tzinfo=pytz.UTC) return block.date
return microsite.get_value( 'COURSE_ABOUT_VISIBILITY_PERMISSION', settings.COURSE_ABOUT_VISIBILITY_PERMISSION )
courses = sorted( courses, key=lambda course: (course.has_ended(), course.start is None, course.start), reverse=False ) return courses
studio_link = None if course.course_edit_method == "Studio": studio_link = get_cms_course_link(course, page) return studio_link
if not is_entrance_exams_enabled(): return False if not course.entrance_exam_enabled: return False if not course.entrance_exam_id: return False return True
from courseware.module_render import get_module_for_descriptor field_data_cache = FieldDataCache([descriptor], course.id, request.user) return get_module_for_descriptor( request.user, request, descriptor, field_data_cache, course.id, course=course )
self.set_many({key: value})
if key.scope not in self._allowed_scopes: raise InvalidScopeError(key, self._allowed_scopes)
cache_key = self._cache_key_for_kvs_key(kvs_key) if cache_key not in self._cache: raise KeyError(kvs_key.field_name) field_object = self._cache[cache_key] return json.loads(field_object.value)
self.set_many({kvs_key: value})
cache_key = self._cache_key_for_kvs_key(kvs_key) field_object = self._cache.get(cache_key) if field_object is None: raise KeyError(kvs_key.field_name) field_object.delete() del self._cache[cache_key]
return self._cache_key_for_kvs_key(kvs_key) in self._cache
field_object = self._cache.get(self._cache_key_for_kvs_key(kvs_key)) if field_object is None: return None else: return field_object.modified
raise NotImplementedError()
raise NotImplementedError()
self.set_many({kvs_key: value})
try: return self._client.get( self.user.username, kvs_key.block_scope_id, fields=[kvs_key.field_name], ).updated except self._client.DoesNotExist: return None
cache_key = self._cache_key_for_kvs_key(kvs_key) if cache_key not in self._cache: raise KeyError(kvs_key.field_name) return self._cache[cache_key][kvs_key.field_name]
cache_key = self._cache_key_for_kvs_key(kvs_key) return ( cache_key in self._cache and kvs_key.field_name in self._cache[cache_key] )
return key.block_scope_id
return (field_object.usage_id.map_into_course(self.course_id), field_object.field_name)
return (key.block_scope_id, key.field_name)
return (field_object.module_type, field_object.field_name)
return (BlockTypeKeyV1(key.block_family, key.block_scope_id), key.field_name)
return field_object.field_name
return key.field_name
scope_map = defaultdict(set) for descriptor in descriptors: for field in descriptor.fields.values(): scope_map[field.scope].add(field) return scope_map
self.course_key = course_key self.user_id = user_id self._locations_to_scores = {} self._has_fetched = False
return location in self._locations_to_scores
type = 'courseware' title = ugettext_noop('Course') priority = 10 view_name = 'courseware' is_movable = False is_default = False
raise NotImplementedError()
return self.link_value
return (super(ExternalDiscussionCourseTab, cls).validate(tab_dict, raise_error) and key_checker(['link'])(tab_dict, raise_error))
return (super(ExternalLinkCourseTab, cls).validate(tab_dict, raise_error) and key_checker(['link', 'name'])(tab_dict, raise_error))
return reverse_func(view_name, args=[unicode(course.id), index])
queue = [course] while len(queue) > 0: node = queue.pop() queue.extend(node.get_children()) return True
MODULESTORE = TEST_DATA_MONGO_MODULESTORE __test__ = True
MODULESTORE = TEST_DATA_SPLIT_MODULESTORE __test__ = True url_name = 'course'
with open(filename) as f: results = f.read() os.remove(filename) return results
tmp_dir = mkdtemp() try: course_dir = export_course_to_directory(course_key, tmp_dir) compress_directory(course_dir, filename) finally: shutil.rmtree(tmp_dir, ignore_errors=True)
return ACCESS_DENIED if is_prerequisite_courses_enabled() else ACCESS_GRANTED
response = ( _visible_to_nonstaff_users(courselike) and _can_access_descriptor_with_start_date(user, courselike, courselike.id) ) return ( ACCESS_GRANTED if (response or _has_staff_access_to_descriptor(user, courselike, courselike.id)) else response )
return _can_enroll_courselike(user, courselike)
return ACCESS_GRANTED if (can_load() or can_enroll()) else ACCESS_DENIED
return ( _has_catalog_visibility(courselike, CATALOG_VISIBILITY_CATALOG_AND_ABOUT) or _has_staff_access_to_descriptor(user, courselike, courselike.id) )
return ( _has_catalog_visibility(courselike, CATALOG_VISIBILITY_CATALOG_AND_ABOUT) or _has_catalog_visibility(courselike, CATALOG_VISIBILITY_ABOUT) or _has_staff_access_to_descriptor(user, courselike, courselike.id) )
course_key = ccx_key.to_course_locator() return _has_access_course_key(user, action, course_key)
if perm != 'global': debug("Deny: invalid permission '%s'", perm) return ACCESS_DENIED return ACCESS_GRANTED if GlobalStaff().has_user(user) else ACCESS_DENIED
if perm != 'global': return ACCESS_DENIED return ( ACCESS_GRANTED if GlobalStaff().has_user(user) or SupportStaffRole().has_user(user) else ACCESS_DENIED )
return _has_instructor_access_to_location(user, descriptor.location, course_key)
return _has_staff_access_to_location(user, descriptor.location, course_key)
return VisibilityError() if descriptor.visible_to_staff_only else ACCESS_GRANTED
return MilestoneError() if any_unfulfilled_milestones(course_id, user.id) else ACCESS_GRANTED
return MilestoneError() if get_pre_requisite_courses_not_completed(user, course_id) else ACCESS_GRANTED
return ACCESS_GRANTED if course.catalog_visibility == visibility_type else ACCESS_DENIED
return ACCESS_GRANTED if descriptor.mobile_available else MobileAvailabilityError()
return ( auth.user_has_role(user, CourseBetaTesterRole(descriptor.id)) or _has_staff_access_to_descriptor(user, descriptor, descriptor.id) or _is_descriptor_mobile_available(descriptor) )
return self.has_access
return { "has_access": self.has_access, "error_code": self.error_code, "developer_message": self.developer_message, "user_message": self.user_message }
def __init__(self, url): super(Redirect, self).__init__() self.url = url
return True
pass
pass
pass
self.user = user
dog_stats_api.increment( 'DjangoXBlockUserStateClient.{}'.format(evt_name), timestamp=evt_time, sample_rate=self.API_DATADOG_SAMPLE_RATE, )
dog_stats_api.histogram( 'DjangoXBlockUserStateClient.{}'.format(evt_name), value, timestamp=evt_time, sample_rate=self.API_DATADOG_SAMPLE_RATE, )
if scope != Scope.user_state: raise ValueError("Only Scope.user_state is supported") raise NotImplementedError()
if requested_child == 'first': return children[0] elif requested_child == 'last': return children[-1] else: return children[0]
if user is None: return False if user.is_authenticated(): return CourseEnrollment.is_enrolled(user, course.id) else: return False
course_key = CourseKey.from_string(course_id) with modulestore().bulk_operations(course_key): return _progress(request, course_key, student_id)
return render_to_response('financial-assistance/financial-assistance.html', { 'header_text': FINANCIAL_ASSISTANCE_HEADER })
newrelic.agent.add_custom_parameter('course_id', unicode(self.course_key)) newrelic.agent.add_custom_parameter('org', unicode(self.course_key.org))
self._redirect_if_needed_to_pay_for_course() self._redirect_if_needed_to_register() self._redirect_if_needed_for_prereqs() self._redirect_if_needed_for_course_survey()
if must_answer_survey(self.course, self.effective_user): raise Redirect(reverse('course_survey', args=[unicode(self.course.id)]))
gated_content = gating_api.get_gated_content(self.course, self.effective_user) if gated_content: if unicode(self.section.location) in gated_content: raise Http404
language_preference = get_user_preference(self.real_user, LANGUAGE_KEY) if not language_preference: language_preference = settings.LANGUAGE_CODE return language_preference
return self.masquerade and self.masquerade.role == 'student'
return self._find_block(self.course, self.chapter_url_name, 'chapter', CONTENT_DEPTH - 1)
if self.chapter: return self._find_block(self.chapter, self.section_url_name, 'section')
save_child_position(self.course, self.chapter_url_name) save_child_position(self.chapter, self.section_url_name)
return u'grades'
block_structure.request_xblock_fields(*cls.FIELDS_TO_COLLECT) cls._collect_max_scores(block_structure)
pass
for module in cls._iter_scorable_xmodules(block_structure): cls._collect_max_score(block_structure, module)
score = module.max_score() block_structure.set_transformer_block_field(module.location, cls, 'max_score', score)
user = User.objects.get(email=email) user.set_password(new_password) user.save() history = PasswordHistory() history.create(user)
email, password = self._setup_user() self._login(email, password) email, password = self._setup_user(is_staff=True) self._login(email, password)
pass
self.logout()
resp = self.client.get('/copyright') self.assertEqual(resp.status_code, 404)
super(CoachAccessTestCaseCCX, cls).setUpClass() cls.course = CourseFactory.create()
access.has_access(None, 'staff', 'global', None)
user.masquerade_settings = { self.course_key: CourseMasquerade(self.course_key, role=role) }
self.assertEqual( 'student', access.get_user_role(self.anonymous_user, self.course_key) )
overview = CourseOverview.get_from_id(self.course_default.id) with self.assertRaises(ValueError): access.has_access(self.user, '_non_existent_action', overview)
return ItemFactory.create( parent_location=parent.location, category="video", display_name="Group {} Sees This Video".format(group), )
return ItemFactory.create( parent_location=parent.location, category="problem", display_name="Group {} Sees This Problem".format(group), data="<h1>No Problem Defined Yet!</h1>", )
super(AboutWithCappedEnrollmentsTestCase, self).setUp()
course_mode = CourseMode( course_id=course.id, mode_slug=CourseMode.DEFAULT_MODE_SLUG, mode_display_name=CourseMode.DEFAULT_MODE_SLUG, min_price=10, ) course_mode.save()
resp = self.client.get('/') self.assertEqual(resp.status_code, 200) self.assertContains(resp, 'footer-edx-v3')
resp = self.client.get('/') self.assertEqual(resp.status_code, 200) self.assertContains(resp, 'footer-openedx')
self._assert_chapter_loaded(self.course, self.chapter)
self.assertFalse(user_can_skip_entrance_exam(self.anonymous_user, self.course))
self.request.user = self.anonymous_user self.assertFalse(user_has_passed_entrance_exam(self.request, self.course))
while block.parent: block = block.get_parent() return block
user = UserFactory() user.name = 'mock_user' user.is_staff = is_staff user.is_enrolled = is_enrolled user.is_authenticated = lambda: is_authenticated return user
return tab.is_enabled(course, user=user)
serialized_tab = tab.to_json() deserialized_tab = tab.from_json(serialized_tab) self.assertEquals(serialized_tab, deserialized_tab)
old_value = tab[key] new_value = 'New Value' tab[key] = new_value self.assertEquals(tab[key], new_value) tab[key] = old_value self.assertEquals(tab[key], old_value)
for tab in tab_list: if tab.type == tab_type: return True return False
if viewname == "django_comment_client.forum.views.forum_form_discussion" and args == [unicode(course.id)]: return "default_discussion_link"
self.check_discussion( tab_list=self.tabs_with_discussion, discussion_link_in_course="other_discussion_link", expected_discussion_link="other_discussion_link", expected_can_display_value=True, )
for tab_list in [[], self.tabs_with_discussion, self.tabs_without_discussion]: self.check_discussion( tab_list=tab_list, expected_discussion_link=not None, expected_can_display_value=False, )
self.check_discussion( tab_list=self.tabs_with_discussion, expected_discussion_link="default_discussion_link", expected_can_display_value=True, )
self.check_discussion( tab_list=self.tabs_without_discussion, expected_discussion_link=not None, expected_can_display_value=False, )
return get_test_system(course_id=self.course.id)
return reverse( 'xblock_handler', args=(unicode(self.course.id), quote_slashes(self.item_url), 'xmodule_handler', dispatch) )
user = User() user.save() DarkLangConfig( released_languages=languages, changed_by=user, enabled=True ).save()
content = content or SRT_content srt_file = tempfile.NamedTemporaryFile(suffix=".srt") srt_file.content_type = 'application/x-subrip; charset=utf-8' srt_file.write(content) srt_file.seek(0) return srt_file
content_location = StaticContent.compute_location( location.course_key, asset_name ) try: contentstore().find(content_location) except NotFoundError: return False else: return True
store = contentstore() assets, __ = store.get_all_content_for_course(location.course_key) for asset in assets: asset_location = asset['asset_key'] del_cached_content(asset_location) store.delete(asset_location)
item.sub = filename
item.video_bumper["transcripts"][lang] = filename
raise NotImplementedError
course = CourseFactory.create(start=datetime(2013, 9, 16, 7, 17, 28)) course = modulestore().get_course(course.id) return course
text = views.course_about(self.request, unicode(course_key)).content return text
result = Fragment() if 'activate_block_id' in context: result.add_content(u"Activate Block ID: {block_id}</p>".format(block_id=context['activate_block_id'])) return result
def setUp(self): super(TestRenderXBlockSelfPaced, self).setUp() SelfPacedConfiguration(enabled=True).save() def course_options(self): return {'self_paced': True}
cdn_response_video_url = settings.CDN_VIDEO_URLS["CN"] + self.original_video_file self.assertEqual( rewrite_video_url(settings.CDN_VIDEO_URLS["CN"], self.original_video_url), cdn_response_video_url )
invalid_cdn_url = 'http://http://fakecdn.com/' self.assertIsNone(rewrite_video_url(invalid_cdn_url, self.original_video_url))
self.assertIsNone(rewrite_video_url(None, None))
self.assertIsNone(rewrite_video_url("", ""))
context = { "profiles": [self.TEST_PROFILE], "allow_cache_miss": "True" if allow_cache_miss else "False" } return self.video.student_view_data(context)
self.video.edx_video_id = self.TEST_EDX_VIDEO_ID self.setup_val_video(associate_course_in_val=False) result = self.get_result(allow_cache_miss) if allow_cache_miss: self.verify_result_with_val_profile(result) else: self.verify_result_with_fallback_and_youtube(result)
super(TestLTIModuleListing, self).setUp()
youtube_str = '1.00:p2Q6BrNhdh8' youtube_str_hack = '1.0:p2Q6BrNhdh8' self.assertEqual( VideoDescriptor._parse_youtube(youtube_str), VideoDescriptor._parse_youtube(youtube_str_hack) )
self.course = self.store.get_course(self.course.id)
return self.course.id.make_usage_key('problem', problem_url_name)
location = self.problem_location(problem_url_name) modx_url = self.modx_url(location, "problem_get") resp = self.client.get(modx_url) return resp
problem_location = self.problem_location(problem_url_name) modx_url = self.modx_url(problem_location, 'problem_reset') resp = self.client.post(modx_url) return resp
problem_location = self.problem_location(problem_url_name) modx_url = self.modx_url(problem_location, 'problem_show') resp = self.client.post(modx_url) return resp
self.course.grading_policy = grading_policy self.update_course(self.course, self.student_user.id) self.refresh_course()
grade_summary = self.get_grade_summary() self.assertEqual(grade_summary['percent'], percent)
return [s.earned for s in self.get_grade_summary()['totaled_scores']['Homework']]
self.basic_setup() self.check_grade_percent(0) self.assertEqual(self.get_grade_summary()['grade'], None)
self.test_b_grade_exact()
self.weighted_setup() self.submit_question_answer('FinalQuestion', {'2_1': 'Correct', '2_2': 'Correct'}) self.check_grade_percent(0.75)
return [reverse(name, kwargs={'course_id': course.id.to_deprecated_string()}) for name in names]
self._assert_no_redirect(self.course_without_survey)
self._assert_survey_redirect(self.course)
resp = self.client.post( self.postback_url, self.student_answers ) self.assertEquals(resp.status_code, 200) self._assert_no_redirect(self.course)
self._assert_no_redirect(self.course_with_bogus_survey)
if value == 'course_org_filter': return alternate return default
course = CourseFactory.create(org='edX', course='999') self.assertEquals(course_image_url(course), '/c4x/edX/999/asset/{0}'.format(course.course_image))
course = CourseFactory.create(static_asset_path="foo") self.assertEquals( course_image_url(course), '/static/foo/images/course_image.jpg' )
course = CourseFactory.create(course_image=u'things_stuff.jpg', static_asset_path="foo") self.assertEquals( course_image_url(course), '/static/foo/things_stuff.jpg' )
course = self.process_xml(xml.CourseFactory.build()) self.assertEquals(course_image_url(course), '/static/xml_test_course/images/course_image.jpg')
url = reverse( 'info', kwargs={ 'course_id': unicode(self.course.id), } ) return self.client.get(url)
request = get_request_for_user(user) request.method = method request.META = {'CONTENT_TYPE': ['application/json']} request.body = body request.session = session or {} return request
content = self.get_courseware_page().content self.assertTrue(self.sequential_display_name in content, "Subsection should be visible") self.assertEqual(staff_debug_expected, 'Staff Debug Info' in content)
return UserFactory()
self.verify_staff_debug_present(False)
self.verify_show_answer_present(False)
return StaffFactory(course_key=self.course.id)
self.logout() self.login(self.test_user.email, 'test')
self.logout() self.login(self.student_user.email, 'test')
return self.submit_question_answer( self.problem_display_name, {'2_1': response1, '2_2': response2} )
return json.loads(self.look_at_question(self.problem_display_name).content)['progress_detail']
url = reverse("progress", kwargs={"course_id": unicode(self.course.id)}) return self.client.get(url)
result = staticfiles.finders.find('images/favicon.ico') self.assertEqual(result, settings.REPO_ROOT / 'lms/static/images/favicon.ico')
OverrideFieldData.provider_classes = None OverrideModulestoreFieldData.provider_classes = None
super(OverrideFieldDataTests, cls).setUpClass() cls.course = CourseFactory.create(enable_ccx=True)
return OverrideFieldData.wrap(TESTUSER, self.course, DictFieldData({ 'foo': 'bar', 'bees': 'knees', }))
return self.current_group.get(user.id, {}).get(user_partition.id)
partition.scheme.set_group_for_user(user, partition, group)
block = modulestore().get_item(block_location) block.group_access = access_dict modulestore().update_item(block, 1)
block = modulestore().get_item(block_location) block.user_partitions = partitions modulestore().update_item(block, 1)
UserPartition.scheme_extensions = None super(GroupAccessTestCase, self).tearDown()
self.assertIs( bool(access.has_access(user, 'load', modulestore().get_item(block_location), self.course.id)), is_accessible )
with self.assertRaises(AssertionError): self.kvs.get(self.other_key_factory(self.existing_field_name))
with self.assertRaises(AssertionError): self.kvs.set(self.other_key_factory(self.existing_field_name), "new_value")
key1 = user_state_key('field_a') key2 = user_state_key('field_b') new_value = 'new value' newer_value = 'newer value' return {key1: new_value, key2: newer_value}
mock = MagicMock(return_value=return_value) new_patch = patch(function_name, new=mock) new_patch.start() self.addCleanup(new_patch.stop) return mock
new_patch = patch(function_name, new=mock) new_patch.start() self.addCleanup(new_patch.stop) return mock
submissions_score_set_handler(None, **SUBMISSION_SET_KWARGS) self.get_user_mock.assert_called_once_with('anonymous_id')
for missing in SUBMISSION_SET_KWARGS: kwargs = SUBMISSION_SET_KWARGS.copy() del kwargs[missing] submissions_score_set_handler(None, **kwargs) self.signal_mock.assert_not_called()
self.get_user_mock = self.setup_patch('courseware.models.user_by_anonymous_id', None) submissions_score_set_handler(None, **SUBMISSION_SET_KWARGS) self.signal_mock.assert_not_called()
submissions_score_reset_handler(None, **SUBMISSION_RESET_KWARGS) self.get_user_mock.assert_called_once_with('anonymous_id')
for missing in SUBMISSION_RESET_KWARGS: kwargs = SUBMISSION_RESET_KWARGS.copy() del kwargs[missing] submissions_score_reset_handler(None, **kwargs) self.signal_mock.assert_not_called()
module_class = EmptyXModule
self.runtime.publish( self, 'grade', { 'value': json_data['grade'], 'max_value': 1 } )
for entry in toc: if entry['url_name'] == url_name: return entry return None
chapter = self._find_url_name(toc, chapter_url_name) if chapter: return self._find_url_name(chapter['sections'], section_url_name) return None
for entry in toc: if entry['url_name'] == url_name: return entry return None
chapter = self._find_url_name(toc, chapter_url_name) if chapter: return self._find_url_name(chapter['sections'], sequential_url_name) return None
self.course = CourseFactory.create(default_store=store) self.course.course_image = '' url = course_image_url(self.course) self.assertEqual('static/test.png', url)
field_data_cache = FieldDataCache.cache_for_descriptor_descendents( course_id, self.staff_user, descriptor ) return render.get_module( self.staff_user, self.request, location, field_data_cache, )
self.setup_mongo_course() result_fragment = self.module.render(STUDENT_VIEW, context=self.default_context) self.assertIn('View Unit in Studio', result_fragment.content)
self.setup_mongo_course(course_edit_method='XML') result_fragment = self.module.render(STUDENT_VIEW, context=self.default_context) self.assertNotIn('View Unit in Studio', result_fragment.content)
self.setup_mongo_course() result_fragment = self.module.render(STUDENT_VIEW, context=self.default_context) self.assertIn('View Unit in Studio', result_fragment.content)
self.setup_mongo_course(course_edit_method='XML') result_fragment = self.module.render(STUDENT_VIEW, context=self.default_context) self.assertNotIn('View Unit in Studio', result_fragment.content)
frag = Fragment(u"Hello there!") return frag
return {'content': 'test1', 'data_field': 'test2'}
event = self.handle_callback_and_get_context_info(mock_tracker, problem_display_name) return event['module']
module = self.get_module_for_user(self.student_user) module.system.publish(module, 'grade', grade_dict) return module
super(TestEventPublishing, self).setUp() self.mock_user = UserFactory() self.mock_user.id = 1 self.request_factory = RequestFactory()
module_class = EmptyXModuleWithChildren has_children = True
self.assertChildren(block, self.children_for_user[user])
self.assertChildren(block, self.all_children)
self.assertEquals(set(child_usage_ids), set(child.scope_ids.usage_id for child in block.get_children()))
students_to_gradesets = {} students_to_errors = {} for student, gradeset, err_msg in iterate_grades_for(course_id, students): students_to_gradesets[student] = gradeset if err_msg: students_to_errors[student] = err_msg return students_to_gradesets, students_to_errors
score = MagicMock() score.possible = possible score.earned = earned return score
return BlockUsageLocator( course_key=self.course_key, block_type=block_type, block_id=block_id )
self.client.login(username=self.user.username, password='test')
return {}
self.__init__(**state)
masquerade_settings = getattr(user, 'masquerade_settings', {}) return masquerade_settings.get(course_key, None)
course_masquerade = get_course_masquerade(user, course_key) return course_masquerade.role if course_masquerade else None
course_masquerade = get_course_masquerade(user, course_key) return bool(course_masquerade and course_masquerade.user_name)
self.kvs = kvs self.session = session self.session_data = session.setdefault(MASQUERADE_DATA_KEY, {})
return repr(tuple(key))
wait_for_problem('Problem 2')
try: assert_is_none(world.browser.get_alert()) except NoAlertPresentException: pass
assert correctness in ['correct', 'incorrect'] assert problem_type in PROBLEM_DICT answer_problem(world.scenario_dict['COURSE'].number, problem_type, correctness)
profile_path = reverse('learner_profile', kwargs={'username': "no_such_user"}) response = self.client.get(path=profile_path) self.assertEqual(404, response.status_code)
super(TestGenerateCourseBlocks, self).setUp() self.course_1 = CourseFactory.create() self.course_2 = CourseFactory.create() self.command = generate_course_blocks.Command()
for course_key in courses: self.assertFalse(is_course_in_block_structure_cache(course_key, self.store))
for course_key in courses: self.assertTrue(is_course_in_block_structure_cache(course_key, self.store))
return _get_block_structure_manager(course_key).get_collected()
return _get_block_structure_manager(course_key).update_collected()
store = modulestore() course_usage_key = store.make_course_usage_key(course_key) return BlockStructureManager(course_usage_key, store, _get_cache())
return "visibility"
return block_structure.get_transformer_block_field( block_key, cls, cls.MERGED_VISIBLE_TO_STAFF_ONLY, False )
return "start_date"
return block_structure.get_transformer_block_field( block_key, cls, cls.MERGED_START_DATE, False )
return "library_content"
try: return StudentModule.objects.get( student=user, course_id=course_key, module_state_key=block_key, state__contains='"selected": [[' ) except StudentModule.DoesNotExist: return None
json_result = [] for key in keys: info = block_structure.get_transformer_block_field( key, ContentLibraryTransformer, 'block_analytics_summary' ) json_result.append(info) return json_result
return "split_test"
return "user_partitions"
self.state = state
return '{}_{}'.format(block_type, block_ref)
xblocks = (blocks[ref] for ref in refs) return set([xblock.location for xblock in xblocks])
return modulestore().get_item(self.xblock_keys[block_index])
return modulestore().update_item(block, ModuleStoreEnum.UserID.test)
store = modulestore() with store.branch_setting(ModuleStoreEnum.Branch.draft_preferred, course.id): store.publish(course.location, ModuleStoreEnum.UserID.test)
return modulestore().make_course_key(org, course, run).make_usage_key(block_type, block_id)
if enum_value == cls.released: return cls.LAST_MONTH elif enum_value == cls.future: return cls.NEXT_MONTH else: return DEFAULT_START_DATE
if block.fields[field_name].is_set_on(block): return getattr(block, field_name) else: return default_value
if not string.islower(): raise ValidationError(_(u"This value must be all lowercase."))
module, klass = settings.BADGING_BACKEND.rsplit('.', 1) module = import_module(module) return getattr(module, klass)()
return self.badgeassertion_set.filter(user=user)
return self.backend.award(self, user, evidence_url=evidence_url)
if course_id: return cls.objects.filter(user=user, badge_class__course_id=course_id) return cls.objects.filter(user=user)
return deserialize_count_specs(self.courses_completed)
return deserialize_count_specs(self.courses_enrolled)
return "{}/v1/issuer/issuers/{}".format(settings.BADGR_BASE_URL, settings.BADGR_ISSUER_SLUG)
return "{}/badges".format(self._base_url)
return "{}/{}".format(self._badge_create_url, slug)
return "{}/assertions".format(self._badge_url(slug))
return {'Authorization': 'Token {}'.format(settings.BADGR_API_TOKEN)}
def award(self, badge_class, user, evidence_url=None): return BadgeAssertionFactory(badge_class=badge_class, user=user)
return BadgrBackend()
self.assertEqual(headers, {'Authorization': 'Token 12345'})
self.check_headers(self.handler._get_headers())
BadgrBackend.badges.append(EXAMPLE_SLUG) self.handler._create_badge = Mock() self.handler._ensure_badge_created(self.badge_class) self.assertFalse(self.handler._create_badge.called)
config = CourseEventBadgesConfiguration.current().enrolled_settings enrollments = user.courseenrollment_set.filter(is_active=True).count() award_badge(config, enrollments, user)
return site_prefix() + reverse( 'certificates:html_view', kwargs={'user_id': user_id, 'course_id': unicode(course_key)}) + '?evidence_visit=1'
about_path = reverse('about_course', kwargs={'course_id': unicode(course_key)}) return u'{}{}'.format(site_prefix(), about_path)
self.assertEqual( course_complete.badge_description(self.course, 'honor'), 'Completed the course "Badged" (honor, 2015-05-19 - 2015-05-20)' )
self.course.end = None self.assertEqual( course_complete.badge_description(self.course, 'honor'), 'Completed the course "Badged" (honor)' )
return '/api/badges/v1/assertions/user/{}/'.format(self.user.username)
if wildcard: return '*' else: return unicode(badge_class.course_id)
if check_course: return RandomBadgeClassFactory.create(course_id=self.course.location.course_key, **kwargs) return RandomBadgeClassFactory.create(**kwargs)
qs_args = { 'issuing_component': badge_class.issuing_component, 'slug': badge_class.slug, } if check_course: qs_args['course_id'] = self.get_course_id(wildcard, badge_class) return qs_args
status_code = 400 default_detail = "The course key provided was invalid."
return queryset.order_by('-created')
return ImageFile(open(TEST_DATA_ROOT / 'badges' / name + '.png'))
self.assertRaises( ValidationError, CourseCompleteImageConfiguration(mode='test2', icon=get_image('unbalanced')).full_clean )
award = Mock()
self.assertIsInstance(BadgeClass().backend, DummyBackend)
self.assertRaises( IntegrityError, BadgeClass.get_badge_class, slug='new_slug', issuing_component='new_component', image_file_handle=get_image('good') )
self.assertRaises( ValidationError, BadgeClass( slug='test', issuing_component='test2', criteria='test3', description='test4', image=get_image('unbalanced') ).full_clean )
validate_badge_image(get_image('good'))
unbalanced = ImageFile(get_image('unbalanced')) self.assertRaises(ValidationError, validate_badge_image, unbalanced)
scheme = u"https" if settings.HTTPS == "on" else u"http" return u'{}://{}'.format(scheme, settings.SITE_NAME)
if not badges_enabled(): return return function(*args, **kwargs)
return settings.FEATURES.get('ENABLE_OPENBADGES', False)
pattern = klass(re) pattern.md = md pattern.ext = self md.inlinePatterns.add(name, pattern, "<reference")
return _is_staff_for_article(article, user)
self.assertContains(resp, "Home") self.assertContains(resp, "Course")
return URLPath.create_article(parent, slug, title=slug)
settings.WIKI_ENABLED = True self.course.allow_public_wiki_access = True self.assertIsNotNone(self.get_wiki_tab(self.user, self.course))
settings.WIKI_ENABLED = False self.assertIsNone(self.get_wiki_tab(self.user, self.course)) self.assertIsNone(self.get_wiki_tab(self.instructor, self.course))
try: float(slug) except ValueError: return False return True
user = self._authenticate(username=username, password=password) if user is not None: request.user = user return True return False
if dot_models.Application.objects.filter(client_id=self._get_client_id(request)).exists(): return self.dot_adapter else: return self.dop_adapter
backend = self.select_backend(request) view = self.get_view_for_backend(backend) return view(request, *args, **kwargs)
return request.POST.get('client_id')
return { 'email': self._attach_email_claim, 'profile': self._attach_profile_claim }
payload['email'] = user.email
return models.Application.objects.create( name=name, user=user, client_id=client_id, client_type=models.Application.CLIENT_CONFIDENTIAL, authorization_grant_type=authorization_grant_type, redirect_uris=redirect_uri, )
return models.Application.objects.create( name=name, user=user, client_id=client_id, client_type=models.Application.CLIENT_PUBLIC, authorization_grant_type=models.Application.GRANT_PASSWORD, redirect_uris=redirect_uri, )
return models.Application.objects.get(**filters)
return token.application
return models.AccessToken.objects.get(token=token_string)
if not scopes: scopes = ['default'] return ' '.join(scopes)
return models.Client.objects.create( name=name, user=user, client_id=client_id, redirect_uri=redirect_uri, client_type=constants.CONFIDENTIAL, )
return models.Client.objects.create( name=name, user=user, client_id=client_id, redirect_uri=redirect_uri, client_type=constants.PUBLIC, )
return models.Client.objects.get(**filters)
return token.client
return models.AccessToken.objects.get(token=token_string)
return ' '.join(scopes)
return self.client.post(self.url, self._post_body(user, client, token_type))
raise NotImplementedError()
return self.dop_adapter.create_confidential_client( name='test_app', user=user, client_id=client_id, redirect_uri=DUMMY_REDIRECT_URL )
return RequestFactory().post('/', {'client_id': client_id})
attr = getattr(student, feature) try: DjangoJSONEncoder().default(attr) return attr except TypeError: return unicode(attr)
return dict((feature, getattr(student, feature)) for feature in features)
return 'dval'
pass
if not predicate: raise ProfileDistribution.ValidationError()
return { 'gender': {'user__profile__gender': value}, 'level_of_education': {'user__profile__level_of_education': value}, }[feature]
survey = None exists = SurveyForm.objects.filter(name=name).exists() if exists: survey = SurveyForm.objects.get(name=name) elif throw_if_not_found: raise SurveyFormNotFound() return survey
return SurveyAnswer.get_answers(self, user, limit_num_users=limit_num_users)
return SurveyAnswer.do_survey_answers_exist(self, user)
SurveyAnswer.objects.filter(form=self, user=user).delete()
return SurveyForm.get_field_names_from_html(self.form)
return SurveyAnswer.objects.filter(form=form, user=user).exists()
redirect_url = request.GET.get('redirect_url') return view_student_survey(request.user, survey_name, redirect_url=redirect_url)
anon_user = Client() resp = anon_user.get(self.view_url) self.assertEquals(resp.status_code, 302)
resp = self.client.get(reverse('view_survey', args=['NonExisting'])) self.assertEquals(resp.status_code, 302)
anon_user = Client() resp = anon_user.post( self.postback_url, self.student_answers ) self.assertEquals(resp.status_code, 302)
resp = self.client.post( reverse('submit_answers', args=['NonExisting']), self.student_answers ) self.assertEquals(resp.status_code, 404)
self.assertTrue(is_survey_required_for_course(self.course))
self.survey.save_user_answers(self.student, self.student_answers, None) self.assertFalse(must_answer_survey(self.course, self.student))
return SurveyForm.create(self.test_survey_name, self.test_form)
with self.assertRaises(SurveyFormNotFound): SurveyForm.get(self.test_survey_name)
self.assertIsNone(SurveyForm.get(self.test_survey_name, throw_if_not_found=False))
survey = self._create_test_survey() self.assertIsNotNone(survey) self.assertEquals(unicode(survey), self.test_survey_name)
with self.assertRaises(ValidationError): SurveyForm.create('badform', '<input name="oops" /><<<>')
self._create_test_survey() with self.assertRaises(SurveyFormNameAlreadyExists): self._create_test_survey()
survey = self._create_test_survey() self.assertEquals(len(survey.get_answers()), 0)
survey = self._create_test_survey() self.assertFalse(survey.has_user_answered_survey(self.student)) self.assertEquals(len(survey.get_answers()), 0)
context = {'payment_support_email': microsite.get_value('payment_support_email', settings.PAYMENT_SUPPORT_EMAIL)} return render_to_response("commerce/checkout_cancel.html", context)
context = {'payment_support_email': microsite.get_value('payment_support_email', settings.PAYMENT_SUPPORT_EMAIL)} return render_to_response("commerce/checkout_error.html", context)
request = RequestCache.get_current_request() return getattr(request, 'user', None)
user, created = User.objects.get_or_create(username=USERNAME, email=EMAIL) if created: user.set_unusable_password() user.save()
add_enrollment(user.username, unicode(course_key), mode)
payload = { "course_id": unicode(course_id or self.course.id) } if marketing_email_opt_in: payload["email_opt_in"] = True return self.client.post(self.url, payload)
actual = json.loads(response.content)['detail'] self.assertEqual(actual, expected_msg)
actual_response = json.loads(response.content) self.assertEqual(actual_response, TEST_PAYMENT_DATA)
self.assertEqual(response.status_code, 500) actual = json.loads(response.content)['detail'] self.assertIn('Call to E-Commerce API failed', actual)
self.assertFalse(CourseEnrollment.is_enrolled(self.user, self.course.id)) self.assert_no_events_were_emitted()
self.client.logout() self.assertEqual(403, self._post_to_view().status_code)
response = getattr(self.client, method)(self.url) self.assertEqual(405, response.status_code)
with mock_create_basket(exception=exceptions.Timeout): response = self._post_to_view() self.assertValidEcommerceInternalRequestErrorResponse(response) self.assertUserNotEnrolled()
with mock_create_basket(exception=exceptions.SlumberBaseException): response = self._post_to_view() self.assertValidEcommerceInternalRequestErrorResponse(response) self.assertUserNotEnrolled()
self.assertProfessionalModeBypassed()
self.assertProfessionalModeBypassed()
with mock_basket_order(basket_id=1, exception=exceptions.HttpNotFoundError): response = self.client.get(self.path) self.assertEqual(response.status_code, 404)
authentication_classes = (OAuth2Authentication, SessionAuthentication,) permission_classes = (IsAuthenticated,) serializer_class = CourseSerializer pagination_class = None def get_queryset(self): return list(Course.iterator())
course = Course( validated_data["id"], self._new_course_mode_models(validated_data["modes"]), verification_deadline=validated_data["verification_deadline"] ) course.save() return course
validated_data["modes"] = self._new_course_mode_models(validated_data["modes"]) instance.update(validated_data) instance.save() return instance
if dt: return JSONEncoder().default(dt) return None
self.client.logout() response = self.client.get(self.path, content_type=JSON_CONTENT_TYPE) self.assertEqual(response.status_code, 401)
self.client.logout() response = getattr(self.client, method)(self.path, content_type=JSON_CONTENT_TYPE) self.assertEqual(response.status_code, 401)
response = getattr(self.client, method)(self.path, content_type=JSON_CONTENT_TYPE) self.assertEqual(response.status_code, 403)
self.client.logout() self.assert_can_create_course(HTTP_X_EDX_API_KEY=settings.EDX_API_KEY)
with mock_order_endpoint(order_number=self.ORDER_NUMBER, exception=exceptions.HttpNotFoundError): response = self.client.get(self.path) self.assertEqual(response.status_code, 404)
course_key = 'non/existing/keyone' error_msg = u"Course {} does not exist.".format(course_key) with self.assertRaisesRegexp(serializers.ValidationError, error_msg): validate_course_id(course_key)
mode = CourseMode(mode_slug=slug) self.assertEqual(self.course.get_mode_display_name(mode), expected_display_name)
self.client.login(username=self.user.username, password='test')
self.client.logout() response = self.client.post(reverse('commerce:checkout_receipt')) self.assertEqual(response.status_code, 302)
response = self.client.post(reverse('commerce:checkout_receipt'), params={'basket_id': 1}, data=post_data) self.assertEqual(response.status_code, 200) return response
CommerceConfiguration.objects.create( checkout_on_ecommerce_service=enabled, single_course_checkout_page=checkout_page )
is_microsite.return_value = True is_enabled = EcommerceService().is_enabled(self.user) self.assertTrue(is_enabled)
url = EcommerceService().payment_page_url() self.assertEqual(url, 'http://ecommerce_url/test_basket/')
UNENROLL_DONE.send(sender=None, course_enrollment=self.course_enrollment, skip_refund=skip_refund)
with mock.patch('commerce.signals.refund_seat') as mock_refund_seat: self.send_signal() self.assertFalse(mock_refund_seat.called)
with mock_create_refund(status=403): refund_seat(self.course_enrollment, UserFactory()) self.assertTrue(mock_log_warning.called)
with mock_create_refund(status=500): self.send_signal() self.assertTrue(mock_log_exception.called)
with mock_create_refund(status=200, response=[1, 2, 3]): self.send_signal() self.assertTrue(mock_send_notification.called)
with mock_create_refund(status=200, response=[]): self.send_signal() self.assertFalse(mock_send_notification.called)
with mock_create_refund(status=200, response=[1, 2, 3]): self.send_signal() self.assertTrue(mock_send_notification.called) self.assertTrue(mock_log_warning.called)
with self.assertRaises(NotImplementedError): send_refund_notification(self.course_enrollment, [1, 2, 3])
httpretty.register_uri(httpretty.POST, urljoin(ZENDESK_URL, '/api/v2/tickets.json'), status=status, body='{}', content_type=JSON)
tags = tags or [u'auto_refund'] create_zendesk_ticket(name, email, subject, body, tags)
with mock.patch('requests.post') as mock_post: self.call_create_zendesk_ticket() self.assertFalse(mock_post.called)
with mock.patch('requests.post', side_effect=Timeout) as mock_post: self.call_create_zendesk_ticket() self.assertTrue(mock_post.called)
def __init__(self, message, status=HTTP_200_OK): data = {'detail': message} super(DetailResponse, self).__init__(resp_obj=data, status=status)
allow_user = user.is_active or user.is_anonymous() return allow_user and self.config.checkout_on_ecommerce_service
ecommerce_url_root = helpers.get_value('ECOMMERCE_PUBLIC_URL_ROOT', settings.ECOMMERCE_PUBLIC_URL_ROOT) return urljoin(ecommerce_url_root, self.config.single_course_checkout_page)
return str(uuid.uuid4())
days_good_for = settings.VERIFY_STUDENT["DAYS_GOOD_FOR"] return datetime.now(pytz.UTC) - timedelta(days=days_good_for)
return cls.objects.filter( user=user, status="approved", created_at__gte=(earliest_allowed_date or cls._earliest_allowed_date()) ).exists()
return cls.verification_valid_or_pending(user, earliest_allowed_date, queryset).exists()
days_good_for = settings.VERIFY_STUDENT["DAYS_GOOD_FOR"] return self.created_at + timedelta(days=days_good_for)
return ( self.created_at < deadline and self.expiration_datetime > deadline )
return self.error_msg
if user_enrollment_mode not in CourseMode.VERIFIED_MODES: return 'N/A' user_is_verified = cls.user_is_verified(user) if not user_is_verified: return 'Not ID Verified' else: return 'ID Verified'
try: deadline = cls.objects.get(course_key=course_key) return deadline.deadline except cls.DoesNotExist: return None
cache.delete(VerificationDeadline.ALL_DEADLINES_CACHE_KEY)
return u"{checkpoint} in {course}".format( checkpoint=self.checkpoint_name, course=self.course_id )
try: return self.checkpoint_status.filter(user_id=user_id).latest() except ObjectDoesNotExist: return None
cls.objects.create(checkpoint=checkpoint, user=user, status=status)
for checkpoint in checkpoints: cls.objects.create(checkpoint=checkpoint, user=user, status=status)
try: verification_status = cls.objects.filter(checkpoint__photo_verification=photo_verification).latest() return verification_status.checkpoint.checkpoint_location except cls.DoesNotExist: return ""
all_checks_points = cls.objects.filter( user_id=user_id, checkpoint__course_id=course_key ) check_points = {} for check in all_checks_points: check_points[check.checkpoint.checkpoint_location] = check.status return check_points
return u"verification.{}.{}".format(user_id, unicode(course_key))
pass
pass
cls.objects.create(checkpoint=checkpoint, user_id=user_id, course_id=course_id)
has_skipped = cls.objects.filter(user_id=user_id, course_id=course_id).exists() return has_skipped
return u"skipped_reverification.{}.{}".format(user_id, unicode(course_key))
return base64.urlsafe_b64encode(aes_encrypt(data, key))
return aes_decrypt(base64.urlsafe_b64decode(encoded_data), key)
cipher = aes_cipher_from_key(key) padded_data = pad(data) return cipher.encrypt(padded_data)
cipher = aes_cipher_from_key(key) padded_data = cipher.decrypt(encrypted_data) return unpad(padded_data)
return AES.new(key, AES.MODE_CBC, generate_aes_iv(key))
return md5(key + md5(key).hexdigest()).hexdigest()[:AES.block_size]
bytes_to_pad = AES.block_size - len(data) % AES.block_size return data + (bytes_to_pad * chr(bytes_to_pad))
num_padded_bytes = ord(padded_data[-1]) return padded_data[:-num_padded_bytes]
key = RSA.importKey(rsa_pub_key_str) cipher = PKCS1_OAEP.new(key) encrypted_data = cipher.encrypt(data) return encrypted_data
key = RSA.importKey(rsa_priv_key_str) cipher = PKCS1_OAEP.new(key) return cipher.decrypt(data)
headers_str = "{}\n\n{}".format(method, header_string(headers_dict)) body_str = body_string(body_dict) message = headers_str + body_str return message
from_address = theming_helpers.get_value( 'email_from_address', settings.DEFAULT_FROM_EMAIL ) user = User.objects.get(id=user_id) user.email_user(subject, message, from_address)
if obj: return self.readonly_fields + ('status', 'checkpoint', 'user', 'response', 'error') return self.readonly_fields
pass
user = UserFactory.create(username="rusty", password="test") self.client.login(username="rusty", password="test")
CourseEnrollmentFactory.create( user=self.user, course_id=course_key, mode=mode )
CourseEnrollment.unenroll(self.user, course_key)
session = self.client.session session["donation_for_course"] = { unicode(course_id): amount } session.save()
response_dict = self._get_page_data(response) self.assertEqual(response_dict['course_mode_slug'], expected_mode)
response_dict = self._get_page_data(response) self.assertEqual(response_dict['current_step'], expected_current_step) self.assertEqual(expected_steps, [ step['name'] for step in response_dict['display_steps'] ])
response_dict = self._get_page_data(response) self.assertEqual(response_dict['message_key'], expected_message)
response_dict = self._get_page_data(response) self.assertEqual(response_dict['full_name'], full_name)
response_dict = self._get_page_data(response) self.assertEqual(response_dict['contribution_amount'], expected_amount)
self.assertEqual(self.client.session.get('attempting_upgrade'), is_upgrade)
self.assertRedirects(response, reverse('dashboard'))
url = reverse('verify_student_start_flow', kwargs={'course_id': unicode(course_id)}) self.assertRedirects(response, url)
url = reverse('verify_student_verify_now', kwargs={'course_id': unicode(course_id)}) self.assertRedirects(response, url, status_code)
url = reverse('verify_student_upgrade_and_verify', kwargs={'course_id': unicode(course_id)}) self.assertRedirects(response, url)
return ''
return dict(zip(('request', 'user', 'course_key', 'course_mode', 'amount'), patched_create_order.call_args[0]))
return uuid4().hex.decode('ascii')
return dict(zip(('user', 'course_key', 'course_mode', 'processor'), patched_create_order.call_args[0]))
request = RequestFactory().get('/url') request.user = self.user account_settings = get_account_settings(request) self.assertEqual(account_settings['name'], full_name)
last_request = httpretty.last_request() return json.loads(last_request.body)
return True
self._assert_can_reverify()
url = reverse("verify_student_reverify") return self.client.get(url)
response = self._get_reverify_page() self.assertContains(response, "reverify-container")
response = self._get_reverify_page() self.assertContains(response, "reverify-blocked")
response = self._submit_photos(self.course_key, self.reverification_location, self.IMAGE_DATA) self.assertEquals(response.status_code, 400)
checkpoint = VerificationCheckpoint(course_id=self.course_key, checkpoint_location=self.reverification_location) checkpoint.save()
attempt = SoftwareSecurePhotoVerification(user=self.user, photo_id_key="dummy_photo_id_key") attempt.mark_ready() attempt.save() attempt.submit()
return reverse( 'verify_student_incourse_reverify', kwargs={ "course_id": unicode(course_key), "usage_id": checkpoint_location } )
def __init__(self, name): self.name = name
def __init__(self, access_key, secret_key): pass def get_bucket(self, bucket_name): return MockBucket(bucket_name)
response = requests.Response() response.status_code = 400 return response
context_dict = self.response_post_params(request.user) return render_to_response("verify_student/test/fake_softwaresecure_response.html", context_dict)
_listen_for_course_publish('store', self.course.id) self.assertEqual(VerificationDeadline.deadline_for_course(self.course.id), self.course.end)
response = self.client.get( '/verify_student/software-secure-fake-response' ) self.assertEqual(response.status_code, 404)
self.client.logout() response = self.client.get( '/verify_student/software-secure-fake-response' ) self.assertEqual(response.status_code, 302)
kwargs['course_id'] = self.course.id.to_deprecated_string() url = reverse(url_name, kwargs=kwargs) return url
self.save()
return json.dumps({'message': 'Task revoked before running'})
for row in rows: yield [unicode(item).encode('utf-8') for item in row]
self.root_path = root_path if not os.path.exists(root_path): os.makedirs(root_path)
output_buffer = StringIO() csvwriter = csv.writer(output_buffer) csvwriter.writerows(self._get_utf8_encoded_rows(rows)) self.store(course_id, filename, output_buffer)
pass
pass
task_type = 'problem_responses_csv' task_class = calculate_problem_responses_csv task_input = {'problem_location': problem_location} task_key = "" return submit_task(request, task_type, task_class, course_key, task_input, task_key)
task_type = 'grade_course' task_class = calculate_grades_csv task_input = {} task_key = "" return submit_task(request, task_type, task_class, course_key, task_input, task_key)
task_type = 'grade_problems' task_class = calculate_problem_grade_report task_input = {} task_key = "" return submit_task(request, task_type, task_class, course_key, task_input, task_key)
task_type = 'detailed_enrollment_report' task_class = enrollment_report_features_csv task_input = {} task_key = "" return submit_task(request, task_type, task_class, course_key, task_input, task_key)
task_type = 'may_enroll_info_csv' task_class = calculate_may_enroll_csv task_input = {'features': features} task_key = "" return submit_task(request, task_type, task_class, course_key, task_input, task_key)
task_type = 'exec_summary_report' task_class = exec_summary_report_csv task_input = {} task_key = "" return submit_task(request, task_type, task_class, course_key, task_input, task_key)
task_type = 'proctored_exam_results_report' task_class = proctored_exam_results_csv task_input = {'features': features} task_key = "" return submit_task(request, task_type, task_class, course_key, task_input, task_key)
task_type = 'cohort_students' task_class = cohort_students task_input = {'file_name': file_name} task_key = "" return submit_task(request, task_type, task_class, course_key, task_input, task_key)
pass
return xmodule_instance_args.get('task_id', UNKNOWN_TASK_ID) if xmodule_instance_args is not None else UNKNOWN_TASK_ID
return xmodule_instance_args.get('xqueue_callback_url_prefix', '') if xmodule_instance_args is not None else ''
return submit_rescore_problem_for_all_students(self.create_task_request(instructor), InstructorTaskModuleTestCase.problem_location(problem_url_name))
return submit_rescore_problem_for_student(self.create_task_request(instructor), InstructorTaskModuleTestCase.problem_location(problem_url_name), student)
module = self.get_student_module(username, descriptor) state = json.loads(module.state) return state['attempts']
return submit_reset_problem_attempts_for_all_students(self.create_task_request(instructor), location)
return submit_delete_problem_state_for_all_students(self.create_task_request(instructor), location)
self.assertDictContainsSubset({'attempted': 2, 'succeeded': 2, 'failed': 0}, task_result)
return dict([item for d in dicts for item in d.items()])
course_email = CourseEmail.create( self.course.id, self.instructor, [SEND_TO_MYSELF, SEND_TO_STAFF, SEND_TO_LEARNERS], "Test Subject", "<p>This is a test message</p>" ) return course_email.id
instructor_task = api_call() instructor_task = InstructorTask.objects.get(id=instructor_task.id) instructor_task.task_state = PROGRESS instructor_task.save() with self.assertRaises(AlreadyRunningError): api_call()
api_call = lambda: generate_certificates_for_students( self.create_task_request(self.instructor), self.course.id ) self._test_resubmission(api_call)
return regenerate_certificates( self.create_task_request(self.instructor), self.course.id, [CertificateStatuses.downloadable, CertificateStatuses.generating] )
with self.assertRaises(SpecificStudentIdMissingError): generate_certificates_for_students( self.create_task_request(self.instructor), self.course.id, student_set='specific_student', specific_student_id=None )
return TEST_COURSE_KEY.make_usage_key('problem', problem_url_name)
return self._create_progress_entry(student, task_state=SUCCESS)
return u'{0}@test.com'.format(username)
if self.current_user != username: self.logout() user_email = User.objects.get(username=username).email self.login(user_email, "test") self.current_user = username
return self._create_user(username, email, is_staff=True)
return self._create_user(username, email, is_staff=False, mode=mode)
mock_request = Mock() mock_request.GET = mock_request.POST = {'task_id': task_id} response = instructor_task_status(mock_request) status = json.loads(response.content) return status
return StudentModule.objects.get(course_id=self.course.id, student=User.objects.get(username=username), module_type=descriptor.location.category, module_state_key=descriptor.location, )
request = Mock() request.GET = request.POST = {'task_id': task_id} return instructor_task_status(request)
instructor_task = self._create_email_subtask_entry( total=total, attempted=attempted, succeeded=succeeded, skipped=skipped, task_state=SUCCESS, ) return self._test_get_status_from_result(instructor_task.task_id)
for _ in range(num_students): random_id = uuid4().hex[:8] self.create_student(username='student{0}'.format(random_id))
self._enroll_students_in_course(self.course.id, extra_count) return {}
return "http://fake-edx-s3.edx.org/"
return self.keys
return MockBucket(bucket_name)
pass
return LocalFSReportStore.from_config(config_name='GRADES_DOWNLOAD')
return {'xqueue_callback_url_prefix': 'dummy_value', 'request_info': {}, }
task_entry = self._create_input_entry() with self.assertRaises(ValueError): task_class(task_entry.id, self._get_xmodule_instance_args())
task_entry = self._create_input_entry(course_id="bogus/course/id") with self.assertRaises(ItemNotFoundError): self._run_task_with_mock_celery(task_class, task_entry.id, task_entry.task_id)
task_entry = self._create_input_entry() with self.assertRaises(ItemNotFoundError): self._run_task_with_mock_celery(task_class, task_entry.id, task_entry.task_id)
self.define_option_problem(PROBLEM_URL_NAME) self._test_run_with_task(task_class, action_name, 0)
self._test_missing_current_task(generate_certificates)
self._test_run_with_task( generate_certificates, 'certificates generated', 0, 0, expected_attempted=1, expected_total=1 )
return dict(zip( header_row, [ unicode(user.id), user.email, user.username, ] + grade ))
return open(file_name)
user_profile = UserFactory(username=user.username, email=user.email).profile user_profile.allow_certificate = not is_embargoed user_profile.save()
return [ self.create_student( username='student_{}'.format(index), email='student_{}@example.com'.format(index) ) for index in xrange(number_of_students) ]
pass
options = dict(d) task_id = options['task_id'] del options['task_id'] return SubtaskStatus.create(task_id, **options)
return cls(task_id, **options)
return self.__dict__
return self.retried_nomax + self.retried_withmax
return 'SubtaskStatus<%r>' % (self.to_dict(),)
return unicode(repr(self))
return modules_to_update.filter(state__contains='"done": true')
self._test_group_id_passed_to_user_profile( mock_request, True, self.moderator, profiled_user, requested_cohort.id, pass_group_id )
self._set_mock_request_data(mock_request, { "threads_count": threads_count, "comments_count": comments_count, })
if is_comment_too_deep(parent=None): return JsonError(_("Comment level too deep")) return _create_comment(request, CourseKey.from_string(course_id), thread_id=thread_id)
if is_comment_too_deep(parent=cc.Comment(comment_id)): return JsonError(_("Comment level too deep")) return _create_comment(request, CourseKey.from_string(course_id), parent_id=comment_id)
comment = cc.Comment.find(comment_id) result = _vote_or_unvote(request, course_id, comment, value) comment_voted.send(sender=None, user=request.user, post=comment) return result
return _vote_or_unvote(request, course_id, cc.Comment.find(comment_id), undo_vote=True)
thread = cc.Thread.find(thread_id) result = _vote_or_unvote(request, course_id, thread, value) thread_voted.send(sender=None, user=request.user, post=thread) return result
return _vote_or_unvote(request, course_id, cc.Thread.find(thread_id), undo_vote=True)
user = cc.User.from_django_user(request.user) commentable = cc.Commentable.find(commentable_id) user.follow(commentable) return JsonResponse({})
user = cc.User.from_django_user(request.user) thread = cc.Thread.find(thread_id) user.unfollow(thread) return JsonResponse({})
user = cc.User.from_django_user(request.user) commentable = cc.Commentable.find(commentable_id) user.unfollow(commentable) return JsonResponse({})
try: return content and not content['closed'] except KeyError: return False
try: return content and content['user_id'] == str(user.id) except KeyError: return False
payload = json.loads(response.content) thread = extract_thread(payload) if extract_thread else payload self._assert_thread_contains_group_info(thread)
pass
pass
self.assertEqual( utils.get_discussion_category_map(self.course, requesting_user or self.user), expected )
self.assertEqual( utils.get_discussion_category_map(self.course, self.instructor, cohorted_if_in_list, exclude_unstarted), expected )
add_lookup('main', '', package=__name__) self.assertEqual(utils.render_mustache('test.mustache', {}), 'Testing 1 2 3.\n')
request = RequestFactory().request() request.user = user all_tabs = get_course_tab_list(request, self.course) return any(tab.type == 'discussion' for tab in all_tabs)
super(IsCommentableCohortedTestCase, self).setUp() self.toy_course_key = ToyCourseFactory.create().id
return {k: dic.get(k) for k in keys}
return dict([(k, v) for k, v in dic.iteritems() if v is not None])
return isinstance(v, str) and len(v.strip()) == 0
return dict(dic1.items() + dic2.items())
try: role = Role.objects.get(name=rolename, course_id=course_id) except Role.DoesNotExist: return False return role.users.filter(username=uname).exists()
pass
try: cached_mapping = CourseStructure.objects.get(course_id=course.id).discussion_id_map if not cached_mapping: raise DiscussionIdMapIsNotCached() return cached_mapping.get(discussion_id) except CourseStructure.DoesNotExist: raise DiscussionIdMapIsNotCached()
return dict(map(get_discussion_id_map_entry, get_accessible_discussion_modules(course, user)))
content = json.dumps(data, cls=i4xEncoder) super(JsonResponse, self).__init__(content, content_type='application/json; charset=utf-8')
super(HtmlResponse, self).__init__(html, content_type='text/plain')
request.view_name = view_func.__name__
return ( MAX_COMMENT_DEPTH is not None and ( MAX_COMMENT_DEPTH < 0 or (parent and parent["depth"] >= MAX_COMMENT_DEPTH) ) )
if instance.module_type in StudentModuleHistoryExtended.HISTORY_SAVING_TYPES: history_entry = StudentModuleHistoryExtended(student_module=instance, version=None, created=instance.modified, state=instance.state, grade=instance.grade, max_grade=instance.max_grade) history_entry.save()
StudentModuleHistoryExtended.objects.filter(student_module=instance).all().delete()
return status in cls.PASSED_STATUSES
linkedin = 'LinkedIn' facebook = 'Facebook' twitter = 'Twitter'
return super(EligibleCertificateManager, self).get_queryset().exclude( status__in=(CertificateStatuses.audit_passing, CertificateStatuses.audit_notpassing) )
try: return cls.objects.get(user=student, course_id=course_id) except cls.DoesNotExist: pass return None
return self.status == CertificateStatuses.downloadable
self.active = False self.save()
if is_prerequisite_courses_enabled(): fulfill_course_milestone(course_key, user)
queryset = (ExampleCertificate.objects).select_related('example_cert_set').filter(example_cert_set=self) for cert in queryset: yield cert
return uuid.uuid4().hex
return self.example_cert_set.course_key
try: latest = cls.objects.filter(course_key=course_key).latest() except cls.DoesNotExist: return False else: return latest.enabled
CertificateGenerationCourseSetting.objects.create( course_key=course_key, enabled=is_enabled )
try: json.loads(self.configuration) except ValueError: raise ValidationError('Must be valid JSON string.')
instance = cls.current() json_data = json.loads(instance.configuration) if instance.enabled else {} return json_data
course_badge_check(user, course_key)
completion_check(user)
return reverse( self.namespaced_url, kwargs={ 'course_id': self.course.id, 'username': username } )
self.assert_oauth_status(self.dot_access_token, status.HTTP_200_OK)
self.assert_oauth_status("fooooooooooToken", status.HTTP_401_UNAUTHORIZED)
enable_self_generated_certs.delay(unicode(course_key))
certificate_html_view_configuration_model = apps.get_model("certificates", "CertificateHtmlViewConfiguration") certificate_html_view_configuration_model.objects.all().delete()
return CertificateStatuses.is_passing_status(cert_status)
try: cert = GeneratedCertificate.eligible_certificates.get( user__username=username, course_id=course_key ) except GeneratedCertificate.DoesNotExist: return None return format_certificate_for_user(username, cert)
asset_url = '' try: template_asset = CertificateTemplateAsset.objects.get(asset_slug=asset_slug) asset_url = template_asset.asset.url except CertificateTemplateAsset.DoesNotExist: pass return asset_url
data = dict( logo_src=branding_api.get_logo_url(), logo_url=branding_api.get_base_url(is_secure), ) return data
symbol = 'courseware.grades.grade' with patch(symbol) as mock_grade: mock_grade.return_value = {'grade': 'Pass', 'percent': 0.75} yield
self.assertIsNone( certs_api.get_certificate_for_user(self.student_no_cert.username, self.course_1.id) )
self.assertEqual( certs_api.get_certificates_for_user(self.student_no_cert.username), [] )
url = certs_api.get_certificate_url(self.student.id, self.course.id) self.assertEqual(url, "")
actual_enabled = certs_api.cert_generation_enabled(course_key) self.assertEqual(expect_enabled, actual_enabled)
with patch.object(XQueueCertInterface, 'add_example_cert') as mock_queue: yield mock_queue
certs_in_queue = [call_args[0] for (call_args, __) in mock_queue.call_args_list] self.assertEqual(len(certs_in_queue), expected_num) for cert in certs_in_queue: self.assertTrue(isinstance(cert, ExampleCertificate))
actual_status = certs_api.example_certificates_status(self.COURSE_KEY) self.assertEqual(list(expected_statuses), actual_status)
microsite.set_by_domain(domain) return func(request, *args, **kwargs)
cert_set = ExampleCertificateSet.objects.create(course_key=self.COURSE_KEY) return ExampleCertificate.objects.create( example_cert_set=cert_set, description=self.DESCRIPTION, template=self.TEMPLATE )
with patch.object(XQueueInterface, 'send_to_queue') as mock_send: mock_send.return_value = (0, None) if success else (1, self.ERROR_MSG) yield mock_send
content = json.loads(response.content) self.assertEqual(response.status_code, 200) self.assertEqual(content['return_code'], 0)
config = CertificateHtmlViewConfiguration(enabled=enabled, configuration=configuration_string) config.save() return config
command = self.command.Command() return command.handle(*args, **kwargs)
cert = GeneratedCertificate.eligible_certificates.get(user=user, course_id=course_key) self.assertEqual(cert.status, expected_status)
super(RegenerateCertificatesTest, self).setUp() self.course = self.courses[0]
super(UngenerateCertificatesTest, self).setUp() self.course = self.courses[0]
url = reverse("certificates:search") + "?user=" + user_filter if course_filter: url += '&course_id=' + course_filter return self.client.get(url)
return True
url = TEST_SERVER_HOST if path: url += path return url
default_headers = { 'HTTP_AUTHORIZATION': 'Bearer ' + self.access_token } default_headers.update(headers) response = self.client.get(uri, follow=True, **default_headers) return response
return self.http_get( reverse(self.view, kwargs={'course_id': course_id or self.course_id}), **headers )
raise NotImplementedError
raise NotImplementedError
response = self.http_get_for_course(self.invalid_course_id) self.assertEqual(response.status_code, 404)
return bool( settings.DEBUG or has_access(user, CourseStaffRole.ROLE, course) or has_access(user, CourseInstructorRole.ROLE, course) )
if not self.user_can_access_course(user, course): raise Http404
super(CourseViewMixin, self).perform_authentication(request) if request.user.is_anonymous() and not settings.DEBUG: raise AuthenticationFailed
return course.id.org
return course.id.run
return course.id.course
request = self.context['request'] return request.build_absolute_uri(reverse('course_structure_api:v0:detail', kwargs={'course_id': course.id}))
if mode == "login": return external_auth_login(request) elif mode == "register": return external_auth_register(request)
data = {} if email: data['email'] = email return self.client.post(path=reverse('password_change_request'), data=data)
return u"{url}?auth_entry={auth_entry}&{param_str}".format( url=reverse("social:begin", kwargs={"backend": backend_name}), auth_entry=auth_entry, param_str=self._finish_auth_url_param(login_params), )
return urlencode({ 'next': '/account/finish_auth?{}'.format(urlencode(params)) })
self.create_programs_config(program_listing_enabled=True) response = self.client.get(path=self.view_path) self.assertContains(response, '<li class="tab-nav-item">')
self.create_programs_config(program_listing_enabled=False) response = self.client.get(path=self.view_path) self.assertContains(response, '<li class="item nav-global-01">')
self.settings_patcher.stop() super(GatingTestCase, self).tearDown()
result = _get_xblock_parent(self.vert1) self.assertEqual(result.location, self.seq1.location)
result = _get_xblock_parent(self.vert1, 'unit') self.assertIsNone(result)
evaluate_prerequisite(self.course, self.prob2.location, self.user.id) self.assertFalse(mock_module_score.called)
evaluate_prerequisite(self.course, self.prob1.location, self.user.id) self.assertFalse(mock_module_score.called)
try: config = cls.objects.get(course_key=course_key) return config.verified_cohort_name except cls.DoesNotExist: return None
expected_response = { "enabled": False } self._verify_cohort_settings_response(expected_response)
self.enrollment.update_enrollment(mode=CourseMode.VERIFIED)
return datetime.now(UTC()) > self.start
if self.due is None: return False return datetime.now(UTC()) > self.due
if self.structure_json: return json.loads(self.structure_json) return None
stripped, ccx = strip_ccx(to_strip) yield stripped, partial(restore_ccx_collection, ccx_id=ccx)
self.__dict__['_modulestore'] = modulestore
return getattr(self._modulestore, name)
setattr(self._modulestore, name, value)
delattr(self._modulestore, name)
with remove_ccx(course_key) as (course_key, restore): return restore(self._modulestore.fill_in_run(course_key))
usage_key, _ = strip_ccx(usage_key) return self._modulestore.has_item(usage_key, **kwargs)
with remove_ccx(usage_key) as (usage_key, restore): return restore( self._modulestore.get_item(usage_key, depth, **kwargs) )
with remove_ccx(course_key) as (course_key, restore): return restore(self._modulestore.get_items(course_key, **kwargs))
with remove_ccx(course_key) as (course_key, restore): return restore(self._modulestore.get_course( course_key, depth=depth, **kwargs ))
with remove_ccx(course_id) as (course_id, restore): return restore(self._modulestore.has_course( course_id, ignore_case=ignore_case, **kwargs ))
course_key, _ = strip_ccx(course_key) return self._modulestore.delete_course(course_key, user_id)
with remove_ccx(location) as (location, restore): return restore( self._modulestore.get_parent_location(location, **kwargs) )
with remove_ccx(usage_key) as (usage_key, restore): orig_key, version = self._modulestore.get_block_original_usage(usage_key) return restore(orig_key), version
with remove_ccx(course_id) as (course_id, restore): return restore(self._modulestore.get_modulestore_type(course_id))
with remove_ccx(course_key) as (course_key, restore): return restore(self._modulestore.get_orphans(course_key, **kwargs))
with remove_ccx(course_key) as (course_key, restore): return restore(self._modulestore.create_item( user_id, course_key, block_type, block_id=block_id, fields=fields, **kwargs ))
with remove_ccx(parent_usage_key) as (parent_usage_key, restore): return restore(self._modulestore.create_child( user_id, parent_usage_key, block_type, block_id=block_id, fields=fields, **kwargs ))
with remove_ccx(course_key) as (course_key, restore): return restore(self._modulestore.import_xblock( user_id, course_key, block_type, block_id, fields=fields, runtime=runtime, **kwargs ))
with remove_ccx(dest_key) as (dest_key, restore): return restore(self._modulestore.copy_from_template( source_keys, dest_key, user_id, **kwargs ))
with remove_ccx(xblock) as (xblock, restore): return restore(self._modulestore.update_item( xblock, user_id, allow_not_found=allow_not_found, **kwargs ))
with remove_ccx(location) as (location, restore): return restore( self._modulestore.delete_item(location, user_id, **kwargs) )
with remove_ccx(location) as (location, restore): return restore( self._modulestore.revert_to_published(location, user_id) )
with remove_ccx(course_key) as (course_key, restore): return restore(self._modulestore.create_xblock( runtime, course_key, block_type, block_id=block_id, fields=fields, **kwargs ))
with remove_ccx(xblock) as (xblock, restore): return restore(self._modulestore.has_published_version(xblock))
with remove_ccx(location) as (location, restore): return restore( self._modulestore.publish(location, user_id, **kwargs) )
with remove_ccx(location) as (location, restore): return restore( self._modulestore.unpublish(location, user_id, **kwargs) )
with remove_ccx(location) as (location, restore): return restore( self._modulestore.convert_to_draft(location, user_id) )
with remove_ccx(xblock) as (xblock, restore): return restore(self._modulestore.has_changes(xblock))
course_key, _ = strip_ccx(course_key) return self._modulestore.check_supports(course_key, method)
course_id, _ = strip_ccx(course_id) with self._modulestore.branch_setting(branch_setting, course_id): yield
if not ccx: raise Http404 schedule = get_ccx_schedule(course, ccx) json_schedule = json.dumps(schedule, indent=4) return HttpResponse(json_schedule, content_type='application/json')
course_object, course_key, error_code, http_status = get_valid_course(course_id, is_ccx) self.check_object_permissions(self.request, course_object) return course_object, course_key, error_code, http_status
return unicode(CCXLocator.from_course_locator(obj.course.id, obj.id))
self.assertEqual(resp_obj.status_code, http_code) self.assertIn('error_code', resp_obj.data) self.assertEqual(resp_obj.data['error_code'], error_code_str)
resp = self.client.patch(self.detail_url, data, format='json', HTTP_AUTHORIZATION=self.auth) self.expect_error_fields(expected_errors, resp)
queue = deque([block]) while queue: item = queue.popleft() yield item queue.extend(item.get_children())
with self.store.bulk_operations(key): course = self.store.get_course(key) return course
OverrideFieldData.provider_classes = None
factory = RequestFactory() request = factory.get('ccx_coach_dashboard') request.user = MagicMock() return request
unit['hidden'] = False for child in unit.get('children', ()): unhide(child)
node.visible_to_staff_only = True self.mstore.update_item(node, self.coach.id)
request = RequestFactory().request() request.user = self.user all_tabs = get_course_tab_list(request, course) return any(tab.type == 'ccx_coach' for tab in all_tabs)
with self.settings(FEATURES={'CUSTOM_COURSES_EDX': ccx_feature_flag}): course = self.ccx_enabled_course if enable_ccx else self.ccx_disabled_course self.assertEquals( expected_result, self.check_ccx_tab(course) )
from lms.djangoapps.ccx.utils import get_ccx_from_ccx_locator return get_ccx_from_ccx_locator(course_id)
super(TestGetCourseChapters, self).setUp() self.course_key = self.course.location.course_key
mocked_attr.return_value = {'foo': 'bar'} self.assertEqual(utils.get_course_chapters(self.course_key), [])
course_key = course.id if view_as_ccx: course_key = CCXLocator.from_course_locator(course_key, self.ccx.id) return progress( self.request, course_id=unicode(course_key), student_id=self.student.id )
return check_mongo_calls_range(max_finds=calls)
return check_sum_of_calls(XBlock, ['__init__'], instantiations, instantiations, include_arguments=False)
override_field_for_ccx(self.ccx, self.course, field, value)
expected = self.course actual = self.ccx.course self.assertEqual(expected, actual)
staff = UserFactory.create(password="test") role = CourseStaffRole(self.course.id) role.add_users(staff) return staff
instructor = UserFactory.create(password="test") role = CourseInstructorRole(self.course.id) role.add_users(instructor) return instructor
role = CourseCcxCoachRole(self.course.id) role.add_users(self.coach)
ccx = CcxFactory(course_id=self.course.id, coach=self.coach) override_field_for_ccx(ccx, self.course, 'max_student_enrollments_allowed', max_students_allowed) return ccx
from django.core import mail return mail.outbox
return [x for sub in seq for x in sub]
try: ccx = CustomCourseForEdX.objects.get( id=ccx_id, course_id=course.id, coach=coach ) except CustomCourseForEdX.DoesNotExist: return None return ccx
try: validate_email(identifier) except ValidationError: return False return True
return getattr(course, 'enable_ccx', False)
if not isinstance(course_key, CCXLocator): send_ccx_course_published.delay(unicode(course_key))
app_label = "rss_proxy"
return auth.user_has_role(user, CourseStaffRole(CourseKey.from_string(course_id)))
_change_access(course, user, level, 'allow', send_email)
_change_access(course, user, level, 'revoke', send_email)
return { 'user': self.user, 'enrollment': self.enrollment, 'allowed': self.allowed, 'auto_enroll': self.auto_enroll, }
language = language or settings.LANGUAGE_CODE with override_language(language): return get_subject_and_message(subject_template, message_template, param_dict)
subject = render_to_string(subject_template, param_dict) message = render_to_string(message_template, param_dict) return subject, message
raise NotImplementedError()
raise NotImplementedError()
raise NotImplementedError()
raise NotImplementedError()
expected_info = [ 'created', 'sent_to', 'email', 'number_sent', 'requester', ] return {info: None for info in expected_info}
password = generate_random_string(password_length) while password in generated_passwords: password = generate_random_string(password_length) generated_passwords.append(password) return password
return { 'username': user.username, 'email': user.email, 'first_name': user.first_name, 'last_name': user.last_name, }
code_length = getattr(settings, 'REGISTRATION_CODE_LENGTH', 8) return generate_random_string(code_length)
return { 'username': user.username, 'email': user.email, 'first_name': user.first_name, 'last_name': user.last_name, }
invoice_copy_preference = True invoice_preference_value = get_user_preference(request.user, INVOICE_KEY) if invoice_preference_value is not None: invoice_copy_preference = invoice_preference_value == 'True' return JsonResponse({ 'invoice_copy': invoice_copy_preference })
course = get_course_by_id(SlashSeparatedCourseKey.from_deprecated_string(course_id)) unit = find_unit(course, request.GET.get('url')) return JsonResponse(dump_module_extensions(course, unit))
student = require_student_from_identifier(request.GET.get('student')) course = get_course_by_id(SlashSeparatedCourseKey.from_deprecated_string(course_id)) return JsonResponse(dump_student_extensions(course, student))
url = reverse('instructor_dashboard', kwargs={'course_id': unicode(course_key)}) if section is not None: url += u'#view-{section}'.format(section=section) return url
course_key = CourseKey.from_string(course_id) certs_api.generate_example_certificates(course_key) return redirect(_instructor_dash_url(course_key, section='certificates'))
return bool(user and has_access(user, 'staff', course, course.id))
is_small_course = False enrollment_count = CourseEnrollment.objects.num_enrolled_in(course_key) max_enrollment_for_buttons = settings.FEATURES.get("MAX_ENROLLMENT_INSTR_BUTTONS") if max_enrollment_for_buttons is not None: is_small_course = enrollment_count <= max_enrollment_for_buttons return is_small_course
return []
error = unicode(self) return HttpResponseBadRequest(json.dumps({'error': error}))
try: return view(request, course_id=course_id) except DashboardError, error: return error.response()
unique_student_identifier = strip_if_string(unique_student_identifier) if "@" in unique_student_identifier: student = User.objects.get(email=unique_student_identifier) else: student = User.objects.get(username=unique_student_identifier) return student
try: return get_student_from_identifier(unique_student_identifier) except User.DoesNotExist: raise DashboardError( _("Could not find student matching identifier: {student_identifier}").format( student_identifier=unique_student_identifier ) )
try: return dateutil.parser.parse(datestr).replace(tzinfo=utc) except ValueError: raise DashboardError(_("Unable to parse date: ") + datestr)
if node.location.to_deprecated_string() == url: return node for child in node.get_children(): found = find(child, url) if found: return found return None
if getattr(node, 'due', None): units.append(node) else: for child in node.get_children(): visit(child)
title = getattr(node, 'display_name', None) if not title: title = node.location.to_deprecated_string() return title
password = generate_unique_password([], 12) self.assertEquals(len(password), 12) for letter in password: self.assertNotIn(letter, 'aAeEiIoOuU1l')
generated_password = ['first'] password = generate_unique_password(generated_password, 12) self.assertNotEquals(password, 'first')
paid_course = CourseFactory.create() CourseModeFactory.create(course_id=paid_course.id, min_price=50, mode_slug=CourseMode.HONOR) CourseInstructorRole(paid_course.id).add_users(self.instructor) return paid_course
request = Mock() request.user = self.instructor return request
self.times_called += 1 if self.times_called % 2 == 0: return True, 'Task Completed' return False, 'Task Errored In Some Way'
attr_dict = {key: getattr(self, key) for key in self.FEATURES} attr_dict['created'] = attr_dict['created'].isoformat() return attr_dict
email_id = kwargs.get('id', 0) return self.emails[email_id]
self.check_emails_sent(50, task_history_request)
self.check_emails_sent(1, task_history_request, True)
self.check_emails_sent(50, task_history_request, True)
test_user = UserFactory() GeneratedCertificateFactory.create( user=test_user, course_id=course_id, mode=mode, status=status )
self.expect_error_on_file_content( 'username,email\n', "The file must contain a 'cohort' column containing cohort names." )
self.expect_error_on_file_content( 'cohort\n', "The file must contain a 'username' column, an 'email' column, or both." )
self.expect_error_on_file_content( '', "The file must contain a 'cohort' column containing cohort names." )
self.expect_error_on_file_content( '', "The file must end with the extension '.csv'.", file_suffix='.notcsv' )
self.client.login(username=self.non_staff_user.username, password='test') response = self.call_add_users_to_cohorts('') self.assertEqual(response.status_code, 403)
self.verify_success_on_file_content( 'username,cohort\nfoo_username,bar_cohort', mock_store_upload, mock_cohort_task )
self.verify_success_on_file_content( 'email,cohort\nfoo_email,bar_cohort', mock_store_upload, mock_cohort_task )
self.verify_success_on_file_content( 'username,email,cohort\nfoo_username,bar_email,baz_cohort', mock_store_upload, mock_cohort_task )
self.verify_success_on_file_content( 'username,email,cohort\rfoo_username,bar_email,baz_cohort', mock_store_upload, mock_cohort_task )
user = UserFactory() allow_access(self.course, user, 'beta') self.assertTrue(CourseBetaTesterRole(self.course.id).has_user(user))
raise tools.DashboardError("Oh noes!")
return "Oh yes!"
super(TestRequireStudentIdentifier, self).setUp() self.student = UserFactory.create()
url = self.homework.location.to_deprecated_string() found_unit = tools.find_unit(self.course, url) self.assertEqual(found_unit.location, self.homework.location)
url = "i4x://MITx/999/chapter/notfound" with self.assertRaises(tools.DashboardError): tools.find_unit(self.course, url)
return self.check_outbox(u"Vous avez été")
self.assertEqual(1, len(mail.outbox)) self.assertIn(expected_message, mail.outbox[0].subject) self.assertIn(expected_message, mail.outbox[0].body)
super(BadImplementationAbstractEnrollmentReportProvider, self).get_user_profile(user_id)
super(BadImplementationAbstractEnrollmentReportProvider, self).get_enrollment_info(user, course_id)
super(BadImplementationAbstractEnrollmentReportProvider, self).get_payment_info(user, course_id)
provider = PaidCourseEnrollmentReportProvider() self.assertIsNotNone(provider) self.assertTrue(isinstance(provider, PaidCourseEnrollmentReportProvider))
return StudentModule.objects.get( student=self.user, course_id=self.course_key, module_state_key=location ).state
def __init__(self, email, user, cenr, cea): self.email = email self.user = user self.cenr = cenr self.cea = cea
email_params = get_email_params(self.course, True) email_params["email_address"] = "user@example.com" email_params["full_name"] = "Jean Reno" return email_params
return render_message_to_string( self.subject_template, self.message_template, self.get_email_params(), language=language )
return render_message_to_string( subject_template, message_template, self.get_email_params_ccx() )
return 'Enrollment data is now available in <a href="http://example.com/courses/{}" ' \ 'target="_blank">Example</a>.'.format(unicode(self.course.id))
response = self.client.get(self.url) self.assert_no_xss(response, '<script>alert("XSS")</script>')
reg_item = PaidCourseRegistration.add_to_order(cart, course_key) return reg_item
return {key: getattr(self, key) for key in self.FEATURES}
self.task_input = "THIS IS INVALID JSON"
return self.target_type
return [FakeEmail.FakeTarget()]
response = self.client.get(self.url) if is_visible: self.assertContains(response, "Student-Generated Certificates") else: self.assertNotContains(response, "Student-Generated Certificates")
response = self.client.get(self.url) expected_html = '<button class="is-disabled" disabled>Enable Student-Generated Certificates</button>' self.assertContains(response, expected_html)
response = self.client.get(self.url) expected_html = ( 'Enable Student-Generated Certificates' if is_enabled else 'Disable Student-Generated Certificates' ) self.assertContains(response, expected_html)
expected_redirect = reverse( 'instructor_dashboard', kwargs={'course_id': unicode(self.course.id)} ) expected_redirect += '#view-certificates' self.assertRedirects(response, expected_redirect)
result = self.service.delete_student_attempt( self.student.username, unicode(self.course.id), 'foo/bar/baz', requesting_user=self.student, ) self.assertIsNone(result)
result = self.service.delete_student_attempt( 'bad_student', unicode(self.course.id), 'foo/bar/baz', requesting_user=self.student, ) self.assertIsNone(result)
result = self.service.delete_student_attempt( self.student.username, unicode(self.course.id), self.other_problem_urlname, requesting_user=self.student, ) self.assertIsNone(result)
return 'edx.mit.edu'
return False
if isinstance(obj, OpaqueKey): return unicode(obj) return JSONEncoder.default(self, obj)
self.course.tabs.append(CourseTab.load("notes")) self.course.advanced_modules = ["notes"]
request = RequestFactory().request() request.user = user all_tabs = get_course_tab_list(request, course) return any([tab.name == u'My Notes' for tab in all_tabs])
for (user, token) in self.tokens.items(): UserPreference.objects.create(user=user, key=NOTIFICATION_PREF_KEY, value=token)
padding_len = AES.block_size - len(input_str) % AES.block_size return input_str + padding_len * chr(padding_len)
num_pad_bytes = ord(input_str[-1]) if num_pad_bytes < 1 or num_pad_bytes > AES.block_size or num_pad_bytes >= len(input_str): raise UsernameDecryptionException("padding") return input_str[:-num_pad_bytes]
if not request.user.is_authenticated(): raise PermissionDenied delete_user_preference(request.user, NOTIFICATION_PREF_KEY) return HttpResponse(status=204)
tag = match.group(0) if ( ALLOWED_BASIC_TAG_PATTERN.match(tag) or ALLOWED_A_PATTERN.match(tag) or ALLOWED_IMG_PATTERN.match(tag) ): return tag else: return ""
return TAG_PATTERN.sub(_sanitize_tag, source)
course_key = CourseKey.from_string(course_id) with modulestore().bulk_operations(course_key): response = get_course_topics(request, course_key) return Response(response)
return Response(get_thread(request, thread_id))
return Response(create_thread(request, request.data))
if request.content_type != MergePatchParser.media_type: raise UnsupportedMediaType(request.content_type) return Response(update_thread(request, thread_id, request.data))
delete_thread(request, thread_id) return Response(status=204)
return Response(create_comment(request, request.data))
delete_comment(request, comment_id) return Response(status=204)
return self.cleaned_data.get("page") or 1
return min(self.cleaned_data.get("page_size") or 10, 100)
return self.cleaned_data.get("order_by") or "last_activity_at"
return self.cleaned_data.get("order_direction") or "desc"
value = self.cleaned_data["following"] if value is False: raise ValidationError("The value of the 'following' parameter must be true.") else: return value
self.page_num = page_num self.num_pages = num_pages
return self.page_num < self.num_pages
return self.page_num > 1
return self.page_num + 1
return self.page_num - 1
self.page = _Page(page_num, num_pages) self.base_url = request.build_absolute_uri() self.count = result_count super(DiscussionAPIPagination, self).__init__()
return self.count
return self.page.num_pages
next_url = None if self.page.has_next(): next_url = replace_query_param(self.base_url, "page", self.page.next_page_number()) return next_url
if not value.strip(): raise ValidationError("This field may not be blank.")
if self.instance: raise ValidationError("This field is not allowed in an update.") return value
return user_id in self.context["staff_user_ids"] or user_id in self.context["ta_user_ids"]
return ( obj["anonymous"] or obj["anonymous_to_peers"] and not self.context["is_requester_privileged"] )
return None if self._is_anonymous(obj) else obj["username"]
return ( "Staff" if user_id in self.context["staff_user_ids"] else "Community TA" if user_id in self.context["ta_user_ids"] else None )
if self._is_anonymous(obj) or obj["user_id"] is None: return None else: user_id = int(obj["user_id"]) return self._get_user_label(user_id)
return render_body(obj["body"])
return self.context["cc_requester"]["id"] in obj.get("abuse_flaggers", [])
return obj["id"] in self.context["cc_requester"]["upvoted_ids"]
return obj.get("votes", {}).get("up_count", 0)
return sorted(get_editable_fields(obj, self.context))
return bool(obj["pinned"])
return self.context["group_ids_to_names"].get(obj["group_id"])
return obj["id"] in self.context["cc_requester"]["subscribed_thread_ids"]
return self.get_comment_list_url(obj, endorsed=True)
return self.get_comment_list_url(obj, endorsed=False)
endorsement = obj.get("endorsement") if endorsement: return self._get_user_label(int(endorsement["user_id"])) else: return None
endorsement = obj.get("endorsement") return endorsement["time"] if endorsement else None
return ( context["is_requester_privileged"] or context["cc_requester"]["id"] == cc_content["user_id"] )
return module.sort_key or module.discussion_target
return sorted(modules_by_category[category], key=get_module_sort_key)
_check_fields( get_editable_fields(cc_content, context), data, "This field is not editable." )
if form_value: user.follow(cc_content) else: user.unfollow(cc_content)
if form_value: cc_content.flagAbuse(user, cc_content) else: cc_content.unFlagAbuse(user, cc_content, removeAll=False)
cc_thread, context = _get_thread_and_context( request, thread_id, retrieve_kwargs={"user_id": unicode(request.user.id)} ) serializer = ThreadSerializer(cc_thread, context=context) return serializer.data
cc_thread, context = _get_thread_and_context(request, thread_id) if can_delete(cc_thread, context): cc_thread.delete() thread_deleted.send(sender=None, user=request.user, post=cc_thread) else: raise PermissionDenied
return context["cc_requester"]["id"] == cc_content["user_id"]
return context["is_requester_privileged"] or _is_author(cc_content, context)
ret = get_editable_fields( Comment(user_id=context["cc_requester"]["id"], type="comment"), context ) ret |= NON_UPDATABLE_COMMENT_FIELDS return ret
course.tabs = [tab for tab in course.tabs if not tab.type == 'discussion'] modulestore().update_item(course, user_id)
course_with_disabled_forums = CourseFactory.create() CourseEnrollmentFactory.create(user=user, course_id=course_with_disabled_forums.id) _remove_discussion_tab(course_with_disabled_forums, user.id) return course_with_disabled_forums
ItemFactory.create( parent_location=self.course.location, category="discussion", discussion_id=topic_id, discussion_category=category, discussion_target=subcategory, **kwargs )
return get_course_topics(self.request, self.course.id)
course = course or self.course self.register_get_threads_response(threads, page, num_pages) ret = get_thread_list(self.request, course.id, page, page_size, topic_id_list) return ret
overrides = overrides.copy() if overrides else {} overrides.setdefault("course_id", unicode(self.course.id)) return make_minimal_cs_thread(overrides)
self.register_get_thread_response(thread) return get_comment_list(self.request, thread["id"], endorsed, page, page_size)
self.register_comment() update_comment(self.request, "test_comment", {}) for request in httpretty.httpretty.latest_requests: self.assertEqual(request.method, "GET")
self.assertEqual(response.status_code, expected_status) parsed_content = json.loads(response.content) self.assertEqual(parsed_content, expected_content)
overrides = overrides.copy() if overrides else {} overrides.setdefault("course_id", unicode(self.course.id)) return make_minimal_cs_thread(overrides)
course = course or self.course role = Role.objects.create(name=role_name, course_id=course.id) role.users = users
return ThreadSerializer(thread, context=get_context(self.course, self.request)).data
context = get_context(self.course, self.request, make_minimal_cs_thread(thread_data)) return CommentSerializer(comment, context=context).data
httpretty.register_uri( httpretty.POST, re.compile(r"http://localhost:4567/api/v1/(\w+)/threads"), body=_get_thread_callback(thread_data) )
httpretty.register_uri( httpretty.PUT, "http://localhost:4567/api/v1/threads/{}".format(thread_data["id"]), body=_get_thread_callback(thread_data) )
httpretty.register_uri( httpretty.GET, "http://localhost:4567/api/v1/threads/{id}".format(id=thread_id), body="", status=status_code )
httpretty.register_uri( httpretty.GET, "http://localhost:4567/api/v1/threads/{id}".format(id=thread["id"]), body=json.dumps(thread), status=200 )
httpretty.register_uri( httpretty.GET, "http://localhost:4567/api/v1/comments/{id}".format(id=comment_id), body="", status=status_code )
self.register_flag_response("thread", thread_id)
self.register_flag_response("comment", comment_id)
self.assert_query_params_equal(httpretty.last_request(), expected_params)
return self.client.patch( self.url, json.dumps(request_data), content_type="application/merge-patch+json" )
return { "cc_requester": User(id=requester_id), "is_requester_privileged": is_requester_privileged, "course": CourseFactory(cohort_config={"cohorted": is_cohorted}), "thread": thread, }
return "<p>{raw_body}</p>".format(raw_body=raw_body)
return [ credentials_factories.UserCredential( id=1, username='test', credential=credentials_factories.ProgramCredential() ), credentials_factories.UserCredential( id=2, username='test', credential=credentials_factories.ProgramCredential() ) ]
self.assertContains(response, 'programData') self.assertContains(response, self.data['name'])
self.create_programs_config(program_details_enabled=False) response = self.client.get(self.details_page) self.assertEquals(response.status_code, 404)
PERSONAL = 'personal' BUSINESS = 'business' ORDER_TYPES = ( (PERSONAL, 'personal'), (BUSINESS, 'business'), )
return cls.objects.filter(user=user, status='cart').exists()
return sum(i.line_cost for i in self.orderitem_set.filter(status=self.status))
for item in self.orderitem_set.all(): if item.is_discounted: item.unit_cost = item.list_price item.save()
self.orderitem_set.all().delete()
self.status = 'refunded' self.save() orderitems = OrderItem.objects.filter(order=self).select_subclasses() self._emit_order_event('Refunded Order', orderitems)
return self.qty * self.unit_cost
self.purchased_callback() self.status = 'purchased' self.fulfilled_time = datetime.now(pytz.utc) self.save()
self.status = 'paying' self.save()
raise NotImplementedError
return self.pk_with_subclass, set([])
return OrderItemSubclassPK(type(self), self.pk)
return self.list_price if self.list_price else self.unit_cost
return 'shoppingcart/receipt.html'
return ''
try: return cls.objects.get(Q(invoice_id=invoice_id), Q(status='completed') | Q(status='refunded')) except InvoiceTransaction.DoesNotExist: return None
return { 'qty': self.qty, 'unit_price': unicode(self.unit_price), 'currency': self.currency }
snapshot = super(CourseRegistrationCodeInvoiceItem, self).snapshot() snapshot['course_id'] = unicode(self.course_id) return snapshot
cls.objects.create( invoice=invoice, snapshot=json.dumps(invoice.snapshot()) )
if isinstance(instance, Invoice): InvoiceHistory.save_invoice_snapshot(instance) elif hasattr(instance, 'invoice'): InvoiceHistory.save_invoice_snapshot(instance.invoice)
return cls.objects.filter(order__isnull=False, course_id=course_id)
return cls.objects.filter(invoice__isnull=False, course_id=course_id)
return cls.objects.filter(registration_code__code=course_reg_code).exists()
try: code_redemption = cls.objects.get(registration_code__code=code, registration_code__course_id=course_id) except cls.DoesNotExist: code_redemption = None return code_redemption
code_redemption = RegistrationCodeRedemption(registration_code=course_reg_code, redeemed_by=user) code_redemption.save() return code_redemption
return super(SoftDeleteCouponManager, self).get_queryset().filter(is_active=True)
return super(SoftDeleteCouponManager, self).get_queryset()
return (self.expiration_date - timedelta(days=1)).strftime("%B %d, %Y") if self.expiration_date else None
discount = Decimal("{0:.2f}".format(Decimal(percentage_discount / 100.00) * value)) return value - discount
return cls.objects.filter(order__status='purchased', coupon__course_id=course_id).aggregate(Count('coupon'))
return cls.objects.filter(course_id=course_key, status=status).count()
try: return cls.objects.filter(course_id=course_id, user=user, course_enrollment=course_enrollment, status='purchased').latest('id') except PaidCourseRegistration.DoesNotExist: return None
return course_id in [ item.course_id for item in order.orderitem_set.all().select_subclasses("paidcourseregistration") if isinstance(item, cls) ]
try: return PaidCourseRegistrationAnnotation.objects.get(course_id=self.course_id).annotation except PaidCourseRegistrationAnnotation.DoesNotExist: return u""
return course_id in [ item.course_id for item in order.orderitem_set.all().select_subclasses("courseregcodeitem") if isinstance(item, cls) ]
try: return CourseRegCodeItemAnnotation.objects.get(course_id=self.course_id).annotation except CourseRegCodeItemAnnotation.DoesNotExist: return u""
self.course_enrollment.change_mode(self.mode) self.course_enrollment.activate()
class Meta(ConfigurationModel.Meta): app_label = "shoppingcart"
pass
return self.pk_with_subclass, set([self._tax_deduction_msg()])
return self._tax_deduction_msg()
return self.pdf.current_page_count() == 1
self.pdf.insert_page_break() self.draw_border() y_pos = self.draw_logos() return y_pos
for item in REPORT_TYPES: if report_type in item: return item[1](start_date, end_date, start_letter, end_letter) raise ReportTypeDoesNotExistException
cart = Order.get_cart_for_user(request.user) cart.reset_cart_items_prices() CouponRedemption.remove_coupon_redemption_from_cart(request.user, cart) return HttpResponse('reset')
try: access_group = Group.objects.get(name=settings.PAYMENT_REPORT_GENERATOR_GROUP) except Group.DoesNotExist: return False return access_group in user.groups.all()
return datetime.datetime.strptime(date_input.strip(), "%Y-%m-%d").replace(tzinfo=pytz.UTC)
obj.is_active = False obj.save()
model = CourseRegistrationCodeInvoiceItem extra = 0 can_delete = False readonly_fields = ( 'qty', 'unit_price', 'currency', 'course_id', ) def has_add_permission(self, request): return False
model = InvoiceTransaction extra = 0 readonly_fields = ( 'created', 'modified', 'created_by', 'last_modified_by' )
return render_to_string('shoppingcart/cybersource_form.html', { 'action': get_purchase_endpoint(), 'params': get_signed_purchase_params(cart), })
return PROCESSOR_MODULE.render_purchase_form_html(cart, **kwargs)
return PROCESSOR_MODULE.get_purchase_endpoint()
return get_processor_config().get('PURCHASE_ENDPOINT', '')
return u'<p class="error_msg">{msg}</p>'.format(msg=msg)
if name == 'cybersource_config_key': return 'test_microsite' else: return None
self.assertEqual(processor_hash('test'), 'GqNJWF7X7L07nEhqMAZ+OVyks1Y=') self.assertEqual(processor_hash('edx '), '/KowheysqM2PFYuxVKg0P8Flfk4=')
self.assertNotEqual(order.processor_reply_dump, '')
raise NotImplementedError
return None
self.assertEqual(unicode(self.annotation), u'{} : {}'.format(self.course_key.to_deprecated_string(), self.TEST_ANNOTATION))
self.add_to_cart() self.request.user = self.user context = user_has_cart_context_processor(self.request) self.assertFalse(context['should_display_shopping_cart_func']())
self.add_to_cart() self.request.user = self.user context = user_has_cart_context_processor(self.request) self.assertFalse(context['should_display_shopping_cart_func']())
self.request.user = AnonymousUser() context = user_has_cart_context_processor(self.request) self.assertFalse(context['should_display_shopping_cart_func']())
self.request.user = self.user context = user_has_cart_context_processor(self.request) self.assertFalse(context['should_display_shopping_cart_func']())
val = Decimal("{0:.2f}".format(Decimal(self.percentage_discount / 100.00) * cost)) return cost - val
coupon = Coupon(code=code, description='testing code', course_id=course_key, percentage_discount=self.percentage_discount, created_by=self.user, is_active=is_active) coupon.save()
if mode_slug is None: mode_slug = self.course_mode.mode_slug course_reg_code = CourseRegistrationCode( code=self.reg_code, course_id=course_key, created_by=self.user, mode_slug=mode_slug, is_valid=is_valid ) course_reg_code.save()
mode = CourseModeFactory.create() mode.course_id = self.course.id mode.min_price = min_price mode.mode_slug = mode_slug mode.expiration_date = expiration_date mode.save() return mode
self.login_user() reg_item = PaidCourseRegistration.add_to_order(self.cart, course_key, mode_slug=self.course_mode.mode_slug) return reg_item
if use_post: response = self.client.post(url) else: response = self.client.get(url) self.assertEquals(response.status_code, 404)
coupon = Coupon(code=code, description='testing code', course_id=course_key, percentage_discount=self.percentage_discount, created_by=self.user, is_active=is_active) coupon.save()
self.client.login(username=self.user.username, password="password")
self.client.login(username=self.user.username, password="password")
resp = self.client.get(self._receipt_url) self.assertContains(resp, expected_text)
self.client.login(username=self.user.username, password="password")
self.assertFalse(_can_download_report(self.user))
Group(name=settings.PAYMENT_REPORT_GENERATOR_GROUP).save() self.assertFalse(_can_download_report(self.user))
grp = Group(name=settings.PAYMENT_REPORT_GENERATOR_GROUP) grp.save() self.user.groups.add(grp) self.assertTrue(_can_download_report(self.user))
return { 'item_description': 'Course %s Description' % index, 'quantity': index, 'list_price': 10, 'discount': discount, 'item_total': 10 }
item = OrderItem(user=self.user, order=Order.get_cart_for_user(self.user)) with self.assertRaises(NotImplementedError): item.purchased_callback()
total_amount = PaidCourseRegistration.get_total_amount_of_purchased_item(course_key=self.course_key) self.assertEqual(total_amount, 0.00)
Coupon.objects.create( code=code, description='testing code', course_id=course_key, percentage_discount=self.percentage_discount, created_by=self.user, is_active=is_active )
self.client.login(username=username, password="password")
super(DonationTest, self).setUp() self.user = UserFactory.create() self.cart = Order.get_cart_for_user(self.user)
total_amount = Invoice.get_invoice_total_amount_for_course(self.course_key) self.assertEqual(total_amount, 123.45)
total_amount_paid = InvoiceTransaction.get_total_amount_of_paid_course_invoices(self.course_key) self.assertEqual(float(total_amount_paid), 0)
contact_info = self._latest_history()['contact_info'] for key, value in kwargs.iteritems(): self.assertEqual(contact_info[key], value)
items = self._latest_history()['items'] self.assertItemsEqual(items, expected_items)
transactions = self._latest_history()['transactions'] self.assertItemsEqual(transactions, expected_transactions)
return super(PaymentFakeView, self).dispatch(*args, **kwargs)
matched = match.group(0) if matched == ';;': return ';' elif matched == ';_': return '/' else: return matched
return re.sub(r'(;;|;_)', _unquote_slashes, text)
return xblock_local_resource_url(block, uri)
real_user = self.runtime.get_real_user(self.runtime.anonymous_student_id) return real_user
pass
pass
pass
course_key = CourseLocator(org="mockx", course="100", run="2015") return BlockUsageLocator(course_key, block_type='mock_type', block_id="mock_id")
return urlparse(self.runtime.handler_url(self.block, 'handler', query=query_string)).query
return urlparse(self.runtime.handler_url(self.block, handler_name, suffix=suffix)).path
return self.user
return self.user
i18n_service = self.runtime.service(self.mock_block, 'i18n') self.assertIsNotNone(i18n_service) self.assertIsInstance(i18n_service, ModuleI18nService)
self.mock_block.service_declaration.return_value = None with self.assertRaises(NoSuchServiceError): self.runtime.service(self.mock_block, 'i18n')
for user_partition in self.user_partitions: if user_partition.id == user_partition_id: return user_partition raise NoSuchUserPartitionError("could not find a UserPartition with ID [{}]".format(user_partition_id))
try: json.loads(self.configuration) except ValueError: raise ValidationError('Must be valid JSON string.')
return request.build_absolute_uri(staticfiles_storage.url(name))
return BrandingApiConfig.current().enabled
urls = microsite.get_value("urls", default={}) return urls.get(name) or EMPTY_URL
return _absolute_url(is_secure=is_secure, url_path="")
return get_url("TOS_AND_HONOR")
return get_url("PRIVACY")
return render_to_response(*args, **kwargs)
request = self.factory.get('/') request.user = AnonymousUser() mako_middleware_process_request(request) student.views.index(request)
super(TestFooter, self).setUp() cache.clear()
config = BrandingApiConfig(enabled=enabled) config.save()
course.tabs.append(CourseTab.load("edxnotes")) modulestore().update_item(course, user_id)
return "original_get_html"
self.course.edxnotes = False self.assertEqual("original_get_html", self.problem.get_html())
self.assertEqual("original_get_html", self.problem.get_html())
self.problem.system.is_author_mode = True self.assertEqual("original_get_html", self.problem.get_html())
self.course.edxnotes = _edxnotes self.assertEqual(helpers.is_feature_enabled(self.course), _edxnotes)
mock_get.return_value.content = "Error" self.assertRaises(EdxNotesParseError, helpers.get_notes, self.request, self.course)
mock_get.return_value.content = json.dumps({}) self.assertRaises(EdxNotesParseError, helpers.get_notes, self.request, self.course)
mock_get.return_value.content = "Error" self.assertRaises(EdxNotesParseError, helpers.get_notes, self.request, self.course)
mock_get.return_value.content = json.dumps({"1": 2}) self.assertRaises(EdxNotesParseError, helpers.get_notes, self.request, self.course)
mock_get.return_value.content = json.dumps(NOTES_API_EMPTY_RESPONSE) self.assertItemsEqual( NOTES_VIEW_EMPTY_RESPONSE, helpers.get_notes(self.request, self.course) )
mock_course_module = MagicMock() mock_course_module.position = 3 mock_course_module.get_display_items.return_value = [] self.assertIsNone(helpers.get_course_position(mock_course_module))
mock_course_module = MagicMock(id=self.course.id, position=None) mock_course_module.get_display_items.return_value = [MagicMock()] self.assertIsNone(helpers.get_course_position(mock_course_module))
response = self.client.get(self.notes_page_url) self.assertEqual(response.status_code, 404)
response = self.client.get(self.notes_url, {"text": "test"}) self.assertEqual(response.status_code, 404)
self.client.logout() response = self.client.get(self.get_token_url) self.assertEqual(response.status_code, 302)
response = self.client.post(self.visibility_url) self.assertEqual(response.status_code, 404)
user = UserFactory.create(username="ma1", email="ma1@ma1.info", password="edx") self.assertFalse(EdxNotesTab.is_enabled(self.course, user=user))
FEATURES['ENABLE_EDXNOTES'] = enable_edxnotes with override_settings(FEATURES=FEATURES): self.assertEqual(EdxNotesTab.is_enabled(self.course), enable_edxnotes)
return HttpResponse(get_edxnotes_id_token(request.user), content_type='text/plain')
return get_id_token(user, CLIENT_NAME)
return reverse("get_token", kwargs={ "course_id": unicode(course_id), })
children = [unicode(child) for child in children] return children.index(usage_key)
return get_endpoint(settings.EDXNOTES_PUBLIC_API, path)
return get_endpoint(settings.EDXNOTES_INTERNAL_API, path)
username = self.cleaned_data.get('username') return username or ''
return self.cleaned_data['return_type'] or 'dict'
if not permissions.can_access_self_blocks(requesting_user, course_key): raise PermissionDenied( "Course blocks for '{requesting_username}' cannot be accessed." .format(requesting_username=requesting_user.username) ) return requesting_user
return has_access(requesting_user, CourseStaffRole.ROLE, course_key)
return has_access(requesting_user, CourseStaffRole.ROLE, course_key)
def __init__(self): self.items = []
block_structure.request_xblock_fields('is_proctored_enabled') block_structure.request_xblock_fields('is_practice_exam')
return block_structure.get_transformer_block_field( block_key, cls, cls.BLOCK_DEPTH, )
with self.assertRaises(PermissionDenied): self.get_form(expected_valid=False)
with self.assertRaises(Http404): self.get_form(expected_valid=False)
form = self.get_form(expected_valid=True) self.assertDictEqual(form.cleaned_data, self.cleaned_data)
self.assertSetEqual( {block['id'] for block in response.data}, self.non_orphaned_block_usage_keys, )
self.assertSetEqual( set(response.data['blocks'].iterkeys()), self.non_orphaned_block_usage_keys, )
if predicate: self.assertIn(member, container) else: self.assertNotIn(member, container)
if predicate: self.assertTrue(expression) else: self.assertFalse(expression)
if serialized_block['id'] == unicode(self.html_block.location): self.assertTrue(serialized_block['visible_to_staff_only']) else: self.assertFalse(serialized_block['visible_to_staff_only'])
if context is None: context = self.serializer_context return BlockSerializer( context['block_structure'], many=True, context=context, )
if context is None: context = self.serializer_context return BlockDictSerializer( context['block_structure'], many=False, context=context, )
self.assertEqual(course_id, str(course.id))
request = Request(self.request_factory.get('/')) request.user = requesting_user with check_mongo_calls(0): return course_detail(request, target_user.username, course_key)
request = Request(self.request_factory.get('/')) request.user = requesting_user with check_mongo_calls(0): return list_courses(request, specified_user.username, org=org, filter_=filter_)
self.assertEqual(len(courses), 1) self.verify_course(courses[0])
self.assertTrue(self.client.login(username=requesting_user.username, password=TEST_PASSWORD)) if make_inactive: requesting_user.is_active = False requesting_user.save()
if user is None: user = self.honor_user request = Request(self.request_factory.get('/')) request.user = user return request
course_overview = CourseOverview.get_from_id(course.id) return self.serializer_class(course_overview, context={'request': self._get_request()}).data
super(RegisterPage, self).__init__(browser) self._course_id = course_id
return "{base}/register?course_id={course_id}&enrollment_action={action}".format( base=BASE_URL, course_id=self._course_id, action="enroll", )
self.wait_for_element_visibility(selector, 'Success div is shown')
return self.q(css=".submission-success h4").text
return ( self.q(css="#login-anchor").is_present() and self.q(css="#register-anchor").is_present() and self.current_form is not None )
return self.q(css="#register-email").attrs('value')[0]
return self.q(css="#register-name").attrs('value')[0]
return self.q(css="#register-username").attrs('value')[0]
return self.q(css=".submission-error li").text
errors = self.errors return (bool(errors), errors)
if self.q(css=".submission-success").visible: return self.q(css=".submission-success h4").text
success = self.success return (bool(success), success)
self.q(css='input.calibration-feedback-button').first.click()
rubric = RubricPage(self.browser) rubric.wait_for_page(timeout=60) return rubric
return self.q(css='.problem-header').text[0]
return self.q(css="div.problem p").text
return self.q(css="div.problems-wrapper").text[0]
return self.q(css="div.problem span.message").text[0]
return self.q(css="div.problem div.problem-hint").html[0].split(' <', 1)[0]
return self.q(css="div.problem div.problem-hint").text[0]
mathjax_container = self.q(css="div.problem p .MathJax_SVG") return mathjax_container.visible and mathjax_container.present
mathjax_container = self.q(css="div.problem div.problem-hint .MathJax_SVG") return mathjax_container.visible and mathjax_container.present
fields = self.q(css='div.problem div.capa_inputtype.textline input') fields = fields.nth(input_num) if input_num is not None else fields fields.fill(text)
self.q(css='div.problem button.check').click() self.wait_for_ajax()
self.q(css='div.problem button.save').click() self.wait_for_ajax()
self.q(css='div.problem button.reset').click() self.wait_for_ajax()
self.wait_for_element_visibility('div.problem section.inputtype div .status', 'wait for status icon')
msg = "Wait for status to be {}".format(message) self.wait_for_element_visibility(status_selector, msg)
self.q(css='div.problem button.hint-button').click() self.wait_for_ajax()
self.q(css='div.problem .choicegroup input[value="' + choice_value + '"]').click() self.wait_for_ajax()
return self.q(css="div.problem div.capa_inputtype.textline div.correct span.status").is_present()
return self.q(css="div.problem section.inputtype div.correct span.status").is_present()
return self.q(css="div.problem section.inputtype div.partially-correct span.status").is_present()
return self.q(css="div.problem section.inputtype div.incorrect span.status").is_present()
self.q(css='div.problem .clarification:nth-child({index}) i[data-tooltip]'.format(index=index + 1)).click()
return self.q(css='.problem-header').text[0]
input_css = "$('.CodeMirror')[0].CodeMirror.setValue('{}');".format(response_str) self.browser.execute_script(input_css)
self.q(css='input.save').click() self.wait_for_ajax()
return "{}#{} {}".format( self.BODY_SELECTOR, self.item_id, selector, )
BODY_SELECTOR = ".note-section" TITLE_SELECTOR = ".course-subtitle"
tag_links = self.q(css=self._bounded_selector(self.TAG_SELECTOR)) if len(tag_links) == 0: return None return[tag_link.text for tag_link in tag_links]
self.q(css=self._bounded_selector(self.TAG_SELECTOR)).filter(lambda el: tag_name in el.text).click()
return self.q(css="{} .action-close".format(self.TAB_SELECTOR)).present
self.q(css="{} .action-close".format(self.TAB_SELECTOR)).first.click()
children = self.q(css=self.CHILD_SELECTOR) return [self.CHILD_CLASS(self.browser, child.get_attribute("id")) for child in children]
BODY_SELECTOR = "#recent-panel" TAB_SELECTOR = ".tab#view-recent-activity"
BODY_SELECTOR = "#structure-panel" TAB_SELECTOR = ".tab#view-course-structure" CHILD_SELECTOR = ".note-group" CHILD_CLASS = EdxNotesChapterGroup
BODY_SELECTOR = "#tags-panel" TAB_SELECTOR = ".tab#view-tags" CHILD_SELECTOR = ".note-group" CHILD_CLASS = EdxNotesTagsGroup
BODY_SELECTOR = "#search-results-panel" TAB_SELECTOR = ".tab#view-search-results"
self.current_view = self.MAPPING[tab_name](self.browser) self.current_view.visit()
self.current_view.close() self.current_view = self.MAPPING["recent"](self.browser)
return self.q(css=".inline-error").visible
element = self.q(css=".inline-error").first if element and self.is_error_visible: return element.text[0] else: return None
children = self.q(css='.note') return [EdxNotesPageItem(self.browser, child.get_attribute("id")) for child in children]
children = self.q(css='.note-group') return [EdxNotesChapterGroup(self.browser, child.get_attribute("id")) for child in children]
children = self.q(css='.note-section') return [EdxNotesSubsectionGroup(self.browser, child.get_attribute("id")) for child in children]
children = self.q(css='.note-group') return [EdxNotesTagsGroup(self.browser, child.get_attribute("id")) for child in children]
return len(self.q(css='div.wrapper-note-excerpts').results)
element = self.q(css=".is-empty").first if element: return element.text[0] else: return None
body = self.q(css=selector)[0] ActionChains(self.browser).move_to_element(body).perform() return self
self.q(css=selector).first.click() return self
self.q(css=".action-toggle-notes").first.click() return self
components = self.q(css=".edx-notes-wrapper") return [AnnotatableComponent(self.browser, component.get_attribute("id")) for component in components]
notes = [] for component in self.components: notes.extend(component.notes) return notes
self.browser.refresh() return self.components
notes = self.q(css=self._bounded_selector(".annotator-hl")) return [EdxNoteHighlight(self.browser, note, self.item_id) for note in notes]
for element in self.q(css=self._bounded_selector(selector)): note = EdxNoteHighlight(self.browser, element, self.item_id) note.show().remove()
viewer_is_visible = self.q(css=self._bounded_selector(self.VIEWER_SELECTOR)).visible editor_is_visible = self.q(css=self._bounded_selector(self.EDITOR_SELECTOR)).visible return viewer_is_visible or editor_is_visible
self.wait_for_element_visibility( self._bounded_selector(self.ADDER_SELECTOR), "Adder is visible." )
self.wait_for_element_visibility( self._bounded_selector(self.VIEWER_SELECTOR), "Note Viewer is visible." )
self.wait_for_element_visibility( self._bounded_selector(self.EDITOR_SELECTOR), "Note Editor is visible." )
selector = self._bounded_selector(".annotator-outer") self.wait_for_element_invisibility(selector, text)
ActionChains(self.browser).move_to_element(self.element).click().perform() return self
self.q(css=self.NOTE_SELECTOR).first.click() return self
ActionChains(self.browser).move_to_element(self.element).perform() self.wait_for_viewer_visibility() return self
self.q(css=self._bounded_selector(".annotator-close")).first.click() self.wait_for_notes_invisibility("Note is canceled.") return self
self.q(css=self._bounded_selector(".annotator-save")).first.click() self.wait_for_notes_invisibility("Note is saved.") self.wait_for_ajax() return self
self.q(css=self._bounded_selector(".annotator-delete")).first.click() self.wait_for_notes_invisibility("Note is removed.") self.wait_for_ajax() return self
self.q(css=self._bounded_selector(".annotator-edit")).first.click() self.wait_for_editor_visibility() return self
self.q(css=self._bounded_selector(".annotator-item textarea")).first.fill(value)
self.q(css=self._bounded_selector(".annotator-item input")).first.fill(" ".join(tags))
super(DashboardPage, self).__init__(browser)
text_items = self.q(css='section#my-courses').text if len(text_items) > 0: return text_items[0] else: return ""
def _get_course_name(el): return el.text return self.q(css='h3.course-title > a').map(_get_course_name).results
message = self.q(css='div.wrapper-msg') if message.present: return message.text[0] return None
return self.q(css='li.prerequisites > .tip').visible
return self.q(css='ul.listing-courses')
return self.q(css='a.action-{}'.format(widget_name))
return self.q(css='ul.listing-courses .course-item')
return self.q(css='ul.listing-courses .course-item .info-date-block').first.text[0]
self.q(css='.dropdown').first.click()
self.q(css='.label-username').first.click()
return self.q(css='.dropdown-menu li a').text
self.q(css='.dropdown-menu li a').nth(1).click()
self.q(css='.dropdown-menu li a').nth(2).click()
mathjax_container = self.q(css=".static_tab_wrapper .MathJax_SVG") EmptyPromise( lambda: mathjax_container.present and mathjax_container.visible, "MathJax is not visible" ).fulfill()
return self._is_on_tab(tab_name)
url = "{base}/verify_student/{entry_point}/{course}/".format( base=BASE_URL, entry_point=self._entry_point, course=self._course_id ) return url
self.q(css=".contribution-option > input").first.click()
self.q(css=".payment-button").click() FakePaymentPage(self.browser, self._course_id).wait_for_page()
self.q(css="#verify_now_button").click() PaymentAndVerificationFlow(self.browser, self._course_id, entry_point='verify-now').wait_for_page()
self.q(css="#verify_later_button").click() DashboardPage(self.browser).wait_for_page()
self.q(css="#next_step_button").click() next_page_object.wait_for_page()
super(FakePaymentPage, self).__init__(browser) self._course_id = course_id
message = self.q(css='BODY').text[0] match = re.search('Payment page', message) return True if match else False
super(CertificatePage, self).__init__(browser) self.user_id = user_id self.course_id = course_id
return self.q(css='section.about-accomplishments').present
return BASE_URL + "/" + self.url_path + "/user/" + self.user_id + "/course/" + self.course_id
return self.q(css='section.banner-user')
return self.q(css='button.action-linkedin-profile')
return self.q(css=self.BOOKMARKS_BUTTON_SELECTOR).visible
return self.q(css=self.BOOKMARKS_BUTTON_SELECTOR).visible
self.q(css=self.BOOKMARKS_BUTTON_SELECTOR).first.click() if wait_for_results: EmptyPromise(self.results_present, "Bookmarks results present").fulfill()
return self.q(css='#my-bookmarks').present
return self.q(css='.bookmarks-results-header').text[0]
return self.q(css='.bookmarks-empty-header').text[0]
return self.q(css='.bookmarks-empty-detail-title').text[0]
return len(self.q(css=self.BOOKMARKED_ITEMS_SELECTOR).results)
breadcrumbs = self.q(css=self.BOOKMARKED_BREADCRUMBS).text return [breadcrumb.replace('\n', '').split('-') for breadcrumb in breadcrumbs]
return self.q(css='.conditional-wrapper').visible
return self.q(css='.hidden-contents').visible
super(IndexPage, self).__init__(browser)
element = self.q(css=BANNER_SELECTOR) return element.visible and element.text[0].startswith("Welcome to the Open edX")
return self.q(css=BANNER_SELECTOR)
return self.q(css=INTRO_VIDEO_SELECTOR)
return self.q(css=VIDEO_MODAL_SELECTOR)
self.q(css="input[name=cancel]").click()
self.q(css="input[name=authorize]").click()
return self.q(css=".error").present
return len(self.q(css='nav.course-navigation a.chapter'))
return len(self.q(css=self.section_selector))
return len(self.q(css=self.subsection_selector))
return self.q(css=self.xblock_component_selector)
return len(self.xblock_components)
return self.q(css=self.xblock_component_selector).attrs('data-block-type')[index]
element = self.q(css="#content .container-footer .course-license") if element.is_present(): return element.text[0] return None
sequential_position_css = '#sequence-list #tab_{0}'.format(sequential_position - 1) self.q(css=sequential_position_css).first.click()
tab_id = self._active_sequence_tab.attrs('id')[0] return int(tab_id.split('_')[1])
active_tab = self._active_sequence_tab return active_tab and previous_tab_id != active_tab.attrs('data-id')[0]
return self.q(css='button.start-timed-exam[data-start-immediately="false"]').is_present()
self.q(css=".xblock-student_view .timed-exam .start-timed-exam").first.click() self.wait_for_element_presence(".proctored_exam_status .exam-timer", "Timer bar")
return self.q(css="div.proctored-exam.completed").visible
return self.q(css='#content .container section.course-content .sequential-status-message')
return self.entrance_exam_message_selector.is_present()
return self.entrance_exam_message_selector.is_present() \ and "You have passed the entrance exam" in self.entrance_exam_message_selector.text[0]
return self.q(css=".proctored_exam_status .exam-timer").is_present()
return [part.strip() for part in self.q(css='.path').text[0].split('>')]
EmptyPromise(lambda: self.q(css='.bookmark-button').visible, "Bookmark button visible").fulfill() return True
return 'bookmarked' if self.q(css='.bookmark-button.bookmarked').present else ''
return self.q(css='.active .bookmark-icon').visible
return self.q(css='.breadcrumb').present
edit_button = self.q(css='.fa-pencil') edit_button.click()
return str(self.q(css='.main-article h1').text[0])
return "/wiki/" + self.article_name + "/_edit"
return self.q(css='.CodeMirror-scroll').present
type_in_codemirror(self, 0, content)
return self.q(css=self.search_results_selector)
return self.q(css=self.search_bar_selector).present
self.q(css=self.search_bar_selector + ' input[type="text"]').fill(text)
self.enter_search_term(text) self.search()
query = self.q(css='.ui-loading-indicator') return query.present and 'is-hidden' not in query.attrs('class')[0].split()
EmptyPromise(self._is_loading_in_progress, "Loading is in progress.").fulfill()
return self.q(css='#dashboard-search-results')
return self.q(css=self.search_bar_selector).present
self.q(css=self.search_bar_selector + ' input[type="text"]').fill(text)
self.q(css=self.search_bar_selector + ' [type="submit"]').click() self.wait_for_element_visibility('.search-info', 'Search results are shown')
self.q(css='a[data-section=membership]').first.click() membership_section = MembershipPage(self.browser) membership_section.wait_for_page() return membership_section
self.q(css='a[data-section=data_download]').first.click() data_download_section = DataDownloadPage(self.browser) data_download_section.wait_for_page() return data_download_section
self.q(css='a[data-section=student_admin]').first.click() student_admin_section = StudentAdminPage(self.browser) student_admin_section.wait_for_page() return student_admin_section
self.q(css='a[data-section=certificates]').first.click() certificates_section = CertificatesPage(self.browser) certificates_section.wait_for_page() return certificates_section
self.q(css='a[data-section=special_exams]').first.click() timed_exam_section = SpecialExamsPage(self.browser) timed_exam_section.wait_for_page() return timed_exam_section
self.q(css='a[data-section=send_email]').first.click() email_section = BulkEmailPage(self.browser) email_section.wait_for_page() return email_section
return '.send-email {}'.format(selector)
recipient_selector_css = "input[name='send_to'][value='{}']".format(recipient) self.q(css=self._bounded_selector(recipient_selector_css))[0].click()
return MembershipPageAutoEnrollSection(self.browser)
return '.cohort-management {}'.format(selector)
query = self.q(css=self._bounded_selector("#cohort-select option")) return len(query) > 0, query
return label.split(' (')[0]
return int(label.split(' (')[1].split(')')[0])
self.q(css=self._bounded_selector("div.form-actions .action-save")).first.click()
attributes = self.q(css=self._bounded_selector('.cohort-management-assignment-type-settings')).attrs('class') if 'is-disabled' in attributes[0].split(): return True return False
query = self.q(css=self._bounded_selector('.copy-error')) if query.visible: return query.text[0] return ''
return self._cohort_name(self.q(css=self._bounded_selector(".group-header-title .title-value")).text[0])
return [ self._cohort_name(opt.text) for opt in self._get_cohort_options().filter(lambda el: el.get_attribute('value') != "") ]
return self._cohort_name( self._get_cohort_options().filter(lambda el: el.is_selected()).first.text[0] )
return self._cohort_count( self._get_cohort_options().filter(lambda el: el.is_selected()).first.text[0] )
textinput = self.q(css=self._bounded_selector("#cohort-name")).results[0] textinput.clear() textinput.send_keys(cohort_name)
css = self._bounded_selector(self.assignment_type_buttons_css) self.q(css=css).filter(lambda el: el.get_attribute('value') == assignment_type).first.click()
return self.q(css=self._bounded_selector('.cohort-management-group-setup .setup-value')).first.text[0]
self.q(css=self._bounded_selector(".tab-manage_students")).first.click()
return self.q( css=self._bounded_selector("#cohort-management-group-add-students") ).results[0].get_attribute("value")
return self.q(css=self._bounded_selector("a.link-to-group-settings")).first.click()
selector_query = self.q(css=self._bounded_selector(self.content_group_selector_css)) return [ option.text for option in get_options(selector_query) if option.text != "Not selected" ]
self.select_content_group_radio_button() select_option_by_text( self.q(css=self._bounded_selector(self.content_group_selector_css)), content_group )
radio_button = self.q(css=self._bounded_selector(self.select_content_group_button_css)).results[0] radio_button.click() return radio_button.is_selected()
title_css = ".csv-upload .message-title" detail_css = ".csv-upload .summary-item" return self._get_messages(title_css, detail_css)
return self._get_cohort_messages("confirmations", wait_for_messages)
return self._get_cohort_messages("errors")
message = self.q(css=self._bounded_selector(".input-group-other .copy-error")) if not message: return None return message.results[0].text
self.q(css=self._bounded_selector("a.link-cross-reference[data-section=data_download]")).first.click()
return self.q(css=self._bounded_selector('.cohorts-state')).selected
if state != self.is_cohorted: self.q(css=self._bounded_selector('.cohorts-state')).first.click() self.wait_for_ajax()
self.q(css=self._bounded_selector(".toggle-cohort-management-discussions")).first.click() self.wait_for_element_visibility("#cohort-management-discussion-topics", "Waiting for discussions to appear")
self.q(css=self._bounded_selector(".check-discussion-subcategory-%s" % key)).first.click()
self.q(css=self._bounded_selector(".check-all-inline-discussions")).first.click()
return self.q(css=self._bounded_selector(".check-all-inline-discussions:checked"))
return self.q(css=self._bounded_selector(".check-cohort-inline-discussions:checked"))
self.q(css=self._bounded_selector(".check-cohort-inline-discussions")).first.click()
inline_topics = self.q(css=self._bounded_selector('.check-discussion-subcategory-inline')) return all(topic.get_attribute('disabled') == 'true' for topic in inline_topics)
return self.q(css=self._bounded_selector('.check-discussion-category:checked')).is_present()
cohorted_topics = self.q(css=self._bounded_selector('.check-discussion-subcategory-%s:checked' % key)) return len(cohorted_topics.results)
save_button_css = '%s %s' % (self.discussion_form_selectors[key], '.action-save') self.q(css=self._bounded_selector(save_button_css)).first.click()
return (self.q(css=self._bounded_selector('.cohort-management-nav')).visible and self.q(css=self._bounded_selector('.wrapper-cohort-supplemental')).visible)
return self.q(css=self.auto_enroll_browse_button_selector).is_present()
return self.q(css=self.auto_enroll_upload_button_selector).is_present()
self.q(css=self.auto_enroll_upload_button_selector).click()
notification_selector = '.auto_enroll_csv .results .message-%s' % section_type self.wait_for_element_presence(notification_selector, "%s Notification" % section_type.title()) return self.q(css=notification_selector).is_present()
self._upload_file('auto_reg_enrollment.csv')
self._upload_file('auto_reg_enrollment_errors_warnings.csv')
self._upload_file('image.jpg')
file_path = InstructorDashboardPage.get_asset_path(filename) self.q(css=self.auto_enroll_browse_button_selector).results[0].send_keys(file_path) self.click_upload_file_button()
return self.q(css="a#add-allowance").present
return self.q(css="table.allowance-table tr.allowance-items").present
return self.q(css="div.modal div.modal-header").present and self._are_all_assets_present()
self.q(css="a#add-allowance").click() self.wait_for_element_presence("div.modal div.modal-header", "Popup should be visible")
return self.q(css="#search_attempt_id").present
return self.q(css="a.remove-attempt").present
return self.q(css='input[name=list-profiles-csv]')
return self.q(css='input[name=calculate-grades-csv]')
return self.q(css='input[name=problem-grade-report]')
return self.q(css="#report-downloads-table .file-download-link>a")
return self.q(css='input[name=export-ora2-data]')
EmptyPromise( lambda: len(self.report_download_links) >= 1, 'Waiting for downloadable report' ).fulfill()
return self.report_download_links.map(lambda el: el.text)
return self.q(css='a[data-section=student_admin].active-section').present
return self.q(css='{} input[name=entrance-exam-student-select-grade]'.format(self.EE_CONTAINER))
return self.q(css='{} input[name=reset-entrance-exam-attempts]'.format(self.EE_CONTAINER))
return self.q(css='{} input[name=rescore-entrance-exam]'.format(self.EE_CONTAINER))
return self.q(css='{} input[name=skip-entrance-exam]'.format(self.EE_CONTAINER))
return self.q(css='{} input[name=delete-entrance-exam-state]'.format(self.EE_CONTAINER))
return self.q(css='{} input[name=entrance-exam-task-history]'.format(self.EE_CONTAINER))
return self.q(css='{} .request-response-error'.format(self.EE_CONTAINER)).first
return self.student_email_input.is_present()
return self.reset_attempts_button.is_present()
return self.rescore_submission_button.is_present()
return self.delete_student_state_button.is_present()
return self.background_task_history_button.is_present()
return self.q(css='{} .entrance-exam-task-history-table'.format(self.EE_CONTAINER)).is_present()
return self.reset_attempts_button.click()
return self.rescore_submission_button.click()
return self.skip_entrance_exam_button.click()
return self.delete_student_state_button.click()
return self.background_task_history_button.click()
input_box = self.student_email_input.first.results[0] input_box.send_keys(email_addres)
self.wait_for_element_visibility( 'div.certificate-invalidation-container', 'Certificate invalidations section is visible.' ) self.wait_for_element_visibility('#invalidate-certificate', 'Invalidate Certificate button is visible')
self.browser.refresh() self.wait_for_page()
return self.q(css=' '.join([self.PAGE_SELECTOR, css_selector]))
self.get_selector('#generate-exception-certificates').click()
self.get_selector('#certificate-exception').fill(student)
self.get_selector('#add-exception').click()
self.get_selector('#certificate-invalidation-user').fill(student)
self.get_selector('#invalidate-certificate').click()
return self.get_selector('#btn-start-generating-certificates')
return self.get_selector('#disabled-btn-start-generating-certificates')
return self.get_selector('div.certificate-generation-status')
return self.get_selector('div.running-tasks-container')
return self.get_selector('div.certificate-exception-container')
return self.get_selector('div.white-listed-students table tr:last-child td')
return self.get_selector('.certificate-exception-container div.message')
return self.get_selector('div.certificate-invalidation-container table tr:last-child td')
return len(self.q(css='section.updates section article').results)
self.provide_info(email, password) self.submit()
loading_css = "#loading-indicator" courses_css = '.courses-listing' return self.q(css=courses_css).visible \ and self.q(css=loading_css).present \ and not self.q(css=loading_css).visible
return self.q(css=".courses-listing-item")
return self.q(css="#clear-all-filters")
super(TrackSelectionPage, self).__init__(browser) self._course_id = course_id
url = "{base}/course_modes/choose/{course_id}/".format( base=BASE_URL, course_id=self._course_id ) return url
return self.q(css=".wrapper-register-choose").is_present()
return self.q(css=self.VIEW_MODE_OPTIONS_CSS).filter(lambda el: el.is_selected()).first.text[0]
self.q(css=self.VIEW_MODE_OPTIONS_CSS).filter(lambda el: el.text.strip() == view_mode).first.click() self.wait_for_ajax()
self.q(css='a.instructor-info-action').first.click() staff_debug_page = StaffDebugPage(self.browser) staff_debug_page.wait_for_page() return staff_debug_page
self.q(css='input.check').first.click() self.wait_for_ajax()
self.q(css="li.next").click() self.wait_for_ajax()
if user: self.q(css='input[id^=sd_fu_]').first.fill(user) self.q(css='section.staff-modal a.staff-debug-reset').click()
if user: self.q(css='input[id^=sd_fu_]').fill(user) self.q(css='section.staff-modal a.staff-debug-sdelete').click()
if user: self.q(css='input[id^=sd_fu_]').first.fill(user) self.q(css='section.staff-modal a.staff-debug-rescore').click()
return self.q(css='div.instructor-dashboard-wrapper-2').present
return self.q(css='div.batch-enrollment').present
super(CoursePage, self).__init__(browser) self.course_id = course_id
return BASE_URL + "/courses/" + self.course_id + "/" + self.url_path
return '{tabpanel_id} {css}'.format(tabpanel_id=getattr(self, 'tabpanel_id', ''), css=css)
self.q(css=self._bounded_selector('a.action-view')).first.click()
return self.q(css=self._bounded_selector('.team-card'))
return self.q(css=self._bounded_selector('h3.card-title')).map(lambda e: e.text).results
return self.q(css=self._bounded_selector('p.card-description')).map(lambda e: e.text).results
return self.q(css=self._bounded_selector('.member-count')).map(lambda e: e.text).results
return self.q(css='.page-header .breadcrumbs')[0].text
self.q(css='a.nav-item').filter(text='All Topics')[0].click()
self.q(css='a.nav-item').filter(text=topic)[0].click()
return self.q(css='body.view-teams').present
return self.q(css='.is-active').attrs('data-url')[0]
self.q(css=BROWSE_BUTTON_CSS).click()
self.wait_for( lambda: len(self.q(css='.team-card')) == expected_count, description="Expected number of teams is wrong" )
self.q(css='a.nav-item').filter(text='All Topics')[0].click()
self.q(css='a.nav-item').filter(text=topic)[0].click()
return self.q(css='.warning').results[0].text
button_classes = self.q(css=MY_TEAMS_BUTTON_CSS).attrs('class') if len(button_classes) == 0: return False return 'is-active' in button_classes[0]
button_classes = self.q(css=BROWSE_BUTTON_CSS).attrs('class') if len(button_classes) == 0: return False return 'is-active' in button_classes[0]
return self.q(css=TOPIC_CARD_CSS).results
return self.q(css='#tabpanel-browse ' + CARD_TITLE_CSS).map(lambda e: e.text).results
return self.q(css='p.card-description').map(lambda e: e.text).results
self.q(css=TEAMS_LINK_CSS).filter( text='View Teams in the {topic_name} Topic'.format(topic_name=topic_name) )[0].click() self.wait_for_ajax()
self.q( css='#paging-header-select option[value={sort_order}]'.format(sort_order=sort_order) ).click() self.wait_for_ajax()
super(BaseTeamsPage, self).__init__(browser, course_id) self.topic = topic
has_correct_url = self.url.endswith(self.url_path) teams_list_view_present = self.q(css='.teams-main').present return has_correct_url and teams_list_view_present
return self.q(css=TEAMS_HEADER_CSS + ' .page-title')[0].text
return self.q(css=TEAMS_HEADER_CSS + ' .page-description')[0].text
return self.q( css='#paging-header-select option' ).filter( lambda e: e.is_selected() ).results[0].text.strip()
return self.q(css=CARD_TITLE_CSS).map(lambda e: e.text).results
query = self.q(css=CREATE_TEAM_LINK_CSS) if query.present: query.first.click() self.wait_for_ajax()
query = self.q(css='.search-team-descriptions') if query.present: query.first.click() self.wait_for_ajax()
query = self.q(css='.browse-teams') if query.present: query.first.click() self.wait_for_ajax()
self.q( css='#paging-header-select option[value={sort_order}]'.format(sort_order=sort_order) ).click() self.wait_for_ajax()
return self.header_description.startswith(u"Showing results for")
super(TeamManagementPage, self).__init__(browser, course_id) self.topic = topic self.url_path = "teams/#topics/{topic_id}/create-team".format(topic_id=self.topic['id'])
return self.q(css='.team-edit-fields').present
return self.q(css='.page-header .page-title')[0].text
return self.q(css='.page-header .page-description')[0].text
return self.q(css='.create-team.wrapper-msg .copy')[0].text
self.q(css='.create-team .action-primary').first.click() self.wait_for_ajax()
self.q(css='.create-team .action-cancel').first.click() self.wait_for_ajax()
return self.q(css='.action-delete').first
self.q(css='.action-edit-members').first.click() self.wait_for_ajax()
return self.q(css='.action-edit-members').present
return len(self.q(css='.team-member'))
self.q(css='.action-remove-member').first.click()
confirm_prompt(self, require_notification=False) self.wait_for_ajax()
confirm_prompt(self, cancel=True)
self.wait_for_ajax() if self.team: if not self.url.endswith(self.url_path): return False return self.q(css='.team-profile').present
return self.q(css='div.discussion-module').attrs('data-discussion-id')[0]
return self.q(css='.page-header .page-title')[0].text
return self.q(css=TEAMS_HEADER_CSS + ' .page-description')[0].text
return self.q(css='.page-content-secondary .team-members .team-member').present
return self.q(css='.page-content-secondary .team-capacity :last-child').text[0]
return self.q(css='.page-content-secondary .team-country :last-child').text[0]
return self.q(css='.page-content-secondary .team-language :last-child').text[0]
query = self.q(css='.page-content-secondary > .team-user-membership-status') return query.text[0] if query.present else ''
return self.q(css='.leave-team-link').present
return len(self.q(css='.page-content-secondary .team-member'))
self.q(css='.page-content-secondary .members-info .team-member').first.click()
return self.q(css='.page-content-secondary .tooltip-custom').text[0]
self.wait_for( lambda: self.team_capacity_text == self.format_capacity_text(num_members, max_size), description="Team capacity text is not correct" )
return '{num_members} / {max_size} {members_text}'.format( num_members=num_members, max_size=max_size, members_text='Member' if num_members == max_size else 'Members' )
self.wait_for_ajax() return self.q(css='.join-team .join-team-message').text[0]
return self.q(css='.join-team .action-primary').present
return self.q(css='.join-team .join-team-message').present
return self.q(css='.discussion-module .new-post-btn').present
return self.q(css='.form-actions .action-edit-team').present
return self.q(css='.program-card').present
return self.q(css='.sidebar').present
url = BASE_URL + '/dashboard/programs/123/program-name/' def is_browser_on_page(self): return self.q(css='.js-program-details-wrapper').present
url = '{base}/course_modes/create_mode/{course_id}/'.format( base=BASE_URL, course_id=self._course_id ) query_string = urllib.urlencode(self._parameters) if query_string: url += '?' + query_string return url
return self.q(css='a.problem-button').text
index = self.problem_list.index(problem_name) + 1 self.q(css='a.problem-button:nth-of-type({})'.format(index)).first.click()
query = self.q(css='.u-field-{}'.format(field_id)) return query.text[0] if query.present else None
EmptyPromise( lambda: self.field(field_id) is not None, "Field with id \"{0}\" is in DOM.".format(field_id) ).fulfill()
self.wait_for_field(field_id) query = self.q(css='.u-field-{} .u-field-title'.format(field_id)) return query.text[0] if query.present else None
self.wait_for_field(field_id) query = self.q(css='.u-field-{} .u-field-message'.format(field_id)) return query.text[0] if query.present else None
self.wait_for_field(field_id) query = self.q(css='.u-field-{} .u-field-message-help'.format(field_id)) return query.text[0] if query.present else None
EmptyPromise( lambda: message in (self.message_for_field(field_id) or ''), "Messsage \"{0}\" is visible.".format(message) ).fulfill()
EmptyPromise( lambda: indicator == self.indicator_for_field(field_id), "Indicator \"{0}\" is visible.".format(self.indicator_for_field(field_id)) ).fulfill()
self.wait_for_field(field_id) query = self.q(css='.u-field-{} .u-field-value'.format(field_id)) if not query.present: return None return query.text[0]
self.wait_for_field(field_id) self.wait_for_ajax() return self.q(css='.u-field-{} .u-field-value .u-field-value-readonly'.format(field_id)).text[0]
self.wait_for_field(field_id) query = self.q(css='.u-field-link-title-{}'.format(field_id)) return query.text[0] if query.present else None
return EmptyPromise( lambda: self.link_title_for_link_field(field_id) == expected_title, "Link field with link title \"{0}\" is visible.".format(expected_title) ).fulfill()
self.wait_for_field(field_id) query = self.q(css='.u-field-{} {}'.format(field_id, field_type)) if query.present: query.first.click()
return self.q(css='{}[data-id="{}"]'.format(self.BODY_SELECTOR, self.locator)).present
return '{}[data-id="{}"] {}'.format( self.BODY_SELECTOR, self.locator, selector )
child_blocks = self.q(css=self._bounded_selector("div[data-id]")) return frozenset(child.text for child in child_blocks)
self.wait_for_ajax() video_selector = '{0}'.format(CSS_CLASS_NAMES['video_container']) self.wait_for_element_presence(video_selector, 'Video is initialized')
return not self.q(css=CSS_CLASS_NAMES['video_spinner']).visible
selector = self.get_element_selector(CSS_CLASS_NAMES['poster']) return self.q(css=selector).visible
selector = self.get_element_selector(CSS_CLASS_NAMES['poster']) self.q(css=selector).click()
if vertical: return '{vertical} {video_element}'.format( vertical=self.get_video_vertical_selector(self.current_video_display_name), video_element=class_name) else: return class_name
self.current_video_display_name = video_display_name
is_present = self.q(css=selector).present return is_present, is_present
selector = self.get_element_selector(CSS_CLASS_NAMES['video_container']) auto_play = json.loads(self.q(css=selector).attrs('data-metadata')[0])['autoplay'] return auto_play
selector = self.get_element_selector(CSS_CLASS_NAMES['error_message']) return self.q(css=selector).visible
selector = self.get_element_selector(CSS_CLASS_NAMES['video_spinner']) return self.q(css=selector).visible
selector = self.get_element_selector(CSS_CLASS_NAMES['error_message']) return self.q(css=selector).text[0]
selector = self.get_element_selector(VIDEO_BUTTONS[button_id]) return self.q(css=selector).visible
self._captions_visibility(True)
self._captions_visibility(False)
self._closed_captions_visibility(True)
self._closed_captions_visibility(False)
self.wait_for_ajax() caption_state_selector = self.get_element_selector(CSS_CLASS_NAMES['captions']) return self.q(css=caption_state_selector).visible
self.wait_for_ajax() closed_caption_state_selector = self.get_element_selector(CSS_CLASS_NAMES['closed_captions']) return self.q(css=closed_caption_state_selector).visible
self.wait_for_captions() captions_selector = self.get_element_selector(CSS_CLASS_NAMES['captions_text']) subs = self.q(css=captions_selector).html return ' '.join(subs)
self.wait_for_closed_captions() closed_captions_selector = self.get_element_selector(CSS_CLASS_NAMES['closed_captions']) subs = self.q(css=closed_captions_selector).html return ' '.join(subs)
self.wait_for_captions() captions_selector = self.q(css=CSS_CLASS_NAMES['captions_text_getter']) captions_selector.click()
speed_selector = self.get_element_selector(CSS_CLASS_NAMES['video_speed']) return self.q(css=speed_selector).text[0]
self.wait_for(lambda: self.speed == expected_speed, "Video speed changed")
element = self.q(css=selector).results[0] return element.size
return self.browser.get_cookie(cookie_name)
selector = self.get_element_selector(VIDEO_MENUS["language"] + ' li.is-active') return self.q(css=selector).first.attrs('data-lang-code')[0]
selector = self.get_element_selector(VIDEO_MENUS[menu_name]) return self.q(css=selector).present
selector = self.get_element_selector(CSS_CLASS_NAMES['video_time']) current_seek_position = self.q(css=selector).text[0] return current_seek_position.split('/')[0].strip()
return int(self.position.split(':')[1])
self._wait_for( lambda: self.state == state, 'State is {state}'.format(state=state) )
return self.state != 'buffering'
self.browser.refresh() self.wait_for_video_player_render()
self._wait_for( lambda: self.position == position, 'Position is {position}'.format(position=position) )
selector = self.get_element_selector(VIDEO_BUTTONS['quality']) return self.q(css=selector).visible
selector = self.get_element_selector(VIDEO_BUTTONS['quality']) classes = self.q(css=selector).attrs('class')[0].split() return 'active' in classes
selector = self.get_element_selector(VIDEO_MENUS['transcript-skip']) return self.q(css=selector).visible
captions_rendered_selector = self.get_element_selector(CSS_CLASS_NAMES['captions_rendered']) self.wait_for_element_presence(captions_rendered_selector, 'Captions Rendered')
cc_rendered_selector = self.get_element_selector(CSS_CLASS_NAMES['closed_captions']) self.wait_for_element_visibility(cc_rendered_selector, 'Closed captions rendered')
cc_rendered_selector = self.get_element_selector(CSS_CLASS_NAMES['closed_captions']) self.wait_for_element_invisibility(cc_rendered_selector, 'Closed captions hidden')
return self.q(css=self.thread_selector + " " + selector)
text_list = self._find_within(selector).text return text_list[0] if text_list else None
return self._get_element_text(".group-visibility-label")
self.wait_for_ajax() return self._get_element_text(".response-count")
return len(self._find_within(".discussion-response"))
return self._get_element_text(".response-display-count")
return self._get_element_text(".load-response-button")
self._find_within(".load-response-button").click() EmptyPromise( self.is_ajax_finished, "Loading more Responses" ).fulfill()
return self._is_element_visible(".add-response-btn")
return self._is_element_visible(".response_{} .edit-post-body".format(response_id))
self.wait_for( lambda: self._is_element_visible(".MathJax_SVG"), description="MathJax Preview is rendered" )
self.wait_for_ajax() return self._is_element_visible(".response_{} .response-body".format(comment_id))
with self._secondary_action_menu_open(".response_{} .discussion-response".format(response_id)): return self._is_element_visible(".response_{} .discussion-response .action-edit".format(response_id))
link_href = self._find_within(".post-body p a").attrs('href') return link_href[0] if link_href else None
self._find_within(".response_{} .discussion-response .wmd-input".format(response_id)).fill(new_body)
return ( self.q(css="#new-url-input-field-message.has-error").visible and self.q(css="#new-url-desc-input-field-message.has-error").visible )
return self._is_element_visible(".response_{} .action-show-comments".format(response_id))
return self._is_element_visible("#wmd-input-comment-body-{}".format(response_id))
return self._is_element_visible("#comment_{} .response-body".format(comment_id))
with self._secondary_action_menu_open("#comment_{}".format(comment_id)): return self._is_element_visible("#comment_{} .action-delete".format(comment_id))
with self._secondary_action_menu_open("#comment_{}".format(comment_id)): return self._is_element_visible("#comment_{} .action-edit".format(comment_id))
return self._is_element_visible(".edit-comment-body[data-id='{}']".format(comment_id))
self._find_within("#comment_{} .wmd-input".format(comment_id)).fill(new_body)
return self.q(css="body.discussion .forum-nav-sort-control").present
options = self.q(css="body.discussion .forum-nav-sort-control option") return options.filter(lambda el: el.is_selected())[0].get_attribute("value")
self.q(css="body.discussion .forum-nav-sort-control option[value='{0}']".format(sort_by)).click()
self.browser.refresh()
return self.browser.execute_script("return $('{}').is(':focus')".format(selector))
return len(self.q(css=".forum-nav-thread").results) == thread_count
EmptyPromise( lambda: self.is_focused_on_element(selector), "Focus is on other element" ).fulfill()
return self.q(css=self._discussion_selector + " " + selector)
self._find_within(".discussion-show").first.click() EmptyPromise( self.is_discussion_expanded, "Discussion expanded" ).fulfill()
return self._find_within('.discussion-thread#thread_{}'.format(thread_id)).present
self._find_within(".forum-thread-expand").first.click() EmptyPromise( lambda: bool(self.get_response_total_text()), "Thread expanded" ).fulfill()
return self.browser.execute_script("return $('{}').is(':focus')".format(selector))
self.new_post_button.click() EmptyPromise( lambda: ( self.new_post_form ), "New post action succeeded" ).fulfill()
elements = self.q(css="ol.course-tabs .new-post-btn") return elements.first if elements.visible and len(elements) == 1 else None
seq_css = 'ol#sequence-list>li>.nav-item>.sequence-tooltip' return self.q(css=seq_css).map(self._clean_seq_titles).results
chapter_css = '.course-navigation .chapter .group-heading' return self.q(css=chapter_css).map(lambda el: el.text.strip()).results
desc = "currently at section '{0}' and subsection '{1}'".format(section_title, subsection_title) return EmptyPromise( lambda: self.is_on_section(section_title, subsection_title), desc )
return self.REMOVE_SPAN_TAG_RE.search(element.get_attribute('innerHTML')).groups()[0].strip()
return self.q(css=".badges-modal").is_focused()
super(LearnerProfilePage, self).__init__(browser) self.username = username
return BASE_URL + "/u/" + self.username
return all([ self.q(css='body.view-profile .account-settings-container').present, not self.q(css='ui-loading-indicator').visible ])
return 'all_users' if self.q(css=PROFILE_VISIBILITY_SELECTOR.format('all_users')).selected else 'private'
return self.q(css="button[data-url='accomplishments']").visible
return [Badge(element, self.browser) for element in self.q(css=".badge-display:not(.badge-placeholder)")]
self.wait_for_ajax() return self.q(css='.u-field-{}'.format(field_id)).visible
self.wait_for_field(field_id) self.make_field_editable(field_id) return self.mode_for_field(field_id) == 'edit'
self.wait_for_ajax() return self.q(css='#u-field-select-account_privacy').visible
return self.icon_for_field(field_id, FIELD_ICONS[field_id])
self.wait_for_ajax() return self.q(css='#u-field-message-account_privacy').text[0]
self.wait_for_ajax() return self.q(css='#u-field-message-account_privacy').visible
self.wait_for_field('image') default_links = self.q(css='.image-frame').attrs('src') return 'profiles/default' in default_links[0] if default_links else False
mouse_hover_action = ActionChains(self.browser).move_to_element(element) mouse_hover_action.perform()
self.wait_for_field('image') return self.q(css='.u-field-upload-button').visible
self.wait_for_field('image') self.wait_for_ajax() return self.q(css='.message-banner p').text[0]
url = AUTH_BASE_URL + "/auto_auth" query_str = urllib.urlencode(self._params) if query_str: url += "?" + query_str return url
if self._user_info is None: user_info = self.get_user_info() if user_info is not None: self._user_info = self.get_user_info() return self._user_info
return self.q(css='.annotatable-title').text[0]
mouse_hover_action = ActionChains(self.browser).move_to_element(element) mouse_hover_action.perform()
annotation_input_selector = self.active_problem_selector('.annotation-input') return self.q(css=annotation_input_selector).visible
self.q(css=self.active_problem_selector('.annotation-return')).click()
return self.q(css=self.PAGINATION_HEADER_TEXT_CSS).text[0]
return int(self.q(css=self.CURRENT_PAGE_NUMBER_CSS).text[0])
return int(self.q(css=self.TOTAL_PAGES_CSS).text[0])
self.q(css=self.PAGE_NUMBER_INPUT_CSS).results[0].send_keys(unicode(page_number), Keys.ENTER) self.wait_for_ajax()
self.q(css=self.NEXT_PAGE_BUTTON_CSS).click() self.wait_for_ajax()
self.q(css=self.PREVIOUS_PAGE_BUTTON_CSS).click() self.wait_for_ajax()
return self.is_enabled(self.NEXT_PAGE_BUTTON_CSS)
return self.is_enabled(self.PREVIOUS_PAGE_BUTTON_CSS)
return 'is-disabled' not in self.q(css=css).attrs('class')[0]
return page.q(css='.wrapper-notification-mini.is-shown').present
return page.q(css='.wrapper-notification-mini.is-hiding').present
url = BASE_URL + "/logout" def is_browser_on_page(self): return self.q(css='.btn-login').present
super(AcidView, self).__init__(browser) if isinstance(context_selector, unicode): context_selector = context_selector.encode('utf-8') self.context_selector = context_selector
selector = '{} .acid-block {} .pass'.format(self.context_selector, test_selector) return bool(self.q(css=selector).results)
selector = '{} .acid-parent-block {} .pass'.format(self.context_selector, test_selector) return bool(self.q(css=selector).execute(try_interval=0.1, timeout=3))
return self.test_passed('.js-init-run')
return all([ self.child_test_passed('.child-counts-match'), self.child_test_passed('.child-values-match') ])
return self.test_passed('.local-resource-test')
super(CrowdsourcehinterProblemPage, self).__init__(browser)
return self.q(css='div.csh_hint_text').text
return self.q(css='div.csh_hint_text').attrs('student_answer')
self.q(css='div.csh_rate_hint').click() self.wait_for_ajax()
if raw: self.set_raw_content(content) else: self.set_content(content) self.save()
if raw: self.set_raw_content(content) else: self.set_content(content) self.cancel()
self.q(css=self.editor_mode_css).click() self.browser.execute_script("tinyMCE.activeEditor.setContent('%s')" % content)
url = BASE_URL + "/signup" def is_browser_on_page(self): return self.q(css='body.view-signup').present
return all([ self.q(css='body.view-group-configurations').present, self.q(css='div.ui-loading.is-hidden').present ])
return self._get_groups(self.experiment_groups_css)
return self._get_groups(self.content_groups_css)
css = prefix + ' .wrapper-collection' return [GroupConfiguration(self, prefix, index) for index in xrange(len(self.q(css=css)))]
self.q(css=self.experiment_groups_css + " .new-button").first.click()
self.q(css=self.content_groups_css + " .new-button").first.click()
self.q(css=self.content_groups_css + " .action-add").first.click()
return self.q(css='.wrapper-content ' + prefix + ' .no-content')
return self.q(css=self.experiment_groups_css).present or self.q(css=".experiment-groups-doc").present
return self.page.q(css=self.get_selector(css=selector))
self.find_css('a.group-toggle').first.click()
return self.find_css('a.group-toggle.hide-groups').present
self.find_css('button.action-add-group').first.click()
return self.find_css(css).first.text[0]
self.find_css('p.group-configuration-usage-text a').first.click()
self.find_css('li.group-configuration-usage-unit a').nth(index).click()
self.find_css('.action-edit .edit').first.click()
return self.find_css('.actions .delete').present
self.find_css('.actions .delete').first.click() confirm_prompt(self.page)
self.find_css('.action-primary').first.click() self.page.wait_for_ajax()
self.find_css('.action-secondary').first.click()
if self.find_css('.collection-edit').present: return 'edit' elif self.find_css('.collection').present: return 'details'
return self.get_text('.group-configuration-id .group-configuration-value')
return self.get_text('.message-status.error')
css = '.group-configuration-usage-unit' return self.find_css(css).text
return self.get_text('.title')
self.find_css('.collection-name-input').first.fill(value)
return self.get_text('.group-configuration-description')
self.find_css('.group-configuration-description-input').first.fill(value)
return self.find_css('.wrapper-delete-button').first.attrs('data-tooltip')[0]
return self.page.q(css=self.prefix + selector)
css = '.group-name' return self.find_css(css).first.text[0]
css = '.group-name' self.find_css(css).first.fill(value)
css = '.group-allocation' return self.find_css(css).first.text[0]
css = '.action-close' return self.find_css(css).first.click()
self.q(css='.nav-item .new-button').click()
return self.q(css=selector)[0].text
self.q(css=selector)[0].send_keys(value)
return self.q(css='body.view-course-create-rerun').present
return self.q(css=self.COURSE_RUN_INPUT).text[0]
set_input_value(self, self.COURSE_RUN_INPUT, value)
url = BASE_URL + "/howitworks" def is_browser_on_page(self): return self.q(css='body.view-howitworks').present
self.browser.refresh() self.wait_for_page()
self.wait_for_element_visibility( '#pre-requisite-course', 'Prerequisite course element is available' ) return self.get_elements('#pre-requisite-course')
self.wait_for_element_visibility( '#entrance-exam-enabled', 'Entrance exam checkbox is available' ) return self.get_element('#entrance-exam-enabled')
self.wait_for_element_visibility( '#alert-confirmation-title', 'Alert confirmation title element is available' ) return self.get_element('#alert-confirmation-title')
return self.pacing_css + ':checked'
return self.q(css='#course-pace-toggle-tip').results[0].text
EmptyPromise( lambda: self.q(css="#pre-requisite-course").present, 'Prerequisite course dropdown selector is displayed' ).fulfill()
press_the_notification_button(self, "save") if wait_for_confirmation: self.wait_for_element_visibility( '#alert-confirmation-title', 'Save confirmation message is visible' )
return browser.execute_script("return typeof(jQuery) == 'undefined' || jQuery.active == 0")
raise NotImplementedError
return self.q(css='body.view-team').present and not self.q(css='.ui-loading').present
return self.q(css='.user-list .user-item').map( lambda el: UserWrapper(self.browser, el.get_attribute('data-email')) ).results
return [user.name for user in self.users]
return self.q(css='.create-user-button').present
self.q(css='.create-user-button').first.click() self.wait_for(lambda: self.new_user_form_visible, "Add user form is visible")
return self.q(css='.form-create.create-user .user-email-input').visible
self.q(css='.form-create.create-user .user-email-input').fill(email)
target_users = [user for user in self.users if user.email == email] assert len(target_users) == 1 return target_users[0]
self.wait_for_element_visibility('.create-user-button', "Add team member button is available") self.click_add_button() self.set_new_user_email(email) self.click_submit_new_user_form() self.wait_for_page()
target_user = self.get_user(email) target_user.click_delete() self.wait_for_page()
return self.q(css='.prompt.{dialog_type}'.format(dialog_type=dialog_type)).visible
return self.q(css='.prompt.{dialog_type} .message'.format(dialog_type=dialog_type)).text[0]
return "{}/library/{}/team/".format(BASE_URL, unicode(self.locator))
url_path = "course_team"
return self.q(css=self.selector).present
return '{} {}'.format(self.selector, selector)
return self.q(css=self._bounded_selector('.user-username')).text[0]
return self.q(css=self._bounded_selector('.flag-role .value')).text[0]
return self.q(css=self._bounded_selector('.flag-role .msg-you')).present
return self.q(css=self._bounded_selector('.add-admin-role')).present
return self.q(css=self._bounded_selector('.add-admin-role')).text[0]
self.q(css=self._bounded_selector('.add-admin-role')).click() wait_for_ajax_or_reload(self.browser)
return self.q(css=self._bounded_selector('.remove-admin-role')).present
return self.q(css=self._bounded_selector('.remove-admin-role')).text[0]
self.q(css=self._bounded_selector('.remove-admin-role')).click() wait_for_ajax_or_reload(self.browser)
return self.q(css=self._bounded_selector('.action-delete:not(.is-disabled) .remove-user')).present
return self.q(css=self._bounded_selector('.notoggleforyou')).present
self.browser.refresh()
return self.q(css='.signatory-title-value').first.html[0]
return self.q(css='.actual-course-number .certificate-value').first.text[0]
return self.q(css='.course-number-override .certificate-value').first.text[0]
return self.q(css='.course-number-override')
css = self.certficate_css + ' .wrapper-collection' return [CertificateSectionPage(self, self.certficate_css, index) for index in xrange(len(self.q(css=css)))]
return self.q(css='.wrapper-content ' + self.certficate_css + ' .no-content').present
return self.q(css='.wrapper-content ' + self.certficate_css + ' .no-content').text[0]
return self.q(css='.wrapper-content ' + self.certficate_css + ' .no-content a.new-button').text[0]
EmptyPromise( lambda: self.q(css=self.certficate_css + " .new-button").present, 'Create first certificate button is displayed' ).fulfill()
EmptyPromise( lambda: self.q(css=self.certficate_css + " .action-add").present, 'Add certificate button is displayed' ).fulfill()
self.wait_for_first_certificate_button() self.q(css=self.certficate_css + " .new-button").first.click()
self.wait_for_add_certificate_button() self.q(css=self.certficate_css + " .action-add").first.click()
self.selector = prefix + ' .certificates-list-item-{}'.format(index) self.index = index super(CertificateSectionPage, self).__init__(container.browser, **container.course_info)
return self.q(css=".certificates").present
return ' '.join([self.selector, css])
return self.q(css=self.get_selector(css=css_selector))
return self.find_css(css).first.text[0]
return self.get_text('.message-status.error')
if self.find_css('.collection-edit').present: return 'edit' elif self.find_css('.collection').present: return 'details'
return self.get_text('.certificate-id .certificate-value')
return self.get_text('.name')
self.find_css('.collection-name-input').first.fill(value)
return self.get_text('.certificate-description')
self.find_css('.certificate-description-input').first.fill(value)
return self.get_text('.course-title-override .certificate-value')
self.find_css('.certificate-course-title-input').first.fill(value)
EmptyPromise( lambda: self.find_css('.actions .delete.action-icon').present, 'Certificate delete button is displayed' ).fulfill()
EmptyPromise( lambda: self.find_css('a.detail-toggle.hide-details').present, 'Certificate details are expanded' ).fulfill()
disable_animations(self) self.find_css('.action-primary').first.click() self.wait_for_ajax()
self.find_css('.action-primary').first.click() self.wait_for_ajax()
self.find_css('.action-add-signatory').first.click()
self.find_css('.action-edit .edit').first.click()
self.find_css('.action-secondary').first.click()
self.find_css('a.detail-toggle').first.click()
self.wait_for_certificate_delete_button() self.find_css('.actions .delete.action-icon').first.click()
return self.q(css=self.prefix + " .signatory-details-list, .signatory-edit-list").present
selector = self.prefix + ' .signatory-{}-view-{}'.format(self.mode, self.index) return ' '.join([selector, css])
return self.q(css=self.get_selector(css=css_selector))
return self.find_css('.signatory-panel-body .signatory-name-value').first.text[0]
self.find_css('.signatory-name-input').first.fill(value)
return self.find_css('.signatory-panel-body .signatory-title-value').first.text[0]
self.find_css('.signatory-title-input').first.fill(value)
return self.find_css('.signatory-panel-body .signatory-organization-value').first.text[0]
self.find_css('.signatory-organization-input').first.fill(value)
self.q(css='button.signatory-panel-close').click() self.mode = 'details' self.wait_for_signatory_detail_view()
EmptyPromise( lambda: self.q(css='.signatory-panel-delete').present, 'Delete icon is displayed' ).fulfill()
EmptyPromise( lambda: self.q(css='a.button.action-primary').present, 'Delete prompt is displayed' ).fulfill()
EmptyPromise( lambda: self.find_css('.signatory-panel-body .signatory-name-input').present, 'On signatory edit view' ).fulfill()
EmptyPromise( lambda: self.find_css('.signatory-panel-body .signatory-name-value').present, 'On signatory details view' ).fulfill()
EmptyPromise( lambda: self.q(css='.assetupload-modal .action-upload').present, 'Signature image upload dialog opened' ).fulfill()
EmptyPromise( lambda: self.q(css=".action-upload-signature").first.present, 'Signature image upload button available' ).fulfill()
EmptyPromise( lambda: self.q(css=".current-signature-image .signature-image").present, 'Signature image available' ).fulfill()
return self.q(css='body.view-export').present
self.q(css='a.action-export').click()
return self.q(css='.prompt.error').visible
self.q(css='.prompt.error .action-primary').click()
EmptyPromise(self.is_error_modal_showing, 'Error Modal Displayed', timeout=30).fulfill()
return self.q(css='.choose-file-button').present
return EmptyPromise( lambda: self.q(css='#replace-courselike-button')[0], "Upload button appears", timeout=30 ).fulfill()
return self.q(css='#view-updated-button').visible
if completed: return 'is-complete', "'{}' is marked complete" else: return 'is-not-started', "'{}' is in not-yet-started status"
EmptyPromise(self.is_upload_finished, 'Upload Finished', timeout=30).fulfill()
return self.q(css='#fileupload .error-block').visible
return self.q(css='.wrapper-status').visible
return self.q(css='.item-progresspoint-success-date').visible
EmptyPromise(self.is_timestamp_visible, 'Timestamp Visible', timeout=30).fulfill()
EmptyPromise(self.is_filename_error_showing, 'Upload Error Displayed', timeout=30).fulfill()
return self.status_message == 'Contains staff only content' if self.has_status_message else False
set_input_value(self, self._bounded_selector(self.NAME_INPUT_SELECTOR), new_name)
modal = self.edit() modal.is_explicitly_locked = is_locked modal.save()
if not child_class: child_class = self.CHILD_CLASS return self.children(child_class)[index]
click_css( self, self._bounded_selector(self.ADD_BUTTON_SELECTOR), require_notification=require_notification, )
click_css(self, self._bounded_selector('.delete-button'), require_notification=False) confirm_prompt(self, cancel)
return '{}[data-locator="{}"] {}'.format( self.BODY_SELECTOR, self.locator, selector )
return ContainerPage(self.browser, self.locator).visit()
return self.child(title)
return self.children()
return self.child_at(index)
self.q(css=self._bounded_selector(self.ADD_BUTTON_SELECTOR)).click()
return self.child(title)
return self.children()
return self.child_at(index)
self.add_child()
MISSING = 0 COLLAPSE = 1 EXPAND = 2
click_css(self, '.view-live-button', require_notification=False) self.browser.switch_to_window(self.browser.window_handles[-1])
return self.child(title)
return self.child_at(index)
self.q(css='{} .section-name'.format(parent_css)).first.click()
if page_refresh: self.browser.refresh() return self.q(css='{} .section-name'.format(parent_css)).text
return self.q(css='{} .section-name input'.format(parent_css)).present
return self.children()
click_css(self, '.wrapper-mast nav.nav-actions .button-new')
element_css = self.BOTTOM_ADD_SECTION_BUTTON if click_child_icon: element_css += " .fa-plus" click_css(self, element_css)
self.q(css=self.EXPAND_COLLAPSE_CSS).click()
self.reindex_button.click()
self.q(css=".subsection-header-actions .configure-button").nth(index).click() self.wait_for_element_presence('.course-outline-modal', 'Subsection settings modal is present.')
self.q(css="input.proctored_exam").first.click() self.q(css=".action-save").first.click() self.wait_for_ajax()
self.q(css="input.no_special_exam").first.click()
self.q(css="input.timed_exam").first.click()
self.q(css="input.proctored_exam").first.click()
self.q(css="input.practice_exam").first.click()
return self.q(css=".field-time-limit").visible
return self.q(css=".field-exam-review-rules").visible
return self.q(css=".field-hide-after-due").visible
self.q(css=".settings-tab-button[data-tab='access']").first.click() self.wait_for_element_visibility('#is_prereq', 'Gating settings fields are present.')
return self.q(css=self.BOTTOM_ADD_SECTION_BUTTON).first
return self.q(css='.outline .no-content').is_present()
return self.q(css='.wrapper-alert.is-shown').is_present()
self.q(css='.dismiss-button').click()
return self.q(css=".button.button-reindex")[0]
for section in self.sections(): if section.is_collapsed: section.expand_subsection() for subsection in section.subsections(): if subsection.is_collapsed: subsection.expand_subsection()
return self.children(CourseOutlineChild)
return self.q(css=".license-value").first.text[0]
return self.q(css='.wrapper-alert-error.is-shown').is_present()
return self.q(css='.warning-heading-text').text[0]
return self.q(css='.components-list-heading-text').text[0]
return self.q(css='.advance-modules-remove-text').visible
return self.q(css='.advance-modules-remove-text').text[0]
return self.q(css='.components-list').visible
return self.q(css='.components-list li>a').text
return self.q(css='.advance-modules-list li').text
return " ".join([self.MODAL_SELECTOR, selector])
return self.page.q(css=self.MODAL_SELECTOR).present
return self.page.q(css=self._bounded_selector(selector))
self.find_css(selector).nth(index).click()
self.click(".action-save") self.page.wait_for_ajax()
self.click(".action-publish") self.page.wait_for_ajax()
self.click(".action-cancel")
return self.find_css("#start_date").present
return self.find_css("#start_time").present
return self.find_css("#due_date").present
return self.find_css("#due_time").present
return self.find_css("#grading_type").present
self.page.q(css=input_selector).fill(time) self.page.q(css=input_selector).results[0].send_keys(Keys.ENTER)
return self.find_css("#start_date").first.attrs('value')[0]
self.set_date('release_date', "#start_date", date)
return self.find_css("#start_time").first.attrs('value')[0]
self.set_time("#start_time", time)
return self.find_css("#due_date").first.attrs('value')[0]
self.set_date('due_date', "#due_date", date)
return self.find_css("#due_time").first.attrs('value')[0]
self.set_time("#due_time", time)
element = self.find_css('#grading_type')[0] return self.get_selected_option_text(element)
return self.find_css('#staff_lock')[0].is_selected()
return self.find_css('.staff-lock .tip-warning').visible
return self.q(css='.course-run>.value').text
return self.q(css='.new-library-button').present
self.q(css='.new-library-button').first.click() self.wait_for_ajax()
return self.q(css='.wrapper-create-library').visible
self.q(css='.wrapper-create-library .new-library-save').click()
return self.q(css='.new-course-button')
return self.q(css='.wrapper-create-course').visible
self.q(css='.new-course-button').first.click() self.wait_for_ajax()
self.q(css='.wrapper-create-course .new-course-save').first.click() self.wait_for_ajax()
return self.q(css='.wrapper-notification-error.is-shown')
self.wait_for_element_visibility( ".wrapper-notification-error.is-shown .message", "Error message is visible" ) return self.error_notification.results[0].find_element_by_css_selector('.message').text
return self.q(css='.wrapper-create-course #new-course-org')
for course in self.list_courses(): if course['org'] == org and course['number'] == number and course['run'] == run: return True return False
for lib in self.list_libraries(): if all([lib[key] == kwargs[key] for key in kwargs]): return True return False
self.wait_for_element_visibility( '#settings-language-value', 'Language selector element is available' ) return self.q(css='#settings-language-value')
return self.q(css='#course-index-tabs .programs-tab a').present
self.q(css='#course-index-tabs .programs-tab a').click() self.wait_for_element_visibility("div.programs-tab.active", "Switch to programs tab")
return self.q(css='.nav-actions a.new-program-button').present
self._click_programs_tab() return self.q(css='div.programs-tab.active a.new-program-button').present
self.q(css='nav.%s * .previous-page-link' % position)[0].click() self.wait_until_ready()
self.q(css='nav.%s * .next-page-link' % position)[0].click() self.wait_until_ready()
page_input = self.q(css="#page-number-input")[0] page_input.click() page_input.send_keys(str(number)) page_input.send_keys(Keys.RETURN) self.wait_until_ready()
return self.q(css="span.current-page")[0].get_attribute('innerHTML')
url_path = "tabs" def is_browser_on_page(self): return self.q(css='body.view-static-pages').present
return self.q(css=self.type_filter_element).present
return self.q(css='span.filter-column').visible
self.q(css=".filterable-column .nav-item").click()
super(BaseComponentEditorView, self).__init__(browser) self.locator = locator
return '{}[data-locator="{}"] {}'.format( self.BODY_SELECTOR, self.locator, selector )
return None
click_css(self, 'a.action-save')
click_css(self, 'a.action-cancel', require_notification=False)
elem = self.get_setting_element(label) select = Select(elem) select.select_by_value(value) self.save()
elem = self.get_setting_element(label) if elem: select = Select(elem) return select.first_selected_option.text else: return None
return self.q(css=self._bounded_selector(self.OPTION_SELECTOR)).results
results = [] for option in self.all_options: button = option.find_element_by_css_selector('input.input') if button.is_selected(): results.append(option) return results
url_path = "course_info" def is_browser_on_page(self): return self.q(css='body.view-updates').present
self.wait_for_ajax() self.wait_for_element_presence(MODAL_SELECTOR, 'Validation Modal is present')
self.browser.refresh() self.wait_for_page()
self.browser.execute_script("window.scrollTo" + str(self.coordinates_for_scrolling(UNDO_BUTTON_SELECTOR))) self.q(css=UNDO_BUTTON_SELECTOR).click() self.wait_for_ajax()
self.browser.execute_script("window.scrollTo" + str(self.coordinates_for_scrolling(MANUAL_BUTTON_SELECTOR))) self.q(css=MANUAL_BUTTON_SELECTOR).click()
return self.q(css=MODAL_SELECTOR).present
return self.q(css=ERROR_ITEM_NAME_SELECTOR).text
return self.q(css=ERROR_ITEM_CONTENT_SELECTOR).text
for key, value in key_value_map.iteritems(): index = self._get_index_of(key) type_in_codemirror(self, index, value) self.save()
result_map = {} for key in key_list: index = self._get_index_of(key) val = get_codemirror_value(self, index) result_map[key] = val return result_map
query = self.q(css=SETTINGS_NAME_SELECTOR) return query.attrs('id')
return "{}/container/{}".format(BASE_URL, self.locator)
EmptyPromise( lambda: self.q(css='div.add-xblock-component').present, 'Wait for the menu of components to be present' ).fulfill()
return self._get_xblocks()
return self._get_xblocks(".is-inactive ")
return self._get_xblocks(".is-active ")
return self.q(css='.pub-status').first.text[0]
return self.q(css='.wrapper-release .title').first.text[0]
return self.q(css='.wrapper-release .copy').first.text[0]
return self.q(css='.wrapper-last-draft').first.text[0]
return self.q(css='.wrapper-last-publish').first.text[0]
return self.q(css='.bit-publishing .wrapper-visibility .copy .inherited-from').visible
return self.q(css='.bit-publishing .wrapper-visibility').first.text[0]
return self.q(css='.action-publish').first
click_css(self, 'a.action-discard', 0, require_notification=False) confirm_prompt(self) self.wait_for_ajax()
for attr in self.q(css='a.action-staff-lock>i').attrs('class'): if 'fa-check-square-o' in attr: return True return False
self.q(css='.button-view').first.click() self._switch_to_lms()
self.q(css='.button-preview').first.click() self._switch_to_lms()
click_css(self, 'a.duplicate-button', source_index)
return _click_edit(self, '.edit-button', '.xblock-studio_view')
return self.q(css=self.ADD_MISSING_GROUPS_SELECTOR).present
return self.q(css=".xblock-message.information").first.text[0]
return "is-editing" in self.q(css=self.NAME_FIELD_WRAPPER_SELECTOR).first.attrs("class")[0]
self.q(css='.add-xblock-component-button[data-type={}]'.format(category_type)).first.click() return self.q(css='.{}-type-tabs>li>a'.format(category_type)).text
css = '#tab{tab_index} button[data-category={category_type}] span'.format( tab_index=tab_index, category_type=category_type ) return self.q(css=css).html
return '{}[data-locator="{}"] {}'.format( self.BODY_SELECTOR, self.locator, selector )
return self.q(css=self._bounded_selector('.xblock-student_view'))[0].text
return self.q(css=self._bounded_selector('.xblock-author_view'))[0].text
return self.q(css=self._bounded_selector(self.VALIDATION_SELECTOR)).present
return self.q(css=self._bounded_selector('{} p.{}'.format(self.VALIDATION_SELECTOR, css_class)))
return self._validation_paragraph('warning').present
return self._validation_paragraph('error').present
return self._validation_paragraph('not-configured').present
return self._validation_paragraph('warning').text[0]
return self._validation_paragraph('error').text[0]
return self._validation_paragraph('not-configured').text[0]
return self.q(css=self._bounded_selector('a.duplicate-button'))
return self.q(css=self._bounded_selector('a.delete-button'))
return self.q(css=self._bounded_selector('.visibility-button')).is_present()
return ContainerPage(self.browser, self.locator).visit()
return _click_edit(self, '.edit-button', '.xblock-studio_view', self._bounded_selector)
return _click_edit(self, '.visibility-button', '.xblock-visibility_view', self._bounded_selector)
self._click_button('advanced_tab')
self._click_button('basic_tab')
self._click_button('settings_tab')
type_in_codemirror(self, index, text, find_prefix='$("{}").find'.format(self.editor_selector))
self._click_button('save_settings')
self.q(css=self.COMPONENT_BUTTONS[button_name]).first.click() self.wait_for_ajax()
self.q(css=self._bounded_selector('span.message-text a')).first.click()
return not self.q(css=self._bounded_selector('.wrapper-xblock article')).present
return self.q(css=self._bounded_selector('span.message-text a')).first.text[0]
def is_browser_on_page(self): return self.q(css='body.view-subsection').present
super(CoursePage, self).__init__(browser) self.course_info = { 'course_org': course_org, 'course_num': course_num, 'course_run': course_run }
page.wait_for_component_menu() click_css(page, 'button>span.large-discussion-icon', menu_index)
set_input_value(page, css, value).send_keys(Keys.ENTER) page.wait_for_ajax()
url_path = "settings/grading" def is_browser_on_page(self): return self.q(css='body.grading').present
return "{}/library/{}".format(BASE_URL, unicode(self.locator))
return self.q(css='body.view-library').present
return self.q(css='h1.page-header-title').text
self.wait_for_ajax() super(LibraryEditPage, self).wait_until_ready()
return self._get_xblocks()
return all([not xblock.is_placeholder() for xblock in self.xblocks])
self._action_btn_for_xblock_id(xblock_id, "duplicate").click() wait_for_notification(self) self.wait_for_ajax()
return self.q(css='.wrapper-xblock.level-page .studio-xblock-wrapper').filter( lambda el: el.get_attribute('data-locator') == xblock_id )
return self._div_for_xblock_id(xblock_id)[0].find_element_by_css_selector( '.header-actions .{action}-button.action-button'.format(action=action) )
return self.get_selected_option_text(self.LIBRARY_LABEL)
self.set_select_value(self.LIBRARY_LABEL, library_name) EmptyPromise(lambda: self.library_name == library_name, "library_name is updated in modal.").fulfill()
return int(self.get_setting_element(self.COUNT_LABEL).get_attribute('value'))
self.set_select_value(self.SCORED_LABEL, str(scored)) EmptyPromise(lambda: self.scored == scored, "scored is updated in modal.").fulfill()
return self.get_setting_element(self.PROBLEM_TYPE_LABEL).get_attribute('value')
self.set_select_value(self.PROBLEM_TYPE_LABEL, value) EmptyPromise(lambda: self.capa_type == value, "problem type is updated in modal.").fulfill()
elem = self.get_setting_element(label) select = Select(elem) select.select_by_value(value)
return self.q(css='article.content-primary').visible
return not self.q(css='div.ui-loading').visible
return cls(xblock_wrapper.browser, xblock_wrapper.locator)
return self.q(css=CLASS_SELECTORS['video_controls']).visible
element = self.q(css='.setting-replace')[0] ActionChains(self.browser).move_to_element(element).click(element).perform()
self.q(css=BUTTON_SELECTORS[button_name]).nth(index).click() if require_notification: wait_for_notification(self) self.wait_for_ajax()
return os.sep.join(__file__.split(os.sep)[:-5]) + '/data/uploads/' + filename
self.upload_asset(handout_filename)
self.click_button('handout_clear')
return self.q(css=BUTTON_SELECTORS['handout_download']).visible
return len(self.q(css='.xblock-header').filter( lambda el: 'xblock-header-video' in el.get_attribute('class')).results)
caption_line_selector = ".subtitles li[data-index='{index}']".format(index=line_number - 1) self.q(css=caption_line_selector).results[0].send_keys(Keys.ENTER)
caption_line_selector = ".subtitles li[data-index='{index}']".format(index=line_number - 1) attributes = self.q(css=caption_line_selector).attrs('class') return 'focused' in attributes
return self.q(css=CLASS_SELECTORS['slider_range']).visible
_, setting = self._get_setting_entry(field_name) return self._verify_setting_entry(setting, field_name, field_value)
return len(self.q(css='.wrapper-translations-settings .list-settings-item').results)
language_codes = self.translations() index = language_codes.index(old_lang_code) self.select_translation_language(new_lang_code, index) self.upload_asset(transcript_name, asset_type='transcript', index=index)
translations_selector = '.metadata-video-translations .remove-setting' return self.q(css=translations_selector).attrs('data-lang')
self.q(css='.remove-action').filter(lambda el: language_code == el.get_attribute('data-lang')).click()
return self.q(css='#upload_error').text[0]
if message_type == 'status': self.wait_for_element_visibility(CLASS_SELECTORS[message_type], '{} message is Visible'.format(message_type.title())) return self.q(css=CLASS_SELECTORS[message_type]).text[0]
self.browser.execute_script(script) time.sleep(DELAY) self.wait_for_ajax()
_, setting = self._get_setting_entry(field_name) setting.find_element_by_class_name('setting-clear').click()
url = BASE_URL + "/auto_auth" query_str = urllib.urlencode(self._params) if query_str: url += "?" + query_str return url
super(StudioPagePerformanceTest, self).setUp() AutoAuthPage(self.browser, staff=True).visit()
self.record_visit_outline()
self.record_visit_unit( 'Lecture 1 - Doing the Right Thing', 'Discussion Prompt: Ethics of Torture', 'Discussion Prompt: Ethics of Torture' )
self.record_visit_outline()
courseware_page = CoursewarePage(self.browser, self.course_id) self._make_har_file(courseware_page)
dashboard_page = DashboardPage(self.browser) self._make_har_file(dashboard_page)
course_info_page = CourseInfoPage(self.browser, self.course_id) self._make_har_file(course_info_page)
pass
pass
self._api_base = api_base self._configuration = configuration
return {key: val for key, val in self.session.cookies.items()}
return { 'Content-type': 'application/json', 'Accept': 'application/json', 'X-CSRFToken': self.session_cookies.get('csrftoken', '') }
pass
pass
pass
return {key: val for key, val in self.session.cookies.items()}
return { 'Content-type': 'application/json', 'Accept': 'application/json', 'X-CSRFToken': self.session_cookies.get('csrftoken', '') }
pass
self.children.extend(args) return self
for desc in xblock_descriptions: loc = self.create_xblock(parent_loc, desc) self._create_xblock_children(loc, desc.children)
return json.dumps({ k: v.encode('utf-8') if isinstance(v, basestring) else v for k, v in post_dict.items() })
xblocks = self._get_nested_xblocks(self) if category: xblocks = [x for x in xblocks if x.category == category] return xblocks
xblocks = list(xblock_descriptor.children) for child in xblock_descriptor.children: xblocks.extend(self._get_nested_xblocks(child)) return xblocks
self.category = category self.display_name = display_name self.data = data self.metadata = metadata self.grader_type = grader_type self.publish = publish self.children = [] self.locator = None self.fields = kwargs
self.children.extend(args) return self
return "<CourseFixture: org='{org}', number='{number}', run='{run}'>".format(**self._course_dict)
self._updates.append(update)
self._handouts.append(asset_name)
self._assets.extend(asset_name)
self._textbooks.append({"chapters": chapters, "tab_title": book_title})
self._advanced_settings.update(settings)
self._configure_course()
return "/assets/" + self._course_key + "/"
course_key = CourseKey.from_string(self._course_key) return unicode(course_key.make_usage_key('course_info', 'handouts'))
pass
return "<LibraryFixture: org='{org}', number='{number}'>".format(**self.library_info)
self._create_library() self._create_xblock_children(self.library_location, self.children) return self
return self._library_key
lib_key = CourseKey.from_string(self._library_key) return unicode(lib_key.make_usage_key('library', 'library'))
requests.put( '{}/set_config'.format(COMMENTS_STUB_URL), data=self.get_config_data() )
raise NotImplementedError()
self.cohort_management_page.select_cohort(cohort_name) self.assertEquals(self.cohort_management_page.get_selected_cohort(), cohort_name) self.assertIn(expected_description, self.cohort_management_page.get_cohort_group_setup())
unique_username = 'user' + str(uuid.uuid4().hex)[:12] unique_email = unique_username + "@example.com" return unique_username, unique_email
self.browser.refresh() self.cohort_management_page.wait_for_page() self.instructor_dashboard_page.select_cohort_management() self.cohort_management_page.wait_for_page() self.cohort_discussion_topics_are_visible()
self.reload_page() self.assertEqual(self.cohort_management_page.get_cohorted_topics_count(key), cohorted_topics)
self.cohort_management_page.select_cohort(cohort_name) self.cohort_management_page.set_cohort_associated_content_group(content_group) self._verify_settings_saved_and_reload(cohort_name)
pass
self.setup_cohort_config(self.course_fixture) self.cohort_1_name = "Cohort 1" self.cohort_1_id = self.add_manual_cohort(self.course_fixture, self.cohort_1_name)
self.assertIsNotNone(self.page.new_post_button) self.page.click_new_post_button() self.assertIsNotNone(self.page.new_post_form)
selected_sort = self.sort_page.get_selected_sort_preference() self.assertEqual(selected_sort, "activity")
super(DashboardSearchTest, self).tearDown() os.remove(self.TEST_INDEX_FILENAME)
LogoutPage(self.browser).visit() AutoAuthPage(self.browser, username=username, email=email, staff=staff).visit()
course_outline.visit() subsection = course_outline.section_at(0).subsection_at(0) subsection.expand_subsection() unit = subsection.unit_at(0) unit.publish()
self._auto_auth(self.USERNAME, self.EMAIL, False) self.dashboard.visit()
return [{u"description": i, u"name": i, u"id": i} for i in map(str, xrange(num_topics))]
self.assertEqual(expected_team['name'], team_card_name) self.assertEqual(expected_team['description'], team_card_description)
return event['event_type'].startswith('edx.team.')
self.set_team_configuration(None) self.verify_teams_present(False)
self.set_team_configuration({u"max_team_size": 10, u"topics": []}) self.verify_teams_present(False)
self.assertEqual(self.browse_teams_page.header_name, self.topic['name']) self.assertEqual(self.browse_teams_page.header_description, self.topic['description'])
self.assertEqual(search_results_page.header_name, 'Team Search') self.assertEqual( search_results_page.header_description, 'Showing results for "{search_query}"'.format(search_query=search_query) )
self.assertEqual(self.team_management_page.header_page_name, title) self.assertEqual(self.team_management_page.header_page_description, description) self.assertEqual(self.team_management_page.header_page_breadcrumbs, breadcrumbs)
AutoAuthPage(self.browser, course_id=self.course_id).visit() self.team_page.visit() self.assertFalse(self.team_page.edit_team_button_present)
self.logout_page.visit() AutoAuthPage(self.browser, username=username, email=email, course_id=self.course_id, staff=staff).visit()
return frozenset(child.data for child in self.library_fixture.children)
AutoAuthPage(self.browser, username=username, email=email, course_id=self.course_id, staff=staff).visit()
self._create_search_index() super(StudioLibraryContainerCapaFilterTest, self).setUp()
return frozenset(child.display_name for child in self.library_fixture.children)
self._change_library_content_settings(count=count, capa_type=capa_type) self._auto_auth(self.USERNAME, self.EMAIL, False) self._goto_library_block_page() return self.library_content_page.children_headers
course_listings = self.dashboard_page.get_course_listings() self.assertEqual(len(course_listings), 1)
LogoutPage(self.browser).visit() StudioAutoAuthPage(self.browser, username=username, email=email, course_id=self.course_id, staff=staff).visit()
self._auto_auth(self.USERNAME, self.EMAIL, False) self.courseware_search_page.visit()
raise NotImplementedError()
AutoAuthPage(self.browser, username=username, email=email, course_id=self.course_id, staff=True).visit()
self.course_info_page.visit() self.tab_nav.go_to_tab('Course') acid_block = AcidView(self.browser, '.xblock-student_view[data-block-type=acid]') self.validate_acid_block_view(acid_block)
AutoAuthPage(self.browser).visit()
return dict(parse_qsl(urlparse(url).query))
query = self._qs(self.browser.current_url) return 'access_denied' in query['error']
query = self._qs(self.browser.current_url) return 'code' in query
self.courseware_page.visit() staff_page = StaffPage(self.browser, self.course_id) self.assertEqual(staff_page.staff_view_mode, 'Staff') return staff_page
return event['event_type'] == self.USER_SETTINGS_CHANGED_EVENT_NAME
return event['event_type'] == self.CHANGE_INITIATED_EVENT_NAME
return self.relative_path_to_absolute_uri(self.ACCOUNT_SETTINGS_REFERER)
self.assert_no_matching_events_were_emitted({'event_type': self.USER_SETTINGS_CHANGED_EVENT_NAME})
super(AccountSettingsPageTest, self).setUp() self.full_name = XSS_INJECTION self.username, self.user_id = self.log_in_as_unique_user(full_name=self.full_name) self.visit_account_settings_page()
self.assertEqual(self.account_settings_page.title_for_field(field_id), title) self.assertEqual(self.account_settings_page.value_for_readonly_field(field_id), value)
self._test_readonly_field('username', 'Username', self.username)
self._test_dropdown_field( u'pref-lang', u'Language', u'English', [u'Dummy Language (Esperanto)', u'English'], reloads_on_save=True, )
self._test_dropdown_field( u'country', u'Country or Region', u'', [u'Pakistan', u'Palau'], )
super(ForgotPasswordPageTest, self).setUp() self.user_info = self._create_user() self.reset_password_page = ResetPasswordPage(self.browser)
auto_auth = AutoAuthPage(self.browser).visit() user_info = auto_auth.user_info LogoutPage(self.browser).visit() return user_info
return event['event_type'].startswith('edx.course.enrollment.')
self.course_info_page.visit() self.tab_nav.go_to_tab('Course') self.courseware_page.verify_tooltips_displayed()
select_option_by_value(self.settings_page.pre_requisite_course_options, self.pre_requisite_course_id) self.settings_page.save_changes()
page = DashboardPage(self.browser) page.wait_for_page() self.assertIn( 'The course you are looking for is closed for enrollment', page.banner_text )
AutoAuthPage(self.browser).visit() url = BASE_URL + "/course_modes/choose/" + self.course_id self.browser.get(url) self._assert_dashboard_message()
instructor_dashboard_page = InstructorDashboardPage(self.browser, self.course_id) instructor_dashboard_page.visit() return instructor_dashboard_page
self.auto_enroll_section.upload_correct_csv_file() self.assertTrue(self.auto_enroll_section.is_notification_displayed(section_type=self.auto_enroll_section.NOTIFICATION_SUCCESS))
AutoAuthPage(self.browser, username=username, email=email, course_id=self.course_id, staff=staff).visit()
self.assert_matching_events_were_emitted( event_filter={'name': u'edx.instructor.report.requested', 'report_type': report_type} )
self.assert_matching_events_were_emitted( event_filter={'name': u'edx.instructor.report.downloaded', 'report_url': report_url} )
report_name = u"ORA_data" self.data_download_section.generate_ora2_response_report_button.click() self.data_download_section.wait_for_available_report() self.verify_report_download(report_name)
self.assertTrue(self.certificates_section.pending_tasks_section.visible)
self.assertTrue(self.certificates_section.certificate_exceptions_section.visible)
AutoAuthPage( self.browser, username="testcert", email="cert@example.com", password="testuser", course_id=self.course_id ).visit()
AutoAuthPage( self.browser, username="testprogress", email="progress@example.com", password="testuser", course_id=self.course_id ).visit()
LogoutPage(self.browser).visit() StudioAutoAuthPage(self.browser, username=username, email=email, course_id=self.course_id, staff=staff).visit()
self.courseware_search_page.visit() staff_page = StaffPage(self.browser, self.course_id) self.assertEqual(staff_page.staff_view_mode, 'Staff') return staff_page
self._auto_auth(self.cohort_default_student_username, self.cohort_default_student_email, False) self.courseware_search_page.visit()
CourseFixture(**self.course_info).install() course_id = self.course_id if enroll else None AutoAuthPage(self.browser, course_id=course_id).visit()
self.stub_api() self.auth(enroll=False) self.listing_page.visit() self.assertTrue(self.listing_page.is_sidebar_present) self.assertFalse(self.listing_page.are_cards_present)
self.stub_api() self.auth() self.listing_page.visit() self.assertTrue(self.listing_page.is_sidebar_present) self.assertTrue(self.listing_page.are_cards_present)
return ['Test Section {}'.format(index), 'Test Subsection {}'.format(index), 'Test Problem {}'.format(index)]
AutoAuthPage(self.browser, username=username, email=email, course_id=self.course_id, staff=staff).visit()
AutoAuthPage(self.browser, username=username, email=email, course_id=self.course_id, staff=staff).visit()
self.courseware_page.go_to_sequential_position(position) self.problem_page.wait_for_element_presence( self.problem_page.CSS_PROBLEM_HEADER, 'wait for problem header' ) self.assertEqual(self.problem_page.problem_name, problem_name)
username = "test_{uuid}".format(uuid=self.unique_id[0:6]) auto_auth_page = AutoAuthPage(self.browser, username=username).visit() user_id = auto_auth_page.get_user_id() return username, user_id
profile_page.value_for_dropdown_field('language_proficiencies', 'English') profile_page.value_for_dropdown_field('country', 'United Arab Emirates') profile_page.set_value_for_textarea_field('bio', 'Nothing Special')
if is_editable: self.assertTrue(profile_page.privacy_field_visible) self.assertEqual(profile_page.visible_fields, self.PRIVATE_PROFILE_FIELDS)
username, user_id = self.log_in_as_unique_user() profile_page = self.visit_profile_page(username) self.verify_profile_page_is_public(profile_page)
self.assertTrue(profile_page.profile_has_default_image) self.assertTrue(profile_page.profile_has_image_with_public_access())
),
),
),
self.assertEqual(item.title, title) self.assertEqual(item.subtitles, subtitles)
self.assertEqual(item.title, title) self.assertEqual(item.notes, notes)
LogoutPage(self.browser).visit() AutoAuthPage(self.browser, username=username, email=email, course_id=self.course_id, staff=staff).visit()
self._auto_auth(self.STAFF_USERNAME, self.STAFF_EMAIL, True) self.course_outline.visit() section = self.course_outline.section_at(section_index) section.change_name(self.EDITED_CHAPTER_NAME)
self._auto_auth(self.STAFF_USERNAME, self.STAFF_EMAIL, True) self.course_outline.visit() self.course_outline.start_reindex() self.course_outline.wait_for_ajax()
self._auto_auth(self.USERNAME, self.EMAIL, False) self.courseware_search_page.visit() self.courseware_search_page.search_for_term(search_term) return search_term in self.courseware_search_page.search_results.html[0]
self._auto_auth(self.USERNAME, self.EMAIL, False) self.courseware_search_page.visit()
super(ProblemTypeTestBase, self).setUp() self.courseware_page.visit() self.problem_page = ProblemPage(self.browser)
msg = "Wait for status to be {}".format(status) selector = ', '.join(self.status_indicators[status]) self.problem_page.wait_for_element_visibility(selector, msg)
raise NotImplementedError()
if correct: self.problem_page.click_choice("choice_0") self.problem_page.click_choice("choice_2") else: self.problem_page.click_choice("choice_1")
if correct: self.problem_page.click_choice("choice_choice_2") else: self.problem_page.click_choice("choice_choice_1")
if correct: self.problem_page.click_choice("choice_2") else: self.problem_page.click_choice("choice_1")
answer = 'Option 2' if correct else 'Option 3' selector_element = self.problem_page.q( css='.problem .option-input select') select_option_by_text(selector_element, answer)
textvalue = 'correct string' if correct else 'incorrect string' self.problem_page.fill_answer(textvalue)
textvalue = "pi + 1" if correct else str(random.randint(-2, 2)) self.problem_page.fill_answer(textvalue)
textvalue = "x^2+2*x+y" if correct else 'x^2' self.problem_page.fill_answer(textvalue)
pass
pass
pass
self.problem_page.q( css='div.problem input.ctinput[type="{}"]'.format(self.choice_type) ).nth(input_num).click()
self.problem_page.q( css='div.problem input.ctinput[type="text"]' ).nth(input_num).fill(value)
choice = 0 if correct else 1 input_value = "8" if correct else "5" self._select_choice(choice) self._fill_input_text(input_value, choice)
raise NotImplementedError()
LogoutPage(self.browser).visit() AutoAuthPage(self.browser, username=username, email=email, staff=staff).visit()
self.page.visit()
actual_events = self.wait_for_events(event_filter={'event_type': event_type}, number_of_matches=1) self.assert_events_match(event_data, actual_events)
xblocks = self.course_fixture.get_nested_xblocks(category="vertical") for index in range(num_units): self._bookmark_unit(xblocks[index].locator)
self.bookmarks_page.click_bookmarks_button() self.assertTrue(self.bookmarks_page.results_present()) self.assertEqual(self.bookmarks_page.results_header_text(), 'My Bookmarks')
self.courseware_page.visit() annotation_component_page = AnnotationComponentPage(self.browser) self.assertEqual( annotation_component_page.component_name, 'Test Annotation Module'.format() ) return annotation_component_page
full_path = path(__file__).abspath().dirname() / "data" / rel_path with open(full_path) as data_file: return data_file.read()
if os.path.exists(filename): os.remove(filename)
disable_jquery_animations(page) disable_css_animations(page)
enable_jquery_animations(page) enable_css_animations(page)
page.browser.execute_script("jQuery.fx.off = true;")
page.browser.execute_script("jQuery.fx.off = false;")
try: select = Select(query.first.results[0]) select.select_by_visible_text(value) return True except StaleElementReferenceException: return False
try: select = Select(query.first.results[0]) return (True, select.first_selected_option.text) except StaleElementReferenceException: return (False, None)
return Select(select_browser_query.first.results[0]).options
default_store = os.environ.get('DEFAULT_STORE', 'draft') return CourseLocator(org, number, run, deprecated=(default_store == 'draft'))
select = Select(browser_query.first.results[0]) ddl_selected_value = select.first_selected_option.get_attribute('value') return ddl_selected_value == value
text_present = False text_list = page.q(css=css_selector).text if len(text_list) > 0 and (text in text_list): text_present = True return text_present
WebDriverWait(browser, 6).until(EC.alert_is_present()) return browser.switch_to.alert
return 'Page not found (404)' in browser.find_element_by_tag_name('h1').text
self.event_collection.drop() self.start_time = datetime.now()
return self.matching_events_were_emitted( start_time=start_time, event_filter=event_filter, number_of_matches=number_of_matches )
matching_events = self.get_matching_events_from_time(start_time=start_time, event_filter=event_filter) return len(matching_events) >= number_of_matches, matching_events
return urlparse.urljoin(BASE_URL, relative_path)
super(UniqueCourseTest, self).__init__(*args, **kwargs)
pass
youtube_stub_config_url = cls.URL + 'get_config' response = requests.get(youtube_stub_config_url) if response.ok: return json.loads(response.content) else: return {}
return UserPartition( partition_id, name, description, groups, MockUserPartitionScheme(scheme) ).to_json()
with open(self.TEST_INDEX_FILENAME, "w+") as index_file: json.dump({}, index_file)
self.courseware_page.visit() csh_problem_page = CrowdsourcehinterProblemPage(self.browser) self.assertGreater(len(self.browser.find_elements_by_class_name('crowdsourcehinter_block')), 0) return csh_problem_page
current_time = json.loads(video_event['event'])['currentTime'] self.assertAlmostEqual(current_time, time_in_seconds, delta=1)
return event['event_type'] in ('load_video', 'play_video', 'pause_video')
self.video.wait_for_position('0:05') self.video.click_player_button('skip_bumper')
self.video.wait_for_position('0:05') self.video.click_player_button('do_not_show_again')
self.video.wait_for_state(state)
additional_data = { u'video_bumper': { u'value': { "transcripts": {}, "video_id": "video_001" } } } self.course_fixture.add_advanced_settings(additional_data)
self.navigate_to_course_unit() self.edit_component() self.open_advanced_tab() self.video.upload_handout(handout_filename) if save_settings: self.save_unit_settings()
self._create_course_unit_with_handout('textbook.pdf', save_settings=False) self.assertEqual(self.video.download_handout('application/pdf', is_editor=True), (True, True))
self._create_course_unit_with_handout('asset.html', save_settings=False) self.video.clear_handout() self.save_unit_settings() self.assertFalse(self.video.is_handout_button_visible)
if subtitles: self.assets.append('subs_3_yD_cEKoCk.srt.sjson') self.navigate_to_course_unit()
self._create_video_component() self.edit_component() self.assertTrue(self.video.verify_settings())
self._install_course_fixture() self._navigate_to_courseware_video_and_render()
self._install_course_fixture() self._navigate_to_courseware_video_no_render()
self.auth_page.visit() self.user_info = self.auth_page.user_info self.course_info_page.visit() self.tab_nav.go_to_tab('Course')
self._navigate_to_courseware_video() self.video.wait_for_video_player_render()
self._navigate_to_courseware_video() self.video.wait_for_video_class()
self.courseware.go_to_sequential_position(position) self.video.wait_for_video_player_render()
self.navigate_to_video() self.assertFalse(self.video.is_button_shown('transcript_button'))
self.video.wait_for( lambda: (text in self.video.closed_captions_text), u'Closed captions contain "{}" text'.format(text), timeout=5 )
if subtitles: self.assets.append('subs_{}.srt.sjson'.format(subtitle_id)) self.navigate_to_course_unit()
if youtube_stub_config: YouTubeStubConfig.configure(youtube_stub_config) if subtitles: self.assets.append('subs_3_yD_cEKoCk.srt.sjson') self.navigate_to_course_unit()
self._install_course_fixture() self._navigate_to_course_unit_page()
self.unit_page.xblocks[xblock_index].edit()
self._create_course_unit(youtube_stub_config={'youtube_api_blocked': True}) self.assertFalse(self.video.is_button_shown('play'))
self._create_course_unit() self.assertFalse(self.video.is_autoplay_enabled)
self._create_course_unit(subtitles=True) self.video.hide_captions() self.assertFalse(self.video.is_captions_visible())
self._create_course_unit(subtitles=True) self.assertTrue(self.video.is_captions_visible())
self._create_course_unit(subtitles=True) self.video.show_captions() self.video.focus_caption_line(2) self.assertTrue(self.video.is_caption_line_focused(2))
for page in self.pages: page.visit()
self.auth_page.visit() self.dashboard_page.visit()
unit = self.go_to_unit_page() component = unit.xblocks[1] self.modify_display_name_and_verify(component)
container = self.go_to_nested_container_page() self.modify_display_name_and_verify(container)
component.edit_visibility() return ComponentVisibilityEditorView(self.browser, component.locator)
self.assertTrue(component.has_validation_error) self.assertEqual(component.validation_error_text, self.VALIDATION_ERROR_LABEL) self.assertEqual([self.VALIDATION_ERROR_MESSAGE], component.validation_error_messages)
self.course_fixture._update_xblock(component.locator, {'metadata': metadata}) self.browser.refresh() self.container_page.wait_for_page()
unit.view_published_version() self.assertEqual(len(self.browser.window_handles), 2) self.courseware.wait_for_page()
page = StaffPage(self.browser, self.course_id) EmptyPromise(page.is_browser_on_page, 'Browser is on staff page in LMS').fulfill() return page
self._verify_and_return_staff_page().set_staff_view_mode('Student') self.assertEqual(0, self.courseware.num_xblock_components)
self._verify_and_return_staff_page().set_staff_view_mode('Student') self._verify_components_visible(expected_components)
self.assertEqual(expected_title, unit.release_title) self.assertEqual(expected_date, unit.release_date)
self.assertTrue(expected_published_prefix in unit.last_published_text) self.assertTrue(expected_saved_prefix in unit.last_saved_text)
self.set_programs_api_configuration() self.dashboard_page.visit() self.assertFalse(self.dashboard_page.is_programs_tab_present()) self.assertFalse(self.dashboard_page.is_new_program_button_present())
self._cleanup_index_file() super(StudioLibraryContainerTest, self).tearDown()
library_fixture.add_children( XBlockFixtureDesc("html", "Html1"), XBlockFixtureDesc("html", "Html2"), XBlockFixtureDesc("html", "Html3"), )
return StudioLibraryContainerXBlockWrapper.from_xblock_wrapper(xblock)
good_status, is_tarball_mimetype = self.export_page.download_tarball() self.assertTrue(good_status) self.assertTrue(is_tarball_mimetype)
self.assertEqual(self.export_page.header_text, 'Course Export')
super(TestLibraryExport, self).setUp() self.export_page = ExportLibraryPage(self.browser, self.library_key) self.export_page.visit()
self.assertEqual(self.export_page.header_text, 'Library Export')
library_fixture.add_children( XBlockFixtureDesc("problem", "Bad Problem", data='<'), )
return []
self.import_page.upload_tarball(self.tarball_name) self.assertEqual(self.import_page.finished_target_url(), self.landing_page.url)
self.import_page.upload_tarball('funny_cat_video.mp4') self.import_page.wait_for_filename_error()
self.assertEqual(self.import_page.header_text, 'Course Import')
NEVER_PUBLISHED = 1 UNPUBLISHED_CHANGES = 2 PUBLISHED = 3 VALUES = [NEVER_PUBLISHED, UNPUBLISHED_CHANGES, PUBLISHED]
self.is_released = is_released self.publish_state = publish_state self.is_locked = is_locked
self._verify_unit_warning( self.UnitState(is_released=True, publish_state=self.PublishState.NEVER_PUBLISHED, is_locked=True), self.STAFF_ONLY_WARNING )
self._verify_unit_warning( self.UnitState(is_released=True, publish_state=self.PublishState.NEVER_PUBLISHED, is_locked=False), self.NEVER_PUBLISHED_WARNING )
self._verify_unit_warning( self.UnitState(is_released=True, publish_state=self.PublishState.UNPUBLISHED_CHANGES, is_locked=True), self.STAFF_ONLY_WARNING )
self._verify_unit_warning( self.UnitState(is_released=True, publish_state=self.PublishState.UNPUBLISHED_CHANGES, is_locked=False), self.LIVE_UNPUBLISHED_WARNING )
self._verify_unit_warning( self.UnitState(is_released=True, publish_state=self.PublishState.PUBLISHED, is_locked=True), self.STAFF_ONLY_WARNING )
self._verify_unit_warning( self.UnitState(is_released=True, publish_state=self.PublishState.PUBLISHED, is_locked=False), None )
self._verify_unit_warning( self.UnitState(is_released=False, publish_state=self.PublishState.NEVER_PUBLISHED, is_locked=True), self.STAFF_ONLY_WARNING )
self._verify_unit_warning( self.UnitState(is_released=False, publish_state=self.PublishState.NEVER_PUBLISHED, is_locked=False), self.NEVER_PUBLISHED_WARNING )
self._verify_unit_warning( self.UnitState(is_released=False, publish_state=self.PublishState.UNPUBLISHED_CHANGES, is_locked=True), self.STAFF_ONLY_WARNING )
self._verify_unit_warning( self.UnitState(is_released=False, publish_state=self.PublishState.UNPUBLISHED_CHANGES, is_locked=False), self.FUTURE_UNPUBLISHED_WARNING )
self._verify_unit_warning( self.UnitState(is_released=False, publish_state=self.PublishState.PUBLISHED, is_locked=True), self.STAFF_ONLY_WARNING )
self._verify_unit_warning( self.UnitState(is_released=False, publish_state=self.PublishState.PUBLISHED, is_locked=False), None )
self.assertTrue(item.is_staff_only) if hasattr(item, 'children'): for child in item.children(): self._verify_descendants_are_staff_only(child)
self.course_outline_page.visit() section = self.course_outline_page.section_at(0) section.subsection_at(0).set_staff_lock(True) self.assertFalse(section.has_staff_lock_warning)
self.assertEqual(item.name, old_name) item.change_name(new_name) self.assertFalse(item.in_editable_form()) self.assertEqual(item.name, expected_name)
pass
for section in self.course_outline_page.sections(): self.assertEqual(collapsed, section.is_collapsed)
for section in self.course_outline_page.sections(): section.expand_subsection()
self.course_outline_page.visit() self.assertEquals(self.course_outline_page.expand_collapse_link_state, ExpandCollapseLinkState.COLLAPSE) self.verify_all_sections(collapsed=False)
pass
self.course_outline_page.visit() self.assertEquals(self.course_outline_page.expand_collapse_link_state, ExpandCollapseLinkState.MISSING)
pass
section = self.course_outline_page.section(SECTION_NAME) subsection = section.subsection(SUBSECTION_NAME) unit = subsection.expand_subsection().unit(UNIT_NAME) return (section, subsection, unit)
self.advanced_settings.visit() self.advanced_settings.set_values({"Advanced Module List": json.dumps(block_types)})
self.course_outline_page.visit() self.assertFalse(self.course_outline_page.deprecated_warning_visible)
self.page.visit() self.page.wait_until_no_loading_indicator()
self.page = CourseTeamPage( self.browser, self.course_info['org'], self.course_info['number'], self.course_info['run'] ) self._go_to_course_team_page()
return ( course1['org'] == course2['display_organization'] and course1['number'] == course2['display_coursenumber'] and course1['run'] == course2['run'] )
self.page.modal_dialog_visible(dialog_type) self.assertIn(dialog_message, self.page.modal_dialog_text(dialog_type))
self.group_configurations_page.visit() self.assertFalse(self.group_configurations_page.experiment_group_sections_present)
select_element = page.q(css=selector) self.assertTrue(select_element.is_present()) return [option.text for option in Select(select_element[0]).options]
message = self.textbook_page.get_element_text('.wrapper-content .no-textbook-content') self.assertIn("You haven't added any textbooks", message)
super(LibraryEditPageTest, self).setUp() self.lib_page = LibraryEditPage(self.browser, self.library_key) self.lib_page.visit() self.lib_page.wait_until_ready()
super(LibraryNavigationTest, self).setUp() self.lib_page = LibraryEditPage(self.browser, self.library_key) self.lib_page.visit() self.lib_page.wait_until_ready()
self.blocks = [XBlockFixtureDesc('html', str(i)) for i in xrange(1, 41)] library_fixture.add_children(*self.blocks)
self.page = LibraryUsersPage(self.browser, self.library_key) self.page.visit()
super(AdvancedProblemComponentTest, self).setUp(is_staff=is_staff)
super(StudioCourseTest, self).setUp() self.test_xss = test_xss self.install_course_fixture(is_staff)
pass
self.outline.visit() subsection = self.outline.section(section_name).subsection(subsection_name) return subsection.expand_subsection().unit(unit_name).go_to()
pass
super(CourseRerunTest, self).setUp(is_staff=True) self.dashboard_page = DashboardPage(self.browser)
pass
unit = self.go_to_unit_page() verify_ordering(self, unit, [{"": ["Unit HTML", "Unit Problem"]}])
self.assertTrue(self.settings_detail.entrance_exam_field)
self.assertEqual(self.settings_detail.course_pacing, 'Instructor-Paced')
self.settings_detail.course_pacing = 'Self-Paced' self.settings_detail.save_changes() self.settings_detail.refresh_page() self.assertEqual(self.settings_detail.course_pacing, 'Self-Paced')
pass
pass
return self.scope_ids.usage_id.course_key.org
return self.scope_ids.usage_id.course_key.library
return ''.join(traceback.format_exception(*exc_info))
if not isinstance(validation, Validation): raise TypeError("Copy must be called with a Validation instance") studio_validation = cls(validation.xblock_id) studio_validation.messages = validation.messages return studio_validation
super(StudioValidation, self).__init__(xblock_id) self.summary = None
if not isinstance(message, ValidationMessage): raise TypeError("Argument must of type ValidationMessage") self.summary = message
return super(StudioValidation, self).empty and not self.summary
self.fed.append(data)
self.fed.append('&%s;' % name)
return ''.join(self.fed)
raise NotImplementedError("Specific Modulestores must implement get_definition_id")
return def_id.block_type
return aside_id.usage_key
return aside_id.definition_key
return aside_id.aside_type
return aside_id.aside_type
def_key = AsideDefinitionKeyV1(definition_id, aside_type) usage_key = AsideUsageKeyV1(usage_id, aside_type) return (def_key, usage_key)
raise NotImplementedError("Specific Modulestores must provide implementations of create_usage")
raise NotImplementedError("Specific Modulestores must provide implementations of create_definition")
raise NotImplementedError( "get_html() must be provided by specific modules - not present in {0}" .format(self.__class__))
if not fragment.js_init_fn: fragment.initialize_js('XBlockToXModuleShim') fragment.json_init_args = {'xmodule-type': block.js_module_name}
return self.runtime
return course_metadata_utils.display_name_with_default(self)
return self.display_name_with_default
return self._field_data
self._asides.append(aside)
return self._asides
self._source = source self._name = name
return Fragment(self.get_html())
return u'{cat}/{name}'.format(cat=location.category, name=location.name)
return Fragment(self.get_html())
raise NotImplementedError("Applications must monkey-patch this function before using local_resource_url for studio_view")
return self.load_item(usage_id, for_parent=for_parent)
if block_type in self.disabled_xblock_types: return self.default_class return super(DescriptorSystem, self).load_block_type(block_type)
raise NotImplementedError("edX Platform doesn't currently implement XBlock resource urls")
super(XMLParsingSystem, self).__init__(**kwargs) self.process_xml = process_xml
if isinstance(value, UsageKey): return value return course_key.make_usage_key_from_deprecated_string(value)
return self.__dict__.get(attr)
self.__dict__[attr] = val
assert self.xmodule_instance is not None return self.handler_url(self.xmodule_instance, 'xmodule_handler', '', '').rstrip('/?')
if self._module_system: delattr(self._module_system, name) delattr(self._descriptor_system, name)
def get(self, _key): return None def set(self, key, value, timeout=None): pass
return location.name
return datetime.now(UTC()) > end_date if end_date is not None else False
return advertised_start is None and start == DEFAULT_START_DATE
data = String(scope=Scope.content, default='')
css = {'scss': [resource_string(__name__, 'css/codemirror/codemirror.scss')]} js = {'coffee': [resource_string(__name__, 'js/src/raw/edit/xml.coffee')]} js_module_name = "XMLEditingDescriptor"
js = {'coffee': [resource_string(__name__, 'js/src/raw/edit/metadata-only.coffee')]} js_module_name = "MetadataOnlyEditingDescriptor" mako_template = "widgets/metadata-only-edit.html"
css = {'scss': [resource_string(__name__, 'css/codemirror/codemirror.scss')]} js = {'coffee': [resource_string(__name__, 'js/src/raw/edit/json.coffee')]} js_module_name = "JSONEditingDescriptor"
self.save() self.runtime.modulestore.update_item(self, user.id)
if youtube_id: return 'https://www.youtube.com/watch?v={0}'.format(youtube_id) else: return ''
return self.runtime.service(self, "request_cache")
return edxval_api.get_video_info_for_course_and_profiles(unicode(course_id), video_profile_names)
content_location = Transcript.asset_location(location, name) content = StaticContent(content_location, name, mime_type, content) contentstore().save(content) return content_location
filedata = json.dumps(subs, indent=2) filename = subs_filename(subs_id, language) return save_to_store(filedata, filename, 'application/json', item.location)
filename = subs_filename(subs_id, lang) Transcript.delete_asset(item.location, filename)
html5_ids = [x.split('/')[-1].rsplit('.', 1)[0] for x in html5_sources] return html5_ids
if lang == 'en': return u'subs_{0}.srt.sjson'.format(subs_id) else: return u'{0}_subs_{1}.srt.sjson'.format(lang, subs_id)
asset_filename = subs_filename(subs_id, lang) if not filename else filename return Transcript.get_asset(location, asset_filename)
return contentstore().find(Transcript.asset_location(location, filename))
return StaticContent.compute_location(location.course_key, filename)
raise NotImplementedError()
try: return self[key] except KeyError: return default
return not self == other
return key_checker(['type'])(tab_dict, raise_error)
json_dict = kwargs.copy() json_dict['type'] = type_name return cls.from_json(json_dict)
to_json_val = {'type': self.type, 'name': self.name} if self.is_hidden: to_json_val.update({'is_hidden': True}) return to_json_val
return reverse_func(self.type, args=[course.id.to_deprecated_string(), self.url_slug])
return True
return (super(StaticTab, cls).validate(tab_dict, raise_error) and key_checker(['name', 'url_slug'])(tab_dict, raise_error))
to_json_val = super(StaticTab, self).to_json() to_json_val.update({'url_slug': self.url_slug}) return to_json_val
return next((tab for tab in tab_list if tab.get('url_slug') == url_slug), None)
return next((tab for tab in tab_list if tab.type == tab_type), None)
return next((tab for tab in tab_list if tab.tab_id == tab_id), None)
return lambda course, reverse_url_func: reverse_url_func(reverse_name, args=[course.id.to_deprecated_string()])
return Fragment(self.get_html())
return 'latex' not in template['template_id'] or course.use_latex_compiler
non_editable_fields = super(HtmlDescriptor, self).non_editable_metadata_fields non_editable_fields.append(HtmlDescriptor.use_latex_compiler) return non_editable_fields
pass
template_dir_name = "about" module_class = AboutModule
pass
template_dir_name = None module_class = StaticTabModule
return Fragment(self.get_html())
raise NotImplementedError('Subclasses must implement course_partition')
for partition in self.course_partitions: if partition.id == user_partition_id: return partition return None
pass
pass
pass
return { "id": self.id, "name": self.name, "version": Group.VERSION }
if self.current_group: return self.current_group groups = user_partition.groups if not groups or len(groups) == 0: return None return groups[0]
UserPartition.scheme_extensions = None
def __init__(self, partitions, **kwargs): super(StaticPartitionService, self).__init__(**kwargs) self._partitions = partitions @property def course_partitions(self): return self._partitions
return AUTHOR_VIEW if has_author_view(block) else STUDENT_VIEW
author_view = module_attr(AUTHOR_VIEW) has_author_view = True
partitions_service = self.runtime.service(self, 'partitions') if not partitions_service: return None return partitions_service.get_user_group_id_for_partition(self.user_partition_id)
return self.descriptor.is_configured
return True
for user_partition in self.user_partitions: if user_partition.id == self.user_partition_id: return user_partition return None
return not self.user_partition_id == SplitTestFields.no_partition_selected['value']
try: result = super(StringOrDate, self).from_json(value) except ValueError: return value if result is None: return value else: return result
try: result = super(StringOrDate, self).to_json(value) except: return value if result is None: return value else: return result
return course_metadata_utils.has_course_ended(self.end)
return course_metadata_utils.may_certify_for_course( self.certificates_display_behavior, self.certificates_show_before_end, self.has_ended() )
config = self.cohort_config if config is None: return False return bool(config.get("cohorted"))
if not self.is_cohorted: return False return bool(self.cohort_config.get( "auto_cohort", False))
if self.cohort_config is None: return [] else: return self.cohort_config.get("auto_cohort_groups", [])
topics = self.discussion_topics return [d["id"] for d in topics.values()]
return usage_key.block_type in self.block_types_affecting_grading
return self.location.course_key
i18n = self.runtime.service(self, "i18n") return course_metadata_utils.course_start_datetime_text( self.start, self.advertised_start, format_string, i18n.ugettext, i18n.strftime )
return course_metadata_utils.course_start_date_is_default( self.start, self.advertised_start )
return course_metadata_utils.course_end_datetime_text( self.end, format_string, self.runtime.service(self, "i18n").strftime )
if self.display_coursenumber: return self.display_coursenumber return self.number
if self.display_organization: return self.display_organization return self.org
return ( self.video_upload_pipeline is not None and 'course_video_upload_token' in self.video_upload_pipeline )
return course_metadata_utils.clean_course_key(self.location.course_key, padding_char)
if self.teams_configuration: return len(self.teams_configuration.get('topics', [])) > 0 return False
return self.teams_configuration.get('max_team_size', None)
return self.teams_configuration.get('topics', None)
return [ p for p in self.user_partitions if p.scheme == scheme ]
return datetime.now(UTC()) <= self.start
if self.display_organization: return self.display_organization return self.location.org
child_classes = set(child.get_icon_class() for child in self.get_children()) new_class = 'other' for higher_class in CLASS_PRIORITY: if higher_class in child_classes: new_class = higher_class return new_class
non_editable_fields = super(VerticalBlock, self).non_editable_metadata_fields non_editable_fields.extend([ self.fields['due'], ]) return non_editable_fields
return getattr(problem_class, 'human_name', problem_class.__name__)
return LibraryLocator.from_string(self.source_library_id)
for block_type, block_id in self.selected_children(): yield self.runtime.get_block(self.location.course_key.make_usage_key(block_type, block_id))
return self.descriptor.validate()
return list(self._get_selected_child_blocks())
return self.runtime.service(self, 'library_tools')
if validation.empty: validation.set_summary(summary)
return True
titles = [] for child in self._xmodule.get_child_descriptors(): titles.extend(child.get_content_titles()) return titles
return get_instructions(xmltree)
return any(role.has_permission(permission) for role in user.roles.filter(course_id=course_id))
course = self.runtime.modulestore.get_course(self.course_id) return course
return self.runtime.get_edited_by(self)
return self.runtime.get_edited_on(self)
return self.runtime.get_subtree_edited_by(self)
return self.runtime.get_subtree_edited_on(self)
return self.runtime.get_published_by(self)
return self.runtime.get_published_on(self)
pass
pass
pass
pass
pass
def from_json(self, values): return [UserPartition.from_json(v) for v in values] def to_json(self, values): return [user_partition.to_json() for user_partition in values]
return module.get_explicitly_set_fields_by_scope(Scope.settings)
super(InheritingFieldData, self).__init__(**kwargs) self.inheritable_names = set(inheritable_names)
return InheritingFieldData( inheritable_names=InheritanceMixin.fields.keys(), kvs=kvs, )
return self.inherited_settings[key.field_name]
if self._is_in_bulk_operation(course_key, ignore_case): return self._get_bulk_ops_record(course_key, ignore_case).index else: return self.db_connection.get_course_index(course_key, ignore_case)
if self._is_in_bulk_operation(course_key, False): self._clear_bulk_ops_record(course_key) self.db_connection.delete_course_index(course_key)
bulk_write_record = self._get_bulk_ops_record(course_key) if bulk_write_record.active: bulk_write_record.index = updated_index_entry else: self.db_connection.update_course_index(updated_index_entry, course_context=course_key)
bulk_write_record = self._get_bulk_ops_record(course_key) if bulk_write_record.active: return bulk_write_record.modules[version_guid].get(block_id, None) else: return None
bulk_write_record = self._get_bulk_ops_record(course_key) if bulk_write_record.active: bulk_write_record.modules[version_guid][block_key] = block
bulk_write_record = self._get_bulk_ops_record(course_key) if bulk_write_record.active: try: del bulk_write_record.modules[version_guid][block_key] except KeyError: pass
bulk_write_record = self._get_bulk_ops_record(course_key) if bulk_write_record.active: bulk_write_record.definitions[definition['_id']] = definition else: self.db_connection.insert_definition(definition, course_key)
ids = set(ids) return self.db_connection.find_course_blocks_by_id(list(ids))
self.db_connection.close_connections()
return self.db_connection.mongo_wire_version
if self.request_cache is None: return None return self.request_cache.data.setdefault('course_cache', {}).get(course_version_guid)
if self.request_cache is not None: self.request_cache.data.setdefault('course_cache', {})[course_version_guid] = system return system
version_guids, id_version_map = self.collect_ids_from_matching_indexes(branch, **kwargs) if not version_guids: return for entry in self.find_course_blocks_by_id(version_guids): for course_index in id_version_map[entry['_id']]: yield entry, course_index
version_guids, id_version_map = self.collect_ids_from_matching_indexes(branch, **kwargs) if not version_guids: return for entry in self.find_structures_by_id(version_guids): for course_index in id_version_map[entry['_id']]: yield entry, course_index
return CourseLocator( org=course_info['org'], course=course_info['course'], run=course_info['run'], branch=branch, )
return LibraryLocator( org=library_info['org'], library=library_info['course'], branch=branch, )
return { field: course.fields[field] for field in CourseSummary.course_info_fields if field in course.fields }
return CourseLocator(org, course, run)
locator_cls = CCXBlockUsageLocator if isinstance(course_key, CCXLocator) else BlockUsageLocator return locator_cls(course_key, 'course', 'course')
structure_entry = self._lookup_course(structure_id, head_validation=head_validation) root = structure_entry.structure['root'] result = self._load_items(structure_entry, [root], depth, **kwargs) return result[0]
return ModuleStoreEnum.Type.split
return self.save_asset_metadata_list([asset_metadata, ], user_id, import_only)
if asset_idx is None: raise ItemNotFoundError(asset_key) all_asset_info.pop(asset_idx) return all_asset_info
return [ parent_block_key for parent_block_key, value in structure['blocks'].iteritems() if block_key in value.fields.get('children', []) ]
return structure['blocks'].get(block_key)
structure['blocks'][block_key] = content
return self.find_courses_by_search_target('wiki_slug', wiki_slug)
return {ModuleStoreEnum.Type.split: self.db_connection.heartbeat()}
self.db_connection.ensure_indexes()
missing = index - len(self) + 1 if missing > 0: self.extend([None] * missing) list.__setitem__(self, index, value)
dest_course_id = self._map_revision_to_branch(dest_course_id, revision=revision) return super(DraftVersioningModuleStore, self).clone_course( source_course_id, dest_course_id, user_id, fields=fields, **kwargs )
usage_key = self._map_revision_to_branch(usage_key, revision=revision) return super(DraftVersioningModuleStore, self).has_item(usage_key)
usage_key = self._map_revision_to_branch(usage_key, revision=revision) return super(DraftVersioningModuleStore, self).get_item(usage_key, depth=depth, **kwargs)
course_locator = self._map_revision_to_branch(course_locator, revision=revision) return super(DraftVersioningModuleStore, self).get_items(course_locator, **kwargs)
usage_key = self._map_revision_to_branch(usage_key) return super(DraftVersioningModuleStore, self).get_block_original_usage(usage_key)
for branch in [ModuleStoreEnum.RevisionOption.published_only, ModuleStoreEnum.RevisionOption.draft_only]: super(DraftVersioningModuleStore, self).fix_not_found( self._map_revision_to_branch(course_key, branch), user_id )
course_locator = self._map_revision_to_branch(course_locator) return super(DraftVersioningModuleStore, self).get_course_history_info(course_locator)
course_locator = self._map_revision_to_branch(course_locator) return super(DraftVersioningModuleStore, self).get_course_successors( course_locator, version_history_depth=version_history_depth )
block_locator = self._map_revision_to_branch(block_locator) return super(DraftVersioningModuleStore, self).get_block_generations(block_locator)
return self._get_head(xblock, ModuleStoreEnum.BranchName.published) is not None
source_version = block.edit_info.source_version return source_version if source_version is not None else block.edit_info.update_version
return super(DraftVersioningModuleStore, self)._find_course_assets( self._map_revision_to_branch(course_key) )
self.modulestore = modulestore self.course_key = course_key self.definition_locator = DefinitionLocator(block_type, definition_id) self.field_converter = field_converter
return caches[alias]
if value == 0: return 0 return math.pow(2, math.ceil(math.log(value, 2)))
self.measures.append((name, size))
self.added_tags.extend(kwargs.items())
self._metric_base = metric_base self._sample_rate = sample_rate
if self.database.connection.alive(): return True else: raise HeartbeatFailure("Can't connect to {}".format(self.database.name), 'mongo')
with TIMER.timer("insert_course_index", course_context): course_index['last_update'] = datetime.datetime.now(pytz.utc) self.course_index.insert(course_index)
self.database.connection.close()
return self.database.connection.max_wire_version
return xblock._edited_by
return xblock._edited_on
if not hasattr(xblock, '_published_by'): self.modulestore.compute_published_info_internal(xblock) return getattr(xblock, '_published_by', None)
if not hasattr(xblock, '_published_on'): self.modulestore.compute_published_info_internal(xblock) return getattr(xblock, '_published_on', None)
raise NotImplementedError
raise NotImplementedError
return LibraryLocator( self.courselike_key.org, self.courselike_key.library )
return self.modulestore.get_library(self.courselike_key, depth=None, lazy=False)
root.set('org', self.courselike_key.org) root.set('library', self.courselike_key.library)
CourseExportManager(modulestore, contentstore, course_key, root_dir, course_dir).export()
LibraryExportManager(modulestore, contentstore, library_key, root_dir, library_dir).export()
row_items = [E.TD(x) for x in items] self.table.append(E.TR(*row_items))
return lxml.html.tostring(self.table)
func_name = "H{}".format(level) self.body.append(getattr(E, func_name)(text))
return lxml.html.tostring(self.html, pretty_print=pretty_print)
return random.choice((True, False))
return coin_flip()
return random.randint(1, 100000000)
return 'v{}.0'.format(ver)
all_asset_md = [] for __ in xrange(amount): all_asset_md.append(generate_random_asset_md()) return all_asset_md
split = 'split' mongo = 'mongo'
draft_preferred = 'draft-preferred' published_only = 'published-only'
draft = 'draft-branch' published = 'published-branch' library = 'library'
ascending = 1 descending = 2
return self._active_count > 0
self._active_count += 1
self._active_count -= 1
return self._active_count == 1
def __init__(self, bulk_ops_record_type, **kwargs): super(ActiveBulkThread, self).__init__(**kwargs) self.records = defaultdict(bulk_ops_record_type)
for course_key, record in self._active_bulk_ops.records.iteritems(): if record.active: yield (course_key, record)
if course_key.for_branch(None) in self._active_bulk_ops.records: del self._active_bulk_ops.records[course_key.for_branch(None)]
pass
pass
return self._get_bulk_ops_record(course_key, ignore_case).active
signal_handler = getattr(self, "signal_handler", None) if signal_handler and bulk_ops_record.has_publish_item: signal_handler.send("pre_publish", course_key=course_id)
if self.signal_handler and bulk_ops_record.has_library_updated_item: self.signal_handler.send("library_updated", library_key=library_id) bulk_ops_record.has_library_updated_item = False
return self.to_storable() == edit_info.to_storable()
return not self == edit_info
return not self == block_data
pass
return hasattr(self, method)
raise NotImplementedError()
raise NotImplementedError()
pass
pass
if key not in fields: return False, None field = fields[key] if xblock is not None: return field.is_set_on(block), getattr(xblock, key) else: return True, field
pass
pass
pass
pass
pass
pass
pass
pass
yield
pass
return {}
assert isinstance(course_id, CourseKey) for course in self.get_courses(**kwargs): if course.id == course_id: return course return None
return True
if self.contentstore: self.contentstore.close_connections() super(ModuleStoreReadBase, self).close_connections()
if self.signal_handler: self.signal_handler.send("course_deleted", course_key=course_key)
if self.signal_handler: self.signal_handler.send("item_deleted", usage_key=usage_key, user_id=user_id)
from_xmodule = [entry_point for entry_point in entry_points if entry_point.dist.key == 'xmodule'] return default_select(identifier, from_xmodule)
from_xmodule = [entry_point for entry_point in entry_points if entry_point.dist.key == 'xmodule'] if from_xmodule: return default_select(identifier, from_xmodule) else: return default_select(identifier, entry_points)
item.is_draft = (item.location.revision == MongoRevisionKey.draft) item.location = item.location.replace(revision=MongoRevisionKey.published) return item
if getattr(xblock, 'is_draft', False): published_xblock_location = as_published(xblock.location) try: xblock.runtime.lookup_item(published_xblock_location) except ItemNotFoundError: return False return True
actual_branch_setting = self.get_branch_setting() if actual_branch_setting != expected_branch_setting: raise InvalidBranchSetting( expected_setting=expected_branch_setting, actual_setting=actual_branch_setting )
draft = 'draft' published = None
pass
key = UsageKey.from_string(ref_string) return key.replace(run=self.modulestore.fill_in_run(key.course_key).run)
try: json = self.module_data[location] except KeyError: json = self.modulestore._find_one(location) self.module_data[location] = json return json
return xblock._edit_info.get('edited_by')
return xblock._edit_info.get('edited_on')
return xblock._edit_info.get('subtree_edited_by')
return xblock._edit_info.get('subtree_edited_on')
return xblock._edit_info.get('published_by')
return xblock._edit_info.get('published_date')
if location.category in DIRECT_ONLY_CATEGORIES: return location return location.replace(revision=MongoRevisionKey.draft)
return location.replace(revision=MongoRevisionKey.published)
def __init__(self): super(MongoBulkOpsRecord, self).__init__() self.dirty = False
return super(MongoBulkOpsMixin, self)._is_in_bulk_operation( course_id.for_branch(None), ignore_case )
self.collection.database.connection.close()
self.database.connection._ensure_connected() return self.database.connection.max_wire_version
if self.request_cache is not None: return self.request_cache.data.setdefault('parent-location-{}'.format(branch), ParentLocationCache()) else: return ParentLocationCache()
item['location'] = item['_id'] del item['_id']
category = item['location']['category'] apply_cached_metadata = category not in DETACHED_XBLOCK_TYPES and \ not (category == 'course' and depth == 0) return apply_cached_metadata
return { field: course['metadata'][field] for field in CourseSummary.course_info_fields if field in course['metadata'] }
return CourseLocator(org, course, run, deprecated=True)
return BlockUsageLocator(course_key, 'course', course_key.run)
try: self._find_one(usage_key) return True except ItemNotFoundError: return False
return SON([ (key, id_dict[key]) for key in ('tag', 'org', 'course', 'category', 'name', 'revision') ])
if block_type == 'course': block_id = course_key.run xblock = self.create_xblock(runtime, course_key, block_type, block_id, fields) return self.update_item(xblock, user_id, allow_not_found=True)
parent = self._get_raw_parent_location(as_published(location), ModuleStoreEnum.RevisionOption.draft_preferred) if parent: self._update_single_item(parent, update) self._update_ancestors(parent, update)
return ModuleStoreEnum.Type.mongo
kvs = MongoKeyValueStore( definition_data, None, [], metadata, ) field_data = KvsFieldData(kvs) return field_data
return 'assets.{}'.format(asset_type)
pass
raise NotImplementedError
raise NotImplementedError
raise NotImplementedError
raise NotImplementedError
pass
self.recursive_build(source_courselike, courselike, courselike_key, dest_id)
return courselike
manager = CourseImportManager(*args, **kwargs) return list(manager.run_imports())
manager = LibraryImportManager(*args, **kwargs) return list(manager.run_imports())
assert isinstance(reference, UsageKey) if source_course_id == reference.course_key: return reference.map_into_course(dest_course_id) else: return reference
return (url_name is not None and url_name.startswith(tag) and re.search('[0-9a-fA-F]{12}$', url_name))
return xmlstore.get_item(usage_key, for_parent=for_parent)
return usage_id
return descriptor.id
return policy.get(policy_key(usage_id), {})
return usage_key in self.modules[usage_key.course_key]
return CourseLocator(org, course, run, deprecated=True)
return BlockUsageLocator(course_key, 'course', course_key.run)
return self.courses.values()
return self.get_courses(**kwargs)
return dict((k, self.errored_courses[k].errors) for k in self.errored_courses)
courses = self.get_courses() return [course.location.course_key for course in courses if course.wiki_slug == wiki_slug]
return {'xml': True}
if branch_setting != ModuleStoreEnum.Branch.published_only: raise ValueError(u"Cannot set branch setting to {} on a ReadOnly store".format(branch_setting)) yield
log.warning("_find_course_asset request of XML modulestore - not implemented.") return (None, None)
log.warning("find_asset_metadata request of XML modulestore - not implemented.") return None
log.warning("get_all_asset_metadata request of XML modulestore - not implemented.") return []
return course_key
return LibraryLocator(org=org, library=library)
assert isinstance(library_id, LibraryLocator) for library in self.get_courses(**kwargs): if library.location.library_key == library_id: return library return None
return descriptor.location.library_key
pass
pass
return "{store}[{collection}] already has {element_id} ({exception})".format( store=self.store, collection=self.collection, element_id=self.element_id, exception=Exception.__str__(self, *args, **kwargs), )
def __init__(self, requestedLocation, currentHeadVersionGuid): super(VersionConflictError, self).__init__(u'Requested {}, but current head is {}'.format( requestedLocation, currentHeadVersionGuid ))
super(DuplicateCourseError, self).__init__( u'Cannot create course {}, which duplicates {}'.format(course_id, existing_entry) ) self.course_id = course_id self.existing_entry = existing_entry
check_has_course_method( XMLModuleStore(DATA_DIR, source_dirs=['toy', 'simple']), SlashSeparatedCourseKey('edX', 'toy', '2012_Fall'), locator_key_fields=SlashSeparatedCourseKey.KEY_FIELDS )
content = String(default="content", scope=Scope.content)
expected_fields = CourseSummary.course_info_fields + ['id', 'location'] return all([hasattr(course_summary, field) for field in expected_fields])
return block_type in DETACHED_BLOCK_TYPES
store_fields = ["OPTIONS", "DOC_STORE_CONFIG"] for field in store_fields: self.assertEqual(store_setting1[field], store_setting2[field])
return self.course_locations[string].course_key
mappings = mappings or {} self.store = MixedModuleStore( contentstore, create_modulestore_instance=create_modulestore_instance, mappings=mappings, **self.options ) self.addCleanup(self.store.close_all_connections)
return Fragment(self.FRAG_CONTENT)
return Fragment(self.FRAG_CONTENT)
for child_location, parent_location, revision in expected_results: self.assertEqual( parent_location, self.store.get_parent_location(child_location, revision=revision) )
self.initdb(default_ms) self._create_block_hierarchy() with self.assertRaises(InvalidVersionError): self.store.revert_to_published(self.vertical_x1a, self.user_id)
self.assertEquals( len(self.store.get_items(course_key.for_branch(None), settings={'display_name': display_name})), expected_number )
signal_handler.send.assert_called_with('course_published', course_key=course.id)
super(TestPublishOverExportImport, self).setUp() self.user_id = ModuleStoreEnum.UserID.test self.export_dir = mkdtemp() self.addCleanup(rmtree, self.export_dir, ignore_errors=True)
top_level_export_dir = 'exported_source_course' export_course_to_xml( modulestore, contentstore, source_course_key, export_dir, top_level_export_dir, ) import_course_from_xml( modulestore, 'test_user', export_dir, source_dirs=[top_level_export_dir], static_content_store=contentstore, target_id=source_course_key, create_if_not_present=True, raise_on_failure=True, )
self._enabled = True
self._enabled = False
return self._enabled
block = store.create_child( user_id, parent_loc, block_info.category, block_id=block_info.block_id, fields=block_info.fields, ) for tree in block_info.sub_tree: create_sub_tree(block.location, tree)
with check_number_of_calls(object_with_method, method_name, num_calls, num_calls): yield
return check_sum_of_calls(object_with_method, [method_name], maximum_calls, minimum_calls)
return sum(self.stack_calls(stack) for stack in self._stacks)
if self.include_arguments: return sum(self._stacks[stack].values()) else: return self._stacks[stack]
return iter(sorted(self._stacks.keys(), key=lambda stack: (self.stack_calls(stack), stack), reverse=True))
return self._stacks[stack]
asset_md.thumbnail = 'ABC39XJUDN2' return asset_md
for idx, asset in enumerate(orig): self.assertEquals(assets[idx].asset_id.asset_type, asset[0]) self.assertEquals(assets[idx].asset_id.path, asset[1])
return Fragment(self.FRAG_CONTENT)
result = self.store.get_library(LibraryLocator("non", "existent")) self.assertEqual(result, None)
for element in collection: if element.location.block_id == _id: return element
return modulestore().db_connection.get_structure( course.location.as_object_id(course.location.version_guid) )
return [child.version_agnostic() for child in children]
module_path, _, name = engine_path.rpartition('.') return getattr(import_module(module_path), name)
self.addCleanup(self.cleanup_modulestore) super(ModuleStoreNoSettings, self).setUp()
module_path, _, name = engine_path.rpartition('.') return getattr(importlib.import_module(module_path), name)
actual_explicitly_set_fields = block.get_explicitly_set_fields_by_scope(scope=scope) assertion = self.assertIn if should_be_set else self.assertNotIn for field in expected_explicitly_set_fields: assertion(field, actual_explicitly_set_fields)
has_children = True reference_link = Reference(default=None, scope=Scope.content) reference_list = ReferenceList(scope=Scope.content) reference_dict = ReferenceValueDict(scope=Scope.settings)
pass
pass
lib_key = LibraryLocator("TestOrg", "TestLib") result = self.draft_store.has_course(lib_key) assert_false(result)
pass
pass
class_ = load_function(engine) if issubclass(class_, ModuleStoreDraftAndPublished): options['branch_setting_func'] = lambda: ModuleStoreEnum.Branch.draft_preferred return class_( doc_store_config=doc_store_config, contentstore=contentstore, signal_handler=signal_handler, **options )
return tab_dict
return self.scope_ids.usage_id
assert isinstance(value, UsageKey) self.scope_ids = self.scope_ids._replace( def_id=value, usage_id=value, )
return self._data.get(key, default)
self._data[key] = value
with MongoContentstoreBuilder().build() as contentstore: with self.build_with_contentstore(contentstore) as modulestore: yield contentstore, modulestore
modulestore = XMLModuleStore( DATA_DIR, course_ids=course_ids, default_class='xmodule.hidden_module.HiddenDescriptor', xblock_mixins=XBLOCK_MIXINS, ) yield modulestore
self.store_builders = store_builders self.mappings = mappings or {} self.mixed_modulestore = None
return '{}{:02d}'.format(block_type, num)
self.course_db.update( { (block_type, block_id): _make_course_db_entry( parent_type, parent_id, block_id, idx, child_type, child_base ) } )
raise NotImplementedError()
block_path = course_export_dir if draft: block_path = os.path.join(block_path, 'drafts') return os.path.join(block_path, block_type)
return '{}.xml'.format(block_id)
self.assertEqual(element.tag, tag)
self._assertOLXBase(block_list, draft=True, published=False)
self._assertOLXBase(block_list, draft=False, published=True)
self._assertOLXBase(block_list, draft=True, published=True)
for block_data in block_list: (block_type, block_id) = block_data self.assertOLXMissing(block_type, block_id, draft=True) self.assertOLXMissing(block_type, block_id, draft=False)
try: export_dir = mkdtemp() yield export_dir finally: rmtree(export_dir, ignore_errors=True)
return self.store.get_modulestore_type(self.course.id) == ModuleStoreEnum.Type.split
return self.store.get_modulestore_type(self.course.id) == ModuleStoreEnum.Type.mongo
return self.EXPORTED_COURSE_AFTER_DIR_NAME.format(unicode(uuid.uuid4())[:8])
assert_method = getattr(self, expected_result) assert_method(block_list)
draft, split = range(2)
super(SharedModuleStoreTestCase, cls).setUpClass() cls.start_modulestore_isolation()
return mixed_setting["default"]["OPTIONS"]["stores"]
assert isinstance(location, (NoneType, UsageKey)) if location is None: return None return super(UsageKeyField, self).to_mongo(location.to_deprecated_string())
if 'asides' in kwargs: kwargs['asides'] = prepare_asides_to_store(kwargs['asides']) return func(*args, **kwargs)
if hasattr(locator, 'version_agnostic'): locator = locator.version_agnostic() if hasattr(locator, 'branch'): locator = locator.replace(branch=None) return locator
for store in self.modulestores: if store.get_modulestore_type() == modulestore_type: return store return None
store = self._get_modulestore_for_courselike(course_key) if not hasattr(store, 'fill_in_run'): return course_key return store.fill_in_run(course_key)
store = self._get_modulestore_for_courselike(usage_key.course_key) return store.has_item(usage_key, **kwargs)
store = self._get_modulestore_for_courselike(usage_key.course_key) return store.get_item(usage_key, depth, **kwargs)
assert isinstance(course_key, CourseKey) store = self._get_modulestore_for_courselike(course_key) return store.make_course_usage_key(course_key)
assert isinstance(course_key, CourseKey) store = self._get_modulestore_for_courselike(course_key) try: return store.get_course(course_key, depth=depth, **kwargs) except ItemNotFoundError: return None
assert isinstance(course_key, CourseKey) store = self._get_modulestore_for_courselike(course_key) return store.delete_course(course_key, user_id)
store = self._get_modulestore_for_courselike(asset_key.course_key) return store.find_asset_metadata(asset_key, **kwargs)
store = self._get_modulestore_for_courselike(asset_key.course_key) return store.delete_asset_metadata(asset_key, user_id)
store = self._get_modulestore_for_courselike(location.course_key) return store.get_parent_location(location, **kwargs)
try: store = self._verify_modulestore_support(usage_key.course_key, 'get_block_original_usage') return store.get_block_original_usage(usage_key) except NotImplementedError: return None, None
return self._get_modulestore_for_courselike(course_id).get_modulestore_type()
store = self._get_modulestore_for_courselike(course_key) return store.get_orphans(course_key, **kwargs)
errs = {} for store in self.modulestores: errs.update(store.get_errored_courses()) return errs
store = self._verify_modulestore_support(course_key, 'import_xblock') return store.import_xblock(user_id, course_key, block_type, block_id, fields, runtime, **kwargs)
store = self._verify_modulestore_support(dest_key.course_key, 'copy_from_template') return store.copy_from_template(source_keys, dest_key, user_id)
store = self._verify_modulestore_support(xblock.location.course_key, 'update_item') return store.update_item(xblock, user_id, allow_not_found, **kwargs)
store = self._verify_modulestore_support(location.course_key, 'delete_item') return store.delete_item(location, user_id=user_id, **kwargs)
for modulestore in self.modulestores: modulestore.close_connections()
courses = [] for modulestore in self.modulestores: courses.extend(modulestore.get_courses_for_wiki(wiki_slug, **kwargs)) return courses
store = self._verify_modulestore_support(location.course_key, 'publish') return store.publish(location, user_id, **kwargs)
store = self._verify_modulestore_support(location.course_key, 'unpublish') return store.unpublish(location, user_id, **kwargs)
store = self._verify_modulestore_support(location.course_key, 'convert_to_draft') return store.convert_to_draft(location, user_id)
store = self._verify_modulestore_support(xblock.location.course_key, 'has_changes') return store.has_changes(xblock)
try: self._verify_modulestore_support(course_key, method) return True except NotImplementedError: return False
store = self._get_modulestore_for_courselike(course_key) if hasattr(store, method): return store else: raise NotImplementedError(u"Cannot call {} on store {}".format(method, store))
store = self._verify_modulestore_support(course_id, 'branch_setting') with store.branch_setting(branch_setting, course_id): yield
store = self._get_modulestore_for_courselike(course_id) with store.bulk_operations(course_id, emit_signals): yield
return _write_styles('.xmodule_display', output_root, _list_modules())
return _write_js(output_root, _list_modules())
return _write_styles('.xmodule_edit', output_root, _list_descriptors())
return _write_js(output_root, _list_descriptors())
return [ desc for desc in [ desc for (_, desc) in XModuleDescriptor.load_classes() ] ]
return [ desc.module_class for desc in _list_descriptors() ]
try: os.makedirs(directory) except OSError as exc: if exc.errno == errno.EEXIST: pass else: raise
class_=class_, selector=selector
return name.replace(':', '/')
if isinstance(value, basestring): return value return json.dumps(value, cls=EdxJSONEncoder)
meta = xml_object.find('meta') if meta is None: return '' dmdata = meta.text if remove: xml_object.remove(meta) return dmdata
raise NotImplementedError("%s does not implement definition_from_xml" % cls.__name__)
for field_name, field in cls.fields.items(): if field.scope == Scope.settings and xml_object.get(field_name) is not None: del xml_object.attrib[field_name]
return etree.parse(file_object, parser=EDX_XML_PARSER).getroot()
return True
raise NotImplementedError( "%s does not implement definition_to_xml" % self.__class__.__name__)
for attr, val in attr_dict.iteritems(): if attr in self.ATTRS_ALLOWED_TO_UPDATE: setattr(self, attr, val) else: self.fields[attr] = val
for asset in assets: asset_node = etree.SubElement(node, "asset") asset.to_xml(asset_node)
self.course_id = course_id self._doc_id = doc_id self.asset_md = asset_md
return self._doc_id
return self.asset_md.setdefault(item, default)
return self.asset_md.get(item, default)
pass
pass
pass
root = etree.Element("glassets") with self.assertRaises(ContractNotRespected): AssetMetadata.add_all_assets_as_xml(root, self.course_assets)
sources = xml_element.get('sources') if sources: return [location.strip() for location in sources.split(';')]
def from_json(self, value): if value in ("", "true"): return RANDOMIZATION.ALWAYS elif value == "false": return RANDOMIZATION.PER_STUDENT return value to_json = from_json
self.last_submission_time = datetime.datetime.now(UTC())
return self.lcp.get_score()
return self.lcp.get_max_score()
hint_index = int(data['hint_index']) return self.get_demand_hint(hint_index)
return (self.close_date is not None and datetime.datetime.now(UTC()) > self.close_date)
if self.max_attempts is not None and self.attempts >= self.max_attempts: return True if self.is_past_due(): return True return False
return self.attempts > 0
score_dict = self.get_score() return score_dict['score'] == score_dict['total']
return {'html': self.get_problem_html(encapsulate=False)}
return self.system.render_template('lti.html', self.get_context())
template = self.system.render_template('lti_form.html', self.get_context()) return Response(template, content_type='text/html')
return self.descriptor.runtime.modulestore.get_course(self.course_id)
return self.course_id.to_deprecated_string()
def __init__(self, location): super(InvalidVersionError, self).__init__() self.location = location
def __init__(self, location, msg): super(SerializationError, self).__init__(msg) self.location = location
pass
if self.choice is None: return None return self.descriptor.get_children()[self.choice]
child_descriptor = self.child_descriptor if child_descriptor is None: return None return self.system.get_module(child_descriptor)
if self.child_descriptor is None: return [] return [self.child_descriptor]
contents = String(scope=Scope.content) error_msg = String(scope=Scope.content) display_name = String(scope=Scope.settings)
return library_key.replace(version_guid=None, branch=None)
if not isinstance(library_key, LibraryLocator): library_key = LibraryLocator.from_string(library_key) try: return self.store.get_library( library_key, remove_version=False, remove_branch=False, head_validation=False ) except ItemNotFoundError: return None
if usage_key.block_type != "problem": return False descriptor = self.store.get_item(usage_key, depth=0) assert isinstance(descriptor, CapaDescriptor) return capa_type in descriptor.problem_types
return self.store.check_supports(block.location.course_key, 'copy_from_template')
super(CapaModule, self).__init__(*args, **kwargs)
return self.student_view(context)
return 'latex' not in template['template_id'] or course.use_latex_compiler
resources_dir = None def get_html(self): return self.studio_view(None).content
assert_is_none(get_current_request())
if params is None: params = {} data = copy(self.defaults) data.update(params) return self.request_body_xml_template.format(**data)
return mock_url_prefix + handler_name
self.xmodule.verify_oauth_body_sign(self.get_signed_grade_mock_request())
with self.assertRaises(IndexError): mocked_request = self.get_signed_grade_mock_request(namespace_lti_v1p1=False) self.xmodule.parse_grade_xml_body(mocked_request.body)
with self.assertRaises(LTIError): req = self.get_signed_grade_mock_request() self.xmodule.verify_oauth_body_sign(req)
user = XBlockUser() user.opt_attrs['edx-platform.username'] = 'test user' return user
return sequence.xmodule_runtime.render( sequence, STUDENT_VIEW, { 'requested_child': requested_child, 'next_url': next_url, 'prev_url': prev_url, }, ).content
self.assertIn("'position': {}".format(expected_position), rendered_html)
pass
module = self.descriptor._xmodule self.assertIsInstance(module, ErrorModule)
with self.assertRaises(TestException): module = self.descriptor._xmodule
with assert_raises(TypeError): StudioValidation("id").set_summary("foo")
problem = "<problem>" for problem_type in args: problem += "<{problem_type}></{problem_type}>".format(problem_type=problem_type) problem += "</problem>" return problem
self.problem_type_lookup = {} for problem_type in self.problem_types: block = self.make_block("problem", self.library, data=self._get_capa_problem_type_xml(*problem_type)) self.problem_type_lookup[block.location] = problem_type
non_editable_metadata_fields = self.lc_block.non_editable_metadata_fields self.assertIn(LibraryContentDescriptor.mode, non_editable_metadata_fields) self.assertNotIn(LibraryContentDescriptor.display_name, non_editable_metadata_fields)
super(TestLibraryContentModuleWithSearchIndex, self).setUp() search_index_mock.search = Mock(side_effect=self._get_search_response)
has_children = True field1 = String(default="something", scope=Scope.user_state) field2 = Integer(scope=Scope.user_state)
return text
orig_view_name = None if hasattr(self, '_view_name'): orig_view_name = self._view_name self._view_name = None rt_repr = super(TestModuleSystem, self).__repr__() self._view_name = orig_view_name return rt_repr
return pprint.pformat((args, kwargs)).decode()
return json.loads(self.xmodule.handle_ajax(dispatch, data))
self._assertion_errors.append(formatted_exc)
if self._assertion_errors: raise BulkAssertionError(self._assertion_errors)
if self.__manager: yield else: try: self.__manager = _BulkAssertionManager(self) yield except Exception: raise else: manager = self.__manager self.__manager = None manager.raise_assertion_errors()
base_attr = super(BulkAssertionTest, self).__getattribute__(name) if name.startswith('assert'): return self._wrap_assertion(base_attr) else: return base_attr
self.field_exclusions.add((usage_id, field_name))
self.ignored_asset_keys.add(key_name)
self.assertEqual(expected_block.fields, actual_block.fields) for field in expected_block.fields.values(): self.assertFieldEqual(field, expected_block, actual_block)
if self._xml_string is not None: return self._xml_string return etree.tostring(self._xml_node)
return etree.Element(self.tag)
return xml_import_data.policy.get(policy_key(usage_id), {})
return self._descriptors[location.to_deprecated_string()]
assertion, args = assertion_tuple[0], assertion_tuple[1:] getattr(self, assertion)(*args)
tag = 'split_test'
non_editable_metadata_fields = self.split_test_module.non_editable_metadata_fields self.assertIn(SplitTestDescriptor.due, non_editable_metadata_fields) self.assertIn(SplitTestDescriptor.user_partitions, non_editable_metadata_fields) self.assertNotIn(SplitTestDescriptor.display_name, non_editable_metadata_fields)
return "input_" + cls.answer_key(response_num, input_num)
mock_progress.return_value = True module = CapaFactory.create() module.weight = 0 progress = module.get_progress() self.assertIsNone(progress) self.assertFalse(mock_progress.called)
module = CapaFactory.create() module.get_progress = Mock(wraps=module.get_progress) module.get_html() module.get_progress.assert_called_once_with()
descriptor = CapaDescriptor(get_test_system(), scope_ids=1) descriptor.data = xml if name: descriptor.display_name = name return descriptor
sample_problem_xml = textwrap.dedent(xml)
cls.num += 1 return cls.num
return "input_" + cls.answer_key(input_num)
fut = duedate.get_extended_due_date return fut(node)
node = object() self.assertEqual(self.call_fut(node), None)
node = mock.Mock(due=1, extended_due=None) self.assertEqual(self.call_fut(node), 1)
node = mock.Mock(due=1, extended_due=2) self.assertEqual(self.call_fut(node), 2)
node = mock.Mock(due=2, extended_due=1) self.assertEqual(self.call_fut(node), 2)
node = mock.Mock(due=None, extended_due=1) self.assertEqual(self.call_fut(node), None)
pass
pass
youtube_str = '1.00:p2Q6BrNhdh8' youtube_str_hack = '1.0:p2Q6BrNhdh8' self.assertEqual( VideoDescriptor._parse_youtube(youtube_str), VideoDescriptor._parse_youtube(youtube_str_hack) )
return [child.tag for child in elem]
for key, value in attrs.items(): self.assertEquals(getattr(video, key), value)
return etree.Element( 'video_asset', attrib={'export_edx_video_id': edx_video_id} )
self.descriptor.display_name = '\x1e' with self.assertRaises(ValueError): self.descriptor.definition_to_xml(None)
teams_configuration = {} teams_configuration["topics"] = [] if topics is None else topics if max_team_size is not None: teams_configuration["max_team_size"] = max_team_size self.course.teams_configuration = teams_configuration
super(CourseDescriptorTestCase, self).setUp() self.course = get_dummy_course(start=_TODAY)
self.assertEqual( self.course.clean_id(), "course_ORSXG5C7N5ZGOL3UMVZXIX3DN52XE43FF52GK43UL5ZHK3Q=" ) self.assertEqual( self.course.clean_id(padding_char='$'), "course_ORSXG5C7N5ZGOL3UMVZXIX3DN52XE43FF52GK43UL5ZHK3Q$" )
self.course.start = _LAST_WEEK self.assertTrue(self.course.has_started()) self.course.start = _NEXT_WEEK self.assertFalse(self.course.has_started())
pass
pass
assert False, "student_view should produce valid html"
assert False, "studio_view should produce valid html"
try: html = lxml.html.fragment_fromstring(fragment.content) except lxml.etree.ParserError: assert_student_view_invalid_html(block, fragment.content) else: assert_student_view_valid_html(block, html)
template_packages = [__name__] @classmethod def get_template_dir(cls): return 'templates/test'
@classmethod def get_template_dir(cls): return 'foo'
self.get_dummy_course(START)
self.course.start = _LAST_WEEK self.assertTrue(self.course.has_started()) self.course.start = _NEXT_WEEK self.assertFalse(self.course.has_started())
has_children = True field1 = String(default="something", scope=Scope.user_state) field2 = Integer(scope=Scope.user_state)
def utcoffset(self, _dt): return timedelta(hours=4)
def utcoffset(self, _dt): return None
for cls, fields_list in class_dict.items(): for fields in fields_list: yield (cls, fields)
return get_test_system(*args, **kwargs)
return get_test_descriptor_system(*args, **kwargs)
self.position = position
self.position = position
if xmodule_runtime is None: xmodule_runtime = ModuleSystemFactory() self.xmodule_runtime = xmodule_runtime
runtime = SubFactory(ContainerDescriptorRuntimeFactory) children = range(3)
self.assertEqual( descriptor._xmodule.get_html(), descriptor.render(STUDENT_VIEW).content )
html = descriptor.get_html() rendered_content = descriptor.render(STUDIO_VIEW).content self.assertEqual(html, rendered_content)
with self.assertRaisesRegexp(LTIError, "Content-Type must be"): request = Mock(headers={u'Content-Type': u'Non-existent'}) self.xmodule.verify_lti_2_0_result_rest_headers(request)
for einput in self.BAD_DISPATCH_INPUTS: with self.assertRaisesRegexp(LTIError, "No valid user id found in endpoint URL"): self.xmodule.parse_lti_2_0_handler_suffix(einput)
for ginput, expected in self.GOOD_DISPATCH_INPUTS: self.assertEquals(self.xmodule.parse_lti_2_0_handler_suffix(ginput), expected)
for error_inputs, error_message in self.BAD_JSON_INPUTS: for einput in error_inputs: with self.assertRaisesRegexp(LTIError, error_message): self.xmodule.parse_lti_2_0_result_json(einput)
self.setup_system_xmodule_mocks_for_lti20_request_test() mock_request = self.get_signed_lti20_mock_request(self.GOOD_JSON_PUT) response = self.xmodule.lti_2_0_result_rest_handler(mock_request, None) self.assertEqual(response.status_code, 404)
inherited = String(scope=Scope.settings, default="the default") not_inherited = String(scope=Scope.settings, default="nothing")
assert_equals(expected, deserialize_field(self.test_field(), arg))
def __init__(self, location, content_type): self.location = location self.content_type = content_type
self.cursor = position
chunk = self.data[self.cursor:(self.cursor + chunk_size)] self.cursor += chunk_size return chunk
return { child_descriptor.location: child_descriptor, source_location: source_descriptor }.get(usage_id)
pass
with self.assertRaises(ValueError): self.settings_service.get_settings_bucket(None)
self.assertEqual(settings.XBLOCK_SETTINGS, {self.xblock_setting_key1: 42}) self.assertEqual(self.settings_service.get_settings_bucket(self.xblock_mock), 42)
return self.is_proctored_enabled
self.is_proctored_enabled = value
stack = [node] locations = [] while stack: curr = stack.pop() locations.append(curr.location) if curr.has_children: stack.extend(curr.get_children()) return locations
non_editable_fields = super(SequenceDescriptor, self).non_editable_metadata_fields non_editable_fields.append(self.fields['is_entrance_exam']) return non_editable_fields
pass
self.set_user_module_score(user, None, None)
if isinstance(value, datetime.timedelta) or value is None: return value return self.from_json(value)
bool_dict = [True, "True", "true", "T", "t", "1"] return value in bool_dict
return word.strip().lower()
return dict( sorted( dict_obj.items(), key=lambda x: x[1], reverse=True )[:amount] )
return StaticContent.ASSET_URL_RE.match(path_string) is not None
raise NotImplementedError
raise NotImplementedError
self.fs_files.database.connection.close()
return self.get_attrs(location).get(attr, default)
module_path, _, name = path.rpartition('.') return getattr(import_module(module_path), name)
return get_instructions(xmltree)
return get_instructions(xmltree)
zero = sympy.Symbol('dotzero') identity = sympy.Symbol('dotidentity')
return '<mstyle' in self.expr
return '<math ' in self.expr
pass
if text[-1] in SUFFIXES: return float(text[:-1]) * SUFFIXES[text[-1]] else: return float(text)
return super_float("".join(parse_result))
varname = tokens[0][0] self.variables_used.add(varname)
varname = tokens[0][0] self.functions_used.add(varname)
return 1 / numpy.cos(arg)
return 1 / numpy.sin(arg)
return numpy.arccos(1. / val)
return numpy.arcsin(1. / val)
return 1 / numpy.cosh(arg)
return 1 / numpy.sinh(arg)
return numpy.arccosh(1. / val)
return numpy.arcsinh(1. / val)
if len(children) == 3: return LatexRendered( children[1].latex, parens=children[0].latex, tall=children[1].tall ) else: return children[0]
self.assertEqual(4.0, calc.evaluator({}, {}, '4.'))
self.assertAlmostEqual( -0.28, calc.evaluator({}, {}, 'SiN(6)', case_sensitive=False), delta=1e-3 )
self._each_parens('(x+y)', 'x+y', '(')
self._each_parens('[x+y]', 'x+y', '[')
self._each_parens(r'\{x+y\}', 'x+y', '{')
self._each_parens(r'\left(x^y\right)', 'x^y', '(', tall=True)
self._each_parens(r'\left[x^y\right]', 'x^y', '[', tall=True)
self._each_parens(r'\left\{x^y\right\}', 'x^y', '{', tall=True)
with self.assertRaisesRegexp(Exception, 'Unknown parenthesis'): preview.LatexRendered('x^2', parens='not parens')
self.assertEquals(preview.latex_preview('3.1415'), '3.1415')
self.assertEquals(preview.latex_preview('1.618k'), r'1.618\text{k}')
self.assertEquals(preview.latex_preview('x', variables=['x']), 'x')
self.assertEquals(preview.latex_preview('pi'), r'\pi')
self.assertEquals( preview.latex_preview('epsilon_max', variables=['epsilon_max']), r'\epsilon_{max}' )
self.assertEquals( preview.latex_preview('f(3)', functions=['f']), r'\text{f}(3)' )
self.assertEquals( preview.latex_preview('f(3^2)', functions=['f']), r'\text{f}\left(3^{2}\right)' )
self.assertEquals(preview.latex_preview('sqrt(3)'), r'\sqrt{3}')
self.assertEquals(preview.latex_preview('log10(3)'), r'\log_{10}(3)')
self.assertEquals(preview.latex_preview('log2(3)'), r'\log_2(3)')
self.assertEquals(preview.latex_preview('2^3^4'), '2^{3^{4}}')
self.assertEquals(preview.latex_preview('2^3^(4+5)'), '2^{3^{4+5}}')
self.assertEquals(preview.latex_preview('2||3'), r'2\|3')
self.assertEquals(preview.latex_preview('2*3'), r'2\cdot 3')
self.assertEquals( preview.latex_preview('2*3/4/5'), r'\frac{2\cdot 3}{4\cdot 5}' )
self.assertEquals( preview.latex_preview('(2+3)/(4+5)'), r'\frac{2+3}{4+5}' )
self.assertEquals( preview.latex_preview('2/3*4/5*6'), r'\frac{2}{3}\cdot \frac{4}{5}\cdot 6' )
self.assertEquals( preview.latex_preview('(2+3^2)'), r'\left(2+3^{2}\right)' )
return self._mapping.keys()
self.student_answers = dict() self.correct_map = CorrectMap() self.done = False
maxscore = 0 for responder in self.responders.values(): maxscore += responder.get_max_score() return maxscore
return any(self.correct_map.is_queued(answer_id) for answer_id in self.correct_map)
return self._grade_answers(None)
answer_ids = [] for response in self.responders.keys(): results = self.responder_answers[response] answer_ids.append(results.keys()) return answer_ids
self.do_targeted_feedback(self.tree) html = contextualize_text(etree.tostring(self._extract_html(self.tree)), self.context) return html
print problem.get_html()
pass
pass
pass
return sum(self.maxpoints.values())
new_cmap = self.get_score(student_answers) self.get_hints(convert_files_to_filenames( student_answers), new_cmap, old_cmap) return new_cmap
pass
pass
pass
return hasattr(self, '_has_mask')
return hasattr(self, '_has_shuffle')
return hasattr(self, '_has_answerpool')
self.do_shuffle(self.xml, problem) self.do_answer_pool(self.xml, problem)
return compare_with_tolerance( evaluator({}, {}, ans1), evaluator({}, {}, ans2), self.tolerance )
try: evaluator(dict(), dict(), answer) return True except (StudentInputError, UndefinedVariable): return False
return {self.answer_id: self.initial_display}
internal_result = self.check_formula(ans1, ans2, self.samples) return internal_result == "correct"
var_dict_list = self.randomize_variables(self.samples) try: self.tupleize_answers(answer, var_dict_list) return True except StudentInputError: return False
scoring = self.default_scoring correct_points = scoring.get('correct') return dict([(inputfield.get('id'), correct_points) for inputfield in self.inputfields])
for option in self._find_options(inputfield): if option['choice'] == choice: return option
submitted = self._unpack(student_answer) option_ids = submitted['options_value'] if len(option_ids) == 1: return option_ids[0] return None
return self.answer_values
student_choices = set(choices) required_selected = len(self.correct_choices - student_choices) == 0 no_extra_selected = len(student_choices - self.correct_choices) == 0 correct = required_selected and no_extra_selected return correct
h = hashlib.md5() h.update(str(seed)) return h.hexdigest()
return all(hasattr(file_to_test, method) for method in ['read', 'name'])
return self.cmap
if answer_id in self.cmap: return self.cmap[answer_id]['correctness'] in ['correct', 'partially-correct'] return None
if answer_id in self.cmap: return self.cmap[answer_id]['correctness'] == 'partially-correct' return None
pass
self.assert_has_xpath(xml_root, xpath, context_dict, exact_num=0)
return '<div>{0}</div>'.format(saxutils.escape(repr(context)))
return LoncapaProblem(xml, id='1', seed=seed, capa_system=capa_system or test_capa_system(), capa_module=mock_capa_module())
expected = [(o, o) for o in options] self.assertEqual(f(input), expected)
c=self.cols, tabsize=self.tabsize, m=self.mode, payload=self.payload, ln=self.linenumbers)
element = etree.fromstring(xml_str) state = {'value': 'H2OYeah', } self.the_input = lookup_tag('chemicalequationinput')(test_capa_system(), element, state)
response = self.the_input.handle_ajax("obviously_not_real", {}) self.assertEqual(response, {})
element = etree.fromstring(xml_str) state = {'value': 'x^2+1/2'} self.the_input = lookup_tag('formulaequationinput')(test_capa_system(), element, state)
response = self.the_input.handle_ajax("obviously_not_real", {}) self.assertEqual(response, {})
self.check_group('radiotextgroup', 'choice', 'radio')
self.check_group('checkboxtextgroup', 'choice', 'checkbox')
with self.assertRaises(Exception): self.check_group('invalid', 'choice', 'checkbox')
with self.assertRaisesRegexp(Exception, "Error in xml"): self.check_group('checkboxtextgroup', 'invalid', 'checkbox')
statobj = inputtypes.Status('test') self.assertEqual(str(statobj), 'test') self.assertEqual(unicode(statobj), u'test')
return "str(random.randint(0, 1e9))"
timestr = datetime.strftime(time, dateformat) return {'key': key, 'time': timestr}
problem = self.build_problem(answer="1/3", tolerance=1e-3) correct_responses = ["1/3", "0.333333"] incorrect_responses = [] self.assert_multiple_grade(problem, correct_responses, incorrect_responses)
problem = self.build_problem(answer="1+1j", tolerance=1e-3) self.assert_grade(problem, '1+j', 'correct')
if text == "There was a problem with the staff answer to this problem.": text = "TRANSLATED!" return text
return mapping[math_string]
if math_string != '4': raise err
return self.build_problem( choices=choices, type=in_type, script=script )
with self.assertRaises(Exception): self.build_problem(type="invalidtextgroup")
return eval(xml.text)
element = etree.fromstring(xml_str) renderer = lookup_tag('math')(test_capa_system(), element) self.assertEqual(renderer.mathstr, mathjax_out)
return None
return etree.Element("schematic")
return etree.Element("choiceresponse")
return ResponseXMLFactory.choicegroup_input_xml(**kwargs)
return etree.Element("imageresponse")
return etree.Element("javascriptinput")
return etree.Element('multiplechoiceresponse')
kwargs['choice_type'] = 'multiple' return ResponseXMLFactory.choicegroup_input_xml(**kwargs)
return etree.Element('truefalseresponse')
kwargs['choice_type'] = 'multiple' return ResponseXMLFactory.choicegroup_input_xml(**kwargs)
return etree.Element("optionresponse")
return etree.Element("annotationresponse")
return etree.Element("choicetextresponse")
md5er = hashlib.md5() update_hash(md5er, obj) return md5er.hexdigest()
return []
return {}
return [Attribute('options', transform=cls.parse_options), Attribute('label', ''), Attribute('inline', False)]
return [Attribute('params', None), Attribute('problem_state', None), Attribute('display_class', None), Attribute('display_file', None), ]
return json.dumps(files.split())
return {'queue_len': self.queue_len, }
if self.status in ['correct', 'incorrect', 'partially-correct']: return False else: return True
return [Attribute('src'), Attribute('height'), Attribute('label', ''), Attribute('width'), ]
return [Attribute('height'), Attribute('width'), Attribute('molecules'), Attribute('geometries'), ]
return [Attribute('size', '20'), Attribute('label', ''), ]
if dispatch == 'preview_chemcalc': return self.preview_chemcalc(data) return {}
if dispatch == 'preview_formcalc': return self.preview_formcalc(get) return {}
return [Attribute('file'), Attribute('missing', None)]
return [Attribute('width'), Attribute('height'), Attribute('target_shape') ]
return [Attribute('genex_dna_sequence'), Attribute('genex_problem_number') ]
return { 'input_type': self.html_input_type, 'choices': self.choices }
if LOCAL_DEBUG: print msg if output_type == 'html': f.write(msg + '\n<br>\n')
return a * b / fr.gcd(a, b)
try: return _render_to_html(_get_final_tree(ex)) except ParseException: return err(ex)
if "tags" in kwargs: kwargs["tags"] = _clean_tags(kwargs["tags"]) dog_stats_api.increment(metric_name, *args, **kwargs)
if "tags" in kwargs: kwargs["tags"] = _clean_tags(kwargs["tags"]) dog_stats_api.histogram(metric_name, *args, **kwargs)
from setuptools import setup setup( name="safe_lxml", version="1.0", packages=["safe_lxml"], install_requires=[ "lxml", "defusedxml" ], )
from defusedxml import defuse_stdlib defuse_stdlib() import lxml import lxml.etree from . import etree as safe_etree lxml.etree = safe_etree
def __init__(self, *args, **kwargs): if "resolve_entities" not in kwargs: kwargs["resolve_entities"] = False super(XMLParser, self).__init__(*args, **kwargs)
if uid is None: return None try: return User.objects.get(anonymoususerid__anonymous_user_id=uid) except ObjectDoesNotExist: return None
return self.profile_image_uploaded_at is not None
year_of_birth = self.year_of_birth year = datetime.now(UTC).year if year_of_birth is not None: return self._calculate_age(year, year_of_birth)
if self.level_of_education: return self.__enumerable_to_display(self.LEVEL_OF_EDUCATION_CHOICES, self.level_of_education)
return dict(enumerables)[enum_value]
return cls.PROFILE_COUNTRY_CACHE_KEY.format(user_id=user_id)
user = kwargs['instance'] user._changed_fields = get_changed_fields_dict(user, sender)
user = models.ForeignKey(User, db_index=True) site = models.CharField(max_length=255, db_index=True)
self.new_email = email self.activation_key = uuid.uuid4().hex self.save() return self.activation_key
if not settings.FEATURES['ADVANCED_SECURITY']: return False min_diff_pw = settings.ADVANCED_SECURITY_CONFIG.get( 'MIN_DIFFERENT_STUDENT_PASSWORDS_BEFORE_REUSE', 0 ) return min_diff_pw > 0
if not settings.FEATURES['ADVANCED_SECURITY']: return False min_diff_pw = settings.ADVANCED_SECURITY_CONFIG.get( 'MIN_DIFFERENT_STAFF_PASSWORDS_BEFORE_REUSE', 0 ) return min_diff_pw > 0
if not settings.FEATURES['ADVANCED_SECURITY']: return False min_days_between_reset = settings.ADVANCED_SECURITY_CONFIG.get( 'MIN_TIME_IN_DAYS_BETWEEN_ALLOWED_RESETS' ) return min_days_between_reset
if not settings.FEATURES['ADVANCED_SECURITY']: return False min_days_between_reset = settings.ADVANCED_SECURITY_CONFIG.get( 'MIN_DAYS_FOR_STAFF_ACCOUNTS_PASSWORD_RESETS' ) return min_days_between_reset
if not settings.FEATURES['ADVANCED_SECURITY']: return False min_days_pw_reset = settings.ADVANCED_SECURITY_CONFIG.get( 'MIN_DAYS_FOR_STUDENT_ACCOUNTS_PASSWORD_RESETS' ) return min_days_pw_reset
records = LoginFailures.objects.filter(user=user).order_by('-lockout_until') for extra_record in records[1:]: extra_record.delete() return records.get()
return settings.FEATURES['ENABLE_MAX_FAILED_LOGIN_ATTEMPTS']
try: entry = cls._get_record_for_user(user) entry.delete() except ObjectDoesNotExist: return
enrollment_number = super(CourseEnrollmentManager, self).get_queryset().filter( course_id=course_id, is_active=1 ).count() return enrollment_number
is_course_full = False if course.max_student_enrollments_allowed is not None: is_course_full = self.num_enrolled_in_exclude_admins(course.id) >= course.max_student_enrollments_allowed return is_course_full
return User.objects.filter( courseenrollment__course_id=course_id, courseenrollment__is_active=True )
return User.objects.filter( courseenrollment__course_id=course_id )
try: return cls.objects.get( user=user, course_id=course_key ) except cls.DoesNotExist: return None
paid_course = CourseMode.is_white_label(self.course_id) if paid_course or CourseMode.is_professional_slug(self.mode): return True return False
self.update_enrollment(is_active=True)
self.update_enrollment(is_active=False)
self.update_enrollment(mode=mode)
cache_key = CourseEnrollment.cache_key_name( instance.user.id, unicode(instance.course_id) ) cache.delete(cache_key)
return cls.objects.create( enrolled_by=user, enrolled_email=email, state_transition=state_transition, reason=reason, enrollment=enrollment )
try: manual_enrollment = cls.objects.filter(enrolled_email=email).latest('time_stamp') except cls.DoesNotExist: manual_enrollment = None return manual_enrollment
try: manual_enrollment = cls.objects.filter(enrollment=enrollment).latest('time_stamp') except cls.DoesNotExist: manual_enrollment = None return manual_enrollment
enrolled = CourseEnrollment.objects.users_enrolled_in(course_id=course_id).values_list('email', flat=True) return CourseEnrollmentAllowed.objects.filter(course_id=course_id).exclude(email__in=enrolled)
return (self.role, self.org, self.course_id, self.user_id)
if '@' in username_or_email: return User.objects.get(email=username_or_email) else: return User.objects.get(username=username_or_email)
return self.MODE_TO_CERT_NAME.get( cert_mode, _(u"{platform_name} Certificate for {course_name}") ).format( platform_name=microsite.get_value('platform_name', settings.PLATFORM_NAME), course_name=course_name )
can_skip = False if is_entrance_exams_enabled(): try: record = EntranceExamConfiguration.objects.get(user=user, course_id=course_key) can_skip = record.skip_entrance_exam except EntranceExamConfiguration.DoesNotExist: can_skip = False return can_skip
return u"{namespace}:{name}, {value}".format( namespace=self.namespace, name=self.name, value=self.value, )
return timedelta(microseconds=self.refund_window_microseconds)
self.refund_window_microseconds = int(refund_window.total_seconds() * 1000000)
return u"[{username}] {name}: {value}".format( name=self.name, value=self.value, username=self.user.username, )
cls.objects.filter(user=user, name=name).delete() cls.objects.create(user=user, name=name, value=value)
return has_studio_write_access(user, course_key)
return bool(STUDIO_VIEW_CONTENT & get_user_permissions(user, course_key))
_check_caller_authority(caller, role) role.add_users(*users)
return survey_link.format(UNIQUE_ID=unique_id_for_user(user))
email_opt_in = request.POST.get('email_opt_in') if email_opt_in is not None: email_opt_in_boolean = email_opt_in == 'true' preferences_api.update_email_opt_in(request.user, org, email_opt_in_boolean)
affiliate_id = request.COOKIES.get(settings.AFFILIATE_COOKIE_NAME) if user is not None and affiliate_id is not None: UserAttribute.set_user_attribute(user, settings.AFFILIATE_COOKIE_NAME, affiliate_id)
try: role_name = cls.ROLE REGISTERED_ACCESS_ROLES[role_name] = cls except AttributeError: log.exception(u"Unable to register Access Role with attribute 'ROLE'.") return cls
return any( access_role.role == role and access_role.course_id == course_id and access_role.org == org for access_role in self._roles )
return False
pass
pass
return User.objects.none()
super(CourseRole, self).__init__(role, course_key.org, course_key)
def __init__(self, role, org): super(OrgRole, self).__init__(role, org)
ROLE = 'staff' def __init__(self, *args, **kwargs): super(CourseStaffRole, self).__init__(self.ROLE, *args, **kwargs)
ROLE = 'instructor' def __init__(self, *args, **kwargs): super(CourseInstructorRole, self).__init__(self.ROLE, *args, **kwargs)
ROLE = 'finance_admin' def __init__(self, *args, **kwargs): super(CourseFinanceAdminRole, self).__init__(self.ROLE, *args, **kwargs)
ROLE = 'sales_admin' def __init__(self, *args, **kwargs): super(CourseSalesAdminRole, self).__init__(self.ROLE, *args, **kwargs)
ROLE = 'beta_testers' def __init__(self, *args, **kwargs): super(CourseBetaTesterRole, self).__init__(self.ROLE, *args, **kwargs)
ROLE = 'library_user' def __init__(self, *args, **kwargs): super(LibraryUserRole, self).__init__(self.ROLE, *args, **kwargs)
ROLE = 'ccx_coach' def __init__(self, *args, **kwargs): super(CourseCcxCoachRole, self).__init__(self.ROLE, *args, **kwargs)
def __init__(self, *args, **kwargs): super(OrgStaffRole, self).__init__('staff', *args, **kwargs)
def __init__(self, *args, **kwargs): super(OrgInstructorRole, self).__init__('instructor', *args, **kwargs)
ROLE = LibraryUserRole.ROLE def __init__(self, *args, **kwargs): super(OrgLibraryUserRole, self).__init__(self.ROLE, *args, **kwargs)
ROLE = "course_creator_group" def __init__(self, *args, **kwargs): super(CourseCreatorRole, self).__init__(self.ROLE, *args, **kwargs)
ROLE = "support" def __init__(self, *args, **kwargs): super(SupportStaffRole, self).__init__(self.ROLE, *args, **kwargs)
self.user = user self.role = role
def value_from_datadict(self, data, files, name): value = data.get(name, '') return value.lower() == 'true'
widget = TrueCheckbox
try: year_str = self.cleaned_data["year_of_birth"] return int(year_str) if year_str is not None else None except ValueError: return None
return { key: value for key, value in self.cleaned_data.items() if key in self.extended_profile_fields and value is not None }
for idx in range(num): (user, _, _) = _do_create_account(make_random_form()) if course_key is not None: CourseEnrollment.enroll(user, course_key)
pass
pass
self.check_groups(group_permissions.keys()) for group_name, permission_codenames in group_permissions.items(): self.check_permissions(group_name, permission_codenames)
self.assertFalse(self.signal_fired) self.assertTrue(skip_refund) self.signal_fired = True
return CourseFactory.create( org=course_location.org, number=course_location.course, run=course_location.run )
user.set_password(password) user.save() history = PasswordHistory() history.create(user)
student = self._user_factory_with_history() self.assertFalse(PasswordHistory.is_password_reset_too_soon(student))
course_mode_info = self._enrollment_with_complete_course('honor') self.assertTrue(course_mode_info['show_upsell']) self.assertEquals(course_mode_info['days_for_upsell'], 1)
course_mode_info = self._enrollment_with_complete_course(enrollment_mode) self.assertFalse(course_mode_info['show_upsell']) self.assertIsNone(course_mode_info['days_for_upsell'])
if 'truncated' not in kwargs: kwargs['truncated'] = [] self.assert_event_emitted( USER_SETTINGS_CHANGED_EVENT_NAME, table=self.table, user_id=self.user.id, **kwargs )
response = self.client.post( reverse('change_enrollment'), { 'course_id': course.id.to_deprecated_string(), 'enrollment_action': 'enroll' } ) return response
role = CourseStaffRole(self.course_key) role.add_users(self.student) self.assertGreater(len(role.users_with_role()), 0)
self.client.logout() super(TestCourseListing, self).tearDown()
return force_text(urlsafe_base64_encode(force_bytes(base36_to_int(uidb36 or self.uidb36))))
self.assertTrue(user_has_role(self.user, CourseCreatorRole()))
self.assertTrue(self.enrollment.refundable())
if name == 'SITE_NAME': return 'openedx.localhost' else: return default
return FAKE_MICROSITE.get(name, default)
response = self.client.post(self.url, self.params) self.assertEqual(response.status_code, 400)
self.profile.year_of_birth = year_of_birth self.profile.save()
self.profile.level_of_education = level_of_education self.profile.save()
self.profile.gender = gender self.profile.save()
self.assertIsNone(self.profile.age)
self._set_level_of_education(level_enum) self.assertEqual(self.profile.level_of_education_display, display_level)
self.assertIsNone(self.profile.level_of_education_display)
self._set_gender(gender_enum) self.assertEqual(self.profile.gender_display, display_gender)
self._set_gender(None) self.assertIsNone(self.profile.gender_display)
if self.cert_status is not None: return { 'status': self.cert_status, 'can_unenroll': self.cert_status not in DISABLE_UNENROLL_CERT_STATES } else: return {}
with patch('student.views.cert_info', return_value=None): response = self.client.get(reverse('dashboard')) self.assertEqual(response.status_code, 200)
course = CourseFactory.create( org=course_location.org, number=course_location.course, run=course_location.run ) enrollment = CourseEnrollment.enroll(self.student, course.id) return course, enrollment
config = DashboardConfiguration(recent_enrollment_time_delta=timeout) config.save()
pass
return str((template_name, sorted(context.iteritems())))
settings.ALLOWED_HOSTS.append(hostname) self.addCleanup(settings.ALLOWED_HOSTS.pop)
return json.loads(reactivation_email_for_user(user).content)
response_data = self.reactivation_email(self.unregisteredUser) self.assertFalse(response_data['success'])
try: validate_new_email(self.request.user, email) except ValueError as err: return err.message
try: do_email_change_request(user, email, activation_key) except ValueError as err: return err.message
self.assertFalse(response_data['success']) self.assertEquals(expected_error, response_data['error']) self.assertFalse(self.user.email_user.called)
for email in ('bad_email', 'bad_email@', '@bad_email'): self.assertEqual(self.do_email_validation(email), 'Valid e-mail address required.')
self.assertEqual(self.do_email_validation(self.user.email), 'Old email is the same as the new email.')
UserFactory.create(email=self.new_email) self.assertEqual(self.do_email_validation(self.new_email), 'An account with this e-mail already exists.')
self.assertRolledBack() self.assertFalse(email_user.called)
response = confirm_email_change(self.request, self.key) self.assertEquals( mock_render_to_response(expected_template, expected_context).content, response.content )
self.profile.year_of_birth = year_of_birth self.profile.save()
self.assertTrue(self.profile.requires_parental_consent()) self.assertTrue(self.profile.requires_parental_consent(default_requires_consent=True)) self.assertFalse(self.profile.requires_parental_consent(default_requires_consent=False))
self.profile.meta = {u'foo': u'bar'} self.profile.save() self.assert_no_events_were_emitted()
self.profile.gender = "unknown" with self.assertRaises(IntegrityError): self.profile.save() self.assert_no_events_were_emitted()
self.user.password = u'new password' self.user.save() self.assert_user_setting_event_emitted(setting='password', old=None, new=None)
self.user.passwordhistory_set.add(PasswordHistory(password='new_password')) self.user.save() self.assert_no_events_were_emitted()
self.user.password = u'new password' with self.assertRaises(IntegrityError): self.user.save() self.assert_no_events_were_emitted()
self._auto_auth() self.client.logout() self._auto_auth() self.assertEqual(User.objects.all().count(), 2)
return self.client.get(reverse("dashboard"))
credit_api.update_credit_request_status(uuid, self.PROVIDER_ID, status)
self.base_extauth_bypass_sending_activation_email(True)
self.base_extauth_bypass_sending_activation_email(False)
self.base_extauth_bypass_sending_activation_email(True)
affiliate_id = 'test-partner' self.client.cookies[settings.AFFILIATE_COOKIE_NAME] = affiliate_id user = self.create_account_and_fetch_profile().user self.assertEqual(UserAttribute.get_user_attribute(user, settings.AFFILIATE_COOKIE_NAME), affiliate_id)
self.assert_error(params, "honor_code", expected_error)
self.assert_error(params, field, expected_error)
return u"{}?{}".format(reverse('finish_auth'), urllib.urlencode(params))
return True
self._create_certificate(CourseMode.NO_ID_PROFESSIONAL_MODE) self._check_can_download_certificate_no_id()
for cookie_name in [settings.EDXMKTG_LOGGED_IN_COOKIE_NAME, settings.EDXMKTG_USER_INFO_COOKIE_NAME]: response.delete_cookie( cookie_name.encode('utf-8'), path='/', domain=settings.SESSION_COOKIE_DOMAIN ) return response
dashboard_url = reverse('dashboard') self._test_change_session_hash(dashboard_url, reverse('signin_user') + '?next=' + dashboard_url)
cache.delete_many([instance_key(model, x) for x in instance_or_pk])
class Meta(ConfigurationModel.Meta): app_label = "util"
urlconf = settings.ROOT_URLCONF if urlconf and urlconf in sys.modules: reload(sys.modules[urlconf]) reloaded = import_module(urlconf) reloaded_urls = reloaded.urlpatterns set_urlconf(tuple(reloaded_urls))
return False if str is None else str.lower() == "true"
self.mock_tracker.reset_mock()
def test_patch_unsupported_media_type(self): response = self.client.patch( self.url, json.dumps({}), content_type=self.unsupported_media_type ) self.assertEqual(response.status_code, 415)
CommitOnSuccessManager.ENABLED = False OuterAtomic.ALLOW_NESTED = True if not hasattr(OuterAtomic, 'atomic_for_testcase_calls'): OuterAtomic.atomic_for_testcase_calls = 0 OuterAtomic.atomic_for_testcase_calls += 1 return wrapped_func(*args, **kwargs)
CommitOnSuccessManager.ENABLED = True OuterAtomic.ALLOW_NESTED = False OuterAtomic.atomic_for_testcase_calls -= 1 return wrapped_func(*args, **kwargs)
md4 = hashlib.new("md4") md4.update(string) return md4.hexdigest()
return urllib.quote_plus(smart_str(val))
counts = self.get_counters(request) return sum(counts.values()) >= self.requests
return abs(dt1 - dt2) < allowed_delta
return int((datetime_value - datetime(1970, 1, 1, tzinfo=UTC)).total_seconds())
try: return datetime.utcfromtimestamp(int(timestamp)).replace(tzinfo=UTC) except (ValueError, TypeError): return None
return NAMESPACE_CHOICES
return settings.FEATURES.get('ENTRANCE_EXAMS', False)
return settings.FEATURES.get('ENABLE_PREREQUISITE_COURSES', False) \ and settings.FEATURES.get('MILESTONES_APP', False)
if not is_prerequisite_courses_enabled(): return None from milestones import api as milestones_api milestones_api.remove_course_milestone( course_key, milestone, )
return ' '.join([ descriptor.display_org_with_default, descriptor.display_number_with_default ])
if not settings.FEATURES.get('MILESTONES_APP', False): return None from milestones import api as milestones_api return milestones_api.get_user_milestones({'id': user.id}, namespace)
try: course_key = CourseKey.from_string(key) except InvalidKeyError: course_key = key return isinstance(course_key, CourseKey)
if namespace in NAMESPACE_CHOICES.values(): if namespace == 'entrance_exams': return '{}.{}'.format(unicode(course_key), NAMESPACE_CHOICES['ENTRANCE_EXAM'])
return { 'id': user.id, }
if not settings.FEATURES.get('MILESTONES_APP', False): return None from milestones import api as milestones_api return milestones_api.add_milestone(milestone_data)
if not settings.FEATURES.get('MILESTONES_APP', False): return [] from milestones import api as milestones_api return milestones_api.get_milestones(namespace)
if not settings.FEATURES.get('MILESTONES_APP', False): return {} from milestones import api as milestones_api return milestones_api.get_milestone_relationship_types()
if not settings.FEATURES.get('MILESTONES_APP', False): return None from milestones import api as milestones_api return milestones_api.add_course_milestone(course_id, relationship, milestone)
if not settings.FEATURES.get('MILESTONES_APP', False): return [] from milestones import api as milestones_api return milestones_api.get_course_milestones(course_id)
if not settings.FEATURES.get('MILESTONES_APP', False): return None from milestones import api as milestones_api return milestones_api.add_course_content_milestone(course_id, content_id, relationship, milestone)
if not settings.FEATURES.get('MILESTONES_APP', False): return [] from milestones import api as milestones_api return milestones_api.get_course_content_milestones(course_id, content_id, relationship)
if not settings.FEATURES.get('MILESTONES_APP', False): return None from milestones import api as milestones_api return milestones_api.remove_content_references(content_id)
if not settings.FEATURES.get('MILESTONES_APP', False): return False return bool( get_course_milestones_fulfillment_paths(course_id, {"id": user_id}) )
if not settings.FEATURES.get('MILESTONES_APP', False): return None from milestones import api as milestones_api return milestones_api.get_course_milestones_fulfillment_paths( course_id, user_id )
ticket_url = self._zendesk_instance.create_ticket(data=ticket) return zendesk.get_id_from_url(ticket_url)
self._zendesk_instance.update_ticket(ticket_id=ticket_id, data=update)
accept = parse_accept_header(request.META.get("HTTP_ACCEPT", "")) return media_type in [t for (t, p, q) in accept]
if not organizations_enabled(): return None from organizations import api as organizations_api return organizations_api.add_organization(organization_data=organization_data)
if not organizations_enabled(): return None from organizations import api as organizations_api return organizations_api.add_organization_course(organization_data=organization_data, course_key=course_id)
if not organizations_enabled(): return [] from organizations import api as organizations_api return organizations_api.get_organization(organization_id)
if not organizations_enabled(): return None from organizations import api as organizations_api from organizations.exceptions import InvalidOrganizationException try: return organizations_api.get_organization_by_short_name(organization_short_name) except InvalidOrganizationException: return None
if not organizations_enabled(): return [] from organizations import api as organizations_api return organizations_api.get_organization_courses(organization_id)
if not organizations_enabled(): return [] from organizations import api as organizations_api return organizations_api.get_course_organizations(course_id)
user = User.objects.get(id=user_id) return anonymous_id_for_user(user, None)
RateLimitConfiguration = apps.get_model("util", "RateLimitConfiguration") objects = RateLimitConfiguration.objects if not objects.exists(): objects.create(enabled=True)
if used_ids is None: used_ids = [] cid = random.randint(minimum, maximum) while cid in used_ids: cid = random.randint(minimum, maximum) return cid
def __contains__(self, item): return True def __getitem__(self, item): return "notmigrations"
pass
return string.replace('\r\n', '\n').replace('\r', '\n')
stack = [descriptor] while len(stack) > 0: next_descriptor = stack.pop() stack.extend(get_dynamic_descriptor_children(next_descriptor, user_id, module_creator)) yield next_descriptor
return
return
minimum = 1 maximum = times for i in range(times): self.assertIn(generate_int_id(minimum, maximum), range(minimum, maximum + 1))
test_string = "%%user_id%%" result = Ks.substitute_keywords_with_data( test_string, self.context, ) self.assertEquals(test_string, result)
test_string = "this string has no subtags" result = Ks.substitute_keywords_with_data( test_string, self.context, ) self.assertEquals(test_string, result)
for cache in caches.all(): self.assertIsNone(cache.get(key)) cache.set(key, "Not None")
self.check_caches("mstc_cache_test_key")
self.check_caches("smstc_cache_test_key")
self.assertEqual(expected_message, error.exception.message)
self.assertEqual(should_exist, validator_data["storage"].exists(validator_data["filename"]))
validator_data["storage"] = storage validator_data["filename"] = filename verify_file_presence(True)
self.assertTrue("success_file" in os.path.basename(filename)) store_file_data(storage, filename)
self.assertTrue(storage.exists(file_name)) with storage.open(file_name, 'r') as f: self.assertEqual(expected_content, f.read())
def utcoffset(self, _dt): return timedelta(hours=-3) def dst(self, _dt): return timedelta(0)
response = organizations_helpers.get_organization_by_short_name(self.organization['short_name']) self.assertIsNone(response)
resp = self._build_and_run_request(user, fields) self.assertEqual(resp.status_code, 200)
with self.assertRaises(Http404): self._build_and_run_request(self._anon_user, self._anon_fields)
return cls.current().cache_ttl
return cls.current().cdn_user_agents
super(ContentStoreToyCourseTest, self).setUp() self.staff_usr = AdminFactory.create() self.non_staff_usr = UserFactory.create() self.client = Client()
self.client.logout() resp = self.client.get(self.url_unlocked) self.assertEqual(resp.status_code, 200)
self.client.logout() resp = self.client.get(self.url_locked) self.assertEqual(resp.status_code, 403)
self.client.login(username=self.non_staff_usr, password='test') resp = self.client.get(self.url_locked) self.assertEqual(resp.status_code, 403)
self.client.login(username=self.staff_usr, password='test') resp = self.client.get(self.url_locked) self.assertEqual(resp.status_code, 200)
resp = self.client.get(self.url_unlocked, HTTP_RANGE=header_value) self.assertEqual(resp.status_code, 200) self.assertNotIn('Content-Range', resp)
resp = self.client.get(self.url_unlocked) self.assertEqual(resp.status_code, 200) self.assertEquals('Origin', resp['Vary'])
return self.list_display
return self.list_display
return ( request.path.startswith('/' + XASSET_LOCATION_TAG + '/') or request.path.startswith('/' + AssetLocator.CANONICAL_NAMESPACE) )
expire_dt = now + datetime.timedelta(seconds=cache_ttl) return expire_dt.strftime(HTTP_DATE_FORMAT)
return bool(getattr(content, "locked", False))
config = cls.current() if not config.enabled: return False return block_type in config.disabled_blocks.split()
config = cls.current() if not config.enabled: return () return config.disabled_blocks.split()
return self._convert_django_user_to_xblock_user(self._django_user)
self.assertFalse(xb_user.opt_attrs[ATTR_KEY_IS_AUTHENTICATED]) self.assertIsNone(xb_user.full_name) self.assertListEqual(xb_user.emails, [])
django_user_service = DjangoXBlockUserService(self.anon_user) xb_user = django_user_service.get_current_user() self.assertTrue(xb_user.is_current_user) self.assert_is_anon_xb_user(xb_user)
django_user_service = DjangoXBlockUserService(self.user) xb_user = django_user_service.get_current_user() self.assertTrue(xb_user.is_current_user) self.assert_xblock_user_matches_django(xb_user, self.user)
django_user_service = DjangoXBlockUserService(self.user, user_is_staff=False) anonymous_user_id = django_user_service.get_anonymous_user_id(username=self.user.username, course_id='edx/toy/2012_Fall') self.assertIsNone(anonymous_user_id)
django_user_service = DjangoXBlockUserService(self.user, user_is_staff=True) anonymous_user_id = django_user_service.get_anonymous_user_id(username="No User", course_id='edx/toy/2012_Fall') self.assertIsNone(anonymous_user_id)
XBlockDisableConfig.objects.create( disabled_create_blocks=xblocks, enabled=True ) self.assertEqual( XBlockDisableConfig.disabled_create_block_types(), expected_result )
self.assertEqual(XBlockDisableConfig.disabled_create_block_types(), ['poll', 'survey'])
XBlockDisableConfig.objects.create( disabled_create_blocks='annotatable', enabled=True ) self.assertEqual(XBlockDisableConfig.disabled_create_block_types(), ['annotatable', 'poll', 'survey'])
attribute = getattr(module, attribute_name) return hasattr(attribute, __BACKUP_ATTRIBUTE_NAME)
attribute = getattr(module, attribute_name) setattr(attribute_replacement, __BACKUP_ATTRIBUTE_NAME, attribute) setattr(module, attribute_name, attribute_replacement) return is_patched(module, attribute_name)
if request is not None and hasattr(request, 'META') and header_name in request.META: return request.META[header_name] else: return default
world.browser.execute_script('window.confirm = function(){return true;} ; window.alert = function(){return;}')
world.browser.execute_script('window.confirm = function(){return false;} ; window.alert = function(){return;}')
world.browser.execute_script('window.prompt = function(){return %s;}') % prompt
import ipdb ipdb.set_trace() assert True
return {"username": settings.SAUCE.get('USERNAME'), "access-key": settings.SAUCE.get('ACCESS_ID')}
LOGGER.debug("Flushing the test database...") call_command('flush', interactive=False, verbosity=0) world.absorb({}, 'scenario_dict')
world.auto_capture_screenshots = False
service = SERVICES.get(name, None) if service: fake_server = service['class'](port_num=service['port']) setattr(world, name, fake_server)
self._send_handler_response("GET")
self._send_handler_response("POST")
if self.path.startswith("/set_config"): return StubHttpRequestHandler.do_PUT(self) self._send_handler_response("PUT")
self._send_handler_response("DELETE")
if self.server.delete_note(note_id): self.respond(204, "No Content") else: self.respond(404, "404 Not Found")
query_params = deepcopy(query_params) query_params.update({ "page": page_num, "page_size": page_size }) return url_path + "?" + urlencode(query_params)
self.server.cleanup() self.respond()
notes = deepcopy(self.notes) notes.reverse() return notes
if not isinstance(notes, list): notes = [notes] for note in notes: self.notes.append(note)
note = self.filter_by_id(self.notes, note_id) if note: note[0].update(note_info) return note else: return None
note = self.filter_by_id(self.notes, note_id) if note: index = self.notes.index(note[0]) self.notes.pop(index) return True else: return False
self.notes = list()
return self.filter_by(data, "id", note_id)
return self.filter_by(data, "user", user)
return self.filter_by(data, "usage_id", usage_id)
return self.filter_by(data, "course_id", course_id)
return [note for note in data if note.get(field_name) == value]
self.send_response(200, 'This is LTI Provider.', {'Content-type': 'text/plain'})
data = payload.format(score=0.8) return self._send_lti2(data)
return self._send_lti2(payload)
lti_endpoint = self.server.config.get('lti_endpoint', self.DEFAULT_LTI_ENDPOINT) return lti_endpoint in self.path
root_dir = self.server.config.get('root_dir') path = '{}{}'.format(root_dir, path) return path.split('?')[0]
notes = self.server.get_all_notes() self.assertGreater(len(notes), 0, "Notes are empty.") return notes
def __init__(self, delay, func): self.func = func def start(self): self.func()
self.launch_uri = self.uri + 'wrong_lti_endpoint' response = requests.post(self.launch_uri, data=self.payload) self.assertIn('Invalid request URL', response.content)
response = requests.post(self.launch_uri, data=self.payload) self.assertIn('Wrong LTI signature', response.content)
response = requests.post(self.launch_uri, data=self.payload) self.assertIn('This is LTI tool. Success.', response.content)
LOGGER.debug(self._format_msg(format_str, *args))
LOGGER.error(self._format_msg(format_str, *args))
try: length = int(self.headers.getheader('content-length')) except (TypeError, ValueError): return "" else: return self.rfile.read(length)
path = urlparse.urlparse(self.path).path if path.endswith('/'): return path[:-1] else: return path
self.send_response(200, json.dumps(content), {"Content-Type": "application/json"})
if not args: format_str = urllib.unquote(format_str) return u"{0} - - [{1}] {2}\n".format( self.client_address[0], self.log_date_time_string(), format_str % args )
self.send_response(200)
return 'xqueue/submit' in self.path
pass
wait_for( func=lambda _: EC.presence_of_element_located((By.CSS_SELECTOR, css_selector,)), timeout=timeout, timeout_msg="Timed out waiting for {} to be present.".format(css_selector) )
wait_for( func=lambda _: css_visible(css_selector, index), timeout=timeout, timeout_msg="Timed out waiting for {} to be visible.".format(css_selector) )
wait_for( func=lambda _: EC.invisibility_of_element_located((By.CSS_SELECTOR, css_selector,)), timeout=timeout, timeout_msg="Timed out waiting for {} to be invisible.".format(css_selector) )
wait_for( func=lambda _: EC.element_to_be_clickable((By.CSS_SELECTOR, css_selector,)), timeout=timeout, timeout_msg="Timed out waiting for {} to be clickable.".format(css_selector) )
wait_for_present(css_selector=css, timeout=wait_time) return world.browser.find_by_css(css)
css_click('#{}'.format(elem_id))
wait_for_visible(css_selector, index=index) retry_on_exception(lambda: css_find(css_selector)[index].fill(text)) wait_for(lambda _: css_has_value(css_selector, text, index=index)) return True
assert is_css_present(css_selector) return retry_on_exception(lambda: css_find(css_selector)[index].html)
urls = get_xmodule_urls() return HttpResponse(json.dumps(urls), content_type="application/json")
if value is not UNSET: dct[key] = value
self.assertEquals( value, request.META.get('HTTP_ACCEPT_LANGUAGE', UNSET) )
self.assertEquals( value, request.session.get(LANGUAGE_SESSION_KEY, UNSET) )
DarkLangConfig = apps.get_model("dark_lang", "DarkLangConfig") objects = DarkLangConfig.objects if not objects.exists(): objects.create(enabled=True)
language_options = DarkLangConfig.current().released_languages_list if settings.LANGUAGE_CODE not in language_options: language_options.append(settings.LANGUAGE_CODE) return language_options
if not DarkLangConfig.current().enabled: return self._clean_accept_headers(request) self._activate_preview_language(request)
return "{};q={}".format(lang, priority)
return self.mode_slug
return self._expiration_datetime
for mode in cls.VERIFIED_MODES: if mode in course_mode_dict: return True return False
return cls.PROFESSIONAL in modes_dict or cls.NO_ID_PROFESSIONAL_MODE in modes_dict
return course_mode_tuple.slug in [cls.PROFESSIONAL, cls.NO_ID_PROFESSIONAL_MODE] if course_mode_tuple else False
return slug in [cls.PROFESSIONAL, cls.NO_ID_PROFESSIONAL_MODE]
return course_mode_tuple.slug in cls.VERIFIED_MODES
return mode_slug in cls.VERIFIED_MODES
return course_mode_tuple.slug in cls.CREDIT_MODES
if modes_dict is None: modes_dict = cls.modes_for_course_dict(course_id) if cls.HONOR in modes_dict: return cls.HONOR elif cls.AUDIT in modes_dict: return cls.AUDIT
modes = cls.modes_for_course(course_id) return min(mode.min_price for mode in modes if mode.currency.lower() == currency.lower())
return Mode( self.mode_slug, self.mode_display_name, self.min_price, self.suggested_prices, self.currency, self.expiration_datetime, self.description, self.sku, self.bulk_sku )
return super(ChooseModeView, self).dispatch(*args, **kwargs)
if self.cleaned_data.get("verification_deadline"): return self.cleaned_data.get("verification_deadline").replace(tzinfo=UTC)
super(AdminCourseModeFormTest, self).setUp() self.course = CourseFactory.create()
return CourseMode.objects.get_or_create( course_id=self.course_key, mode_display_name=mode_name, mode_slug=mode_slug, min_price=min_price, suggested_prices=suggested_prices, currency=currency, _expiration_datetime=expiration_datetime, )
self.assertEqual(CourseMode.is_eligible_for_certificate(mode_slug), expected_eligibility)
return CourseMode.objects.get_or_create( course_id=self.course.id, mode_display_name=mode_name, mode_slug=mode_slug, min_price=min_price, suggested_prices=suggested_prices, currency=currency, _expiration_datetime=expiration_datetime, )
return cls.current().enabled
key_fields = key_fields or cls.KEY_FIELDS return 'configuration/{}/key_values/{}'.format(cls.__name__, ','.join(key_fields))
cache_timeout = 300 string_field = models.TextField() int_field = models.IntegerField(default=10)
perms_map = DjangoModelPermissions.perms_map.copy() perms_map['GET'] = perms_map['OPTIONS'] = perms_map['HEAD'] = perms_map['POST']
view = super(AtomicMixin, cls).as_view(**initkwargs) return cls.create_atomic_wrapper(view)
model = self.model
return True
return queryset
return [self.parameter_name]
return self.get_displayable_field_names() + ['edit_link']
pass
return ApiKeyHeaderPermission().has_permission(request, self)
rate = '40/minute' def allow_request(self, request, view): return self.has_api_key_permissions(request) or super(EnrollmentUserThrottle, self).allow_request(request, view)
course_modes = CourseMode.modes_for_course( obj.id, include_expired=self.include_expired, only_selectable=False ) return [ ModeSerializer(mode).data for mode in course_modes ]
course_id = course.id if course else self.course.id for mode_slug in course_modes: CourseModeFactory.create( course_id=course_id, mode_slug=mode_slug, mode_display_name=mode_slug, )
with self.assertRaises(UserOrgTag.DoesNotExist): UserOrgTag.objects.get(user=self.user, org=self.course.id.org, key="email-optin")
self.assert_enrollment_status(username='fake-user', expected_status=status.HTTP_404_NOT_FOUND, as_server=False) self.assert_enrollment_status(username='fake-user', expected_status=status.HTTP_406_NOT_ACCEPTABLE, as_server=True)
resp = self.client.get(self.url) return json.loads(resp.content)
return _ENROLLMENTS
return _get_fake_enrollment(student_id, course_id)
return add_enrollment(student_id, course_id, mode=mode, is_active=is_active)
enrollment = _get_fake_enrollment(student_id, course_id) if enrollment and mode is not None: enrollment['mode'] = mode if enrollment and is_active is not None: enrollment['is_active'] = is_active return enrollment
return _get_fake_course_info(course_id)
for enrollment in _ENROLLMENTS: if student_id == enrollment['student'] and course_id == enrollment['course']['course_id']: return enrollment
for course in _COURSES: if course_id == course['course_id']: return course
return _ENROLLMENT_ATTRIBUTES
m_obj = re.match(r'^/courses/{}'.format(settings.COURSE_ID_PATTERN), input_str) if m_obj: return CourseKey.from_string(m_obj.group('course_id')) return None
course = modulestore().get_course(course_id) if course is None: return None return course.enrollment_domain
choice = random.SystemRandom().choice return ''.join([choice(chars) for _i in range(length)])
return fn(*args, **kwargs)
super(SSLClientTest, self).setUp() self.client = Client() self.factory = RequestFactory() self.mock = Mock()
self._base_test_extauth_auto_activate_user_with_flag(log_user_string="inactive@stanford.edu")
self._base_test_extauth_auto_activate_user_with_flag(log_user_string="user.id: 1")
super(MyFetcher, self).__init__() self.client = client
self.attempt_login(200)
self.attempt_login(403, ns="http%3A%2F%2Fspecs.openid.net%2Fauth%2F2.0")
self.attempt_login(403, return_to="http://apps.cs50.edx.or")
response = self._send_bad_redirection_login() self.assertEquals(response.status_code, 302)
return HttpResponse()
stats = celery.control.inspect().stats() or {} return HttpResponse(json.dumps(stats, indent=4), content_type="application/json")
super(CeleryConfigTest, self).setUp() self.client = Client() self.ping_url = reverse('status.service.celery.ping')
stub = get_request_or_stub() expected_url = "http://{site_name}/foobar".format(site_name=settings.SITE_NAME) self.assertEqual(stub.build_absolute_uri("foobar"), expected_url)
cache = {"course_cache": "blah blah blah"} modulestore().request_cache.data.update(cache)
middleware.RequestCache.clear_request_cache()
return middleware.RequestCache.get_request_cache(name)
return middleware.RequestCache.get_current_request()
def __init__(self): super(_RequestCache, self).__init__() self.data = {} self.request = None
if name is None: return REQUEST_CACHE else: return REQUEST_CACHE.data.setdefault(name, {})
return REQUEST_CACHE.request
REQUEST_CACHE.data = {} REQUEST_CACHE.request = None
response.remove_headers = headers
response = func(*args, **kwargs) remove_headers_from_response(response, *headers) return response
return HttpResponse()
try: record = cls.objects.get(course_id=course_id) return record.embargoed except cls.DoesNotExist: return False
if self.embargoed_countries == '': return [] return [country.strip().upper() for country in self.embargoed_countries.split(',')]
return unicode(course_id) in cls._get_restricted_courses_from_cache()
ordering = ['country']
cache_key = cls.CACHE_KEY.format(course_key=course_key) cache.delete(cache_key) log.info("Invalidated country access list for course %s", course_key)
if isinstance(instance, RestrictedCourse): CourseAccessRuleHistory.save_snapshot(instance) elif isinstance(instance, CountryAccessRule): CourseAccessRuleHistory.save_snapshot(instance.restricted_course)
if self.whitelist == '': return [] return self.IPFilterList([addr.strip() for addr in self.whitelist.split(',')])
model = CountryAccessRule extra = 1 def has_delete_permission(self, request, obj=None): return True
whitelist = self.cleaned_data["whitelist"] return self._valid_ip_addresses(whitelist)
country_model = apps.get_model("embargo", "Country") for country_code, __ in list(countries): country_model.objects.get_or_create(country=country_code)
def test_unicode_values(self): country = Country.objects.create(country='NZ') self.assertEquals(unicode(country), "New Zealand (NZ)")
found_rerun = CourseRerunState.objects.find_first(course_key=self.course_key) found_rerun_state = {key: getattr(found_rerun, key) for key in self.expected_rerun_state} self.assertDictEqual(found_rerun_state, self.expected_rerun_state) return found_rerun
def setUp(self): super(TestCourseActionStateManagerBase, self).setUp() self.course_key = CourseLocator("test_org", "test_course_num", "test_run")
self.assertSetEqual( set(course_action_state.course_key for course_action_state in expected), set(course_action_state.course_key for course_action_state in found))
abstract = True
self.filter(id=entry_id).delete()
return self.update(id=entry_id, updated_user=user, should_display=should_display)
IN_PROGRESS = "in_progress" FAILED = "failed" SUCCEEDED = "succeeded"
self.update_state( course_key=destination_course_key, new_state=self.State.IN_PROGRESS, user=user, allow_not_found=True, source_course_key=source_course_key, display_name=display_name, )
self.update_state( course_key=course_key, new_state=self.State.SUCCEEDED, )
super(PipelineRenderTest, cls).setUpClass() call_task('pavelib.assets.update_assets', args=('lms', '--settings=test'))
oauth2_adapter = adapters.DOPAdapter()
return Response(data=token)
request.user = user request.scopes = [SCOPE_VALUE_DICT[scope]] request.client = client request.state = None request.refresh_token = None request.extra_credentials = None request.grant_type = client.authorization_grant_type
return Response(status=400, data=form_errors)
for backend_path in settings.AUTHENTICATION_BACKENDS: backend = auth.load_backend(backend_path) if backend.get_user(user.id): return backend_path
return self._require_oauth_field("access_token")
return self._require_oauth_field("client_id")
return self.oauth2_adapter.create_public_client( name='Test Public Client', user=user, client_id=client_id, redirect_uri=DUMMY_REDIRECT_URL, )
return self.oauth2_adapter.create_confidential_client( name='Test Confidential Client', user=user, client_id=client_id, redirect_uri=DUMMY_REDIRECT_URL, )
return {'access_token', 'token_type', 'expires_in', 'scope'}
return self.oauth2_adapter.create_public_client( name='Test Public Application', user=user, client_id=client_id, redirect_uri=DUMMY_REDIRECT_URL, )
return self.oauth2_adapter.create_confidential_client( name='Test Confidential Application', user=user, client_id=client_id, redirect_uri=DUMMY_REDIRECT_URL, )
return {'access_token', 'refresh_token', 'token_type', 'expires_in', 'scope'}
return json.loads(response.content)["access_token"]
pass
pass
raise NotImplementedError()
raise NotImplementedError()
return self.create_public_client(self.user, self.client_id)
for class_path in settings.AUTHENTICATION_BACKENDS: auth_class = module_member(class_path) if issubclass(auth_class, base_class): yield auth_class
assert self.prefix is not None return "-".join((self.prefix, ) + tuple(getattr(self, field) for field in self.KEY_FIELDS))
return _PSA_BACKENDS[self.backend_name]
return {}
return self.backend_name == pipeline['backend']
return self.backend_name == social_auth.provider
return remote_id
return '{}.{}'.format(self.backend_class.__module__, self.backend_class.__name__)
super(OAuth2ProviderConfig, self).clean() self.other_settings = clean_json(self.other_settings, dict)
super(SAMLProviderConfig, self).clean() self.other_settings = clean_json(self.other_settings, dict)
return {'idp': self.idp_slug}
return self.backend_name == pipeline['backend'] and self.idp_slug == pipeline['kwargs']['response']['idp_name']
prefix = self.idp_slug + ":" return self.backend_name == social_auth.provider and social_auth.uid.startswith(prefix)
return '{}:{}'.format(self.idp_slug, remote_id)
if self.expires_at and timezone.now() > self.expires_at: return False return bool(self.entity_id and self.sso_url and self.public_key)
return 'configuration/{}/current/{}'.format(cls.__name__, entity_id)
prefix = self.lti_consumer_key + ":" return self.backend_name == social_auth.provider and social_auth.uid.startswith(prefix)
try: return ( self.backend_name == pipeline['backend'] and self.lti_consumer_key == pipeline['kwargs']['response'][LTI_PARAMS_KEY]['oauth_consumer_key'] ) except KeyError: return False
raise NotImplementedError("Not used")
raise NotImplementedError("Not used")
lti_params = response[LTI_PARAMS_KEY] return lti_params['oauth_consumer_key'] + ":" + lti_params['user_id']
if lti_key in lti_params and lti_params[lti_key]: details[details_key] = lti_params[lti_key]
try: return int(value) except (ValueError, TypeError): return 0
return sorted(cls._enabled_providers(), key=lambda provider: provider.name)
return [provider for provider in cls.enabled() if provider.accepts_logins]
for enabled in cls._enabled_providers(): if enabled.is_active_for_pipeline(running_pipeline): return enabled
if request.method != 'POST': return HttpResponseNotAllowed('POST') request.backend.start() return complete(request, backend, *args, **kwargs)
backend_name = forms.ChoiceField(choices=((name, name) for name in _PSA_OAUTH2_BACKENDS))
return ( 'name', 'enabled', 'backend_name', 'secondary', 'skip_registration_form', 'skip_email_verification', 'change_date', 'changed_by', 'edit_link', )
backend_name = forms.ChoiceField(choices=((name, name) for name in _PSA_SAML_BACKENDS))
return ( 'change_date', 'changed_by', 'enabled', 'entity_id', 'org_info_str', 'key_summary', )
return ( 'name', 'enabled', 'lti_consumer_key', 'lti_max_timestamp_age', 'change_date', 'changed_by', 'edit_link', )
return self.redirect_uri
return '1234'
from .models import SAMLProviderConfig return SAMLProviderConfig.current(idp_name).get_config()
try: return self._config.get_setting(name) except KeyError: return self.strategy.setting(name, default)
return self.provider.provider_id + '_unlink_form'
return request.session.get('partial_pipeline')
enabled_provider = provider.Registry.get(provider_id) if not enabled_provider: raise ValueError('Provider %s not enabled' % provider_id) return enabled_provider
return redirect(AUTH_DISPATCH_URLS[AUTH_ENTRY_LOGIN])
return redirect(AUTH_DISPATCH_URLS[AUTH_ENTRY_REGISTER])
current_provider = provider.Registry.get_from_pipeline({'backend': backend.name, 'kwargs': kwargs}) return current_provider and current_provider.skip_email_verification
return social_user.user.username
self.provider_id = provider_id
return [{'username': username, 'remote_id': 'remote_' + username} for username in usernames]
super(ThirdPartyAuthApiPermissionTest, self).setUp() client = self.configure_oauth_client() self.configure_api_permission(client, PROVIDER_ID_TESTSHIB)
request = self.factory.get('/login') request.META['HTTP_REFERER'] = referer return request
def setUp(self): super(TestCase, self).setUp() self.enabled_provider = self.configure_google_provider(enabled=True)
return social_models.DjangoStorage.user.user_model().objects.get(username=username)
self.enable_saml(enabled=False) response = self.client.get(self.METADATA_URL) self.assertEqual(response.status_code, 404)
self.enable_saml() response = self.client.get(self.LOGIN_URL) self.assertEqual(response.status_code, 302)
return (200, headers, self.read_data_file('testshib_metadata.xml'))
self._configure_testshib_provider() super(TestShibIntegrationTest, self).test_login()
self._configure_testshib_provider() super(TestShibIntegrationTest, self).test_register()
now_patch = patch('onelogin.saml2.utils.OneLogin_Saml2_Utils.now', return_value=timestamp) now_patch.start() self.addCleanup(now_patch.stop)
raise NotImplementedError
return self._check_login_or_register_page(self.login_page_url, "loginUrl")
return self._check_login_or_register_page(self.register_page_url, "registerUrl")
return reverse('social:complete', kwargs={'backend': self.PROVIDER_BACKEND})
self.assertEqual(200, response.status_code)
self.assertIsNone(auth.authenticate(password=password, username=username))
self.assertTrue(pipeline.running(request))
self.assertEqual(302, response.status_code) self.assertEqual('/login', response.get('Location'))
self.assertEqual(302, response.status_code) self.assertEqual('/register', response.get('Location'))
social_auths = strategy.storage.user.get_social_auth_for_user( user, provider=self.provider.backend_name) self.assertEqual(0, len(social_auths))
return strategy.storage.user.user_model().objects.get(email=email)
django.test.signals.template_rendered.send(sender=None, template=None, context=kwargs) return orig_render(*args, **kwargs)
for key, value in mappings.iteritems(): setattr(self, key, value)
kwargs.setdefault('enabled', True) SAMLConfiguration(**kwargs).save()
obj = OAuth2ProviderConfig(**kwargs) obj.save() return obj
self.assertTrue(SAMLConfiguration.is_enabled(), "SAML Provider Configuration only works if SAML is enabled.") obj = SAMLProviderConfig(**kwargs) obj.save() return obj
obj = LTIProviderConfig(**kwargs) obj.save() return obj
kwargs.setdefault("name", "Dummy") kwargs.setdefault("backend_name", "dummy") return cls.configure_oauth_provider(**kwargs)
user = User.objects.get(email=email) user.is_active = True user.save()
return OAuth2Client.objects.create(client_type=constants.CONFIDENTIAL)
return ProviderApiPermissions.objects.create(client=client, provider_id=provider_id)
with open(os.path.join(os.path.dirname(__file__), 'data', filename)) as f: return f.read()
return cls.read_data_file('{}.pub'.format(key_name))
return cls.read_data_file('{}.key'.format(key_name))
return Client.objects.create( client_id=self.client_id, client_type=PUBLIC, )
pass
return cls.current().base_url
add_period = lambda x: '.' + x return map(add_period, cls.current().excluded_extensions.split())
processed = request.build_absolute_uri(prefix + rest) return quote + processed + quote
return self.list_display
return self.list_display
is_secure_default = request.is_secure request.is_secure = lambda: False try: yield finally: request.is_secure = is_secure_default
return HttpResponse(json.dumps(request.META))
config = XDomainProxyConfiguration.current() config.enabled = is_enabled if whitelist: config.whitelist = "\n".join(whitelist) config.save() cache.clear()
return self.client.get(reverse('xdomain_proxy'))
request = Mock() request.META = {'HTTP_REFERER': http_referer} request.is_secure = lambda: is_secure return request
self.assertFalse(request.is_secure()) return SENTINEL
if not settings.FEATURES.get('ENABLE_CORS_HEADERS'): raise MiddlewareNotUsed()
verbose_name_plural = "Microsite histories"
archive_object = MicrositeHistory( key=instance.key, site=instance.site, values=instance.values, ) archive_object.save()
_make_archive_copy(instance)
return u'{microsite_key}: {organization}'.format( microsite_key=self.microsite.key, organization=self.organization )
return cls.objects.filter(microsite_id=microsite_pk).values_list('organization', flat=True)
try: item = cls.objects.select_related('microsite').get(organization=org) return item.microsite except ObjectDoesNotExist: return None
return u'{microsite_key}: {template_uri}'.format( microsite_key=self.microsite.key, template_uri=self.template_uri )
unique_together = (('microsite', 'template_uri'),)
return settings.FEATURES.get('USE_MICROSITES', False)
return BACKEND.is_request_in_microsite()
return BACKEND.get_value(val_name, default, **kwargs)
return BACKEND.get_dict(dict_name, default, **kwargs)
return BACKEND.has_override_value(val_name)
return BACKEND.get_value_for_org(org, val_name, default)
return BACKEND.get_all_config()
BACKEND.clear()
BACKEND.set_config_by_domain(domain)
if is_feature_enabled(): BACKEND.enable_microsites_pre_startup(log)
if is_feature_enabled(): BACKEND.enable_microsites(log)
if not is_request_in_microsite(): return return TEMPLATES_BACKEND.get_template(uri)
if not is_request_in_microsite(): return relative_path return TEMPLATES_BACKEND.get_template_path(relative_path, **kwargs)
return False
return False
def __init__(self, **kwargs): super(FilebasedMicrositeBackend, self).__init__(**kwargs)
if Microsite.objects.all()[:1].exists(): return True else: return False
config = {} candidates = Microsite.objects.all() for microsite in candidates: values = microsite.values config[microsite.key] = values return config
raise NotImplementedError()
raise NotImplementedError()
raise NotImplementedError()
raise NotImplementedError()
raise NotImplementedError()
raise NotImplementedError()
raise NotImplementedError()
return getattr(settings, "MICROSITE_CONFIGURATION", False)
if not hasattr(self.current_request_configuration, 'data'): return {} return self.current_request_configuration.data
if hasattr(self.current_request_configuration, 'cache'): return self.current_request_configuration.cache.get(key)
if hasattr(self.current_request_configuration, 'cache'): self.current_request_configuration.cache[key] = value
configuration = self.get_configuration() return configuration.get(val_name, default)
return bool(self.get_configuration())
configuration = self.get_configuration() return val_name in configuration
config = {} for key, value in settings.MICROSITE_CONFIGURATION.iteritems(): config[key] = value return config
self.current_request_configuration.data = {} self.current_request_configuration.cache = {}
microsites_root = settings.MICROSITE_ROOT_DIR if self.has_configuration_set(): settings.DEFAULT_TEMPLATE_ENGINE['DIRS'].append(microsites_root)
return
return page_title_breadcrumbs(*crumbs)
return microsite.get_value('platform_name', settings.PLATFORM_NAME)
return staticfiles_storage.url(microsite.get_value('favicon_path', default))
def setUp(self): super(DatabaseMicrositeTestCase, self).setUp() self.microsite = MicrositeFactory.create() MicrositeOrganizationMappingFactory.create(microsite=self.microsite, organization='TestMicrositeX')
path = microsite_tags.microsite_template_path('footer.html') self.assertEqual("footer.html", path)
return super(NullBackend, self).set_config_by_domain(domain)
return super(NullBackend, self).get_template_path(relative_path, **kwargs)
return super(NullBackend, self).get_value(val_name, default, **kwargs)
return super(NullBackend, self).get_dict(dict_name, default, **kwargs)
return super(NullBackend, self).is_request_in_microsite()
return super(NullBackend, self).has_override_value(val_name)
return super(NullBackend, self).get_value_for_org(org, val_name, default)
return super(NullBackend, self).clear()
microsite.set_by_domain(self.microsite.site.domain) self.assertEqual(microsite.get_value('email_from_address'), self.microsite.values['email_from_address'])
microsite.set_by_domain(self.microsite.site.domain) self.assertTrue(microsite.is_request_in_microsite())
microsite.set_by_domain(self.microsite.site.domain) self.assertEqual(microsite.get_dict('nested_dict'), self.microsite.values['nested_dict'])
microsite.set_by_domain(self.microsite.site.domain) self.assertTrue(microsite.has_override_value('platform_name'))
microsite.set_by_domain(self.microsite.site.domain) self.assertEqual( microsite.get_all_orgs(), set(self.microsite.get_organizations()) )
self.assertTrue(microsite.BACKEND.has_configuration_set()) Microsite.objects.all().delete() self.assertFalse(microsite.BACKEND.has_configuration_set())
microsite.set_by_domain(self.microsite_subdomain) self.assertEqual(microsite.get_value('platform_name'), 'Test Microsite')
microsite.set_by_domain(self.microsite_subdomain) self.assertTrue(microsite.is_request_in_microsite())
microsite.set_by_domain(self.microsite_subdomain) self.assertTrue(microsite.has_override_value('platform_name'))
microsite.set_by_domain(self.microsite_subdomain) configs = microsite.get_all_config() self.assertEqual(len(configs.keys()), 3)
microsite.clear() domain = request.META.get('HTTP_HOST', None) microsite.set_by_domain(domain) return None
microsite.clear() return response
super(NoneToEmptyManager, self).__init__()
return NoneToEmptyQuerySet(self.model, using=self._db)
if hasattr(key, 'version_agnostic') and hasattr(key, 'for_branch'): return key.for_branch(None).version_agnostic() else: return key
if lookup == 'in': stripped_value = [_strip_object(el) for el in value] else: stripped_value = _strip_object(value) return stripped_value
if value is self.Empty: return return super(OpaqueKeyField, self).run_validators(value)
description = "A CourseKey object, saved to the DB in the form of a string" KEY_CLASS = CourseKey
description = "A Location object, saved to the DB in the form of a string" KEY_CLASS = UsageKey
self.prog_name = "{} {}".format(prog_name, subcommand) return super(TrackedCommand, self).create_parser(prog_name, subcommand)
def handle(self, *args, **options): return json.dumps(eventtracker.get_tracker().resolve_context())
out = StringIO() DummyCommand().execute(*args, stdout=out, **kwargs) out.seek(0) return json.loads(out.read())
super(LoggerBackend, self).__init__(**kwargs) self.event_logger = logging.getLogger(name)
super(DjangoBackend, self).__init__(**options) self.name = name
context = event.get('context', {}) if field in context: event[field] = context[field] del context[field] else: event[field] = default_value
tracker.send(event)
if request is not None and hasattr(request, 'META') and header_name in request.META: return request.META[header_name] else: return default
if request is not None and hasattr(request, 'META'): return get_ip(request) else: return default
request = self.create_request( data=self.create_segmentio_event_json(**kwargs), content_type='application/json' ) segmentio.track_segmentio_event(request)
return json.dumps(self.create_segmentio_event(**kwargs))
event = self.create_segmentio_event() for field in fields: if field in event: del event[field] return event
self.events.append(event)
self.tracker = DjangoTracker() tracker.register_tracker(self.tracker)
return self.tracker.backends['mem']
return self.backend.events[idx]
self.assertEquals(len(self.backend.events), 0)
backends = self._reload_backends() self.assertEqual(len(backends), 1) tracker.send({}) self.assertEqual(backends.values()[0].count, 1)
settings.TRACKING_BACKENDS.update({'second': None}) backends = self._reload_backends() self.assertEqual(len(backends), 1)
request = self.request_factory.get(path) return self.get_context_for_request(request)
for key, expected_value in subset.iteritems(): self.assertEquals(superset[key], expected_value)
try: self[key] except KeyError: return default
for key, value in dict_: self[key] = value
return self._match_registry.keys() + self._prefix_registry.keys()
cls.mapping[transformer.match_key] = transformer return transformer
name = event.get(u'name') return cls.mapping[name](event)
return self[u'name']
if self.is_legacy_event: self._set_legacy_event_type() self.process_legacy_fields() self.process_event() self.dump_payload()
self['event_type'] = self.legacy_event_type
pass
pass
return not self.crosses_boundary()
self.event[u'old'] = self.event[u'current_tab'] self.event[u'new'] = self.event[u'current_tab'] + self.offset
raise NotImplementedError
return self.event[u'current_tab'] == self.event[u'tab_count']
return self.event[u'current_tab'] == 1
return self.name_to_event_type_map[self.name]
if self.name in self.name_to_event_type_map: super(VideoEventTransformer, self).transform()
if 'current_time' in self.event: self.event['currentTime'] = self.event.pop('current_time')
if 'open_in_browser_url' in self.context: self['page'] = self.context.pop('open_in_browser_url').rpartition('/')[0]
if self._build_requests_plus_30_for_minus_30(): if self._user_requested_plus_30_skip(): self.event[u'requested_skip_interval'] = -30
try: return self.encrypt_session_key(request.session.session_key) except AttributeError: return ''
try: return request.user.pk except AttributeError: return ''
try: return request.user.username except AttributeError: return ''
ip_address = get_ip(request) if ip_address is not None: return ip_address else: return ''
del context_mock.context self.assertIn("this module is temporarily unavailable", render_to_string("courseware/error-message.html", None))
del context_mock.context self.assertIn("We're having trouble rendering your component", render_to_string("html_error.html", None))
return any(is_marketing_link_set(name) for name in names)
enable_mktg_site = microsite.get_value( 'ENABLE_MKTG_SITE', settings.FEATURES.get('ENABLE_MKTG_SITE', False) ) if enable_mktg_site: return name in settings.MKTG_URLS else: return name in settings.MKTG_URL_LINK_MAP
return dict( [ ("IS_REQUEST_IN_MICROSITE", microsite.is_request_in_microsite()) ] )
microsite_template = microsite.get_template(uri) return ( microsite_template if microsite_template else super(DynamicTemplateLookup, self).get_template(uri) )
if namespace in LOOKUP: del LOOKUP[namespace]
return LOOKUP[namespace].get_template(name)
if not kwargs.get('no_django', False): kwargs['lookup'] = edxmako.LOOKUP['main'] super(Template, self).__init__(*args, **kwargs)
dictionary = dict(mako_context) return loader.render_to_string(file_name, dictionary=dictionary)
REQUEST_CONTEXT.request = request
REQUEST_CONTEXT.request = None return response
context_processors = _builtin_context_processors context_processors += tuple(settings.DEFAULT_TEMPLATE_ENGINE['OPTIONS']['context_processors']) return tuple(import_string(path) for path in context_processors)
role, __ = Role.objects.get_or_create(course_id=course_id, name=FORUM_ROLE_STUDENT) user.roles.add(role)
STANDALONE = 'standalone' COURSE = 'course'
role, created = Role.objects.get_or_create(name=name, course_id=course_key) if created is False: role.course_id = course_key role.save() return role
return self.construct_scalar(node)
item = self.store.get_item(item_location) item.visible_to_staff_only = True self.store.update_item(item, self.user.id)
item = self.store.get_item(item_location) item.group_access[self.content_partition.id] = group_ids self.store.update_item(item, self.user.id)
item = self.store.get_item(item_location) html = item.visibility_view().body_html() for string in substrings: self.assertIn(string, html)
return urlparse(handler_url(self.block, 'handler', query=query_string)).query
return urlparse(handler_url(self.block, handler_name, suffix=suffix)).path
return settings.STATIC_URL + relative_url
return [t.value for t in TagAvailableValues.objects.filter(category=self)]
return TagCategories.objects.all()
return settings.STATIC_URL + relative_url
descriptor = modulestore().get_course(course_key) model = cls(descriptor) return model
descriptor = modulestore().get_course(course_key) descriptor.grade_cutoffs = cutoffs modulestore().update_item(descriptor, user.id) return cutoffs
descriptor = modulestore().get_course(course_key) del descriptor.graceperiod modulestore().update_item(descriptor, user.id)
result = {} metadata = cls.fetch_all(descriptor) for key, value in metadata.iteritems(): if key in cls.filtered_list(): continue result[key] = value return result
instance = kwargs['instance'] instance.orig_state = instance.state
if add: auth.add_users(caller, CourseCreatorRole(), user) else: auth.remove_users(caller, CourseCreatorRole(), user)
user = CourseCreator.objects.get(user=user) if user.state != CourseCreator.GRANTED: user.state = CourseCreator.PENDING user.save()
return obj.user.email
return inst.user.username
user = kwargs['user'] updated_state = kwargs['state'] update_course_creator_group(kwargs['caller'], user, updated_state == CourseCreator.GRANTED)
return str((template_name, context))
self.table_entry.state = state self.creator_admin.save_model(self.request, self.table_entry, None, True)
self.assertFalse(self.creator_admin.has_add_permission(self.request))
self.assertFalse(self.creator_admin.has_delete_permission(self.request))
self.assertTrue(self.creator_admin.has_change_permission(self.request)) self.request.user = self.user self.assertFalse(self.creator_admin.has_change_permission(self.request))
with self.assertRaises(PermissionDenied): add_user_with_status_granted(self.user, self.user) with self.assertRaises(PermissionDenied): update_course_creator_group(self.user, self.user, True)
try: return CourseKey.from_string(arg) except InvalidKeyError: return SlashSeparatedCourseKey.from_deprecated_string(arg)
with open(filename) as f: results = f.read() os.remove(filename) return results
tmp_dir = mkdtemp() try: course_dir = export_course_to_directory(course_key, tmp_dir) compress_directory(course_dir, filename) finally: shutil.rmtree(tmp_dir, ignore_errors=True)
with self.assertRaisesRegexp(CommandError, 'Error: too few arguments'): call_command('delete_orphans')
errstring = "Invalid course_key: 'foo/TestX/TS01/2015_Q7'." with self.assertRaisesRegexp(CommandError, errstring): call_command('delete_course', 'foo/TestX/TS01/2015_Q7')
errstring = "Course with 'TestX/TS01/2015_Q7' key not found." with self.assertRaisesRegexp(CommandError, errstring): call_command('delete_course', 'TestX/TS01/2015_Q7')
errstring = "Error: too few arguments" with self.assertRaisesRegexp(CommandError, errstring): call_command('force_publish')
errstring = "Invalid course key." with self.assertRaisesRegexp(CommandError, errstring): call_command('force_publish', 'TestX/TS01')
errstring = "Course not found." with self.assertRaisesRegexp(CommandError, errstring): call_command('force_publish', unicode('course-v1:org+course+run'))
self.assertFalse(are_permissions_roles_seeded(self.base_course_key)) call_command('import', self.content_dir, self.good_dir) self.assertTrue(are_permissions_roles_seeded(self.base_course_key))
errstring = "Error: too few arguments" with self.assertRaisesRegexp(CommandError, errstring): call_command('export_olx')
errstring = "Unparsable course_id" with self.assertRaisesRegexp(CommandError, errstring): call_command('export_olx', 'InvalidCourseID')
errstring = "Invalid course_id" with self.assertRaisesRegexp(CommandError, errstring): call_command('export_olx', 'x/y/z')
return library.location.library_key
return [mock.call(self.store, course.id) for course in courses]
with self.assertRaisesRegexp(CommandError, ".* requires one or more arguments.*"): call_command('reindex_course')
err_string = "Invalid course_key: '{0}'".format(invalid_key) with self.assertRaisesRegexp(CommandError, err_string): call_command('reindex_course', invalid_key)
return library.location.library_key
return [mock.call(self.store, self._get_lib_key(lib)) for lib in libraries]
with self.assertRaisesRegexp(CommandError, ".* requires one or more arguments.*"): call_command('reindex_library')
with self.assertRaises(InvalidKeyError): call_command('reindex_library', invalid_key)
with self.assertRaisesRegexp(CommandError, "Error: too few arguments"): call_command('fix_not_found')
errstring = "migrate_to_split requires at least two arguments" with self.assertRaisesRegexp(CommandError, errstring): self.command.handle()
errstring = "Invalid location string" with self.assertRaisesRegexp(CommandError, errstring): self.command.handle("foo", "bar")
errstring = "No user found identified by 99" with self.assertRaisesRegexp(CommandError, errstring): self.command.handle("org/course/name", "99")
errstring = "Error: too few arguments" with self.assertRaisesRegexp(CommandError, errstring): call_command('export')
try: user_id = int(identifier) except ValueError: return User.objects.get(email=identifier) return User.objects.get(id=user_id)
return { "id": update["id"], "date": update["date"], "content": update["content"], }
return PushNotificationConfig.is_enabled()
log.debug(message) response['status'] = message return JsonResponse(response, status_code)
disabled_create_block_types = XBlockDisableConfig.disabled_create_block_types() return [c_type for c_type in ADVANCED_COMPONENT_TYPES if c_type not in disabled_create_block_types]
component_class = XBlock.load_class(category, select=settings.XBLOCK_SELECT_FUNCTION) mixologist = Mixologist(settings.XBLOCK_MIXINS) return mixologist.mix(component_class)
_delete_entrance_exam(request, course_key) return _create_entrance_exam( request=request, course_key=course_key, entrance_exam_minimum_score_pct=entrance_exam_minimum_score_pct )
return _delete_entrance_exam(request=request, course_key=course_key)
for dirpath, _dirnames, filenames in os.walk(directory): for filename in filenames: yield (filename, dirpath)
for fname, dirpath in get_all_files(directory): if fname == filename: return dirpath return None
return upload_file.size
return render_to_string(template_name, dictionary, context, namespace="lms." + namespace)
locator = xblock.location parent_location = modulestore().get_parent_location(locator) if parent_location is None: return None return modulestore().get_item(parent_location)
if xblock.category == 'vertical': if parent_xblock is None: parent_xblock = get_parent_xblock(xblock) parent_category = parent_xblock.category if parent_xblock else None return parent_category == 'sequential' return False
category = xblock.category if category == 'course': return 'chapter' elif category == 'chapter': return 'sequential' elif category == 'sequential': return 'vertical' return None
usage_key = UsageKey.from_string(usage_key_string) usage_key = usage_key.replace(course_key=modulestore().fill_in_run(usage_key.course_key)) return usage_key
if not has_studio_write_access(user, course_key): raise PermissionDenied() course_module = modulestore().get_course(course_key, depth=depth) return course_module
pass
pass
if not course.certificates or not course.certificates.get('certificates'): return [] return [cert['id'] for cert in course.certificates['certificates']]
event_name = '.'.join(['edx', 'certificate', 'configuration', event_name]) tracker.emit(event_name, event_data)
return self._certificate_data
return list( { attr: video[attr] for attr in ["edx_video_id", "client_video_id", "created", "duration", "status"] } for video in _get_videos(course) )
conn = s3.connection.S3Connection( settings.AWS_ACCESS_KEY_ID, settings.AWS_SECRET_ACCESS_KEY ) return conn.get_bucket(settings.VIDEO_UPLOAD_PIPELINE["BUCKET"])
md5 = hashlib.md5() md5.update(repr(resource)) return md5.hexdigest()
if is_entrance_exams_enabled(): graders = [grader for grader in graders if grader.get('type') != u'Entrance Exam'] return graders
return has_studio_read_access(self._user, course_key)
return has_studio_write_access(self._user, course_key)
return _create_item(request)
_delete_item(usage_key, request.user)
if ancestor: direct_children_only = lambda parent: parent == ancestor ancestors.append(create_xblock_info( ancestor, include_child_info=include_child_info, course_outline=course_outline, include_children_predicate=direct_children_only )) collect_ancestor_info(get_parent_xblock(ancestor))
return _xblock_type_and_display_name(find_release_date_source(xblock))
source = find_staff_lock_source(xblock) return _xblock_type_and_display_name(source) if source else None
return _('{section_or_subsection} "{display_name}"').format( section_or_subsection=xblock_type_display_name(xblock), display_name=xblock.display_name_with_default)
user_requested_access(request.user) return JsonResponse({"Status": "OK"})
return { 'id': user.id, 'username': user.username, 'email': user.email, 'role': role }
return self.wrap_xblock(block, view_name, Fragment(), context)
if isinstance(key, LibraryLocator): key = unicode(key) return LIBRARY_REST_URL + key
response = self.client.get_json(LIBRARY_REST_URL) self.assertEqual(response.status_code, 404)
response = getattr(self.client, verb)(LIBRARY_REST_URL) self.assertEqual(response.status_code, 405)
response = self.client.ajax_post(LIBRARY_REST_URL, data) self.assertEqual(response.status_code, 400)
response = self.client.get_json(make_url_for_lib(key_str)) self.assertEqual(response.status_code, 404)
self.check_index_and_outline(self.client)
parsed_html = lxml.html.fromstring(response.content) return parsed_html.find_class('course-status')[0].find_class('status-release-value')[0].text_content()
published_video = self.store.publish(self.video.location, self.user.id) self.validate_preview_html(self.video, STUDENT_VIEW, can_add=False)
self.validate_preview_html(self.video, STUDENT_VIEW, can_add=False)
html = self.get_page_html(xblock) self.assertIn(expected_section_tag, html) self.assertRegexpMatches(html, expected_breadcrumbs)
self.validate_preview_html(self.vertical, self.container_view) self.validate_preview_html(self.child_container, self.container_view) self.validate_preview_html(self.child_vertical, self.reorderable_child_view)
return ItemFactory.create( parent_location=parent_location, category=category, display_name=display_name, publish_item=False, user_id=self.user.id, **kwargs )
empty_child_container = self._create_item(self.vertical.location, 'split_test', 'Split Test') self.validate_preview_html(empty_child_container, self.reorderable_child_view, can_add=False)
resp = self.client.delete('/course/bad/course/key/entrance_exam') self.assertEqual(resp.status_code, 400)
resp = self.client.get('/course/bad/course/key/entrance_exam') self.assertEqual(resp.status_code, 400)
resp = self.client.post( '/course/bad/course/key/entrance_exam', {}, http_accept='application/json' ) self.assertEqual(resp.status_code, 400)
resp = self.client.post( '/course/bad/course/key/entrance_exam', {}, http_accept='text/html' ) self.assertEqual(resp.status_code, 400)
resp = self.client.put(self.exam_url) self.assertEqual(resp.status_code, 405)
resp = self.client.get_html(self.url) self.assertEqual(resp.status_code, 200) self.assertIn('course-nav-list', resp.content)
self.check_toggle_tab_visiblity('wiki', True) self.check_toggle_tab_visiblity('wiki', False)
asset = self.get_sample_asset(name, asset_type) response = self.client.post(self.url, {"name": name, "file": asset}) return response
asset_location = StaticContent.get_location_from_path('/c4x/edX/toy/asset/sample_static.txt') content = contentstore().find(asset_location) self.assertEqual(content.locked, locked)
super(ExportTestCase, self).setUp() self.url = reverse_course_url('export_handler', self.course.id)
resp = self.client.get_html(self.url) self.assertEquals(resp.status_code, 200) self.assertContains(resp, "Export My Course Content")
resp = self.client.get(self.url, HTTP_ACCEPT='application/json') self.assertEquals(resp.status_code, 406)
resp = self.client.get(self.url, HTTP_ACCEPT='application/x-tgz') self._verify_export_succeeded(resp)
resp = self.client.get(self.url + '?_accept=application/x-tgz') self._verify_export_succeeded(resp)
self.assertEquals(resp.status_code, 200) self.assertTrue(resp.get('Content-Disposition').startswith('attachment'))
resp = self.client.get_html(url) self.assertEquals(resp.status_code, 404)
return reverse_course_url( 'textbooks_detail_handler', self.course.id, kwargs={'textbook_id': textbook_id} )
return reverse_course_url(self.VIEW_NAME, course_key)
return next( video for video in self.previous_uploads if video["edx_video_id"] == edx_video_id )
pass
configuration_id = content.pop("id") group_ids = [group.pop("id") for group in content["groups"]] return (configuration_id, group_ids)
super(GroupConfigurationsListHandlerTestCase, self).setUp()
return reverse_course_url('group_configurations_list_handler', self.course.id)
response = self.client.get( self._url(), HTTP_ACCEPT="text/plain", ) self.assertEqual(response.status_code, 406)
cid = cid if cid > 0 else self.ID return reverse_course_url( 'group_configurations_detail_handler', self.course.id, kwargs={'group_configuration_id': cid}, )
self._add_user_partitions(scheme_id='cohort') actual = GroupConfiguration.get_or_create_content_group(self.store, self.course) expected = self._get_expected_content_group(usage_for_group=[]) self.assertEqual(actual, expected)
super(TestHeaderMenu, self).setUp()
response = self.client.get(self.programs_path) self.assertEquals(response.status_code, status_code) return response
response = self.client.get(self.programs_path) self.assertRedirects( response, '{login_url}?next={programs}'.format( login_url=settings.LOGIN_URL, programs=self.programs_path ) )
self.create_programs_config(enable_studio_tab=False) response = self.client.get(self.path) self.assertEqual(response.status_code, 404)
self.assertFalse( CourseEnrollment.is_enrolled(self.ext_user, self.course.id), 'Did not expect ext_user to be enrolled in course' )
self.client.ajax_post( self.seq1_url, data={'isPrereq': True} ) mock_add_prereq.assert_called_with(self.course.id, self.seq1.location)
self.client.ajax_post( self.seq1_url, data={'isPrereq': False} ) mock_remove_prereq.assert_called_with(self.seq1.location)
usage_key_string = json.loads(resp.content).get('locator') return UsageKey.from_string(usage_key_string)
return Fragment(self.FRAG_CONTENT)
item = self.store.get_item(usage_key) if verify_is_draft: self.assertTrue(getattr(item, 'is_draft', False)) return item
resp = self.create_xblock(category='vertical', parent_usage_key=parent_usage_key) self.assertEqual(resp.status_code, 200) return self.response_usage_key(resp)
resp = self._get_preview(usage_key, data) self.assertEqual(resp.status_code, expected_code) if content_contains: self.assertIn(content_contains, resp.content) return resp
return modulestore().has_item(location, revision=ModuleStoreEnum.RevisionOption.published_only)
self.assertTrue(self._is_location_published(location)) self.assertFalse(modulestore().has_changes(modulestore().get_item(location)))
self.assertTrue(self._is_location_published(location)) self.assertTrue(modulestore().has_changes(modulestore().get_item(location)))
self._make_draft_content_different_from_published()
split_test = self.get_item_from_modulestore(self.split_test_usage_key, True) self.assertEqual(expected_number, len(split_test.children)) return split_test
return next((template for template in templates if template.get('display_name') == display_name), None)
children = xblock_info['child_info']['children'] self.assertTrue(len(children) > index) return children[index]
return create_xblock_info( modulestore().get_item(location), include_child_info=True, include_children_predicate=ALWAYS, )
return create_xblock_info( modulestore().get_item(location), include_child_info=True, include_children_predicate=ALWAYS, course_outline=True )
xblock = modulestore().get_item(location) xblock.start = start self.store.update_item(xblock, self.user.id)
xblock = modulestore().get_item(location) xblock.visible_to_staff_only = True self.store.update_item(xblock, self.user.id)
xblock = modulestore().get_item(location) xblock.display_name = display_name self.store.update_item(xblock, self.user.id)
self._verify_xblock_info_state(xblock_info, 'staff_only_message', expected_state, path)
self._verify_xblock_info_state(xblock_info, 'visibility_state', expected_state, path, should_equal)
self._verify_xblock_info_state(xblock_info, 'has_explicit_staff_lock', expected_state, path, should_equal)
url = xblock_studio_url(xblock) self.assertIsNotNone(url) resp = self.client.get_html(url) self.assertEqual(resp.status_code, 200) return resp.content
certificate_id = content.pop("id") return certificate_id
super(CertificatesListHandlerTestCase, self).setUp('contentstore.views.certificates.tracker')
return reverse_course_url('certificates.certificates_list_handler', self.course.id)
response = self.client.get( self._url(), HTTP_ACCEPT="text/plain", ) self.assertEqual(response.status_code, 406)
resp = self.client.put(self._url()) self.assertEqual(resp.status_code, 405)
super(CertificatesDetailHandlerTestCase, self).setUp('contentstore.views.certificates.tracker')
cid = cid if cid > 0 else self._id return reverse_course_url( 'certificates.certificates_detail_handler', self.course.id, kwargs={'certificate_id': cid}, )
pass
if not has_studio_read_access(user, course_key): raise PermissionDenied() course_module = modulestore().get_course(course_key, depth=depth) return course_module
if not has_course_author_access(user, course_key): raise PermissionDenied() return CoursewareSearchIndexer.do_course_reindex(modulestore(), course_key)
return create_xblock_info( course_module, include_child_info=True, course_outline=True, include_children_predicate=lambda xblock: not xblock.category == 'vertical', user=request.user )
return not isinstance(course_access.course_id, CCXLocator)
return reverse_course_url('course_rerun_handler', course_key)
return ( 'split_test' in ADVANCED_COMPONENT_TYPES and 'split_test' in course.advanced_modules )
pass
self.configuration = GroupConfiguration.parse(json_string) self.course = course self.assign_id(configuration_id) self.assign_group_ids() self.validate()
try: configuration = json.loads(json_string) except ValueError: raise GroupConfigurationsValidationError(_("invalid JSON")) configuration["version"] = UserPartition.VERSION return configuration
return set([p.id for p in course.user_partitions])
return UserPartition.from_json(self.configuration)
split_tests = store.get_items(course.id, qualifiers={'category': 'split_test'}) return GroupConfiguration._get_content_experiment_usage_info(store, course, split_tests)
items = store.get_items(course.id, settings={'group_access': {'$exists': True}}, include_orphans=False) return GroupConfiguration._get_content_groups_usage_info(course, items)
items = store.get_items(course.id, settings={'group_access': {'$exists': True}}) return GroupConfiguration._get_content_groups_items_usage_info(course, items)
resp = self._login(email, password) data = parse_json(resp) self.assertTrue(data['success']) return resp
super(ForumTestCase, self).setUp() self.course = CourseFactory.create(org='testX', number='727', display_name='Forum Course')
super(CourseKeyVerificationTestCase, self).setUp() self.course = CourseFactory.create(org='edX', number='test_course_key', display_name='Test Course')
course = CourseFactory.create( org=course_location.org, number=course_location.course, run=course_location.run, default_store=store ) self._add_role_access_to_user(user, course.id) return course
if user is not None: for role in [CourseInstructorRole, CourseStaffRole]: role(course_id).add_users(user)
self.client.logout() ModuleStoreTestCase.tearDown(self)
self.assertEqual(len(self.store.get_orphans(course_key)), number)
self._verify_split_test_import( 'split_test_copy', 'split_test_module', 'split1', {"0": 'sample_0', "2": 'sample_2'}, )
self._verify_split_test_import( 'split_test_copy_with_draft', 'split_test_module_draft', 'fb34c21fe64941999eaead421a8711b8', {"0": '9f0941d021414798836ef140fb5f6841', "1": '0faf29473cf1497baa33fcc828b179cd'}, )
try: content = contentstore().find(self.content_location) contentstore().delete(content.location) except NotFoundError: pass
for subs_id in youtube_subs.values(): self.clear_sub_content(subs_id)
mock_get.return_value = Mock(status_code=200, text=response_success, content=response_success) transcript_name = transcripts_utils.youtube_video_transcript_name(youtube_text_api) self.assertEqual(transcript_name, 'Custom')
mock_get.return_value = Mock(status_code=200, text=response_success, content=response_success) transcript_name = transcripts_utils.youtube_video_transcript_name(youtube_text_api) self.assertIsNone(transcript_name)
pass
return SearchEngine.get_search_engine(self.INDEX_NAME)
return {}
fields = field_dictionary if field_dictionary else self._get_default_search() return self.searcher.search(query_string=query_string, field_dictionary=fields, doc_type=self.DOCUMENT_TYPE)
with store.branch_setting(ModuleStoreEnum.Branch.draft_preferred): store.publish(item_location, ModuleStoreEnum.UserID.test)
with store.branch_setting(ModuleStoreEnum.Branch.draft_preferred): store.delete_item(item_location, ModuleStoreEnum.UserID.test)
with store.branch_setting(ModuleStoreEnum.Branch.draft_preferred): store.update_item(item, ModuleStoreEnum.UserID.test)
return CoursewareSearchIndexer.do_course_reindex(store, self.course.id)
trigger_time = datetime.now(UTC) return CoursewareSearchIndexer.index( store, self.course.id, triggered_at=trigger_time, reindex_age=(trigger_time - since_time) )
indexed_count = self.reindex_course(store) self.assertFalse(indexed_count)
self.publish_item(store, self.vertical.location) with self.assertRaises(SearchIndexingError): self.reindex_course(store)
self._perform_test_using_store(store_type, self._test_delete_course_from_search_index_after_course_deletion)
response = self.searcher.search(field_dictionary={"course": self.course_id}) self.assertEqual(response["total"], expected_count)
list_of_ids = [unicode(top_parent_object.location)] for child in top_parent_object.get_children(): list_of_ids.extend(id_list(child)) return list_of_ids
return {"library": unicode(self.library.location.library_key.replace(version_guid=None, branch=None))}
return LibrarySearchIndexer.do_library_reindex(store, self.library.location.library_key)
return [item['data']['content'] for item in response['results']]
indexed_count = self.reindex_library(store) self.assertFalse(indexed_count)
with self.assertRaises(SearchIndexingError): self.reindex_library(store)
return reverse_course_url( 'group_configurations_detail_handler', self.course.id, kwargs={'group_configuration_id': cid}, )
return CoursewareSearchIndexer.do_course_reindex(store, self.course.id)
return self.translations.get(msgid, msgid)
if languages: language = languages[0] if language in locales_map: return FakeTranslations(locales_map[language]) return gettext.NullTranslations()
i18n_service = self.runtime.service(descriptor, 'i18n') self.assertIsNotNone(i18n_service) self.assertIsInstance(i18n_service, ModuleI18nService) return i18n_service
output = self.old_ugettext(*args, **kwargs) return "XYZ " + output
i18n_service = ModuleI18nService(None) self.assertEqual(i18n_service.ugettext(self.test_language), 'XYZ-TEST-LANGUAGE')
if logout_first: self.client.logout() self.client.login(username=self.user.username, password=self.user_password)
return ItemFactory.create( category='library_content', parent_location=course.location, user_id=self.user.id, publish_item=False, source_library_id=unicode(library_key), **(other_settings or {}) )
return ItemFactory.create( category="html", parent_location=self.library.location, user_id=self.user.id, publish_item=False )
if user is None: user = self.user if user not in self.session_data: self.session_data[user] = {} request = Mock(user=user, session=self.session_data[user]) _load_preview_module(request, descriptor)
update_url = reverse_usage_url("xblock_handler", usage_key) return self.client.ajax_post( update_url, data={ 'metadata': metadata, } )
response = self.client.get_json(LIBRARY_REST_URL) self.assertEqual(response.status_code, 200) return parse_json(response)
super(TestLibraryAccess, self).setUp() self.non_staff_user, self.non_staff_user_password = self.create_non_staff_user()
self.client.logout() super(TestLibraryAccess, self).tearDown()
with mock.patch.dict('django.conf.settings.FEATURES', {'ENABLE_MKTG_SITE': True}): self.assertEquals(self.get_about_page_link(), None)
self.assertEquals(self.get_about_page_link(), None)
course_key = SlashSeparatedCourseKey('mitX', '101', 'test') return utils.get_lms_link_for_about_page(course_key)
if tab_types: return [{'tab_type': tab_type} for tab_type in tab_types.split(',')] else: return []
self._test_visible_to_students(False, 'private_unreleased', self.future)
self._test_visible_to_students(False, 'private_released', self.past)
self._test_visible_to_students(False, 'public_unreleased', self.future, publish=True)
self._test_visible_to_students(True, 'public_released', self.past, publish=True)
self._test_visible_to_students(False, 'private_no_start', None)
self._test_visible_to_students(True, 'public_no_start', None, publish=True)
source = utils.find_release_date_source(item) self.assertEqual(source.location, expected_source.location) self.assertEqual(source.start, expected_source.start)
self._update_release_dates(self.date_one, self.date_one, self.date_one) self._verify_release_date_source(self.vertical, self.chapter)
self._update_release_dates(self.date_one, self.date_two, self.date_two) self._verify_release_date_source(self.vertical, self.sequential)
self._update_release_dates(self.date_one, self.date_one, self.date_one) self._verify_release_date_source(self.sequential, self.chapter)
self._update_release_dates(self.date_one, self.date_two, self.date_two) self._verify_release_date_source(self.sequential, self.sequential)
source = utils.find_staff_lock_source(item) self.assertEqual(source.location, expected_source.location) self.assertTrue(source.visible_to_staff_only)
self._update_staff_locks(True, False, False) self._verify_staff_lock_source(self.vertical, self.chapter)
self.assertIsNone(utils.find_staff_lock_source(self.orphan))
self._update_staff_locks(False, False, False) self.assertIsNone(utils.find_staff_lock_source(self.vertical))
self.assertFalse(utils.ancestor_has_staff_lock(self.orphan))
xblock.group_access = value self.store.update_item(xblock, self.user.id)
self.course.user_partitions = partitions self.course = self.store.update_item(self.course, ModuleStoreEnum.UserID.test)
self.block.group_access = group_access self.block = self.store.update_item(self.block, ModuleStoreEnum.UserID.test)
self.client.logout() ModuleStoreTestCase.tearDown(self)
self.assertNotIn('index_in_children_list', attributes) self.assertNotIn('parent_sequential_url', attributes) self.assertNotIn('parent_url', attributes)
self._check_verticals([self.vert_loc])
self.assert_created_course()
test_course_data = self.assert_created_course() course_id = _get_course_id(self.store, test_course_data) course_module = self.store.get_course(course_id) self.assertEquals(course_module.language, 'hr')
test_course_data = self.assert_created_course(number_suffix=uuid4().hex) self.assertTrue(are_permissions_roles_seeded(_get_course_id(self.store, test_course_data)))
test_course_data = self.assert_created_course() course_id = _get_course_id(self.store, test_course_data) delete_course_and_groups(course_id, self.user.id) self.assert_created_course()
self.course_data['org'] = 'University of California, Berkeley' self.assert_course_creation_failed(r"(?s)Unable to create course 'Robot Super Course'.*")
with mock.patch.dict('django.conf.settings.FEATURES', {'DISABLE_COURSE_CREATION': True}): self.assert_created_course()
with mock.patch.dict('django.conf.settings.FEATURES', {'ENABLE_CREATOR_GROUP': True}): self.assert_created_course()
resp = self.client.ajax_post('/course/', self.course_data) self.assertEqual(resp.status_code, 403)
resp = self.client.get_html('/home/') self.assertContains( resp, '<h1 class="page-header">Studio Home</h1>', status_code=200, html=True )
course = CourseFactory.create() self.assertIsInstance(course, CourseDescriptor)
course = CourseFactory.create() item = ItemFactory.create(parent_location=course.location) self.assertIsInstance(item, SequenceDescriptor)
resp = self.client.get_html(get_url('course_handler', course_key, 'course_key_string')) return resp
response = self.client.get_html('/course/edX/test') self.assertEquals(response.status_code, 404)
return html.cssselect('.course-item[data-course-key="{}"]'.format(unicode(course_key)))
return html.cssselect('.courses-processing li[data-course-key="{}"]'.format(unicode(course_key)))
super(TestProctoredExams, self).setUp() self.course = CourseFactory.create( org='edX', course='900', run='test_run', enable_proctored_exams=True )
self.client.logout() super(TestUsersDefaultRole, self).tearDown()
return json.loads(response.content)
return User.objects.get(email=email)
return Registration.objects.get(user__email=email)
return self.get(path, data or {}, follow, HTTP_ACCEPT="text/html", **extra)
return self.get(path, data or {}, follow, HTTP_ACCEPT="application/json", **extra)
nonstaff, password = self.create_non_staff_user() client = AjaxEnabledTestClient() if authenticate: client.login(username=nonstaff.username, password=password) nonstaff.is_authenticated = lambda: authenticate return client, nonstaff
self.course = self.store.get_course(self.course.id)
self.course.save() self.store.update_item(self.course, self.user.id)
self.assertEqual(self.store.has_published_version(item), publish_state)
return Date().to_json(datetime_obj)
test_model = CourseMetadata.fetch(self.fullcourse) self.assertIn('giturl', test_model)
test_model = CourseMetadata.fetch(self.fullcourse) self.assertNotIn('giturl', test_model)
test_model = CourseMetadata.fetch(self.fullcourse) self.assertIn('edxnotes', test_model)
test_model = CourseMetadata.fetch(self.fullcourse) self.assertNotIn('edxnotes', test_model)
super(CourseGraderUpdatesTest, self).setUp() self.url = get_url(self.course.id, 'grading_handler') self.starting_graders = CourseGradingModel(self.course).graders
self._verify_editable(self._get_course_details_response(True))
self._verify_editable(self._get_course_details_response(False))
self._verify_editable(self._get_course_details_response(True))
def __init__(self, location, content): self.location = location self.content = content def get_id(self): return self.location.to_deprecated_son()
self.client.logout() ModuleStoreTestCase.tearDown(self)
global_admin = AdminFactory() for role in (CourseStaffRole, CourseInstructorRole): auth.add_users(global_admin, role(course.id), user)
world.wait_for(lambda _driver: len(world.browser.find_by_css('div.ui-loading.is-hidden')) > 0)
assert_equal(name, world.browser.find_by_css('.problem-header').text)
index = world.get_setting_entry_index(DISPLAY_NAME) world.set_field_value(index, "<script>alert('test')</script>") verify_modified_display_name_with_html() world.save_component()
world.browser.driver.execute_script(script, str(text)) world.wait_for_ajax_complete()
world.edit_component() type_in_codemirror(0, text) world.save_component()
staff_role = CourseStaffRole(course_key) staff_role.remove_users(*staff_role.users_with_role()) instructor_role = CourseInstructorRole(course_key) instructor_role.remove_users(*instructor_role.users_with_role())
if not xblock.has_children: return False for child in xblock.get_children(): if is_visible_to_specific_content_groups(child): return True return False
if not xblock.group_access: return False for partition in get_user_partition_info(xblock): if any(g["selected"] for g in partition["groups"]): return True return False
kwargs_for_reverse = {key_name: unicode(key_value)} if key_name else None if kwargs: kwargs_for_reverse.update(kwargs) return reverse('contentstore.views.' + handler_name, kwargs=kwargs_for_reverse)
return reverse_url(handler_name, 'course_key_string', course_key, kwargs)
return reverse_url(handler_name, 'library_key_string', library_key, kwargs)
return reverse_url(handler_name, 'usage_key_string', usage_key, kwargs)
return settings.FEATURES.get('ENABLE_COURSEWARE_INDEX', False)
def __init__(self, message, error_list): super(SearchIndexingError, self).__init__(message) self.error_list = error_list
return settings.FEATURES.get(cls.ENABLE_INDEXING_KEY, False)
return usage_id
return item.location.version_agnostic().replace(branch=None)
indexed_count = cls.index(modulestore, structure_key) if indexed_count: cls._track_index_request(cls.INDEX_EVENT['name'], cls.INDEX_EVENT['category'], indexed_count) return indexed_count
data = { "indexed_count": indexed_count, 'category': category, } tracker.emit( event_name, data )
return None
pass
return {}
return structure_key
return modulestore.get_course(structure_key, depth=None)
return {"course": unicode(normalized_structure_key), "org": normalized_structure_key.org}
return cls._do_reindex(modulestore, course_key)
CourseAboutSearchIndexer.index_about_information(modulestore, structure)
return normalize_key_for_search(structure_key)
return modulestore.get_library(structure_key, depth=None)
return {"library": unicode(normalized_structure_key)}
return usage_id.replace(library_key=(usage_id.library_key.replace(version_guid=None, branch=None)))
return cls._do_reindex(modulestore, library_key)
return self.source_from(self, **kwargs)
return {"course": unicode(normalized_structure_key), "org": normalized_structure_key.org}
def __init__(self, value): self.value = value
escaped_string_for_js = js_escaped_string(None) self.assertEquals(u"", escaped_string_for_js)
wrapper_frag = Fragment(content=new_content) wrapper_frag.add_frag_resources(fragment) return wrapper_frag
return wrap_fragment(frag, static_replace.replace_static_urls( frag.content, data_dir, course_id, static_asset_path=static_asset_path ))
sanitized_html_id = re.sub(r'[:-]', '_', html_id) return sanitized_html_id
the_dir = tempfile.mkdtemp(suffix=suffix, prefix=prefix, dir=dir) atexit.register(cleanup_tempdir, the_dir) return the_dir
if isinstance(arg, XBlock): return unicode(arg.location) else: return unicode(arg)
return zlib.compress(pickle.dumps(data, pickle.HIGHEST_PROTOCOL))
return remove_root(root, sorted(glob2.glob('{root}/{glob}'.format(root=root, glob=glob))))
return resource_filename(self.module, os.path.join(self.base_dir, name))
if self.base_dir is None: return False return resource_exists(self.module, os.path.join(self.base_dir, path))
path = self.path(name) return FileSystemStorage(path).open(path, mode)
return os.path.getsize(self.path(name))
return datetime.fromtimestamp(os.path.getatime(self.path(name)))
return datetime.fromtimestamp(os.path.getctime(self.path(name)))
return datetime.fromtimestamp(os.path.getmtime(self.path(name)))
raise NotImplementedError("Package resources do not support URLs")
raise NotImplementedError("Deleting files from a package is not supported")
return Response({"developer_message": developer_message}, status=status_code)
func_or_class.authentication_classes = ( OAuth2AuthenticationAllowInactiveUser, SessionAuthenticationAllowInactiveUser ) func_or_class.permission_classes = () if is_authenticated: func_or_class.permission_classes += (IsAuthenticated,) if is_user: func_or_class.permission_classes += (IsUserInUrl,) return func_or_class
return self.retrieve(request, *args, **kwargs)
field_errors = self._validate_patch(request.data) if field_errors: return Response({'field_errors': field_errors}, status=status.HTTP_400_BAD_REQUEST) return self.partial_update(request, *args, **kwargs)
return {'HTTP_AUTHORIZATION': 'Basic ' + base64.b64encode('%s:%s' % (username, password))}
return getattr(self.client, method)(*args, HTTP_X_EDX_API_KEY=TEST_API_KEY, **kwargs)
copy = self.get_json(obj["url"]) self.assertEqual(obj, copy)
self.assertEqual(response.status_code, 200)
self.assertEqual(response.status_code, 403)
self.assertEqual(response.status_code, 400)
self.assertEqual(response.status_code, 405)
return unicode(data)
try: return CourseKey.from_string(data) except InvalidKeyError as ex: raise serializers.ValidationError("Invalid course key: {msg}".format(msg=ex.msg))
return unicode(data)
return self.page.paginator.count
return self.page.paginator.num_pages
return self._get_dot_token(access_token) or self._get_dop_token(access_token)
token_query = dop_models.AccessToken.objects.select_related('user') return token_query.filter(token=access_token).first()
def __init__(self, detail): if isinstance(detail, dict): self.detail = detail else: super(_DictAPIException, self).__init__(detail)
pass
def __init__(self, method, detail=None): if isinstance(detail, dict): self.detail = detail else: super(MethodNotAllowed, self).__init__(method, detail)
def has_permission(self, request, view): if request.user.is_staff: return True return super(IsUserInUrlOrStaff, self).has_permission(request, view)
pass
secret = secret or self.JWT_SECRET_KEY token = jwt.encode(payload, secret) return token
payload = self.default_payload(user=user, ttl=ttl) payload.update(overrides) return self.generate_token(payload)
username = serializers.CharField() email = serializers.CharField()
mock_object = Mock() object_config = { 'pk': obj_id, 'name': "object {}".format(obj_id) } mock_object.configure_mock(**object_config) return mock_object
return self.ROOT + value
self.assertEqual(self.field.to_representation(value), value)
auth = self._create_authorization_header(token) return self.csrf_client.get(target_url, params, HTTP_AUTHORIZATION=auth)
auth = self._create_authorization_header(token) return self.csrf_client.post(target_url, HTTP_AUTHORIZATION=auth)
response = self.get_with_bearer_token('/oauth2-test/') self.assertEqual(response.status_code, status.HTTP_200_OK)
response = self.csrf_client.post( '/oauth2-test/', data={'access_token': self.access_token.token} ) self.assertEqual(response.status_code, status.HTTP_200_OK)
response = self.post_with_bearer_token('/oauth2-test/') self.assertEqual(response.status_code, status.HTTP_200_OK)
self.access_token.delete() response = self.post_with_bearer_token('/oauth2-test/') self.check_error_codes( response, status_code=status.HTTP_401_UNAUTHORIZED, error_code=authentication.OAUTH2_TOKEN_ERROR_NONEXISTENT )
response = self.post_with_bearer_token('/oauth2-test/', token=self.refresh_token.token) self.check_error_codes( response, status_code=status.HTTP_401_UNAUTHORIZED, error_code=authentication.OAUTH2_TOKEN_ERROR_NONEXISTENT )
def __init__(self, user=None, course_id=None): self.user = user self.course_id = course_id
def __init__(self, user=None, course_id=None): super(TestCcxObject, self).__init__(user, course_id) self.coach = user
self.request.user = user self.assertEqual(self.permission.has_object_permission(self.request, None, self.obj), permitted)
user = UserFactory.create(is_staff=True) self.assert_user_has_object_permission(user, True)
user = UserFactory.create() self.obj.user = user self.assert_user_has_object_permission(user, True)
user = UserFactory.create() self.assert_user_has_object_permission(user, False)
self.request.user = UserFactory.create(is_staff=True) self.assertTrue(self.permission.has_permission(self.request, None))
milestones = milestones_api.get_milestones("{usage_key}{qualifier}".format( usage_key=prereq_content_key, qualifier=GATING_NAMESPACE_QUALIFIER )) for milestone in milestones: milestones_api.remove_milestone(milestone.get('id'))
mock_plugin = Mock() mock_plugin.type = tab_type mock_plugin.priority = priority return mock_plugin
return cls.default() | { cls.PAYLOAD_EXTRA_FIELDS }
return frozenset()
if tolerates is None: return cls.default() else: return tolerates
assert_event_matches(expected, actual, tolerate=EventMatchTolerates.strict())
return '\n'.join([(' ' * spaces) + l for l in pprint.pformat(text).splitlines()])
return len(get_event_differences(expected_event, actual_event, tolerate=tolerate)) == 0
self.assertEqual(len(httpretty.httpretty.latest_requests), count)
program_config = self.create_programs_config(enabled=False) actual = get_edx_api_data(program_config, self.user, 'programs') self.assertTrue(mock_warning.called) self.assertEqual(actual, [])
fragment = Fragment(content) fragment.add_css('body {background-color:red;}') fragment.add_javascript('alert("Hi!");') return fragment
if not expected_url.startswith("/"): expected_url = "/" + expected_url self.assertEquals(expected_url, actual_url)
course = CourseFactory.create() self.verify_url( unicode(course.id.make_asset_key('asset', course.course_image)), course_image_url(course) )
course = CourseFactory.create(course_image='', default_store=default_store) self.assertEquals( 'static/test.png', course_image_url(course), )
def __init__(self): self.data = {}
return self.func_to_count(param)
return self.func_to_count(param1, param2)
return realpath(abspath(rpath))
return not resolved(joinpath(base, path)).startswith(base)
self._cache = cache
self._cache.delete(self._encode_root_cache_key(root_block_usage_key)) logger.info( "Deleted BlockStructure %r from the cache.", root_block_usage_key, )
if cls.USE_PLUGIN_MANAGER: return set(cls.get_available_plugins().itervalues()) else: return set()
return self.get_block_keys()
return self._block_relations[usage_key].parents if usage_key in self else []
return self._block_relations[usage_key].children if usage_key in self else []
return self._block_relations.iterkeys()
return traverse_topologically( start_node=self.root_block_usage_key, get_parents=self.get_parents, get_children=self.get_children, filter_func=filter_func, yield_descendants_of_unyielded=yield_descendants_of_unyielded, )
return traverse_post_order( start_node=self.root_block_usage_key, get_children=self.get_children, filter_func=filter_func, )
self._add_to_relations(self._block_relations, parent_key, child_key)
return self.get_transformer_data(transformer, TRANSFORMER_VERSION_KEY, 0)
return self._xblock_map[usage_key]
if not self._requested_xblock_fields: return for xblock_usage_key, xblock in self._xblock_map.iteritems(): for field_name in self._requested_xblock_fields: self._set_xblock_field(xblock_usage_key, xblock, field_name)
VERSION = 0
pass
pass
pass
return [self.modulestore.get_item(child) for child in self.children]
self.get_items_call_count += 1 item = self.blocks.get(block_key) if not item: raise ItemNotFoundError return item
self.set_call_count += 1 self.map[key] = val self.timeout_from_last_call = timeout
return self.map.get(key, default)
del self.map[key]
modulestore = MockModulestore() modulestore.set_blocks({ block_key: MockXBlock(block_key, children=children, modulestore=modulestore) for block_key, children in enumerate(children_map) }) return modulestore
with patch( 'openedx.core.lib.block_structure.transformer_registry.TransformerRegistry.get_registered_transformers' ) as mock_available_transforms: mock_available_transforms.return_value = {transformer for transformer in transformers} yield
parent_map = [[] for _ in children_map] for parent, children in enumerate(children_map): for child in children: parent_map[child].append(parent) return parent_map
cls._set_block_values(block_structure, cls.collect_data_key) cls.collect_call_count += 1
self._set_block_values(block_structure, self.transform_data_key)
cls._assert_block_values(block_structure, cls.collect_data_key)
cls._assert_block_values(block_structure, cls.transform_data_key)
for block_key in block_structure.topological_traversal(): block_structure.set_transformer_block_field( block_key, cls, data_key, cls._create_block_value(block_key, data_key) )
for block_key in block_structure.topological_traversal(): assert ( block_structure.get_transformer_block_field( block_key, cls, data_key, ) == cls._create_block_value(block_key, data_key) )
return data_key + 't1.val1.' + unicode(block_key)
pass
with mock_registered_transformers(self.registered_transformers): self.transformers += self.registered_transformers
self.clear() self.get_collected()
if self.structure_json: return json.loads(self.structure_json) return None
if self.structure: ordered_blocks = OrderedDict() self._traverse_tree(self.structure['root'], self.structure['blocks'], ordered_blocks) return ordered_blocks
def setUp(self): super(SignalDisconnectTestMixin, self).setUp() SignalHandler.course_published.disconnect(listen_for_course_publish)
course = modulestore().get_course(course_key, depth=0) if course is None: raise CourseNotFoundError return course
return course_metadata_utils.clean_course_key(self.location.course_key, padding_char)
return course_metadata_utils.url_name_for_course_location(self.location)
return course_metadata_utils.display_name_with_default(self)
return course_metadata_utils.has_course_started(self.start)
return course_metadata_utils.has_course_ended(self.end)
return course_metadata_utils.course_starts_within(self.start, days)
return course_metadata_utils.course_start_datetime_text( self.start, self.advertised_start, format_string, ugettext, strftime_localized )
return course_metadata_utils.course_start_date_is_default( self.start, self.advertised_start, )
return course_metadata_utils.course_end_datetime_text( self.end, format_string, strftime_localized )
if self.advertised_start: return u'string' elif self.start != DEFAULT_START_DATE: return u'timestamp' else: return u'empty'
if self.advertised_start: return self.advertised_start elif self.start != DEFAULT_START_DATE: return defaultfilters.date(self.start, "DATE_FORMAT") else: return None
return course_metadata_utils.may_certify_for_course( self.certificates_display_behavior, self.certificates_show_before_end, self.has_ended() )
return json.loads(self._pre_requisite_courses_json)
return [ CourseKey.from_string(course_overview['id']) for course_overview in CourseOverview.objects.values('id') ]
return 'self' if self.self_paced else 'instructor'
return unicode(self.id)
tab_id = models.CharField(max_length=50) course_overview = models.ForeignKey(CourseOverview, db_index=True, related_name="tabs")
return (self.small_width, self.small_height)
return (self.large_width, self.large_height)
self.set_config(True) super(CourseOverviewImageSetTestCase, self).setUp()
CourseOverviewImageConfig.objects.all().delete() CourseOverviewImageConfig.objects.create( enabled=enabled, small_width=200, small_height=100, large_width=400, large_height=200 )
return self.list_display
CourseOverview.objects.filter(id=course_key).delete() CourseOverview.load_from_module_store(course_key)
parser.add_argument( '--all', action='store_true', dest='all', default=False, help='Generate course overview for all courses.', )
super(TestGenerateCourseOverview, self).setUp() self.course_key_1 = CourseFactory.create().id self.course_key_2 = CourseFactory.create().id self.command = generate_course_overview.Command()
course_keys = CourseOverview.get_all_course_keys() for expected_course_key in courses: self.assertNotIn(expected_course_key, course_keys)
course_keys = CourseOverview.get_all_course_keys() for expected_course_key in courses: self.assertIn(expected_course_key, course_keys)
with self.assertRaises(CommandError): self.command.handle('not/found', all=False)
self.command.handle('fake/course/id', all=False) self.assertTrue(mock_log.exception.called)
form = self.FORM_CLASS(self.form_data, initial=getattr(self, 'initial', None)) self.assertEqual(form.is_valid(), expected_valid) return form
form = self.get_form(expected_valid=False) self.assertEqual(form.errors, {expected_field: [expected_message]})
form = self.get_form(expected_valid=True) self.assertDictEqual(form.cleaned_data, expected_cleaned_data)
if values and "" in values: raise ValidationError("This field cannot be empty.")
return urljoin(self.internal_service_url, '/api/v{}/'.format(self.api_version_number))
return urljoin(self.public_service_url, '/api/v{}/'.format(self.api_version_number))
js_url = urljoin(self.public_service_url, self.authoring_app_js_path) css_url = urljoin(self.public_service_url, self.authoring_app_css_path) return AuthoringAppConfig(js_url=js_url, css_url=css_url)
return self.cache_ttl > 0
return self.enabled and self.enable_student_dashboard
return ( self.enabled and self.enable_studio_tab and bool(self.authoring_app_js_path) and bool(self.authoring_app_css_path) )
return self.enabled and self.enable_certification
return self.enabled and self.xseries_ad_enabled
return self.enabled and self.program_listing_enabled
programs = get_programs(self.client.user) self.run_modes = self._flatten(programs)
id_token = get_id_token(student, api_config.OAUTH2_CLIENT_NAME) return EdxRestApiClient(api_config.internal_api_url, jwt=id_token)
return client.programs.complete.post({'completed_courses': course_certificates})['program_ids']
return [ credential['credential']['program_id'] for credential in get_user_credentials(student) if 'program_id' in credential['credential'] and credential['status'] == 'awarded' ]
client.user_credentials.post({ 'username': username, 'credential': {'program_id': program_id}, 'attributes': [] })
fields = dict(self.DEFAULTS, **kwargs) ProgramsApiConfig(**fields).save() return ProgramsApiConfig.current()
return [ credentials_factories.UserCredential( id=1, username='test', credential=credentials_factories.ProgramCredential( program_id=1 ) ), credentials_factories.UserCredential( id=2, username='test', credential=credentials_factories.ProgramCredential( program_id=2 ) ) ]
self.create_programs_config(enabled=False) actual = utils.get_programs(self.user) self.assertEqual(actual, [])
self.create_programs_config() mock_init.side_effect = Exception actual = utils.get_programs(self.user) self.assertEqual(actual, []) self.assertTrue(mock_init.called)
self.create_programs_config() self.mock_programs_api(status_code=500) actual = utils.get_programs(self.user) self.assertEqual(actual, [])
self.create_programs_config(enable_student_dashboard=False) actual = utils.get_programs_for_dashboard(self.user, self.COURSE_KEYS) self.assertEqual(actual, {})
self.create_programs_config() self.mock_programs_api(data={'results': []}) actual = utils.get_programs_for_dashboard(self.user, self.COURSE_KEYS) self.assertEqual(actual, {})
return [CourseEnrollmentFactory(user=self.user, course_id=c) for c in course_ids]
self.assertEqual(meter.progress, list(progresses))
return [program['course_codes'][cc]['display_name'] for cc in course_codes]
programs_config = self.create_programs_config(cache_ttl=cache_ttl) self.assertEqual(programs_config.is_cache_enabled, is_cache_enabled)
COURSE_CERT_AWARDED.send(**self.signal_kwargs) self.assertEqual(mock_is_certification_enabled.call_count, 1)
handle_course_cert_awarded(**self.signal_kwargs) self.assertEqual(mock_is_certification_enabled.call_count, 1) self.assertEqual(mock_task.call_count, 0)
user_preference = kwargs["instance"] user_preference._old_value = get_changed_fields_dict(user_preference, sender).get("value", None)
user_preference = kwargs["instance"] emit_setting_changed_event( user_preference.user, sender._meta.db_table, user_preference.key, user_preference._old_value, user_preference.value ) user_preference._old_value = None
return self._extra_fields_setting.get(field_name) in ["required", "optional"]
return self._extra_fields_setting.get(field_name) == "required"
authentication_classes = (authentication.SessionAuthentication,) permission_classes = (ApiKeyHeaderPermission,) queryset = User.objects.all().prefetch_related("preferences") serializer_class = UserSerializer paginate_by = 10 paginate_by_param = "page_size"
try: return data.get('code', None) except AttributeError: return None
visible_serialized_account = {} for field_name in field_whitelist: visible_serialized_account[field_name] = serialized_account.get(field_name, None) return visible_serialized_account
if len(new_name) < NAME_MIN_LENGTH: raise serializers.ValidationError( "The name field must be at least {} characters long.".format(NAME_MIN_LENGTH) ) return new_name
return AccountLegacyProfileSerializer.convert_empty_to_None(value)
return None if value == "" else value
return user_profile.requires_parental_consent()
return AccountLegacyProfileSerializer.get_profile_image(user_profile, user_profile.user)
config = settings.PROFILE_IMAGE_BACKEND storage_class = get_storage_class(config['class']) return storage_class(**config['options'])
return hashlib.md5(settings.PROFILE_IMAGE_SECRET_KEY + username).hexdigest()
return '{name}_{size}.{file_extension}'.format(name=name, size=size, file_extension=file_extension)
name = _make_profile_image_name(username) return {size: _get_profile_image_filename(name, size) for size in _PROFILE_IMAGE_SIZES}
try: existing_user = User.objects.get(username=username) existing_user_profile = UserProfile.objects.get(user=existing_user) except ObjectDoesNotExist: raise UserNotFound() return existing_user, existing_user_profile
self.assertEqual( actual_url, 'http://example-storage.com/profile-images/{name}_{size}.jpg?v={version}'.format( name=expected_name, size=expected_pixels, version=expected_version ) )
self.assertEqual( actual_url, '/static/default_{size}.png'.format(size=expected_pixels) )
return str((template_name, sorted(context.iteritems())))
client = getattr(self, api_client) user = getattr(self, user) client.login(username=user.username, password=self.test_password) return client
response = client.put(self.url, data=json.dumps(json_data), content_type=content_type) self.assertEqual(expected_status, response.status_code) return response
response = client.delete(self.url) self.assertEqual(expected_status, response.status_code) return response
self.send_get(self.anonymous_client, expected_status=401) self.send_patch(self.anonymous_client, {}, expected_status=401)
if preference_visibility == PRIVATE_VISIBILITY: self._verify_private_account_response(response, account_privacy=PRIVATE_VISIBILITY) else: self._verify_full_shareable_account_response(response, ALL_USERS_VISIBILITY, badges_enabled=True)
client = self.login_client(api_client, user) self.send_patch(client, {}, expected_status=403 if user == "staff_user" else 404)
legacy_profile = UserProfile.objects.get(id=self.user.id) name_change_info = legacy_profile.get_meta()["old_names"] self.assertEqual(expected_entries, len(name_change_info)) return name_change_info
all_courses = modulestore().get_courses() orgs_lowercase = [org.lower() for org in org_aliases] return [ course.id for course in all_courses if course.id.org.lower() in orgs_lowercase ]
start_time = time.time() yield execution_time = time.time() - start_time LOGGER.info(u"Execution time: {time} seconds".format(time=execution_time))
while True: rows = cursor.fetchmany(self.QUERY_INTERVAL) if not rows: break for row in rows: yield row
return u",".join([u'"{}"'.format(val) for val in values])
update_email_opt_in(user, org, is_opted_in)
pref = UserOrgTag.objects.filter(user=user).order_by("-modified") return pref[0].modified.isoformat(' ') if len(pref) > 0 else self.DEFAULT_DATETIME_STR
profile = UserProfile.objects.get(user=user) return profile.name
return dict([(pref.key, pref.value) for pref in user.preferences.all()])
user = UserSerializer() class Meta(object): model = UserPreference depth = 1
user = serializers.PrimaryKeyRelatedField(queryset=User.objects.all()) class Meta(object): model = UserPreference depth = 1
return getattr(cls.Meta, 'read_only_fields', '') + getattr(cls.Meta, 'explicit_read_only_fields', '')
self.method = method self.submit_url = submit_url self.fields = [] self._field_overrides = defaultdict(dict)
if isinstance(obj, Promise): return force_text(obj) super(LocalizedJSONEncoder, self).default(obj)
pass
pass
pass
pass
pass
pass
pass
pass
pass
def __init__(self, developer_message, user_message=None): self.developer_message = developer_message self.user_message = user_message
pass
def __init__(self, developer_message, user_message=None): self.developer_message = developer_message self.user_message = user_message
try: user_preferences = get_user_preferences(request.user, username=username) except UserNotAuthorized: return Response(status=status.HTTP_403_FORBIDDEN) except UserNotFound: return Response(status=status.HTTP_404_NOT_FOUND) return Response(user_preferences)
if username is None: username = requesting_user.username try: existing_user = User.objects.get(username=username) except ObjectDoesNotExist: raise UserNotFound() _check_authorized(requesting_user, username, allow_staff) return existing_user
if requesting_user.username != username: if not requesting_user.is_staff or not allow_staff: raise UserNotAuthorized()
if not timestamp: return False try: parse_datetime(timestamp) except ValueError: return False else: return True
return u"Invalid user preference key '{preference_key}'.".format(preference_key=preference_key)
self.send_get(self.anonymous_client, expected_status=401) self.send_patch(self.anonymous_client, {}, expected_status=401)
self.different_client.login(username=self.different_user.username, password=self.test_password) self.send_get(self.different_client, expected_status=404)
self.client.login(username=self.user.username, password=self.test_password) response = self.send_get(self.client) self.assertEqual({}, response.data)
self._do_create_preferences_test(True)
self._do_create_preferences_test(False)
self.url = reverse( self.url_endpoint_name, kwargs={'username': self.user.username, 'preference_key': preference_key} )
self.send_get(self.anonymous_client, expected_status=401) self.send_put(self.anonymous_client, "new_value", expected_status=401) self.send_delete(self.anonymous_client, expected_status=401)
self._do_create_preference_test(True)
self._do_create_preference_test(False)
client = self.login_client(api_client, user) new_value = "new value" self.send_put(client, new_value, expected_status=403 if user == "staff_user" else 404)
return self._tags[course_id].get(key)
self._tags[course_id][key] = value
pass
pass
if raise_error is not None: raise raise_error
return None
serialization_options = { 'favorite_editor': { 'default': 'vim', }, }
users = self.get_json(USER_LIST_URI)["results"] for user in users: if user["id"] == target_user.id: return user["url"] self.fail()
__test__ = True pass
self.user_preference.value = "new value" self.user_preference.save() self.assert_user_setting_event_emitted(setting=self.TEST_KEY, old=self.TEST_VALUE, new="new value")
self.user_preference.delete() self.assert_user_setting_event_emitted(setting=self.TEST_KEY, old=self.TEST_VALUE, new=None)
enable_course_home_improvements = BooleanField( default=False, verbose_name=_("Enable course home page improvements.") )
if self.themed(name): base = self.theme_location else: base = self.location path = safe_join(base, name) return os.path.normpath(path)
return page_title_breadcrumbs(*args)
return microsite.get_value(val_name, default=default, **kwargs)
return microsite.get_template_path(relative_path, **kwargs)
return microsite.is_request_in_microsite()
return urljoin(self.internal_service_url, '/api/v1/')
return urljoin(self.public_service_url, '/api/v1/')
return self.enabled and self.enable_learner_issuance
return self.enabled and self.enable_studio_authoring
fields = dict(self.CREDENTIALS_DEFAULTS, **kwargs) CredentialsApiConfig(**fields).save() return CredentialsApiConfig.current()
return [ factories.UserCredential( id=1, username='test', credential=factories.ProgramCredential() ), factories.UserCredential( id=2, username='test', credential=factories.ProgramCredential() ) ]
self.create_credentials_config() self.mock_credentials_api(self.user) actual = get_user_credentials(self.user) self.assertEqual(actual, self.CREDENTIALS_API_RESPONSE['results'])
self.create_credentials_config(enable_learner_issuance=False) actual = get_user_program_credentials(self.user) self.assertEqual(actual, [])
self.create_credentials_config() self.mock_credentials_api(self.user, data={'results': []}) actual = get_user_program_credentials(self.user) self.assertEqual(actual, [])
return cls.populate(modulestore().get_course(course_key))
raw_video = cls.fetch_about_attribute(course_key, 'video') if raw_video: return cls.parse_video_tag(raw_video)
video_id = cls.fetch_youtube_video_id(course_key) if video_id: return "http://www.youtube.com/watch?v={0}".format(video_id)
recomposed_video_tag = CourseDetails.recompose_video_tag(video_id) cls.update_about_item(course, 'video', recomposed_video_tag, user_id)
validate = URLValidator() try: validate(url) return True except ValidationError: return False
return
missing_course_key = CourseKey.from_string('course-v1:FakeOrganization+CN999+CR-FALL99') self.assertIsNone(ccxconapi.course_info_to_ccxcon(missing_course_key)) self.assertEqual(mock_post.call_count, 0)
mock_response = mock.Mock() mock_response.status_code = 500 mock_post.return_value = mock_response with self.assertRaises(ccxconapi.CCXConnServerError): ccxconapi.course_info_to_ccxcon(self.course_key)
mock_response = mock.Mock() mock_citc.return_value = mock_response course_id = u'course-v1:OrgFoo+CN199+CR-FALL01' tasks.update_ccxcon.delay(course_id) mock_citc.assert_called_once_with(CourseKey.from_string(course_id))
return cls.current().platform_key if cls.is_enabled() else ''
return '{0.org}_{0.course}'.format(course_key)
config = models.CourseTalkWidgetConfiguration.current() config.enabled = enabled config.platform_key = self.PLATFORM_KEY config.save()
try: return CreditProvider.objects.get(active=True, provider_id=provider_id) except cls.DoesNotExist: return None
return self.provider_id
cache.delete(CreditProvider.CREDIT_PROVIDERS_CACHE_KEY)
return cls.objects.get(course_key=course_key, enabled=True)
return unicode(self.course_key)
cache.delete(CreditCourse.CREDIT_COURSES_CACHE_KEY)
cls.objects.filter(id__in=requirement_ids).update(active=False)
try: return cls.objects.get( course__course_key=course_key, active=True, namespace=namespace, name=name ) except cls.DoesNotExist: return None
return cls.objects.filter(requirement__in=requirements, username=username)
return datetime.datetime.now(pytz.UTC) + datetime.timedelta( days=getattr(settings, "CREDIT_ELIGIBILITY_EXPIRATION_DAYS", 365) )
return cls.objects.filter( username=username, course__enabled=True, deadline__gt=datetime.datetime.now(pytz.UTC) ).select_related('course')
return cls.objects.filter( course__course_key=course_key, course__enabled=True, username=username, deadline__gt=datetime.datetime.now(pytz.UTC), ).exists()
return u"{user}, {course}".format( user=self.username, course=self.course.course_key, )
return u"{course}, {provider}, {status}".format( course=self.course.course_key, provider=self.provider.provider_id, status=self.status, )
return self.enabled and self.cache_ttl > 0
course_key = CourseKeyField() class Meta(object): model = CreditCourse exclude = ('id',)
return unicode(obj.course.course_key)
return ( CourseKey.from_string(course_key_or_id) if isinstance(course_key_or_id, basestring) else course_key_or_id )
return finders.FileSystemFinder().find(path)
return CreditCourse.is_credit_course(course_key=course_key)
return CreditEligibility.is_user_eligible_for_credit(course_key, username)
pass
pass
pass
pass
pass
pass
pass
pass
status_code = status.HTTP_400_BAD_REQUEST
def __init__(self, course_key): detail = _('[{course_key}] is not a valid course key.').format(course_key=course_key) super(InvalidCourseKey, self).__init__(detail)
request = api.create_credit_request(self.course_key, self.PROVIDER_ID, self.user.username) self.assertEqual(request['parameters']['user_mailing_address'], '')
statuses = api.get_credit_requests_for_user(self.USER_INFO["username"]) self.assertEqual(statuses[0]["status"], expected_status)
self._mock_ecommerce_courses_api(self.course_key, self.COURSE_API_RESPONSE) response_providers = get_credit_provider_display_names(self.course_key) self.assertListEqual(self.PROVIDERS_LIST, response_providers)
mock_init.side_effect = Exception response = get_credit_provider_display_names(self.course_key) self.assertTrue(mock_init.called) self.assertEqual(response, None)
self._mock_ecommerce_courses_api(self.course_key, self.COURSE_API_RESPONSE) CreditProvider.objects.all().update(active=False) self.assertEqual(get_credit_provider_display_names(self.course_key), [])
user = UserFactory.create() CourseEnrollment.objects.create( user=user, course_id=self.course.id, mode=enrollment_type, is_active=True ) return user
VerificationStatus.add_status_from_checkpoints( checkpoints=[self.first_checkpoint], user=user, status=status )
self.assertEqual(response.status_code, status_code) self.assertDictEqual(response.data, {'detail': msg})
self.client.logout() response = self.client.get(self.path) self.assertEqual(response.status_code, 401)
response = getattr(self.client, method)(self.path) self.assertEqual(response.status_code, 405)
return { 'course_key': unicode(credit_course.course_key), 'enabled': credit_course.enabled }
data = { 'username': username, 'course_key': unicode(course_id) } return self.client.post(self.path, json.dumps(data), content_type=JSON)
response = self.post_credit_request(None, 'a/b/c') self.assert_error_response(response, 'A username must be specified.')
request = CreditRequest.objects.get(uuid=uuid) self.assertEqual(request.status, expected_status)
request_uuid = self._create_credit_request_and_get_uuid() response = self._credit_provider_callback(request_uuid, 'approved', timestamp=timestamp) self.assertEqual(response.status_code, 400)
request_uuid = self._create_credit_request_and_get_uuid() response = self._credit_provider_callback(request_uuid, 'invalid') self.assertEqual(response.status_code, 400)
return '{path}?username={username}&course_key={course_key}'.format( path=reverse(self.view_name), username=eligibility.username, course_key=eligibility.course.course_key )
self.assert_valid_get_response(self.eligibility)
staff = AdminFactory(password=self.password) self.client.logout() self.client.login(username=staff.username, password=self.password) self.assert_valid_get_response(self.eligibility)
try: return self.store.get_item(location) except ItemNotFoundError: return None
raise InvalidCreditRequirements
self.assert_requirement_status(grade, due_date, 'satisfied')
self.assert_requirement_status(0.70, self.EXPIRED_DUE_DATE, 'failed')
self.assert_requirement_status(grade, due_date, None)
if course_id is None: course_id = self.course.id return CourseEnrollment.enroll(self.user, course_id, mode='honor')
self.assertIsNone(self.service.get_credit_state(0, self.course.id))
self.assertIsNone(self.service.get_credit_state(self.user.id, self.course.id))
enrollment = self.enroll() enrollment.is_active = False enrollment.save() self.assertIsNone(self.service.get_credit_state(self.user.id, self.course.id))
xblocks = get_course_blocks(course_key, category) return xblocks
return cls.objects.get_or_create( course_id=course_id, group_type=group_type, name=name )
instance.course_user_group.users.remove(instance.user) instance.course_user_group.save()
return json.loads(self._cohorted_discussions)
self._cohorted_discussions = json.dumps(value)
if course_user_group is None: course_user_group, __ = CourseUserGroup.create(cohort_name, course_id) course_cohort, __ = cls.objects.get_or_create( course_user_group=course_user_group, defaults={'assignment_type': assignment_type} ) return course_cohort
return ( {"cohort_id": cohort.id, "cohort_name": cohort.name, "user_id": user_id} for user_id in user_id_iter for cohort in cohort_iter )
return get_course_cohort_settings(course_key).is_cohorted
cohort = get_cohort(user, course_key, use_cached=use_cached) return None if cohort is None else cohort.id
return CourseUserGroup.objects.get( course_id=course_key, group_type=CourseUserGroup.COHORT, name=name )
return CourseUserGroup.objects.get( course_id=course_key, group_type=CourseUserGroup.COHORT, id=cohort_id )
return CourseUserGroup.objects.filter(course_id=course_key, group_type=CourseUserGroup.COHORT, name=name).exists()
course_cohort = user_group.cohort return course_cohort.assignment_type
random_cohorts = CourseUserGroup.objects.filter( course_id=user_group.course_id, group_type=CourseUserGroup.COHORT, cohort__assignment_type=CourseCohort.RANDOM ) return len(random_cohorts) == 1 and random_cohorts[0].name == user_group.name
return JsonResponse(data)
return re.split(r'[\s,]+', cstr)
CourseUserGroupPartitionGroup( course_user_group=cohort, partition_id=partition_id, group_id=group_id, ).save()
parser.add_argument( '--commit', action='store_true', dest='commit', default=False, help='Really commit the changes, otherwise, just dry run', )
for user in users: CourseEnrollment.enroll(user, course_key)
return username in [user.username for user in cohort.users.all()]
return { 'is_cohorted': True, 'always_cohort_inline_discussions': True, 'cohorted_inline_discussions': [], 'cohorted_course_wide_discussions': [], 'id': 1 }
cohort_tuple = namedtuple("Cohort", "name id user_count assignment_type user_partition_id group_id") return cohort_tuple( name=cohort.name, id=cohort.id, user_count=user_count, assignment_type=assignment_type, user_partition_id=user_partition_id, group_id=group_id )
self.verify_lists_expected_cohorts([])
response_dict = self.put_handler(self.course, expected_response_code=400) self.assertEqual("Cohort name must be specified.", response_dict.get("error"))
cohort = CohortFactory(course_id=self.course.id, users=[]) self._verify_non_staff_cannot_access(users_in_cohort, "GET", [unicode(self.course.id), cohort.id])
cohort = CohortFactory(course_id=self.course.id, users=[]) self._verify_non_staff_cannot_access( add_users_to_cohort, "POST", [unicode(self.course.id), cohort.id] )
response_dict = self.request_add_users_to_cohort("", self.cohort1, self.course) self.verify_added_users_to_cohort( response_dict, self.cohort1, self.course, expected_added=[], expected_changed=[], expected_present=[], expected_unknown=[] )
cohort = CohortFactory(course_id=self.course.id, users=[]) self._verify_non_staff_cannot_access( remove_user_from_cohort, "POST", [unicode(self.course.id), cohort.id] )
cohort = CohortFactory(course_id=self.course.id, users=[]) response_dict = self.request_remove_user_from_cohort(None, cohort) self.verify_removed_user_from_cohort( None, response_dict, cohort, expected_error_msg='No username specified' )
self._verify_non_staff_cannot_access(cohort_discussion_topics, "GET", [unicode(self.course.id)])
if extracted: self.users.add(*extracted) for user in self.users.all(): CohortMembership.objects.create( user=user, course_user_group=self, )
class Meta(object): model = CourseCohort
return "{course}_{run}_{name}".format( course=course.location.course, run=course.url_name, name=name )
return topic_name_to_id(course, name)
super(TestCohorts, self).setUp() self.toy_course_key = ToyCourseFactory.create().id
cohort = CohortFactory(course_id=course_id, name=cohort_name) CourseCohortFactory(course_user_group=cohort, assignment_type=assignment_type) return cohort
course = modulestore().get_course(self.toy_course_key) config_course_cohorts(course, is_cohorted=True) self.assertEqual([], cohorts.get_course_cohorts(course))
link = CourseUserGroupPartitionGroup( course_user_group=cohort, partition_id=partition_id, group_id=group_id, ) link.save() return link
self.assertEqual( CohortPartitionScheme.get_group_for_user( self.course_key, self.student, partition or self.user_partition, use_cached=False ), group )
self.course.user_partitions.append(self.random_user_partition) self.assertIsNone(get_cohorted_user_partition(self.course))
self._verify_masquerade_for_group(self.user_partition.groups[0]) self._verify_masquerade_for_group(self.user_partition.groups[1]) self._verify_masquerade_for_group(None)
self._verify_masquerade_for_all_groups()
return [(unicode(path_item.usage_key), path_item.display_name) for path_item in path]
unique_together = ('user', 'usage_key')
return [parse_path_data(path) for path in self._paths] if self._paths else self._paths
self._paths = [prepare_path_for_serialization(path) for path in value] if value else value
optional_fields = params.get('fields', '').split(',') return DEFAULT_FIELDS + [field for field in optional_fields if field in OPTIONAL_FIELDS]
model = Bookmark fields = ( 'id', 'course_id', 'usage_id', 'block_type', 'display_name', 'path', 'created', )
return "{0},{1}".format(bookmark.user.username, bookmark.usage_key)
pass
bookmark = Bookmark.objects.get(user=user, usage_key=usage_key) bookmark.delete() _track_event('edx.bookmark.removed', bookmark)
return self._bookmarks_cache(course_key, fetch=True)
usage_id = unicode(usage_key) bookmarks_cache = self._bookmarks_cache(usage_key.course_key, fetch=True) for bookmark in bookmarks_cache: if bookmark['usage_id'] == usage_id: return True return False
mock_tracker.assert_any_call( event_name, kwargs, )
with self.assertNumQueries(1): with self.assertRaises(ObjectDoesNotExist): api.get_bookmark(user=self.other_user, usage_key=self.vertical_1.location)
client = APIClient() client.login(username=user.username, password=self.TEST_PASSWORD) return client
url = url + '?' + query_parameters if query_parameters else url response = client.get(url) self.assertEqual(expected_status, response.status_code) return response
response = client.post(url, data=json.dumps(data), content_type=content_type) self.assertEqual(expected_status, response.status_code) return response
response = client.delete(url) self.assertEqual(expected_status, response.status_code) return response
return { 'user': user or self.user, 'usage_key': block.location, 'course_key': block.location.course_key, 'display_name': block.display_name, }
bookmark_data = self.get_bookmark_data(self.vertical_4) bookmark, __ = Bookmark.create(bookmark_data) bookmark_data['display_name'] = self.vertical_4.display_name_with_default self.assert_bookmark_model_is_valid(bookmark, bookmark_data)
for create_data, additional_data_to_expect in data: xblock_cache = XBlockCache.create(create_data) create_data.update(additional_data_to_expect) self.assert_xblock_cache_data(xblock_cache, create_data)
block_info.setdefault('paths', []) block_info['paths'].append(current_path) for child_block_info in block_info['children']: add_path_info(child_block_info, current_path + [block_info])
return cls.api_access_status(user) == cls.APPROVED
try: return cls.objects.get(user=user).status except cls.DoesNotExist: return None
log.info('Approving API request from user [%s].', self.user.id) self.status = self.APPROVED self.save()
log.info('Denying API request from user [%s].', self.user.id) self.status = self.DENIED self.save()
def __unicode__(self): return u'ApiAccessConfig [enabled={}]'.format(self.enabled)
if created: _send_new_pending_email(instance)
if instance.id and not instance.contacted: old_instance = ApiAccessRequest.objects.get(pk=instance.id) if instance.status != old_instance.status: _send_decision_email(instance)
return { 'id': self.id, 'name': self.name, 'query': self.query, 'viewers': self.viewers, }
if ApiAccessRequest.api_access_status(request.user) is not None: return redirect(reverse('api_admin:api-status')) return super(ApiRequestView, self).get(request)
template_name = 'api_admin/terms_of_service.html'
return render_to_response('api_admin/catalogs/search.html')
def render(self, name, value, attrs=None): return super(ViewersWidget, self).render(name, ', '.join(value), attrs)
return [username.strip() for username in value.split(',')]
if ApiAccessConfig.current().enabled: return view_func(view_obj, *args, **kwargs) return HttpResponseNotFound()
response = self.client.get(self.url) self.assertEqual(response.status_code, 200)
self.client.logout() response = self.client.get(self.url) self.assertEqual(response.status_code, 302)
ApiAccessRequestFactory(user=self.user) response = self.client.get(self.url) self.assertRedirects(response, reverse('api_admin:api-status'))
self.assertFalse(ApiAccessRequest.objects.all().exists()) response = self.client.post(self.url, VALID_DATA) self._assert_post_success(response)
ApiAccessConfig(enabled=False).save() response = self.client.get(self.url) self.assertEqual(response.status_code, 404)
ApiAccessConfig(enabled=False).save() response = self.client.post(self.url) self.assertEqual(response.status_code, 404)
response = self.client.get(self.url) self.assertRedirects(response, reverse('api_admin:api-request'))
self.client.logout() response = self.client.get(self.url) self.assertEqual(response.status_code, 302)
ApiAccessConfig(enabled=False).save() response = self.client.get(self.url) self.assertEqual(response.status_code, 404)
other_user = UserFactory() self.assertFalse(ApiAccessRequest.has_api_access(other_user))
return datetime.datetime.utcnow().replace(tzinfo=utc)
return ProfileImageView().post(request, username)
storage = get_profile_image_storage() for name in profile_image_names.values(): storage.delete(name)
return image.convert('RGB')
return image.resize((side_length, side_length), Image.ANTIALIAS)
exif_dict = piexif.load(exif) exif_dict['0th'][piexif.ImageIFD.Orientation] = orientation return piexif.dump(exif_dict)
exif_dict = piexif.load(exif) return exif_dict['0th'].get(piexif.ImageIFD.Orientation)
return ', '.join([', '.join(IMAGE_TYPES[ft].extensions) for ft in IMAGE_TYPES.keys()])
with make_uploaded_file( dimensions=(1, 1), extension=".png", content_type="image/png", force_size=upload_size ) as uploaded_file: self.check_validation_result(uploaded_file, expected_failure_message)
with make_uploaded_file(extension=extension, content_type=content_type) as uploaded_file: self.check_validation_result(uploaded_file, expected_failure_message)
anonymous_client = APIClient() request_method = getattr(anonymous_client, method) response = request_method(self.url) self.check_response(response, 401) self.assert_no_events_were_emitted()
self.assert_user_setting_event_emitted( setting='profile_image_uploaded_at', old=old, new=new )
self.check_anonymous_request_rejected('post') self.assertFalse(mock_log.info.called)
self.assert_user_setting_event_emitted( setting='profile_image_uploaded_at', old=TEST_UPLOAD_DT, new=None )
self.check_anonymous_request_rejected('delete') self.assertFalse(mock_log.info.called)
_view_name = 'profile_image_upload' _replacement_method = 'openedx.core.djangoapps.profile_images.views.ProfileImageView.post'
_view_name = "profile_image_remove" _replacement_method = 'openedx.core.djangoapps.profile_images.views.ProfileImageView.delete'
with patch('openedx.core.djangoapps.safe_sessions.middleware.log.' + log_level) as mock_log: yield mock_log.assert_any_call(log_string)
with self.assert_no_error_logged(): with self.assert_no_warning_logged(): yield
with patch('openedx.core.djangoapps.safe_sessions.middleware.log.warning') as mock_log: yield self.assertFalse(mock_log.called)
with patch('openedx.core.djangoapps.safe_sessions.middleware.log.error') as mock_log: yield self.assertFalse(mock_log.called)
with self.assert_logged(r'SafeCookieData signature error .*|test_session_id|.*: ' + sig_error_string): yield
with self.assert_signature_error_logged('Signature .* does not match'): yield
with self.assert_logged(r'SafeCookieData .* is not bound to user'): yield
with self.assert_logged('SafeCookieData BWC parse error'): yield
with self.assert_logged('SafeCookieData not created due to invalid value for session_id'): yield
with self.assert_logged_with_message( "SafeCookieData user at request '{}' does not match user at response: '{}'".format( user_at_request, user_at_response ), log_level=log_level, ): yield
request = RequestFactory() request.COOKIES = {} request.META = {} request.path = '/' return request
self.assertIsNone(getattr(self.request, 'session', None))
self.assertIsNone(self.request.session.get(SESSION_KEY))
self.assertEquals( SafeSessionMiddleware.get_user_id_from_session(self.request), self.user.id )
with patch('django.http.HttpResponse.set_cookie') as mock_delete_cookie: self.assert_response(set_request_user=set_request_user, set_session_cookie=set_session_cookie) self.assertEquals(mock_delete_cookie.called, expect_delete_called)
if self.request.COOKIES.get(settings.SESSION_COOKIE_NAME): self.client.response.cookies[settings.SESSION_COOKIE_NAME] = self.request.COOKIES[ settings.SESSION_COOKIE_NAME ]
self.assertDictEqual(cookie_data1.__dict__, cookie_data2.__dict__)
return signature[start:end] + 'x' * (end - start) + signature[end:]
def __init__(self, error_message): super(SafeCookieError, self).__init__(error_message) log.error(error_message)
cls._validate_cookie_params(session_id, user_id) safe_cookie_data = SafeCookieData( cls.CURRENT_VERSION, session_id, key_salt=get_random_string(), signature=None, ) safe_cookie_data.sign(user_id) return safe_cookie_data
return self.SEPARATOR.join([self.version, self.session_id, self.key_salt, self.signature])
data_to_sign = self._compute_digest(user_id) self.signature = signing.dumps(data_to_sign, salt=self.key_salt)
request.need_to_delete_cookie = True
return getattr(request, 'need_to_delete_cookie', False)
return getattr(request, 'is_from_logout', False)
default_level = None from_logout = _is_from_logout(request) if from_logout: default_level = logger.getEffectiveLevel() logger.setLevel(ERROR) try: yield finally: if from_logout: logger.setLevel(default_level)
self.login(email, password) self.enroll(self.course, verify=True)
email = staff.email password = 'test' self.login(email, password) self.enroll(self.course, verify=True)
if xblock_name is None: xblock_name = TestRecommender.XBLOCK_NAMES[0] url = self.get_handler_url(handler, xblock_name) return self.client.post(url, json.dumps(resource), '')
resource = {"id": resource_id} edited_recommendations = { key: value + " edited" for key, value in self.test_recommendations[self.resource_id].iteritems() } resource.update(edited_recommendations) return resource
self.check_event_response_by_http_status( 'edit_resource', self.generate_edit_resource(self.non_existing_resource_id), 400 )
self.check_event_response_by_http_status( 'edit_resource', self.generate_edit_resource(self.resource_id), 200 )
resource = self.generate_edit_resource(self.resource_id) for xblock_name in self.XBLOCK_NAMES: self.check_event_response_by_http_status('edit_resource', resource, 200, xblock_name)
resource = {"id": self.non_existing_resource_id, 'event': test_case['event']} self.check_event_response_by_http_status('handle_vote', resource, 400)
resource = {"id": self.resource_id, 'event': test_case['event']} self.check_event_response_by_key('handle_vote', resource, 'newVotes', test_case['new_votes'])
resource = {"id": self.non_existing_resource_id, 'reason': ''} self.check_event_response_by_http_status(test_case, resource, 400)
self.enroll_student(self.STUDENTS[0]['email'], self.STUDENTS[0]['password']) self.attempt_upload_file_and_verify_result(test_case, 'import_resources', self.initial_configuration)
self.enroll_staff(self.staff_user) self.attempt_upload_file_and_verify_result(test_case, 'import_resources', self.initial_configuration)
self.events.append({"event": event, "event_type": event_type}) old_publish(block, event_type, event)
self.events = []
)
super(XBlockTestCase, self).setUp()
self.login(email, password) self.enroll(self.course, verify=True)
email = staff.email password = 'test' self.login(email, password) self.enroll(self.course, verify=True)
if xblock_name is None: xblock_name = TestCrowdsourceHinter.XBLOCK_NAMES[0] url = self.get_handler_url(handler, xblock_name) return self.client.post(url, json.dumps(resource), '')
if xblock_name is None: xblock_name = TestCrowdsourceHinter.XBLOCK_NAMES[0] resp = self.call_event(handler, resource, xblock_name) self.assertEqual(resp[resp_key], resp_val) self.assert_request_status_code(200, self.course_url)
return " ".join(str(arg) for arg in args if arg)
cmd_str = ( 'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={report_dir}' ).format(report_dir=self.pa11y_report_dir) sh(cmd_str)
return None
print colorize('green', "Generating optimized static assets...") sh("paver update_assets --settings=test_static_optimized")
args = [self.system, '--settings=acceptance'] if self.fasttest: args.append('--skip-collect') call_task('pavelib.assets.update_assets', args=args)
super(NoseTestSuite, self).__exit__(exc_type, exc_value, traceback) test_utils.clean_mongo()
return [JsTestSubSuite(test_id, **self.opts) for test_id in Env.JS_TEST_ID_KEYS]
sh("mongo {host}:{port} {repo_root}/scripts/delete-mongo-test-dbs.js".format( host=MONGO_HOST, port=MONGO_PORT_NUM, repo_root=Env.REPO_ROOT, ))
print cmd, logfile run_background_process(cmd, out_log=logfile, err_log=logfile, cwd=cwd)
sh( "mongo {} --eval 'db.dropDatabase()' > /dev/null".format( Env.BOK_CHOY_MONGO_DATABASE, ) )
if not is_mongo_running(): msg = colorize('red', "Mongo is not running locally.") print msg sys.exit(1)
if not is_memcache_running(): msg = colorize('red', "Memcache is not running locally.") print msg sys.exit(1)
if not is_mysql_running(): msg = colorize('red', "MySQL is not running locally.") print msg sys.exit(1)
return run_multi_processes([cmd], out_log=out_log, err_log=err_log)
try: os.makedirs(PREREQS_STATE_DIR) except OSError: if not os.path.isdir(PREREQS_STATE_DIR): raise
for req_file in PYTHON_REQ_FILES: sh("pip install -q --disable-pip-version-check --exists-action w -r {req_file}".format(req_file=req_file))
options.mode = 'run' test_js(options)
options.mode = 'dev' test_js(options)
with open(report_file) as f: violations_list = f.readlines() num_lines = len(violations_list) return num_lines, violations_list
thresholds_option=thresholds_option
with open(filename, "w") as metric_file: metric_file.write(str(metric))
dir_name.rmtree_p() dir_name.mkdir_p()
if "Subprocess return code: 1" not in error_message: return False else: return True
observer.schedule(self, 'common/lib/xmodule/', recursive=True)
if not files: files = ["`{}`".format(coffeescript_files())] sh(cmd( "node_modules/.bin/coffee", "--compile", *files ))
sh('xmodule_assets common/static/xmodule') print("\t\tFinished processing xmodule assets.")
sh(cmd( "touch", 'lms/urls.py', 'cms/urls.py', ))
for sys in systems: sh(django_cmd(sys, settings, "collectstatic --noinput > /dev/null")) print("\t\tFinished collecting {} assets.".format(sys))
opts = parse_bokchoy_opts(options) opts['test_dir'] = 'performance' run_bokchoy(**opts)
parse_coverage( Env.BOK_CHOY_REPORT_DIR, Env.BOK_CHOY_COVERAGERC )
self.check_val('true', True)
self.check_val('false', False)
self.check_val('True', True)
self.check_val('False', False)
self.check_val('0', False)
read_data = "".join(textwrap.dedent(c) for c in content) return patch.object(pavelib.i18n, 'open', create=True, new=mock_open(read_data=read_data))
pass
self.verify_server_task("lms", options)
self.verify_server_task("studio", options)
self.verify_run_all_servers_task(options)
_mock_count.return_value = None with self.assertRaises(SystemExit): call_task('pavelib.quality.run_safecommit_report')
_mock_counts.return_value = {} with self.assertRaises(SystemExit): call_task('pavelib.quality.run_safelint')
_mock_counts.return_value = {'total': 0} call_task('pavelib.quality.run_safelint')
_mock_counts.return_value = {'total': 0} with self.assertRaises(SystemExit): call_task('pavelib.quality.run_safelint', options={"thresholds": "invalid"})
mock_count.return_value = None with self.assertRaises(BuildFailure): pavelib.quality.run_jshint("")
suite = BokChoyTestSuite('', num_processes=2, verbosity=3) with self.assertRaises(BuildFailure): BokChoyTestSuite.verbosity_processes_string(suite)
return tasks.environment.messages
return os.getcwd()
tasks.environment.messages = []
options = self.parse_options_string(options_string) self.reset_task_messages() call_task("pavelib.js_test.test_js_run", options=options) self.verify_messages(options=options, dev_mode=False)
options = self.parse_options_string(options_string) self.reset_task_messages() call_task("pavelib.js_test.test_js_dev", options=options) self.verify_messages(options=options, dev_mode=True)
return ", ".join(DOC_PATHS.keys())
verbose = getattr(options, "verbose", None) cmd = "i18n_tool extract" if verbose: cmd += " -vv" sh(cmd)
sh("i18n_tool generate")
sh("i18n_tool generate")
sh("i18n_tool generate --strict")
sh("i18n_tool transifex push")
sh("i18n_tool transifex pull")
sh('git clean -fdX conf/locale')
pass
resources = find_release_resources() sh("i18n_tool transifex push " + " ".join(resources))
resources = find_release_resources() sh("i18n_tool transifex pull " + " ".join(resources))
fancy = False
stochastic = False
uniform_batch_size = False
uniform_batch_size = True
fancy = None
stochastic = None
_base_iterator_cls = None
while length != self.batch_size: batch = self._base_iterator.next()
self.lengths = [len(s) for s in self._sequence_data] self.len_unique = np.unique(self.lengths)
if self.total_curr_counts == 0: self.reset() raise StopIteration()
while True: self.len_idx = np.mod(self.len_idx+1, len(self.len_unique)) curr_len = self.len_unique[self.len_idx] if self.len_curr_counts[curr_len] > 0: break
curr_batch_size = np.minimum(self._batch_size, self.len_curr_counts[curr_len]) curr_pos = self.len_indices_pos[curr_len]
curr_indices = self.len_indices[curr_len][curr_pos:curr_pos + curr_batch_size]
self.len_indices_pos[curr_len] += curr_batch_size self.len_curr_counts[curr_len] -= curr_batch_size self.total_curr_counts -= curr_batch_size return curr_indices
assert is_flat_specs(data_specs)
if hasattr(self._dataset, 'get'): rval = self._next(next_index) else: rval = self._fallback_next(next_index)
if hasattr(self, 'usesTime') and self.usesTime(): record.asctime = self.formatTime(record, self.datefmt)
if not record.exc_text: record.exc_text = self.formatException(record.exc_info)
s = s + record.exc_text.decode(sys.getfilesystemencoding())
top_level_logger.propagate = False
top_level_logger.setLevel(logging.DEBUG if debug else logging.INFO)
while top_level_logger.handlers: top_level_logger.handlers.pop()
fmt = CustomFormatter() handler = CustomStreamHandler(stdout=stdout, stderr=stderr, formatter=fmt) top_level_logger.addHandler(handler)
top_level_logger.propagate = True
top_level_logger.setLevel(logging.NOTSET)
while top_level_logger.handlers: top_level_logger.handlers.pop()
number_aware_alphabetical_key = cmp_to_key(number_aware_alphabetical_cmp)
new_message = ', '.join(str(arg) for arg in new_exc.args)
yaml_parse = None control = None cuda = None
if six.PY3: py_integer_types = (int, np.integer) py_number_types = (int, float, complex, np.number) else:
new_f.func_name = f.func_name return new_f
return wrapper
unknown = [k for k, w in known.items() if not w] known = dict((k, w) for k, w in known.items() if w)
return first_line.split(':')[2][0:10]
protocol_str = '0'
lush_magic = { 507333717: 'uint8', 507333716: 'int32', 507333713: 'float32', 507333715: 'float64' }
if config_file_path.endswith(suffix_to_strip): config_file_full_stem = config_file_path[0:-len(suffix_to_strip)] else: config_file_full_stem = config_file_path
assert False
encoding = {'encoding': 'latin-1'} if six.PY3 else {}
reraise_as("Couldn't open {0}".format(filepath))
reraise_as(IOError("Cannot open " + path + " but can open " + parent + "."))
assert False
image = image * 255. image = np.cast['uint8'](image)
if len(image.shape) == 3 and image.shape[2] == 1: image = image[:, :, 0]
fd, name = mkstemp(suffix='.png') os.close(fd)
out_shape = [(ishp + tsp) * tshp - tsp for ishp, tshp, tsp in zip(img_shape, tile_shape, tile_spacing)]
if output_pixel_vals: channel_defaults = [0, 0, 0, 255] else: channel_defaults = [0., 0., 0., 1.]
dt = out_array.dtype if output_pixel_vals: dt = 'uint8' out_array[:, :, i] = np.zeros(out_shape, dtype=dt) + \ channel_defaults[i]
out_array[:, :, i] = tile_raster_images( X[i], img_shape, tile_shape, tile_spacing, scale_rows_to_unit_interval, output_pixel_vals)
H, W = img_shape Hs, Ws = tile_spacing
dt = X.dtype if output_pixel_vals: dt = 'uint8' out_array = np.zeros(out_shape, dtype=dt)
this_img = scale_to_unit_interval( this_x.reshape(img_shape))
import logging import os import inspect import zipfile from tempfile import TemporaryFile
import numpy import theano from pylearn2.datasets.utlc import load_ndarray_dataset, load_sparse_dataset from pylearn2.utils import subdict, sharedX
expected = inspect.getargspec(load_ndarray_dataset)[0][1:] data = load_ndarray_dataset(conf['dataset'], **subdict(conf, expected))
if conf.get('normalize_on_the_fly', False): return data
if (valid_repr.shape[1] > valid_repr.shape[0]): valid_repr = numpy.dot(valid_repr, valid_repr.T) test_repr = numpy.dot(test_repr, test_repr.T)
valid_repr = numpy.floor((valid_repr / valid_repr.max())*999) test_repr = numpy.floor((test_repr / test_repr.max())*999)
valid_file = TemporaryFile() test_file = TemporaryFile()
valid_file.seek(0) test_file.seek(0)
if not conf.get('sparse', False): valid_set = valid_set.get_value(borrow=True) test_set = test_set.get_value(borrow=True)
if features is not None: valid_set = valid_set[:, features] test_set = test_set[:, features]
valid_repr = transform_valid(valid_set) test_repr = transform_test(test_set)
save_submission(conf, valid_repr, test_repr)
n_valid = valid_repr.shape[0] n_test = test_repr.shape[0]
import logging import os import functools from itertools import repeat import warnings
from pylearn2.utils.rng import make_np_rng
x, y, z = repr.get_value(borrow=True).T do_3d_scatter(x, y, z)
if classes is not None: label = label[:, classes]
if scipy.sparse.issparse(train): idx = label.sum(axis=1).nonzero()[0] return (train[idx], label[idx])
condition = label.any(axis=1) return tuple(var.compress(condition, axis=0) for var in (train, label))
masks = numpy.asarray([subset.sum(axis=0) for subset in data]).squeeze() nz_feats = combine(masks).nonzero()[0]
flo = numpy.floor sub = numpy.subtract mul = numpy.multiply div = numpy.divide mod = numpy.mod
self.batch_size = batch_size if (isinstance(dataset[0], theano.Variable)): self.dataset = [set.get_value(borrow=True) for set in dataset] else: self.dataset = dataset
set_limit = numpy.ceil(numpy.divide(set_sizes, set_batch)) self.limit = map(int, set_limit)
set_tsign = sub(set_limit, flo(div(set_sizes, set_batch))) set_tsize = mul(set_tsign, flo(div(set_range, set_limit)))
index_tab = [] for i in xrange(3): index_tab.extend(repeat(i, set_range[i]))
self.seed = seed rng = make_np_rng(seed, which_method="permutation") self.permut = rng.permutation(index_tab)
index = counter[chosen] minibatch = self.dataset[chosen][ index * self.batch_size:(index + 1) * self.batch_size ] counter[chosen] = (counter[chosen] + 1) % self.limit[chosen] yield minibatch
assert len(str(e))
from pylearn2.utils import utlc
handlers = logger.handlers level = logger.getEffectiveLevel()
assert handlers == logger.handlers assert level == logger.getEffectiveLevel()
iterator = SequentialSubsetIterator(10, 3, 4) for i in range(4): iterator.next()
iterator = SequentialSubsetIterator(10, 3, 3) for i in range(3): iterator.next()
iterator = SequentialSubsetIterator(10, 3, 5)
self.n_unique_specs = 0
assert source == '' return None
if isinstance(source, (tuple, list)): source, = source
spec_mapping = tuple( self._fill_mapping(sub_space, sub_source) for sub_space, sub_source in safe_zip( space.components, source))
rval = [None] * self.n_unique_specs
self._fill_flat(nested, self.spec_mapping, rval)
return None
idx = mapping if isinstance(flat, (tuple, list)): assert 0 <= idx < len(flat) return flat[idx] else: assert idx == 0 return flat
idx = mapping if isinstance(flat, CompositeSpace): assert 0 <= idx < len(flat.components) return flat.components[idx] else: assert idx == 0 return flat
assert self.n_unique_specs == 1
res_r = int(numpy.floor(last_pool_r/rs)) + 1 res_c = int(numpy.floor(last_pool_c/cs)) + 1
window = tensor.alloc(0.0, batch, channel, res_r, res_c, pr, pc) window.name = 'unravlled_winodows_' + name
res_r = int(numpy.floor(last_pool_r/rs)) + 1 res_c = int(numpy.floor(last_pool_c/cs)) + 1
window = tensor.alloc(0.0, batch, channel, res_r, res_c, pr, pc) window.name = 'unravlled_winodows_' + name
n_classes = n_classes.astype(theano.config.floatX) return sm * (1 - n_classes * min_val) + min_val
precision = [1.] recall = [0.] tp = 0 fp = 0 fn = len(pos_scores) count = fn
p_shared = sharedX(zv[:, 0:rows:pool_rows, 0:cols:pool_cols, :]) h_shared = sharedX(zv) z_shared = sharedX(zv)
p_shared = sharedX(zv[:, :, 0:rows:pool_rows, 0:cols:pool_cols]) h_shared = sharedX(zv) z_shared = sharedX(zv)
p_shared = sharedX(zv[:, 0:rows:pool_rows, 0:cols:pool_cols, :]) h_shared = sharedX(zv) z_shared = sharedX(zv)
grad_shared = sharedX(zv) z_shared = sharedX(zv)
grad_shared = sharedX(zv) z_shared = sharedX(zv)
buckets = 10 bucket_width = 1. / float(buckets) for i in xrange(buckets): lower_lim = i * bucket_width upper_lim = (i+1) * bucket_width
from pylearn2.gui.patch_viewer import PatchViewer
assert max(pd.max(), hd.max()) < .17
assert np.all((ps == 0) + (ps == 1)) assert np.all((hs == 0) + (hs == 1))
buckets = 10 bucket_width = 1. / float(buckets) for i in xrange(buckets): lower_lim = i * bucket_width upper_lim = (i+1) * bucket_width
assert max(pd.max(), hd.max()) < .17
assert np.all((ps == 0) + (ps == 1)) assert np.all((hs == 0) + (hs == 1))
buckets = 10 bucket_width = 1. / float(buckets) for i in xrange(buckets): lower_lim = i * bucket_width upper_lim = (i+1) * bucket_width
from pylearn2.gui.patch_viewer import PatchViewer
assert max(pd.max(), hd.max()) < .17
assert np.all((ps == 0) + (ps == 1)) assert np.all((hs == 0) + (hs == 1))
assert max(pd.max(), hd.max()) < .17
assert np.all((ps == 0) + (ps == 1)) assert np.all((hs == 0) + (hs == 1))
alpha = 1.5 beta = 0.75
ave = kl(Y, Y_hat, 1)
ave = elemwise_kl(Y, Y_hat)
for i in xrange(len(p)): assert p[i] == precision[i], (i, p[i], precision[i]) assert recall[i] == recall[i]
mean = X.mean(axis=1) if subtract_mean:
ddof = 1
if X.shape[1] == 1: ddof = 0
normalizers[normalizers < min_divisor] = 1.
messages = [
beta1 = sqrt_inner_product(bs)
bnorm = beta1 n_params = len(bs)
# dbar = dbarn epln = eplnn dlta = cs * dbar + sn * alpha gbar = sn * dbar - cs * alpha
xnorml = xnorm dl2s = [x for x in xs] xs = [x + tau * d for x, d in zip(xs, ds)]
return None, phi_a1
rvals, _ = scan( armijo, outputs_info=states, n_steps=n_iters, name='armijo', mode=theano.Mode(linker='cvm'), profile=profile)
c1 = TT.as_tensor_variable(c1) c2 = TT.as_tensor_variable(c2) maxiter = n_iters
xmin = TT.switch(cond, constant(numpy.nan), a + (-B + TT.sqrt(radical)) / (3 * A)) return xmin
D = fa C = fpa db = b - a * one
phi_aj = phi(a_j) derphi_aj = derphi(a_j)
phi_aj = phi(a_j) derphi_aj = derphi(a_j)
if obj <= best_obj: best_obj = obj best_alpha = alpha best_alpha_ind = ind
else: assert self.line_search_mode == 'exhaustive'
step_size = x if self.verbose: logger.info('best objective: {0}'.format(mn)) assert not np.isnan(mn)
if new_weight == 1.: self.new_weight.set_value(.01)
cur_out = self._func(*augmented) rval = [x + y for x, y in safe_zip(rval, cur_out)]
skiprows = 1 if headers else 0 x = np.loadtxt(test_path, delimiter=delimiter, skiprows=skiprows)
dataset = obj
HIDDEN_SIZE = 1000 SALT_PEPPER_NOISE = 0.4 GAUSSIAN_NOISE = 0.5
_mbce = MeanBinaryCrossEntropy() reconstruction_cost = lambda a, b: _mbce.cost(a, b) / ds.X.shape[1]
mb_data = MNIST(which_set='test').X[105:106, :]
pw = ParzenWindows(MNIST(which_set='test').X, .20) print(pw.get_ll(history))
_rcost = MeanBinaryCrossEntropy() reconstruction_cost = lambda a, b: _rcost.cost(a, b) / ds.X.shape[1]
c = GSNCost( [ (0, 1.0, reconstruction_cost),
(2, 2.0, classification_cost)
gsn._corrupt_switch = False
#np.sum(np.abs(y_hat - y), axis=1) != 0
data.extend([np.ones((1, 784))] * 2)
already_fixed = {}
currently_fixing = []
rval = shared(obj.get_value()) obj.__getstate__ = None
main(args=[])
from __future__ import print_function
train_algo = SGD( learning_rate = 0.1, cost = MeanSquaredReconstructionError(), batch_size = 10, monitoring_batches = 10, monitoring_dataset = trainset, termination_criterion = EpochCounter(max_epochs=MAX_EPOCHS_UNSUPERVISED), update_callbacks = None )
layer_trainers[-1].main_loop()
train_object.algorithm.termination_criterion.prop_decrease = 0.5 train_object.algorithm.termination_criterion.N = 1
from pylearn2.utils import serial
from pylearn2.datasets import cifar10
from pylearn2.datasets import preprocessing
train = cifar10.CIFAR10(which_set="train")
pipeline.items.append( preprocessing.ExtractPatches(patch_shape=(8, 8), num_patches=150000) )
pipeline.items.append(preprocessing.GlobalContrastNormalization( sqrt_bias=10., use_std=True))
pipeline.items.append(preprocessing.ZCA())
save_path.replace('\\', r'\\')
if not os.path.isdir(os.path.join(local_path, 'h5')): os.makedirs(os.path.join(local_path, 'h5'))
train = SVHN('splitted_train', path=local_path) check_dtype(train)
pipeline = preprocessing.Pipeline() pipeline.items.append(preprocessing.GlobalContrastNormalization(batch_size=5000)) pipeline.items.append(preprocessing.LeCunLCN((32,32)))
train.apply_preprocessor(pipeline, can_fit=True) del train
valid = SVHN('valid', path=local_path) check_dtype(valid) valid.apply_preprocessor(pipeline, can_fit=False)
test = SVHN('test', path=local_path) check_dtype(test) test.apply_preprocessor(pipeline, can_fit=False)
assert test.X.shape[0] % batch_size == 0
train.algorithm.termination_criterion = EpochCounter(max_epochs=1) train.extensions.pop(0) train.save_freq = 0 train.main_loop()
train.algorithm.termination_criterion = EpochCounter(max_epochs=1) train.extensions.pop(0) train.save_freq = 0 train.main_loop()
layers = [model.visible_layer] + model.hidden_layers
sampling_updates = model.get_sampling_updates(layer_to_state, theano_rng) assert layer_to_state[model.visible_layer] in sampling_updates
layer_to_state = model.make_layer_to_state(m) vis_sample = layer_to_state[model.visible_layer]
depth = len(b_list)
_sample_even_odd(W_list, b_list, new_nsamples, beta, odd=marginalize_odd) _activation_even_odd(W_list, b_list, new_nsamples, beta, odd=not marginalize_odd)
new_nsamples[not marginalize_odd] += pa_bias * (1. - beta)
depth = len(b_list)
keep_idx = numpy.arange(not marginalize_odd, depth, 2) for i in keep_idx: fe -= T.dot(samples[i], b_list[i]) * beta
log_ais_w = numpy.zeros(batch_size, dtype=floatX)
dlogz = log_mean(log_ais_w)
inference_fn(x)
nsamples[0].set_value(x) for ii, psample in enumerate(psamples): if ii > 0: nsamples[ii].set_value(psample.get_value())
x_likelihood = numpy.sum((-energy_fn(1.0) + hq - log_z)[:batch_size0])
likelihood = (i * likelihood + x_likelihood) / (i + batch_size0)
depth = len(samples)
wip1 = W_list[i+1] hi_mean += T.dot(samples[i+1], wip1.T) * beta
wi = W_list[i] hi_mean += T.dot(samples[i-1], wi) * beta
W_list = [None] + W_list
depth = len(b_list)
marginalize_odd = (depth % 2) == 0
fe_bp_h1 = free_energy_at_beta(W_list, b_list, nsamples, beta, pa_bias, marginalize_odd=marginalize_odd) free_energy_fn = theano.function([beta], fe_bp_h1)
metrics = {'ais': estimate_likelihood} datasets = {'mnist': MNIST}
from pylearn2.utils import serial from pylearn2.datasets import cifar10 from pylearn2.datasets import preprocessing
num_labels_by_type = numpy.array(norb.SmallNORB.num_labels_by_type, 'int') num_labels_by_type[instance_index] = len(new_to_old_instance)
figure.subplots_adjust(bottom=0.05)
return None
grid_indices = [0, ] * 5
label_to_row_indices = _make_label_to_row_indices(dataset.y)
object_image_index = [0, ] blank_image_index = [0, ] blank_label = _get_blank_label(dataset)
grid_dimension = [0, ]
for axes in all_axes: axes.get_xaxis().set_visible(False) axes.get_yaxis().set_visible(False)
lines[grid_dimension[0]] = '==> ' + lines[grid_dimension[0]]
image_pair = tuple(image_pair[0, :, :, :, 0])
num_dimensions += 1
image_index[0] = add_mod(image_index[0], step, len(row_indices))
gd = grid_dimension[0] grid_indices[gd] = add_mod(grid_indices[gd], step, len(grid_to_short_label[gd]))
image_index[0] = min(image_index[0], len(row_indices))
disable_left_right = (is_blank(grid_indices) and not (grid_dimension[0] in (0, 5)))
ele = (ele * 2 * numpy.pi) / 360. azi = (azi * 2 * numpy.pi) / 360.
grad = numpy.gradient(a) grad_x, grad_y = grad
frgd_img = to_img(data.X[i], 28) frgd_img = frgd_img.convert('L')
textid = 14 while textid == 14: textid = rng.randint(1, 113)
output['texture_id'][i] = textid output['texture_pos'][i] = (px, py)
frgd_arr = to_array(frgd_img) mask_arr = frgd_arr > 0.1
blend_arr = copy(patch_arr) blend_arr[mask_arr] = frgd_arr[mask_arr]
frgd_img = to_img(blend_arr, os)
if not os.path.isdir(orig_path): raise IOError("You need to download the SVHN format2 dataset MAT files " "before running this conversion script.")
cmp_mode = 'equal'
_, model_0_path, model_1_path = sys.argv
record = 0 clean = intersect while len(clean) > 0: bad_channel = [] for channel in clean: channel_0 = channels_0[channel] channel_1 = channels_1[channel]
if record == channel_0.length: bad_channel.append(channel) continue
libv = LibVersion() libv.print_versions() libv.print_exp_env_info(args.print_theano)
import pylearn2.config.yaml_parse
ret_list.append('%s: %s,' % (key, val))
hyper_parameters = expand(flatten(state.hyper_parameters), dict_type=ydict)
final_yaml_str = yaml_template % hyper_parameters
train_obj = pylearn2.config.yaml_parse.load(final_yaml_str)
train_obj.main_loop() state.results = jobman.tools.resolve(state.extract_results)(train_obj) return channel.COMPLETE
import argparse import gc import logging import os
import numpy as np
if os.getenv('DISPLAY') is None: try: import matplotlib matplotlib.use('Agg') except: pass
from pylearn2.utils import serial from pylearn2.utils.logger import ( CustomStreamHandler, CustomFormatter, restore_defaults )
phase_variable = 'PYLEARN2_TRAIN_PHASE' phase_value = 'phase%d' % (number + 1) os.environ[phase_variable] = phase_value
subobj.main_loop(time_budget=time_budget)
del subobj gc.collect()
print_monitor_cv.main(filename)
print_monitor_cv.main(filename, all=True)
os.remove(filename)
return s
prompt = len(channels.values()) > 1
for code in sorted_codes: print(code + '. ' + codebook[code])
else: final_codes ,= set(codebook.keys())
for idx, code in enumerate(sorted(final_codes)):
x = np.arange(len(channel.batch_record))
num_rows = max_num_channels // num_columns if num_rows * num_columns < max_num_channels: num_rows += 1
window_height = window_width * ((num_rows * 1.8) / num_columns) figure, all_axes = pyplot.subplots(num_rows, num_columns, squeeze=False, figsize=(window_width, window_height))
for axes_row in all_axes: for axes in axes_row: axes.get_xaxis().set_visible(False) axes.get_yaxis().set_visible(False)
mid = int(np.floor(kernel_shape/ 2.)) centered_X = X - convout[:,:,mid:-mid,mid:-mid]
sum_sqr_XX = conv2d(T.sqr(X), filters=filters, border_mode='full')
csv_file = open(path, 'r')
row = reader.next()
y = y[:m]
csv_file = open(path, 'r')
row = next(reader)
y = y[:m]
if rbm.nvis < rbm.nhid: width = rbm.nvis type = 'vis' else: width = rbm.nhid type = 'hid'
block_bits = width if (not max_bits or width < max_bits) else max_bits block_size = 2 ** block_bits
visbias_a = visbias
assert rbmA_params[0].shape[0] == rbmB_params[0].shape[0] assert len(rbmA_params[1]) == len(rbmB_params[1])
rbmA_params = [numpy.asarray(q, dtype=config.floatX) for q in rbmA_params] rbmB_params = [numpy.asarray(q, dtype=config.floatX) for q in rbmB_params]
v_sample = tensor.matrix('ais_v_sample') beta = tensor.scalar('ais_beta')
self.log_ais_w = numpy.zeros(n_runs, dtype=config.floatX)
if key_betas is not None: betas = numpy.hstack((betas, key_betas)) betas.sort()
state = self.v_sample0 ki = 0
if self.key_betas is not None and \ ki < len(self.key_betas) and \ bp1 == self.key_betas[ki]:
state = self.sample_fn(bp1, state)
dlogz = self.log_mean(log_ais_w)
self._build_data_specs()
if self._dirty: self.redo_theano()
myiterator = d.iterator(mode=i, batch_size=b, num_batches=n, data_specs=self._flat_data_specs, return_tuple=True, rng=sd)
if len(self._flat_data_specs[1]) == 0: X = () self.run_prereqs(X, d) a(*X)
self.run_prereqs(X, d) a(*X) actual_ne += self._flat_data_specs[0].np_batch_size(X)
self._build_data_specs()
batch_names = ['monitoring_%s' % s for s in self._flat_data_specs[1]] theano_args = self._flat_data_specs[0].make_theano_batch(batch_names)
c_mapping = DataSpecsMapping(channel.data_specs) channel_inputs = c_mapping.flatten(channel.graph_input, return_tuple=True) inputs = c_mapping.flatten(nested_theano_args[i + 1], return_tuple=True)
self.accum.append(function(theano_args, givens=g, updates=u, mode=self.theano_function_mode, name=function_name))
if not hasattr(self, '_datasets'): self._datasets = [self._dataset] del self._dataset
if '_dataset' in d: d['_datasets'] = [d['_dataset']] del d['_dataset']
m_space, m_source = model.get_monitoring_data_specs() spaces.append(m_space) sources.append(m_source)
nested_ipt = mapping.nest(ipt)
channels[prefix + name] = (raw_channels[name], cost_ipt, (spaces[i], sources[i]))
model_channels = model.get_monitoring_channels(nested_ipt[-1]) channels = {} for name in model_channels: channels[name] = (model_channels[name], nested_ipt[-1], (spaces[-1], sources[-1])) custom_channels.update(channels)
if hasattr(self, "doc"): doc = self.doc else: doc = None
from __future__ import print_function
import theano from theano import tensor try: from theano.sparse import SparseType except ImportError: warnings.warn("Could not import theano.sparse.SparseType") from theano.compile.mode import get_default_mode
self._params.update(l._params)
repr = [inputs]
del configure_custom
cluster_ids, mu = milk.kmeans(X, k)
if contains_nan(mu): logger.info('nan found') return X
for i in xrange(k): dists[:, i] = numpy.square((X - mu[i, :])).sum(axis=1)
mmd = min_dists.mean()
break
min_dist_inds = dists.argmin(axis=1)
return False
get_input_space = Model.get_input_space get_output_space = Model.get_output_space
import numpy from theano import tensor
self.nfac = nfac
self.aes = self._layers
self._corrupt_switch = True
self._sample_switch = True
self._bias_switch = True
for i in xrange(1, len(self.aes)): assert (self.aes[i].weights.get_value().shape[0] == self.aes[i - 1].nhid)
act_enc = activation_funcs[i + 1] act_dec = act_enc if i != 0 else activation_funcs[0] aes.append( Autoencoder(layer_sizes[i], layer_sizes[i + 1], act_enc, act_dec, tied_weights=tied) )
set_idxs = safe_zip(*minibatch)[0]
steps = [self.activations[:]]
for _ in xrange(len(self.aes) + walkback): steps.append(self._update(self.activations, clamped=clamped))
def wrap_f_init(*args): data = f_init(*args) length = len(data) / 2 return data[:length], data[length:] return wrap_f_init
state = (self._corrupt_switch, self._sample_switch, self._bias_switch)
return self._compiled_cache[2:]
f_init = compile_f_init() cc = self._compiled_cache self._compiled_cache = (state, indices, f_init, cc[3]) return self._compiled_cache[2:]
f_init = compile_f_init() f_step = compile_f_step() self._compiled_cache = (state, indices, f_init, f_step) return self._compiled_cache[2:]
if not include_first: results = results[1:]
for i, val in minibatch: if val is not None: activations[i] = val
odds = filter(lambda i: i not in skip_idxs, range(1, len(activations), 2))
precor = [None] * len(self.activations) for idx, val in evens_copy + odds_copy: assert precor[idx] is None precor[idx] = val assert None not in precor
clamped_val = clamp * initial
if symbolic: activations[idx] = T.switch(clamp, initial, activations[idx]) else: activations[idx] = np.switch(clamp, initial, activations[idx])
if self._sample_switch: self._apply_corruption(activations, self._layer_samplers, idx_iter) return activations
act_func = None if i == 0: act_func = self.aes[0].act_dec else: act_func = self.aes[i - 1].act_enc
if act_func is not None: activations[i] = act_func(activations[i])
data = np.asarray(data[skip:skip+trials])[:, 0, :, :]
labels = np.zeros_like(mean) labels[np.arange(labels.shape[0]), am] = 1.0
import logging
ml_cost = (self.free_energy_given_v(pos_v).mean() - self.free_energy_given_v(neg_v).mean())
zero_mean = rng.normal(size=shape) * self.sigma return zero_mean + v_mean
W_irange = 2 / numpy.sqrt(nvis * nhid)
self.B = sharedX(numpy.zeros(self.nvis) + B0, name='B', borrow=True)
batch_size = v.shape[0]
h_mean = self.mean_h_given_v(v) h_mean_shape = (batch_size, self.nhid) h_sample = rng.binomial(size=h_mean_shape, n = 1, p = h_mean, dtype = h_mean.dtype)
v_mean, v_var = self.mean_var_v_given_h_s(h_sample, s_sample) v_mean_shape = (batch_size, self.nvis) v_sample = rng.normal(size=v_mean_shape) * tensor.sqrt(v_var) + v_mean
return StackedBlocks(layers)
self.learning_rates = {} self.base_lr = theano._asarray(base_lr, dtype=theano.config.floatX)
self.iteration = sharedX(theano._asarray(0, dtype='int32'), name='iter')
self.annealed = sharedX(base_lr, 'annealed')
ups[self.annealed] = annealed ups[self.iteration] = self.iteration + 1
learn_rates = [annealed * self.learning_rates[p] for p in self.params]
l_ups, learn_rates = self.learning_rate_updates(gradients) safe_update(ups, l_ups)
p_up = dict(self.sgd_updates(self.params, gradients, learn_rates))
safe_update(ups, p_up)
return ups
self.input_space = VectorSpace(dim=self.nvis) self.input_source = 'features' self.latent_space = VectorSpace(dim=self.nhid)
if self.kl_integrator is None: self.kl_integrator = find_integrator_for(self.prior, self.posterior)
z = self.sample_from_p_z(num_samples=num_samples, **kwargs) theta = self.decode_theta(z) X = self.sample_from_p_x_given_z(num_samples=num_samples, theta=theta)
conditional_probs = T.nnet.sigmoid(conditional_params[0]) return self.theano_rng.uniform( size=(num_samples, self.ndim), dtype=theano.config.floatX ) < conditional_probs
return T.nnet.sigmoid(conditional_params[0])
import logging import sys
from pylearn2.blocks import Block from pylearn2.utils import sharedX
self._params = []
if mean is None: mean = X.mean(axis=0) X = X - mean
v, W = self._cov_eigen(X)
self.W = sharedX(W, name='W') self.v = sharedX(v, name='v') self.mean = sharedX(mean, name='mean')
self._update_cutoff()
if self.whiten: W = W / tensor.sqrt(self.v[:self.component_cutoff])
v, W = v[::-1], W[:, ::-1] return v, W
logger.info('computing mean') self.mean_ = numpy.asarray(X.mean(axis=0))[0, :]
return v[::-1], W.T[:, ::-1]
return v[::-1], W[:, ::-1]
mean = X.mean(axis=0) mean_matrix = csr_matrix(mean.repeat(n).reshape((d, n))).T X = X - mean_matrix
return v[::-1], W[:, ::-1]
self._update_cutoff()
self.n_observations = 0 self.minibatch_index = 0
self.Xt = numpy.zeros([self.n_eigen + self.minibatch_size, self.n_dim])
self.x_sum = numpy.zeros([self.n_dim])
self.Ut = numpy.zeros([self.n_eigen, self.n_dim])
row = self.n_eigen + self.minibatch_index self.Xt[row] = x
self.x_sum *= self.gamma self.x_sum += x
normalizer = (1.0 - pow(self.gamma, self.n_observations)) / \ (1.0 - self.gamma)
if self.centering: self.Xt[row] -= self.x_sum / normalizer
for i in range(self.n_eigen + self.minibatch_size): self.G[i,i] += self.regularizer
self.Ut = numpy.dot(self.V[:,-self.n_eigen:].transpose(), self.Xt)
self.Xt[:self.n_eigen,:] = self.Ut
normalizer = (1.0 - pow(self.gamma, self.n_observations - self.minibatch_index)) /\ (1.0 - self.gamma)
if not hasattr(self, 'mask_weights'): self.mask_weights = None
raise NotImplementedError()
raise NotImplementedError()
raise NotImplementedError()
self.desired_space = Conv2DSpace(shape=space.shape, channels=space.num_channels, axes=('c', 0, 1, 'b'))
raise NotImplementedError()
from pylearn2.costs.mlp import L1WeightDecay as _L1WD from pylearn2.costs.mlp import WeightDecay as _WD
dropout_input_mask_value = 0.
assert layer_name is None
if not hasattr(coeffs, '__iter__'): coeffs = [coeffs] * len(self.layers)
if not hasattr(coeffs, '__iter__'): coeffs = [coeffs] * len(self.layers)
if not hasattr(self, 'non_redundant'): self.non_redundant = False if not hasattr(self, 'mask_weights'): self.mask_weights = None
range_ = T.tile(range_.dimshuffle(0, 'x'), (1, self.binary_target_dim)).flatten()
raise NotImplementedError()
raise NotImplementedError()
raise NotImplementedError()
p = T.switch(p > 0., p, self.left_slope * p) return p
if not hasattr(self, 'detector_normalization'): self.detector_normalization = None
mkn = max_kernel_norm dn = detector_normalization on = output_normalization
assert not value or all(0 <= v < self.num_layers for v in value) self.inputs_to_layers[key] = sorted(value)
if len(cur_state_below) == 1: cur_state_below, = cur_state_below
assert not any([key in rval for key in contrib]) assert all([key in params for key in contrib])
scal_points = new_W / norms.dimshuffle('x',0)
dot_update = (old_W * scal_points).sum(axis=0)
import functools import operator
import numpy import theano from theano import tensor from theano.compat.six.moves import zip as izip, reduce
from pylearn2.blocks import Block, StackedBlocks from pylearn2.models import Model from pylearn2.utils import sharedX from pylearn2.utils.theano_graph import is_pure_elemwise from pylearn2.utils.rng import make_np_rng, make_theano_rng from pylearn2.space import VectorSpace
self.s_rng = make_theano_rng(seed, which_method="uniform")
acts = self._hidden_input(inputs) hiddens = self.act_enc(acts) act_grad = tensor.grad(hiddens.sum(), acts) return act_grad
act_grad = self._activation_grad(inputs) jacobian = self.weights * act_grad.dimshuffle(0, 'x', 1) return jacobian
return StackedBlocks(layers)
if 'extensions' not in d: self.extensions = []
self._disallow_censor_updates()
if not hasattr(self, 'names_to_del'): self.names_to_del = set() self.names_to_del = self.names_to_del.union(names)
assert isinstance(num_steps, py_integer_types) assert num_steps > 0
if num_steps != 1: for i in xrange(num_steps): layer_to_state = self.sample(layer_to_state, theano_rng, layer_to_clamp, num_steps=1) return layer_to_state
assert len(self.dbm.hidden_layers) > 0
if layer_to_clamp is None: layer_to_clamp = OrderedDict()
layer_to_updated = OrderedDict()
if i == 0: layer_below = self.dbm.visible_layer else: layer_below = self.dbm.hidden_layers[i-1] state_below = layer_to_state[layer_below] state_below = layer_below.upward_state(state_below)
this_sample = this_layer.sample(state_below=state_below, state_above=state_above, layer_above=layer_above, theano_rng=theano_rng)
for i, this_layer in list(enumerate(self.dbm.hidden_layers))[1::2]:
layer_below = self.dbm.hidden_layers[i-1]
this_sample = this_layer.sample(state_below=state_below, state_above=state_above, layer_above=layer_above, theano_rng=theano_rng)
assert all([layer in layer_to_updated for layer in layer_to_state]) assert all([layer in layer_to_state for layer in layer_to_updated]) assert all([(layer_to_state[layer] is layer_to_updated[layer]) == layer_to_clamp[layer] for layer in layer_to_state])
if Y is not None: state_above = dbm.hidden_layers[-1].downward_state(Y) layer_above = dbm.hidden_layers[-1] assert len(dbm.hidden_layers) > 1
if len(dbm.hidden_layers) > 2: state_below = dbm.hidden_layers[-3].upward_state(H_hat[-3]) else: state_below = dbm.visible_layer.upward_state(V)
H_hat[-1] = Y
for layer, state in safe_izip(dbm.hidden_layers, H_hat): upward_state = layer.upward_state(state) layer.get_output_space().validate(upward_state)
state_below=dbm.hidden_layers[-2].upward_state(H_hat[-1])))
assert V is orig_V assert drop_mask is orig_drop_mask
SuperWeightDoubling = WeightDoubling
drop_mask_Y = T.zeros_like(Y)
state_below=dbm.hidden_layers[-2].upward_state(H_hat[-1])))
assert V is orig_V assert drop_mask is orig_drop_mask
if Y is not None: H_hat[-1] = Y
assert (niter > 1) == (len(dbm.hidden_layers) > 1)
for layer, state in safe_izip(dbm.hidden_layers, H_hat): upward_state = layer.upward_state(state) layer.get_output_space().validate(upward_state)
assert V is orig_V assert drop_mask is orig_drop_mask
if Y is not None: H_hat[-1] = Y
assert (niter > 1) == (len(dbm.hidden_layers) > 1)
assert V is orig_V assert drop_mask is orig_drop_mask
layer_to_chains[self.dbm.visible_layer] = inputs
if rbm == rbm_list[-1]: if targets: assert len(rbm.hidden_layers) == 2 else: assert len(rbm.hidden_layers) == 1 else: assert len(rbm.hidden_layers) == 1
assert len(self.hidden_layers) > 0
assert len(self.hidden_layers) > 0
if not hasattr(self, 'rng'): self.setup_rng()
if not hasattr(self, 'freeze_set'): self.freeze_set = set([])
if not hasattr(self, 'freeze_set'): self.freeze_set = set([])
assert not any([key in rval for key in contrib]) assert all([key in params for key in contrib])
layers = [self.visible_layer] + self.hidden_layers
layers = [self.visible_layer] + self.hidden_layers
if layer_to_clamp is None: layer_to_clamp = OrderedDict()
for layer in layer_to_state: old = layer_to_state[layer] new = updated[layer] if layer_to_clamp[layer]: assert new is old else: add_updates(old, new)
del self.bias_from_marginals
rval = -(self.beta * T.dot(state, self.bias))
raise NotImplementedError()
raise NotImplementedError()
del self.bias_from_marginals
init_bias = \ init_sigmoid_bias_from_array(bias_from_marginals.X / 2. + 0.5)
rval = -(self.beta * T.dot(state, self.ising_bias()))
del self.bias_from_marginals
rval = -T.dot(state, self.bias)
if not hasattr(self, 'mask_weights'): self.mask_weights = None if not hasattr(self, 'max_col_norm'): self.max_col_norm = None
raise NotImplementedError()
raise NotImplementedError()
assert len(state) == 2 if isinstance(coeffs, str): coeffs = float(coeffs) assert isinstance(coeffs, float) _, state = state state = [state] coeffs = [coeffs]
assert len(state) == 2 if isinstance(coeffs, str): coeffs = float(coeffs) assert isinstance(coeffs, float) _, state = state state = [state] coeffs = [coeffs]
raise NotImplementedError()
if not hasattr(self, 'W_lr_scale'): self.W_lr_scale = None
raise NotImplementedError()
if not hasattr(self, 'needs_reformat'): self.needs_reformat = self.needs_reshape del self.needs_reshape
if not hasattr(self, 'needs_reformat'): self.needs_reformat = self.needs_reshape del self.needs_reshape
log_prob_of = (Y * log_prob).sum(axis=1) masked = log_prob_of * drop_mask_Y assert masked.ndim == 1
self.batch_axis=list(axes).index('b') self.axes_to_sum = list(range(len(axes))) self.axes_to_sum.remove(self.batch_axis)
assert len(state) == 2 assert isinstance(coeffs, float) _, state = state state = [state] coeffs = [coeffs]
warnings.warn("Do you really want to regularize the detector units to be more active than the pooling units?")
default_z += T.alloc(*([0.]+[shape[elem] for elem in self.h_space.axes])).astype(default_z.dtype) assert default_z.ndim == 4
assert len(state) == 2 assert isinstance(coeffs, float) _, state = state state = [state] coeffs = [coeffs]
warnings.warn("Do you really want to regularize the detector units to be more active than the pooling units?")
h_rows, h_cols = self.h_space.shape num_h = float(h_rows * h_cols) rval[self.transformer._filters] = 1. /num_h rval[self.b] = 1. / num_h
default_z += T.alloc(*([0.]+[shape[elem] for elem in self.h_space.axes])).astype(default_z.dtype) assert default_z.ndim == 4
raise NotImplementedError()
raise NotImplementedError()
X = dataset.get_design_matrix() m = X.shape[0] assert X.shape[1] == self.nvis
self.prev_floatX = config.floatX config.floatX = 'float64'
config.floatX = self.prev_floatX
self.prev_floatX = config.floatX config.floatX = 'float64'
example_input[0, 0] = -2.5
example_input[1, 3] = 0.0 example_input[1, 4] = 1.0
np.random.seed(12345)
mlp_model = MLP( layers=[mlp_nonlinearity(dim=output_channels, layer_name='mlp', irange=1.0)], batch_size=batch_size, nvis=nvis )
assert_allclose(f(x_mlp).flatten(), g(x).flatten(), rtol=1e-5, atol=5e-5)
assert_raises(NotImplementedError, conv_model.cost, Y, Y_hat) assert_raises(NotImplementedError, mlp_model.cost, Y1, Y1_hat)
from nose.plugins.skip import SkipTest from theano import config from theano import function from theano.sandbox import cuda from theano import tensor as T
f = function([X], output, mode="DEBUG_MODE") f(np.zeros((1, 1)).astype(X.dtype))
axes = ['b', 0, 1, 'c'] random.shuffle(axes) axes = tuple(axes) print('axes:', axes)
rng = np.random.RandomState() mean = rng.uniform(1e-6, 1. - 1e-6, (rows, cols, channels))
axes = ['b', 0, 1, 'c'] random.shuffle(axes) axes = tuple(axes) print('axes:', axes)
for pool_size in [1, 2, 5]: n = num_pools * pool_size
input_space = VectorSpace(1) class DummyDBM(object): def __init__(self): self.rng = rng layer.set_dbm(DummyDBM()) layer.set_input_space(input_space)
mean = layer.mf_update( state_below=T.alloc(0., 1, 1), state_above=None, layer_above=None)
dbm = make_random_basic_binary_dbm( rng = rng, pool_size_1 = pool_size_1, )
p_idx = rng.randint(num_p)
layer_to_state = dbm.make_layer_to_state(1) v_state = layer_to_state[v] h1_state = layer_to_state[h1] h2_state = layer_to_state[h2]
expected_p, expected_h = h1.mf_update( state_below = v.upward_state(v_state), state_above = h2.downward_state(h2_state), layer_above = h2)
wtf_numpy = np.zeros((pool_size_1,)) for i in xrange(pool_size_1): wtf_numpy[i] = on_probs[i] on_probs = wtf_numpy
for pool_size in [1, 2, 5]: do_test(pool_size)
dbm = make_random_basic_binary_dbm( rng = rng, pool_size_1 = pool_size_1,
p_idx = rng.randint(num_p)
layer_to_state = dbm.make_layer_to_state(1) v_state = layer_to_state[v] h1_state = layer_to_state[h1] h2_state = layer_to_state[h2]
expected_p, expected_h = h1.mf_update( state_below = v.upward_state(v_state), state_above = h2.downward_state(h2_state), layer_above = h2)
wtf_numpy = np.zeros((pool_size_1,)) for i in xrange(pool_size_1): wtf_numpy[i] = on_probs[i] on_probs = wtf_numpy
do_test(1)
dbm = make_random_basic_binary_dbm( rng = rng, pool_size_1 = pool_size_1, )
p_idx = rng.randint(num_p)
layer_to_state = dbm.make_layer_to_state(1) v_state = layer_to_state[v] h1_state = layer_to_state[h1] h2_state = layer_to_state[h2]
expected_p, expected_h = h1.mf_update( state_below = v.upward_state(v_state), state_above = h2.downward_state(h2_state), layer_above = h2)
for pool_size in [1, 2, 5]: do_test(pool_size)
num_vis = rng.randint(1,11) n_classes = rng.randint(1, 11)
layer_to_state = dbm.make_layer_to_state(1) v_state = layer_to_state[v] y_state = layer_to_state[y]
expected_y = y.mf_update( state_below = v.upward_state(v_state))
energy = dbm.energy(V = v_state, hidden = [y_state]) unnormalized_prob = T.exp(-energy) assert unnormalized_prob.ndim == 1 unnormalized_prob = unnormalized_prob[0] unnormalized_prob = function([], unnormalized_prob)
wtf_numpy = np.zeros((n_classes,)) for i in xrange(n_classes): wtf_numpy[i] = probs[i] probs = wtf_numpy
num_vis = rng.randint(1,11) n_classes = rng.randint(1, 11)
layer_to_state = dbm.make_layer_to_state(1) v_state = layer_to_state[v] y_state = layer_to_state[y]
expected_y = y.mf_update( state_below = v.upward_state(v_state))
energy = dbm.energy(V = v_state, hidden = [y_state]) unnormalized_prob = T.exp(-energy) assert unnormalized_prob.ndim == 1 unnormalized_prob = unnormalized_prob[0] unnormalized_prob = function([], unnormalized_prob)
wtf_numpy = np.zeros((n_classes,)) for i in xrange(n_classes): wtf_numpy[i] = probs[i] probs = wtf_numpy
num_vis = rng.randint(1,11) n_classes = rng.randint(1, 11)
layer_to_state = dbm.make_layer_to_state(1) v_state = layer_to_state[v] y_state = layer_to_state[y]
expected_y = y.mf_update( state_below = v.upward_state(v_state))
cause_copy = sharedX(np.zeros((num_samples,))).dimshuffle(0,'x') v_state = v_state[0,:] + cause_copy y_state = y_state[0,:] + cause_copy
num_examples = 40 theano_rng = MRG_RandomStreams(2012+11+1)
visible_layer = BinaryVector(nvis=100) hidden_layer = BinaryVectorMaxPool(detector_layer_dim=500, pool_size=1, layer_name='h', irange=0.05, init_bias=-2.0) model = DBM(visible_layer=visible_layer, hidden_layers=[hidden_layer], batch_size=100, niter=1)
np.testing.assert_equal(mlp.get_total_input_dimension(['h0', 'h1']), 4) inp = theano.tensor.matrix()
l = [] for mask in xrange(16): l.append(mlp.masked_fprop(inp, mask)) outsum = reduce(lambda x, y: x + y, l)
mlp_first_part = MLP( layers=[ first_indep_layer ], input_space=VectorSpace(features_in_first_mlp), input_source=('features0') )
mlp_second_part = MLP( layers=[ second_indep_layer ], input_space=VectorSpace(features_in_second_mlp), input_source=('features1') )
shared_dataset = np.random.rand(20, 19).astype(theano.config.floatX)
train_composite = Train(dataset_composite, mlp_composite, SGD(0.0001, batch_size=20)) train_composite.algorithm.termination_criterion = EpochCounter(1) train_composite.main_loop()
X_composite = mlp_composite.get_input_space().make_theano_batch() X_first_part = mlp_first_part.get_input_space().make_theano_batch() X_second_part = mlp_second_part.get_input_space().make_theano_batch()
fl = mlp_composite.layers[0]
assert mlp_composite.get_input_space() == fl.get_input_space()
for i in range(0, 4): np.testing.assert_allclose(fl.get_params()[i].eval(), mlp_composite.get_params()[i].eval())
features_in_first_mlp = 5 features_in_second_mlp = 10 targets_in_first_mlp = 2 targets_in_second_mlp = 2
conv_first_part = ConvElemwise(8, [2, 2], 'sf1', SigmoidConvNonlinearity(), .1) mlp_first_part = MLP(layers=[conv_first_part], input_space=Conv2DSpace(shape=[5, 5], num_channels=2))
train_composite = Train(shared_dataset, mlp_composite, SGD(0.1, batch_size=5, monitoring_dataset=shared_dataset)) train_composite.algorithm.termination_criterion = EpochCounter(1) train_composite.main_loop()
X_composite = mlp_composite.get_input_space().make_theano_batch() X_first_part = mlp_first_part.get_input_space().make_theano_batch() X_second_part = mlp_second_part.get_input_space().make_theano_batch()
self.prev_floatX = config.floatX config.floatX = 'float64'
config.floatX = self.prev_floatX
self.prev_floatX = config.floatX config.floatX = 'float64'
if kl > tol or not (kl <= tol): raise AssertionError("KL divergence between two " "equivalent models should be 0 but is "+ str(kl))
return dict((_instantiate(k, bindings), _instantiate(v, bindings)) for k, v in six.iteritems(proxy))
elif isinstance(proxy, six.string_types): return preprocess(proxy) else: return proxy
if not isinstance(content, str): raise AssertionError("Expected content to be of type str, got " + str(type(content)))
pieces = modulename.split('.') str_e = str(e) found = True in [piece.find(str(e)) != -1 for piece in pieces]
reraise_as(ImportError("Could not import %s; ImportError was %s" % (modulename, str_e)))
yaml.add_multi_constructor('!obj:', multi_constructor_obj) yaml.add_multi_constructor('!pkl:', multi_constructor_pkl) yaml.add_multi_constructor('!import:', multi_constructor_import)
"corruptor" : *corr
loaded = yaml.load(yamlfile) logger.info(loaded) assert loaded['corruptor'] is loaded['dae'].corruptor
assert_(loaded['a'].yaml_src.find("${TEST_VAR}") != -1) del environ['TEST_VAR']
raised = False fmt = OneHotFormatter(max_labels=50) try: fmt.theano_expr(theano.tensor.vector(dtype=theano.config.floatX)) except TypeError: raised = True assert raised
raised = False try: fmt.format(numpy.zeros(10, dtype='float64')) except TypeError: raised = True assert raised
raised = False try: fmt = OneHotFormatter(max_labels=-10) except ValueError: raised = True assert raised
raised = False try: fmt = OneHotFormatter(max_labels=10, dtype='invalid') except TypeError: raised = True assert raised
raised = False try: fmt.theano_expr(theano.tensor.itensor3()) except ValueError: raised = True assert raised
result = list_files('.py') for path in result: logger.info(path)
COMMENT_WITH_NL = tokenize.generate_tokens(['#\n'].pop).send(None)[1] == '#\n'
try: length = len(line.decode('utf-8')) except UnicodeError: pass
yield found + 1, "E201 whitespace after '%s'" % char
indent_next = logical_line.endswith(':')
last_indent = start if verbose >= 3: print("... " + line.rstrip())
rel_indent[row] = expand_indent(line) - indent_level
close_bracket = (token_type == tokenize.OP and text in ']})')
if start[1] != indent[depth]: yield (start, "E124 closing bracket does not match " "visual indentation")
if hang_closing: yield start, "E133 closing bracket is missing indentation"
yield (start, "E128 continuation line " "under-indented for visual indent")
if close_bracket and not hang_closing: yield (start, "E123 closing bracket does not match " "indentation of opening bracket's line") hangs[depth] = hang
indent[depth] = start[1]
pass
(index < 2 or tokens[index - 2][1] != 'class') and not keyword.iskeyword(prev_text)): yield prev_end, "E211 whitespace before '%s'" % text
continue
if need_space is not True and not need_space[1]: yield (need_space[0], "E225 missing whitespace around operator") need_space = False
pass
yield prev_end, "E225 missing whitespace around operator"
pass
need_space = (prev_end, start != prev_end)
yield prev_end, "E225 missing whitespace around operator" need_space = False
self.blank_lines += 1 del self.tokens[0]
text = text.rstrip('\r\n') self.tokens = [(token_type, text) + token[2:]] self.check_logical()
self.elapsed = 0 self.total_errors = 0 self.counters = dict.fromkeys(self._benchmark_keys, 0) self.messages = {}
if code in self.expected: return if self.print_filename and not self.file_errors: print(self.filename) self.file_errors += 1 self.total_errors += 1 return code
options.ignore = tuple(DEFAULT_IGNORE.split(','))
options.ignore = ('',) if options.select else tuple(options.ignore)
if ((filename_match(filename, filepatterns) and not self.excluded(filename, root))): runner(os.path.join(root, filename))
(new_options, __) = parser.parse_args([])
(options, __) = parser.parse_args(arglist, values=new_options)
if not arglist and not parse_argv: arglist = [] (options, args) = parser.parse_args(arglist) options.reporter = None
try: indent = min(len(s) - len(s.lstrip()) for s in docstring if s.strip()) except ValueError: indent = 0
def _str_header(self, name, symbol='`'): return ['**' + name + '**'] + [symbol*(len(name)+4)]
out = self._str_indent(out,indent) return '\n'.join(out)
module = inspect.getmodule(method) if module is not None: if not module.__name__.startswith('pylearn2'): return method_errors
import scipy.sparse
assert not isinstance(batch, list)
if batch is None or (isinstance(batch, tuple) and len(batch) == 0): return True
assert result == any(subbatch_results), ("composite batch had a " "mixture of numeric and " "symbolic subbatches. This " "should never happen.") return result
return (isinstance(batch, np.ndarray) or scipy.sparse.issparse(batch) or str(type(batch)) == "<type 'CudaNdarray'>")
return theano._asarray(arg, dtype=dtype)
self._validate(is_numeric, batch)
self._check_sizes(space)
self.validate(batch)
if '_dtype' not in state_dict: self._dtype = theano.config.floatX
super(IndexSpace, self)._validate_impl(is_numeric, batch)
owner = batch.owner assert 'Subtensor' in str(owner.op) batch = owner.inputs[0]
super(VectorSpace, self)._validate_impl(is_numeric, batch)
return 1
super(VectorSequenceSpace, self)._validate_impl(is_numeric, batch)
return 1
super(IndexSequenceSpace, self)._validate_impl(is_numeric, batch)
default_axes = ('b', 0, 1, 'c')
self.shape = tuple(shape) self.num_channels = num_channels if axes is None: axes = self.default_axes assert len(axes) == 4 self.axes = tuple(axes)
if not hasattr(self, 'num_channels'): self.num_channels = self.nchannels
super(Conv2DSpace, self)._validate_impl(is_numeric, batch)
batch = _undo_op(batch, 'Cast')
if space.axes != self.axes: batch = _undo_op(batch, 'DimShuffle', strict=True)
def __init__(self): super(NullSpace, self).__init__()
self._validate(is_numeric, batch) return 0
from pylearn2.space import (SimplyTypedSpace, VectorSpace, Conv2DSpace, CompositeSpace, VectorSequenceSpace, IndexSequenceSpace, IndexSpace, NullSpace, is_symbolic_batch) from pylearn2.utils import function, safe_zip
if not (isinstance(from_space, VectorSpace) and from_space.sparse): kwargs['batch_size'] = batch_size
fallback_dtype = theano.config.floatX
if isinstance(space, VectorSpace) and space.sparse: del kwargs["batch_size"]
return None, None
if isinstance(from_space, CompositeSpace): if isinstance(to_space, Conv2DSpace): return (NotImplementedError, "CompositeSpace does not know how to format as " "Conv2DSpace")
if isinstance(to_space, CompositeSpace):
n_dtypes = 2 old_nchannels = shape[2] shape[2] = old_nchannels / 2 assert shape[2] * 2 == old_nchannels, \
composite_dtypes = ((None, 'int8'), ('complex128', theano.config.floatX))
for from_space in composite_spaces: for to_dtype in composite_dtypes: test_get_origin_batch(from_space, to_dtype) test_make_shared_batch(from_space, to_dtype) test_make_theano_batch(from_space, to_dtype) test_dtype_setter(from_space, to_dtype)
VS = VectorSpace(dim=27) VS_sparse = VectorSpace(dim=27, sparse=True)
VS_batch = VS.make_theano_batch() new_SVS_batch = VS.format_as(VS_batch, VS_sparse) new_VS_batch = VS.undo_format_as(new_SVS_batch, VS_sparse) assert new_VS_batch is VS_batch assert new_SVS_batch is not VS_batch
VS_batch = VS.make_theano_batch() new_CS_batch = VS.format_as(VS_batch, CS) new_VS_batch = VS.undo_format_as(new_CS_batch, CS) assert new_VS_batch is VS_batch
VS = VectorSpace(dim=27) VS_sparse = VectorSpace(dim=27, sparse=True)
from __future__ import print_function
theano.config.warn.sum_div_dimshuffle_bug = False
if self.corruption_level < 1e-5: return x
pvals = T.alloc(1.0 / num_classes, num_classes) one_hot = self.s_rng.multinomial(size=(num_examples,), pvals=pvals)
super(BinomialSampler, self).__init__(0, *args, **kwargs)
super(MultinomialSampler, self).__init__(0, *args, **kwargs)
assert len(corruptors) >= 1 self._corruptors = corruptors
costMatrix *= T.neq(Y, -1) return model.cost_from_cost_matrix(costMatrix)
return total / len(model_output)
use = zipped[:1]
use = zipped[1:]
return total / coeff_sum
if len(self.costs) > 1: output = self._get_samples_from_model(model, data)
get_space = lambda i: (model.aes[i].get_input_space() if i == 0 else model.aes[i - 1].get_output_space())
spaces = map(lambda c: get_space(c[0]), self.costs)
expected_energy_p = model.energy( layer_to_chains[model.visible_layer], [layer_to_chains[layer] for layer in model.hidden_layers] ).mean()
assert isinstance(model.hidden_layers[-1], Softmax)
assert isinstance(model.hidden_layers[-1], Softmax) layer_to_clamp[model.hidden_layers[-1]] = True layer_to_pos_samples[model.hidden_layers[-1]] = Y hid = model.hidden_layers[:-1]
updates, layer_to_chains = model.get_sampling_updates( layer_to_chains, self.theano_rng, num_steps=self.num_gibbs_steps, return_layer_to_updated=True)
updates, layer_to_chains = model.get_sampling_updates( layer_to_chains, self.theano_rng, num_steps=self.num_gibbs_steps, return_layer_to_updated=True)
assert isinstance(model.hidden_layers[-1], dbm.Softmax)
gsu = model.get_sampling_updates updates, layer_to_chains = gsu(layer_to_chains, self.theano_rng, num_steps=self.num_gibbs_steps, return_layer_to_updated=True)
layer_to_chains = model.sampling_procedure.sample( layer_to_chains, self.theano_rng, layer_to_clamp=layer_to_clamp, num_steps=1 )
layer_to_chains = model.sampling_procedure.sample( layer_to_chains, self.theano_rng, num_steps=self.num_gibbs_steps )
cost = None
layers = model.get_all_layers() states = [final_state['V_hat']] + final_state['H_hat']
Y = None
inpaint_cost = 0.5 * inpaint_cost + 0.5 * new_inpaint_cost
total_cost += l1_act_cost
size = tuple([X.shape[i] for i in xrange(X.ndim)]) if self.sync_channels: del size[X_space.axes.index('c')]
rval = -T.mean(log_hx)-T.mean(log_one_minus_hy) rval.name = 'NCE('+X_name+')'
sampler_updates = self.sampler.updates()
pos_v = data neg_v = self.sampler.particles
ml_cost = (model.free_energy(pos_v).mean() - model.free_energy(neg_v).mean())
X_dense = theano.sparse.dense_from_sparse(X) noise = self.random_stream.binomial(size=X_dense.shape, n=1, prob=self.one_ratio, ndim=None)
P = noise + X_dense P = theano.tensor.switch(P > 0, 1, 0) P = tensor.cast(P, theano.config.floatX)
reg_units = theano.tensor.abs_(model.encode(X)).sum(axis=1).mean()
before_activation = model.reconstruct_without_dec_acti(X, P)
X_dense = theano.sparse.dense_from_sparse(X) noise = self.random_stream.binomial(size=X_dense.shape, n=1, prob=self.ratio, ndim=None)
P = noise + X_dense P = theano.tensor.switch(P > 0, 1, 0) P = tensor.cast(P, theano.config.floatX)
L1_units = theano.tensor.abs_(model.encode(X)).sum(axis=1).mean()
if per_example is None: return None
message = "Error while calling " + str(type(self)) + ".expr" reraise_as(TypeError(message))
composite_space = CompositeSpace(spaces) sources = tuple(sources) return (composite_space, sources)
self.get_data_specs(model)[0].validate(data)
penalty = penalty + abs(var ** self.p).sum()
fn = getattr(model, '%s_data_specs' % self.method)
assert left is not right assert left.fixed_vars is not right.fixed_vars assert left.on_load_batch is not right.on_load_batch
if self._channel_name is None: v = monitor.channels['objective'].val_record else: v = monitor.channels[self._channel_name].val_record
if v[-1] < (1. - self.prop_decrease) * self.best_value: self.countdown = self.N else: self.countdown = self.countdown - 1
return self.countdown > 0
self._original = None
preprocessor = CentralWindow(self._window_shape) for data in self._center: preprocessor.apply(data)
self._original = dict((data, _zero_pad( data.get_topological_view().astype('float32'), self._pad_randomized)) for data in randomize_now)
self.randomize_datasets(randomize_now)
if tag_key is None: tag_key = self.__class__.__name__ self._tag_key = tag_key
self.best_cost = self.coeff * np.inf self.best_model = None
model.tag[self._tag_key]['best_cost'] = self.best_cost
if self.negative_class_index is None: y = T.eq(y, self.positive_class_index)
p = mp.Process(target=train_mlp) p.start()
monitor = lm.LiveMonitor() monitor.update_channels(['train_objective'], start=0, end=2) assert(len(monitor.channels['train_objective'].val_record) == 2)
monitor = lm.LiveMonitor() monitor.update_channels(['train_objective'], start=1, end=2) assert(len(monitor.channels['train_objective'].val_record) == 1)
p.join()
assert_raises( AssertionError, monitor.update_channels, 0 )
assert_raises( AssertionError, monitor.update_channels, [] )
assert_raises( AssertionError, monitor.update_channels, ['train_objective'], start=2, end=1 )
ddata = DummyDataset(axes=('c', 0, 1, 'b')) topo = ddata.get_topological_view()
wf = wf_cls(window_shape=(3, 3), randomize=[ddata], flip=flip)
self.counter = 0
rsp_msg = rsqt_msg.get_response()
st_mode = st.st_mode read_all = stat.S_IRUSR read_all |= stat.S_IRGRP read_all |= stat.S_IROTH
os.chmod(fn, st_mode | read_all)
n_rows = 1 n_cols = np.ceil(n_plots*1./n_rows) n_cols = int(n_cols) half_perimeter = n_cols + 1
max_row = np.sqrt(n_plots) max_row = np.round(max_row) max_row = int(max_row)
colors_hue = np.arange(n_colors) colors_hue = as_floatX(colors_hue) colors_hue *= 1./n_colors
colors_hsv = np.ones((n_colors, 3)) colors_hsv[:, 2] *= .75 colors_hsv[:, 0] = colors_hue
colors_hsv = colors_hsv.reshape((1, )+colors_hsv.shape) colors_rgb = matplotlib.colors.hsv_to_rgb(colors_hsv) colors_rgb = colors_rgb[0]
n_min = plots.shape[1] n_min -= int(np.ceil(plots.shape[1] * self.share)) plots = plots[:, n_min:]
x = np.arange(plots.shape[1]) x += n_min
if self.per_second: seconds = channels['training_seconds_this_epoch'].val_record seconds = np.array(seconds) seconds = seconds.cumsum() x = seconds[x]
plt.figure() for i in xrange(self.n_colors): plt.plot(x, plots[i], color=self.colors_rgb[i], alpha=.5)
for plot in self.plots: if plot.freq is None: plot.freq = self.freq
logger.error("'{0}' not found " "but mandatory".format(this_check)) return False
logger.warning("no '{0}' found".format(this_check))
pass
while f1 != f2: f1=f2 (f2,ext)=os.path.splitext(f1)
from __future__ import print_function
dataset_sources="sources.lst" dataset_web="http://www.stevenpigeon.org/secret" dataset_conf_path="" dataset_data_path="" root_conf_path=None root_data_path=None user_conf_path=None user_data_path=None super_powers=False
packages_sources={} installed_packages_list={}
while f1 != f2: f1=f2 (f2,ext)=os.path.splitext(f1)
dst_path = os.path.dirname(os.path.abspath(dst_filename)) dst_temp_filename=os.tempnam(dst_path);
shutil.move(src_filename, dst_filename)
if not os.path.exists(dataset_conf_path): os.makedirs(dataset_conf_path)
pass
for line in f:
pass
for line in installed_list_file:
read_from_file(os.path.join(dataset_conf_path,"installed.lst"))
atomic_replace(os.path.join(dataset_conf_path,"installed.lst.2"), os.path.join(dataset_conf_path,"installed.lst"))
try: atomic_replace(temp_filename,local_dst) except Exception as e: raise IOError("[ac] %s %s --> %s" % (str(e),temp_filename,local_dst))
if file_access_rights(local_dst,os.W_OK,check_above=True):
try: atomic_replace(temp_filename,local_dst) except Exception as e: raise IOError("[ac] %s %s --> %s" % (str(e),temp_filename,local_dst))
this_tar_file=tarfile.open(tar_filename,"r:bz2")
try: this_tar_file.extractall(dest_path) except Exception as e: raise IOError("[tar] error while extracting '%s'" %tar_filename) else: pass
try: subprocess.check_call( script, stdout=sys.stdout, stderr=sys.stderr ) except Exception: os.chdir(cwd) raise
os.chdir(cwd)
unpack_tarball(src,dst) run_scripts(dst+package.name, scripts=["getscript","postinst"] )
del installed_packages_list[package.name]
package.where=dataset_data_path;
pass
install_package(package,temp_filename,dataset_data_path) update_installed_list("i",package)
if packages_to_upgrade==[]:
packages_really_to_upgrade=[] for this_package in packages_to_upgrade: if this_package in installed_packages_list:
installed_date=installed_packages_list[this_package].timestamp
logger.info(this_package) packages_really_to_upgrade.append(this_package)
pass
if not all_packages: logger.warning("[up] '{0}' is not installed, " "cannot upgrade.".format(this_package)) pass
pass
pass
if this_package in installed_packages_list:
packages_really_to_remove.append(this_package)
pass
pass
warnings.simplefilter("ignore", RuntimeWarning)
try: set_defaults() except Exception as e: logger.exception(e)
pass
for line in installed_list_file: l=line.rstrip().split(' ') if l: self.installed_packages_list[l[0]]=\ this_package=self.package_info(
pass
x=dataset_resolver() logger.info(x.resolve_dataset("toaster-oven")) logger.info(x.resolve_dataset("fake-dataset"))
try: iter(update_callbacks) self.update_callbacks = update_callbacks except TypeError: self.update_callbacks = [update_callbacks]
if self._count >= self.start: return self.final_momentum return self._init_momentum
mean_square_grad = sharedX(param.get_value() * 0.) mean_square_dx = sharedX(param.get_value() * 0.)
new_mean_squared_grad = ( self.decay * mean_square_grad + (1 - self.decay) * T.sqr(grads[param]) )
new_mean_square_dx = ( self.decay * mean_square_dx + (1 - self.decay) * T.sqr(delta_x_t) )
updates[mean_square_grad] = new_mean_squared_grad updates[mean_square_dx] = new_mean_square_dx updates[param] = param + delta_x_t
sum_square_grad = sharedX(param.get_value() * 0.)
new_sum_squared_grad = ( sum_square_grad + T.sqr(grads[param]) )
epsilon = lr_scalers.get(param, 1.) * learning_rate scale = T.maximum(self.eps, T.sqrt(new_sum_squared_grad)) delta_x_t = (-epsilon / scale * grads[param])
updates[sum_square_grad] = new_sum_squared_grad updates[param] = param + delta_x_t
mean_square_grad = sharedX(param.get_value() * 0.)
self.mean_square_grads[param.name] = mean_square_grad
new_mean_squared_grad = (self.decay * mean_square_grad + (1 - self.decay) * T.sqr(grads[param]))
updates[mean_square_grad] = new_mean_squared_grad updates[param] = param + delta_x_t
has_force_batch_size = getattr(model, "force_batch_size", False) train_dataset_is_uneven = \ dataset.get_num_examples() % self.batch_size != 0
nested_args = mapping.nest(theano_args) fixed_var_descr = self.cost.get_fixed_var_descr(model, nested_args) self.on_load_batch = fixed_var_descr.on_load_batch
cost_value.name = 'objective'
updates.update(dict(safe_zip(params, [param - learning_rate * lr_scalers.get(param, 1.) * grads[param] for param in params])))
for param in self.params: value = param.get_value(borrow=True) if not isfinite(value): raise RuntimeError("NaN in " + param.name)
for param in self.params: value = param.get_value(borrow=True) if not isfinite(value): raise RuntimeError("NaN in " + param.name)
return
new_lr = self._base_lr / (self.decay_factor ** self._count) if new_lr <= self.min_lr: self._min_reached = True new_lr = self.min_lr
try: model.add_polyak_channels(self._worker.param_to_mean, algorithm.monitoring_dataset) except AttributeError: pass
space, source = model.get_monitoring_data_specs()
learn_more = model.train_batch(dataset, batch_size) model.monitor.report_batch(batch_size) if not learn_more: break
cost = SumOfCosts([SumOfParams(), (0., DummyCost())]) model = DummyModel(shapes, lr_scalers=scales) dataset = ArangeDataset(1) momentum = 0.5
cost = SumOfCosts([SumOfParams(), (0., DummyCost())]) model = DummyModel(shapes, lr_scalers=scales) dataset = ArangeDataset(1) momentum = 0.5
cost = SumOfCosts([SumOfOneHalfParamsSquared(), (0., DummyCost())]) model = DummyModel(shapes, lr_scalers=scales) dataset = ArangeDataset(1) decay = 0.95
pstate['sg2'] += param_val ** 2 dx_t = - (scale * learning_rate / np.sqrt(pstate['sg2']) * param_val) rval += [param_val + dx_t]
try: AdaGrad(-1.0) allows_null = True except AssertionError: allows_null = False assert not allows_null
cost = SumOfCosts([SumOfOneHalfParamsSquared(), (0., DummyCost())])
return X
assert X.ndim == 2 return T.nnet.softmax(X*self.P)
assert X.ndim == 4 return T.nnet.softmax(X.reshape((X.shape[0], self.dim)) * self.P)
monitoring_dataset = DenseDesignMatrix(X=X, y=Y)
termination_criterion = EpochCounter(5)
monitoring_dataset = DenseDesignMatrix(X=X)
termination_criterion = EpochCounter(5)
monitoring_dataset = DenseDesignMatrix(X=X)
epoch_num = 15 termination_criterion = EpochCounter(epoch_num)
lr_tracker = LearningRateTracker() algorithm = SGD(learning_rate, cost, batch_size=batch_size, monitoring_batches=3, monitoring_dataset=monitoring_dataset, termination_criterion=termination_criterion, update_callbacks=[linear_decay, lr_tracker], set_batch_size=False)
monitoring_dataset = DenseDesignMatrix(X=X)
epoch_num = 15 termination_criterion = EpochCounter(epoch_num)
lr_tracker = LearningRateTracker() algorithm = SGD(learning_rate, cost, batch_size=batch_size, monitoring_batches=3, monitoring_dataset=monitoring_dataset, termination_criterion=termination_criterion, update_callbacks=[annealed_rate, lr_tracker], set_batch_size=False)
monitoring_dataset = DenseDesignMatrix(X=X)
epoch_num = 15 termination_criterion = EpochCounter(epoch_num)
monitoring_dataset = DenseDesignMatrix(X=X)
epoch_num = 6 termination_criterion = EpochCounter(epoch_num)
monitoring_dataset = DenseDesignMatrix(X=X)
epoch_num = 6 termination_criterion = EpochCounter(epoch_num)
monitoring_dataset = DenseDesignMatrix(X=X)
epoch_num = 6 termination_criterion = EpochCounter(epoch_num)
monitoring_dataset = DenseDesignMatrix(X=X)
epoch_num = 6 termination_criterion = EpochCounter(epoch_num)
epoch_num = 5
monitoring_dataset = DenseDesignMatrix(X=X)
epoch_num = 2
monitoring_dataset = DenseDesignMatrix(X=X)
dummy = 'void'
monitor_lr2 = MonitorBasedLRAdjuster(channel_name=dummy)
epoch_num = 1
monitoring_train = DenseDesignMatrix(X=X) monitoring_test = DenseDesignMatrix(X=Y)
epoch_num = 1
monitoring_train = DenseDesignMatrix(X=X) monitoring_test = DenseDesignMatrix(X=Y)
m = 15 monitoring_dataset = get_topological_dataset(rng, rows, cols, channels, m)
termination_criterion = EpochCounter(5)
termination_criterion = EpochCounter(5)
termination_criterion = EpochCounter(5)
termination_criterion = EpochCounter(5)
disturb_mem.disturb_mem() rng = np.random.RandomState([2012, 11, 27])
w = rng.randn(num_features)
cost = SumOfCosts([SumOfParams(), (0., DummyCost())])
return X
cost = SumOfCosts([SumOfParams(), (0., DummyCost())])
train_with_monitoring_datasets( train_dataset=dataset1, monitoring_datasets=no_monitoring_datasets, model_force_batch_size=False, train_iteration_mode='sequential', monitor_iteration_mode='sequential')
train_with_monitoring_datasets( train_dataset=dataset3, monitoring_datasets=no_monitoring_datasets, model_force_batch_size=False, train_iteration_mode='sequential', monitor_iteration_mode='sequential')
train_with_monitoring_datasets( train_dataset=dataset1, monitoring_datasets=even_monitoring_datasets, model_force_batch_size=False, train_iteration_mode='sequential', monitor_iteration_mode='sequential')
train_with_monitoring_datasets( train_dataset=dataset1, monitoring_datasets=uneven_monitoring_datasets, model_force_batch_size=False, train_iteration_mode='sequential', monitor_iteration_mode='sequential')
assert X.ndim == 2 return T.nnet.softmax(X*self.P)
monitoring_dataset = DenseDesignMatrix(X=X)
termination_criterion = EpochCounter(5)
disturb_mem.disturb_mem() rng = np.random.RandomState([2012, 11, 27, 8])
w = rng.randn(num_features)
w = rng.randn(num_features)
assert all(called)
assert unsup_counter.get_value() == train_batches assert sup_counter.get_value() == train_batches
assert grad_counter.get_value() == train_batches * updates_per_batch
nested_args = mapping.nest(theano_args) fixed_var_descr = self.cost.get_fixed_var_descr(model, nested_args) self.on_load_batch = fixed_var_descr.on_load_batch
def capture(f, mapping=mapping): new_f = lambda *args: f(mapping.flatten(args, return_tuple=True)) return new_f
from theano.tensor.nnet.conv import conv2d, ConvOp
return conv2d( x, self._filters, image_shape=self._img_shape, filter_shape=self._filters_shape, subsample=self._subsample, border_mode=self._border_mode, )
channels=3
kern_data_minor = kern_data.transpose([0,2,3,1]).copy() img_data_minor = img_data.transpose([0,2,3,1]).copy()
bias = T.dvector() kerns = T.dvector() input = T.dmatrix() rng = N.random.RandomState(3423489)
AT_xT = self.rmul_T(self.transpose_left(x, False)) rval = self.transpose_right(AT_xT, True) return rval
A_xT = self.rmul(self.transpose_right(x, True)) rval = self.transpose_left(A_xT, True) return rval
xT_AT = self.lmul_T(self.transpose_right(x, False)) rval = self.transpose_left(xT_AT, False) return rval
xT_A = self.lmul(self.transpose_left(x, True)) rval = self.transpose_right(xT_A, True) return rval
def tile_columns(self, **kwargs): raise NotImplementedError('override me')
x_s = x2[:,offset:offset+size] xWlist.append( W.lmul( x_s.reshape( (n_rows,)+W.col_shape()), T)) offset += size
xWlist = [W.lmul(x,T).flatten(2) for W in self._Wlist] rval = tensor.join(1, *xWlist)
def _tile_columns(self): raise NotImplementedError('TODO')
B, IR, IC, C = ishp4 K, KR, KC, CH = kshp4
patch_extractor = sp_extract_patches(IR, IC, KR, KC, CH, RasterOrders.row_col_channel, RasterOrders.row_col_channel, subsample, border_mode, flip_patches=True).tocsc()
patch_stack = patches.reshape((B*OR*OC, KR*KC*CH))
output = tensor.dot(patch_stack, kerns.flatten(2).T).reshape((B, OR, OC, K))
B, C, IR, IC = ishp4 K, CH, KR, KC = kshp4
patch_extractor = sp_extract_patches(IR, IC, KR, KC, CH, RasterOrders.channel_row_col, RasterOrders.channel_row_col, subsample, border_mode, flip_patches=True).tocsc()
patch_stack = patches.reshape((B*OR*OC, KR*KC*CH))
output = tensor.dot(patch_stack, kerns.flatten(2).T).reshape((B, OR, OC, K))
if N.size(imgshp)==2: imgshp = (1,)+imgshp
indices, indptr, spmat_shape, sptype, outshp = \ convolution_indices.conv_eval(imgshp, maxpoolshp, maxpoolshp, mode='valid')
return convolution_indices.evaluate(inshp, kshp, offset, nkern, mode=mode, ws=False)
if N.size(imshp)==2: inshp = (1,)+imshp
lbound = N.array([kshp[0]-1,kshp[1]-1]) if mode=='valid' else N.zeros(2) ubound = lbound + (inshp[1:]-kshp+1) if mode=='valid' else fulloutshp
topleft = N.array([kshp[0]-1,kshp[1]-1])
spmatshp = (outsize*N.prod(kshp)*inshp[0],insize) if ws else\ (nkern*outsize,insize) spmat = scipy_sparse.lil_matrix(spmatshp)
z,zz = 0,0
tapi, ntaps = 0, 0
for ky in oy+N.arange(kshp[0]): for kx in ox+N.arange(kshp[1]):
if all((ky,kx) >= topleft) and all((ky,kx) < botright):
iy,ix = N.array((ky,kx)) - topleft col = iy*inshp[2]+ix +\
(y,x) = (oy,ox) if mode=='full' else (oy,ox) - topleft
row = (y*outshp[1]+x)*inshp[0]*ksize + l + fmapi*ksize if ws else\ y*outshp[1] + x
ntaps += 1
spmat = spmat.ensure_sorted_indices()
if numpy.size(imgshp)==2: imgshp = (1,)+imgshp
indices, indptr, spmat_shape, sptype, outshp, kmap = \ convolution_indices.sparse_eval(imgshp, kshp, nkern, step, mode)
raise NotImplementedError('partial sum')
from nose.plugins.skip import SkipTest import theano.sandbox.cuda as cuda_ndarray if cuda_ndarray.cuda_available == False: raise SkipTest('Optional package cuda disabled')
fgraph = f.maker.env
def run_autoencoder( self,
import matplotlib.pyplot as plt
try: grad_not_implemented = theano.gradient.grad_not_implemented except: def grad_not_implemented(op, idx, ipt): return None
raise NotImplementedError("non-square filter shape", (frows, fcols))
fgroups = hgroups filters_per_group = hcolors_per_group
def left_op(imgs): return self.op(imgs, self.s_filters)
weights_format = ('v', 'h')
self.colors = [np.asarray([1, 1, 0]), np.asarray([1, 0, 1]), np.asarray([0, 1, 0])]
self.xmin = xlim[0] self.xmax = xlim[1] self.delta_x = (self.xmax-self.xmin)/float(self.cols-1)
raise NotImplementedError()
W[0, 1] = .5 W[0, 2] = 1. W[0, 3] = 2.
W[0, 1] = .5 W[0, 2] = 1. W[0, 3] = 2.
self.path = preprocess(self.path) X, y = self._load_data()
dtype = 'uint8' ntrain = 50000
_logger.info('loading file %s' % datasets['test_batch']) data = serial.load(datasets['test_batch'])
Xs = {'train': x[0:ntrain], 'test': data['data'][0:ntest]}
assert start >= 0 assert stop > start assert stop <= X.shape[0] X = X[start:stop, :] y = y[start:stop, :] assert X.shape[0] == y.shape[0]
rval = X.copy()
rval = X.copy()
original_image_shape = (96, 96)
azimuth_degrees = numpy.arange(0, 341, 20)
label_type_to_index = {'category': 0, 'instance': 1, 'elevation': 2, 'azimuth': 3, 'lighting': 4}
num_labels_by_type = (len(_categories),
X = X.reshape(-1, 2 * numpy.prod(self.original_image_shape))
axes = ('b', 's', 0, 1, 'c') view_converter = StereoViewConverter(datum_shape, axes)
mono_shape = shape[:s_index] + (1, ) + shape[(s_index + 1):]
self.dataset_remote_dir = "" self.dataset_local_dir = ""
if self.dataset_local_dir == "": return filename
if not os.path.exists(remote_name): log.error("Error : Specified file %s does not exist" % remote_name) return filename
local_name = os.path.join(self.dataset_local_dir, os.path.relpath(remote_name, self.dataset_remote_dir))
if not os.path.exists(local_name):
if not self.check_enough_space(remote_name, local_name): log.warning(common_msg + "File %s not cached: Not enough free space" % remote_name) self.release_writelock() return filename
self.copy_from_server_to_local(remote_name, local_name) log.info(common_msg + "File %s has been locally cached to %s" % (remote_name, local_name))
self.get_readlock(local_name) self.release_writelock()
return ((storage_used + storage_need) < (storage_total * max_disk_usage))
atexit.register(self.release_readlock, lockdirName=lockdirName)
if (os.path.exists(lockdirName) and os.path.isdir(lockdirName)): os.rmdir(lockdirName)
datasetCache = cache.datasetCache filename = datasetCache.cache_file(filename)
data[s].ndim = len(data[s].shape)
inner_img = img[:, ring_w:img.shape[1] - ring_w, ring_w:img.shape[2] - ring_w]
inner_img = inner_img.reshape(len(output), -1) end_idx = start_idx + inner_img.shape[1] output[:, start_idx: end_idx] = inner_img
idx = 0 start_idx = end_idx for rd in rings: start_idx = downsample_ring(img, idx, rd, output, start_idx) idx += rd
img[:, ring_w:ring_w + inner_h, ring_w:ring_w + inner_w] = inner_img
idx = 0 start_idx = end_idx for rd in rings: start_idx = restore_ring(img, idx, rd, dense_input, start_idx) idx += rd
output[:, i:i + width, j:j + width] = dense_input[ :, idx][:, None, None] idx += 1
out_size = get_encoded_size(img_h, img_w, rings) output = numpy.zeros((batch_size, out_size * chans))
for chan_i in xrange(chans): channel = topo_X[..., chan_i] start_idx = foveate_channel(channel, rings, output, start_idx)
for chan_i in xrange(out_shp[-1]): channel = output[..., chan_i] start_idx = defoveate_channel(channel, rings, dense_X, start_idx)
region[np.logical_and(firstring, righthalf)] = 2 region[np.logical_and(secondring, np.logical_not(righthalf))] = 2
image_dtype = numpy.dtype(image_dtype)
self.label_index_to_name = ('category', 'instance', 'elevation', 'azimuth', 'lighting condition')
if which_norb == 'big': self.label_index_to_name = (self.label_index_to_name +
self.label_name_to_index = {} for index, name in enumerate(self.label_index_to_name): self.label_name_to_index[name] = index
image_length = 96 if which_norb == 'small' else 108
self.X_memmap_info = None self.y_memmap_info = None
result = super(NORB, self).get_topological_view(mat)
mono_shape = shape[:s_index] + (1, ) + shape[(s_index + 1):]
del result['X'] del result['y']
def get_memmap_info(memmap): assert isinstance(memmap, numpy.memmap)
'mode': 'r+' if memmap.mode in ('r+', 'w+') else 'r'}
for memmap in (self.X, self.y): memmap.flush() memmap.setflags(write=False)
data_dir = string_utils.preprocess('${PYLEARN2_DATA_PATH}') info['filename'] = os.path.join(data_dir, info['filename'])
assert hasattr(self, 'shape')
version = float('.'.join(numpy.version.version.split('.')[:2]))
datasetCache = cache.datasetCache im_path = datasetCache.cache_file(im_path)
tmp = X[i, :].copy() X[i, :] = X[j, :] X[j, :] = tmp
self._iter_mode = resolve_iterator_class('sequential')
data_specs[0].np_validate(data) assert not [contains_nan(X) for X in data] raise NotImplementedError()
npy_filename_root = os.path.join(preprocess('${PYLEARN2_DATA_PATH}'), 'icml07data', 'npy', npy_filename)
assert np.isfinite(data_x).all() assert np.isfinite(data_y).all() assert data_x.shape[0] == data_y.shape[0]
self.X_topo_space = self.view_converter.topo_space
sel = np.zeros(self.num_examples, dtype=bool) sel[next_index] = True next_index = sel
if make_new: self.filters = tables.Filters(complib='blosc', complevel=5) self.make_data(which_set, path)
h5file, node = self.init_hdf5(h_file_n, ([sizes[which_set], image_size], [sizes[which_set], 1]), title="SVHN Dataset", y_dtype='int')
rng = make_np_rng(None, 322, which_method="shuffle")
if which_set in ['train', 'test']: data_x, data_y = load_data("{0}{1}_32x32.mat".format(path, which_set))
data_y = data_y - 1
path = preprocess(path) data_x, data_y = self.make_data(which_set, path)
if center and scale: data_x -= 127.5 data_x /= 127.5 elif center: data_x -= 127.5 elif scale: data_x /= 255.
rng = make_np_rng(None, 322, which_method="shuffle")
if which_set in ['train', 'test']: data_x, data_y = load_data("{0}{1}_32x32.mat".format(path, which_set))
'float64': 0x1E3D4C53, 'int32': 0x1E3D4C54, 'uint8': 0x1E3D4C55, 'int16': 0x1E3D4C56
ndim = _read_int32(f) if debug: logger.debug('header ndim {0}'.format(ndim))
transformer = self.transformer_dataset.transformer out_space = self.data_specs[0] if isinstance(out_space, CompositeSpace): out_space = out_space.components[0]
rval_space = out_space
rval = transform(raw_batch)
rval = (transform(raw_batch[0]),) + raw_batch[1:]
FOOD_CONTAINER = 3 FRUIT = 4 FURNITURE = 6 INSECTS = 7 LARGE_OMNIVORES_HERBIVORES = 11 MEDIUM_MAMMAL = 12
if example_range: ex_range = slice(example_range[0], example_range[1]) else: ex_range = slice(None)
data_x = data['images'][set_indices] data_x = np.cast['float32'](data_x) data_x = data_x[ex_range] data_x = data_x.reshape(data_x.shape[0], image_size ** 2)
if which_set != 'unlabeled': data_y = data['labs_ex'][set_indices] data_y = data_y[ex_range] - 1
view_converter = dense_design_matrix.DefaultViewConverter((image_size, image_size, 1), axes)
super(TFD, self).__init__(X=data_x, y=data_y, y_labels=y_labels, view_converter=view_converter)
self.X -= union.mean(axis=0, dtype='float64') std = union.std(axis=0, dtype='float64') std[std < 1e-3] = 1e-3 self.X /= std
tables = None
if not hasattr(view_converter, 'topo_space'): raise NotImplementedError("Not able to get a topo_space " "from this converter: %s" % view_converter)
self.X_topo_space = view_converter.topo_space
self._iter_mode = resolve_iterator_class('sequential') self._iter_topo = False self._iter_targets = False self._iter_data_specs = (self.X_space, 'features')
space, source = data_specs if isinstance(space, CompositeSpace): sub_spaces = space.components sub_sources = source else: sub_spaces = (space,) sub_sources = (source,)
if not hasattr(view_converter, 'topo_space'): raise NotImplementedError("Not able to get a topo_space " "from this converter: %s" % view_converter)
self.X_topo_space = view_converter.topo_space
self.X_topo_space = self.view_converter.topo_space assert not contains_nan(self.X)
init_space, source = self.data_specs X_space, init_y_space = init_space.components new_y_space = VectorSpace(dim=num_classes) new_space = CompositeSpace((X_space, new_y_space)) self.data_specs = (new_space, source)
self.X_topo_space = self.view_converter.topo_space
atom = (tables.Int32Atom() if config.floatX == 'float32' else tables.Int64Atom())
[self.shape[i] for i in (2, 0, 1)])
rval = np.transpose(rval, tuple(self.axes.index(axis) for axis in ('b', 0, 1, 'c')))
if 'axes' not in d: d['axes'] = ['b', 0, 1, 'c'] self.__dict__.update(d)
if 'topo_space' not in self.__dict__: self._update_topo_space()
y = np.array([[y_i] for y_i in y]) assert min(y) == 0 assert max(y) == 2
data_x = np.cast[config.floatX](data['data']) data_x = data_x[MNISTPlus.idx[which_set]]
data_y = None if label_type is not None: data_y = data[label_type].reshape(-1, 1)
if label_type == 'azimuth': data_y = np.cast[config.floatX](data_y / 360.)
data_y = data_y[MNISTPlus.idx[which_set]]
if which_set == 'test': content = content[1:] content = content[:-1]
num_examples = {'train': 32561, 'test': 16281}[which_set] assert len(content) == num_examples, (len(content), num_examples)
content = map(lambda l: l[:-1].split(', '), content)
features = map(lambda l: l[:-1], content) targets = map(lambda l: l[-1], content) del content
space, source = data_specs if isinstance(space, CompositeSpace): sub_spaces = space.components sub_sources = source else: sub_spaces = (space,) sub_sources = (source,)
return mini_batch
assert start >= 0 assert stop > start assert stop <= X.shape[0] X = X[start:stop, :] y = y[start:stop] assert X.shape[0] == y.shape[0]
rval = X.copy()
if not hasattr(self, 'center'): self.center = False if not hasattr(self, 'gcn'): self.gcn = False
rval = X.copy()
if not hasattr(self, 'center'): self.center = False if not hasattr(self, 'gcn'): self.gcn = False
raise NotImplementedError()
im_path = serial.preprocess(im_path) label_path = serial.preprocess(label_path)
datasetCache = cache.datasetCache im_path = datasetCache.cache_file(im_path) label_path = datasetCache.cache_file(label_path)
data_train = Avicenna(which_set='train', standardize=True) assert data_train.X.shape == (150205, 120)
assert np.allclose(dt.mean(dtype='float64'), 0) assert np.allclose(dt.std(dtype='float64'), 1.)
trainer = yaml_parse.load(design_matrix_yaml % {'filename': filename}) trainer.main_loop()
os.remove(filename)
trainer = yaml_parse.load(topo_view_yaml % {'filename': filename}) trainer.main_loop()
os.remove(filename)
trainer = yaml_parse.load(convert_to_one_hot_yaml % {'filename': filename}) trainer.main_loop()
os.remove(filename)
trainer = yaml_parse.load(load_all_yaml % {'filename': filename}) trainer.main_loop()
os.remove(filename)
train, valid, test, transfer = utlc.load_ndarray_dataset("ule", normalize=True, transfer=True) assert train.shape[0] == transfer.shape[0]
train, valid, test, transfer = utlc.load_sparse_dataset("ule", normalize=True, transfer=True) assert train.shape[0] == transfer.shape[0]
norb_train = FoveatedNORB(which_set="train", scale=1, restrict_instances=[4, 6, 7, 8])
topo_tensors = norb.get_topological_view(single_tensor=False) expected_topo_tensors = tuple(expected_topo_tensor[:, i, ...] for i in range(2))
for norb in (SmallNORB('train', stop=1000), NORB(which_norb='small', which_set='train')): test_impl(norb)
{0: 'animal', 1: 'human', 2: 'airplane', 3: 'truck', 4: 'car', 5: 'blank'},
dict(safe_zip(range(10), range(10))),
dict(safe_zip(range(9), numpy.arange(9) * 5 + 30)),
dict(safe_zip(range(0, 36, 2), numpy.arange(0, 360, 20))),
dict(safe_zip(range(5), range(5))),
dict(safe_zip(range(-5, 6), range(-5, 6))),
dict(safe_zip(range(-5, 6), range(-5, 6))),
dict(safe_zip(range(-19, 20), range(-19, 20))),
dict(safe_zip(range(2), (0.8, 1.3))))
for (label_to_value_map, label_to_value_func) in zip(label_to_value_maps, norb.label_to_value_funcs): for label, expected_value in six.iteritems(label_to_value_map): actual_value = label_to_value_func(label) assert expected_value == actual_value
ddm = get_rnd_design_matrix() folds = ddm.split_dataset_nfolds(10) assert folds[0].shape[0] == np.ceil(ddm.get_num_examples() / 10)
d1 = DenseDesignMatrix(topo_view=topo_view) slice_d = from_dataset(d1, 5) assert slice_d.X.shape[1] == d1.X.shape[1] assert slice_d.X.shape[0] == 5
rng = np.random.RandomState([2014, 11, 4]) start = 0 stop = 990 num_examples = 1000 num_feat = 5 num_classes = 2
y = zca_dataset.mapback(zca_dataset.X) assert_allclose(x[start:stop], y)
y = zca_dataset.mapback_for_viewer(zca_dataset.X) z = x/np.abs(x).max(axis=0) assert_allclose(z[start:stop], y, rtol=1e-2)
y = zca_dataset.adjust_for_viewer(x.T).T z = x/np.abs(x).max(axis=0) assert_allclose(z, y)
assert zca_dataset.has_targets()
from pylearn2.datasets import vector_spaces_dataset
def test_split(): skip_if_no_data() n_train = 100 n_valid = 200 n_test = 300
preprocessor = GlobalContrastNormalization(subtract_mean=True, sqrt_bias=0.0, use_std=True)
preprocessor = GlobalContrastNormalization(subtract_mean=False, sqrt_bias=0.0, use_std=False)
assert preprocessor.P_.shape == (self.X.shape[1], self.X.shape[1]) assert_allclose(np.dot(preprocessor.P_, preprocessor.inv_P_), identity, rtol=1e-4)
assert_allclose(np.cov(preprocessed_X.transpose(), bias=1), identity, rtol=1e-4, atol=1e-4)
preprocessor = ZCA(filter_bias=0.0, n_components=3) preprocessed_X = self.get_preprocessed_data(preprocessor)
preprocessor = ZCA(filter_bias=0.0, n_drop_components=2) preprocessed_X = self.get_preprocessed_data(preprocessor) assert_allclose(zca_truncated_X, preprocessed_X, rtol=1e-3)
sut = PCA(self.num_components) sut.apply(self.dataset, True)
assert (np.diag(cm)[:-1] > np.diag(cm)[1:]).all()
np.testing.assert_almost_equal(np.diag(cm), np.ones(cm.shape[0]))
my_pca_preprocessor.apply(training_set, can_fit = True) my_pca_preprocessor.apply(test_set, can_fit = False)
if dataset.y is not None: dataset.y = numpy.repeat(dataset.y, num_patches / X.shape[0])
if dataset.y is not None: dataset.y = dataset.y[::patches.shape[0] / reassembled_shape[0]]
dspace.np_validate(batch) return batch
to_input = self.to_input(batch) return self.orig_view_converter.get_formatted_batch(to_input, dspace)
if not self._whiten and can_fit: assert proc_var[0] > orig_var.max()
self.matrices_save_path = None
for key, matrix in matrices.items(): del result[key]
if 'matrices_save_path' not in state: state['matrices_save_path'] = None
state = dict(state.items() + matrices.items()) del matrices
self.mean_ = numpy.mean(X, axis=0) X -= self.mean_
mid = int(numpy.floor(kernel_shape / 2.)) centered_X = X - convout[:, mid:-mid, mid:-mid, :]
transformer = Conv2D(filters=filters, batch_size=len(input), input_space=input_space, border_mode='full') sum_sqr_XX = transformer.lmul(X ** 2)
X = np.cast['float32'](X) X = X.reshape(-1, 2 * 96 * 96)
y = NORBSmall.load(which_set, 'cat') y_extra = NORBSmall.load(which_set, 'info')
self.class_names = [array[0].encode('utf-8') for array in train['class_names'][0]]
X = np.cast['float32'](train['X'])
y = train['y'][:, 0] - 1 assert y.shape == (5000,)
self.class_names = [array[0].encode('utf-8') for array in test['class_names'][0]]
y = test['y'][:, 0] - 1 assert y.shape == (8000,)
assert X.shape == (96 * 96 * 3, 100000) assert X.dtype == 'uint8'
assert x.ndim == 4 axes = self.input_space.axes assert len(axes) == 4
axes = self.output_axes assert len(axes) == 4
dummy_v = T.tensor4() dummy_v.name = 'dummy_v'
axes = self.input_space.axes assert len(axes) == 4
axes = self.input_space.axes assert len(axes) == 4
assert last_row % stride[0] == 0 num_row_steps = last_row / stride[0] + 1
assert x.ndim == 4 x_axes = self.input_axes assert len(x_axes) == 4
rval_axes = self.output_axes assert len(rval_axes) == 4
axes = self.input_axes assert len(axes) == 4
dummy_v = T.tensor4() sqfilt = T.square(self._filters)
rval, xdummy = z_hs.owner.op.grad((dummy_v, sqfilt), (x,))
axes = self.input_space.axes assert len(axes) == 4
self = layer
check_cuda(str(type(self)))
self.input_space = input_space
self._conv_op = GpuDnnConv() self._desc = GpuDnnConvDesc(border_mode=border_mode, subsample=self._subsample, conv_mode='conv')
assert x.ndim == 4 axes = self._input_space.axes assert len(axes) == 4
axes = self._output_axes assert len(axes) == 4
output = f(np.transpose(self.image, map_to_another_axes)) output_def = np.array(f_def(self.image)) output = np.transpose(output, map_to_default)
X, = data assert X.shape[0] == self.counter_idx + 1 assert X[0,0] == self.counter_idx prereq_counter = self.counter prereq_counter.set_value(prereq_counter.get_value() + 1)
@functools.wraps(fn) def wrapped(*args, **kwargs): orig_mode = config.mode if orig_mode in ["DebugMode", "DEBUG_MODE"]: config.mode = "FAST_RUN"
scipy_works = False
assert_not_debug_mode()
assert config.mode == "DEBUG_MODE" config.mode = orig_mode
cost = sum(costs) model_terms = sum([param.sum() for param in model.get_params()]) cost = cost * model_terms return cost
return (NullSpace(), '')
return (NullSpace(), '')
self.model.enforce_constraints()
continue_learning = (self.model.continue_learning() and extension_continue) assert continue_learning in [True, False, 0, 1] while continue_learning: if self.exceeded_time_budget(t0, time_budget): break
except StopIteration: log.info("Extension requested training halt.") continue_learning = False
self._samples = samples self._sigma = sigma
assert abs(exact_logz - logz) < 0.01*exact_logz
from pylearn2.datasets.mnist import MNIST dataset = MNIST(which_set='train') data = numpy.asarray(dataset.X, dtype=config.floatX)
ais_nodata('mnistvh.mat', do_exact=do_exact, betas=betas)
continue
warnings.warn("TODO: add unit test that iterators uneven property is set correctly.")
#end class
to_string(monitor)
train.save = MethodType(only_run_extensions, train)
model.dataset = dataset
space, source = data_specs if not isinstance(source, tuple): source = (source,)
target_source = self.add_mask_source(self.get_target_space(), 'targets') return target_source
rng = self.mlp.rng if self.irange is None: raise ValueError("Recurrent layer requires an irange value in " "order to initialize its weight matrices")
W = rng.uniform(-self.irange, self.irange, (input_dim, self.dim))
U = rng.randn(self.dim, self.dim) U, _ = scipy.linalg.qr(U)
b = np.zeros((self.dim,))
W, U, b = self._params if self.weight_noise: W = self.add_noise(W) U = self.add_noise(U)
z = mask[:, None] * z + (1 - mask[:, None]) * state_before
rng = self.mlp.rng if self.irange is None: raise ValueError("Recurrent layer requires an irange value in " "order to initialize its weight matrices")
W = rng.uniform(-self.irange, self.irange, (input_dim, self.dim * 4))
b = np.zeros((self.dim * 4,))
z = mask[:, None] * z + (1 - mask[:, None]) * state_before
rng = self.mlp.rng if self.irange is None: raise ValueError("Recurrent layer requires an irange value in " "order to initialize its weight matrices")
W = rng.uniform(-self.irange, self.irange, (input_dim, self.dim * 3))
b = np.zeros((self.dim * 3,))
BLACKLIST = [ 'CompositeLayer',
b01c_shape = [result.shape[0], space.shape[0], space.shape[1], space.num_channels] result = result.flatten() result = tensor.reshape(result, newshape=b01c_shape, ndim=4)
time_step = 5 return np.zeros((time_step, batch_size, self.dim), dtype=dtype)
self.space._validate_impl(is_numeric, batch[0])
time_step = 5 rval = np.zeros((time_step, batch_size), dtype=dtype) rval[:3, :1] = 1 rval[:4, 1:] = 1 return rval
return getattr(self.cost, attr)
f = function([X, y], [gradients[W].sum(), clipped_gradients[W].sum()], allow_input_downcast=True)
np.testing.assert_allclose(f([[1]], [[0]]), [20, 20 / np.sqrt(2)])
if len(set(ml)) != 1: raise ValueError("Composite space is empty or containing " "incompatible index spaces") return ml[0]
self._load_data(which_set, context_len, data_mode)
return SequenceDatasetIterator(self, data_specs, subset_iterator, return_tuple=return_tuple)
num_braces = 0
num_braces = 0
layer_1_detector = FilterActs()(images, filters)
output = FilterActs()(images, filters)
num_braces = 0
num_braces = 0
nb_channel = int(get_scalar_constant_value(images.shape[0])) assert nb_channel % 16 == 0
num_braces = 0
assert images.type.broadcastable == acts.type.broadcastable assert images.type.broadcastable == denoms.type.broadcastable assert images.type.broadcastable == dout.type.broadcastable
num_braces = 0
num_braces = 0
rows_broadcastable = False cols_broadcastable = False
flops = kerns[1] * kerns[2] * 2 #nb flops by output image flops *= out[1] * out[2] flops *= images[0] * kerns[3] * images[3] return flops
num_braces = 0
FilterActs = None WeightActs = None
rows_broadcastable = False cols_broadcastable = False
num_braces = 0
filter_rows_broadcastable = False filter_cols_broadcastable = False output_channels_broadcastable = hid_grads.type.broadcastable[0]
assert images[3] == kerns[3] flops = kerns[1] * kerns[2] * 2 #nb flops by output image flops *= out[1] * out[2] flops *= images[3] * kerns[0] * images[0] return flops
headers = super(WeightActs, self).c_headers() headers.append('weight_acts.cuh') return headers
num_braces = 0
p, h = prob_max_pool_c01b(z, (pool_rows, pool_cols) ) func = function([z], [p, h], mode = mode_with_gpu)
p, h = max_pool_c01b(z, (pool_rows, pool_cols) ) func = function([z], [p, h], mode = mode_without_gpu)
p, h = prob_max_pool_c01b(z, (pool_rows, pool_cols), top_down = t) func = function([z, t], [p, h], mode = mode_with_gpu)
p, h = max_pool_c01b(z, (pool_rows, pool_cols), top_down = t) func = function([z, t], [p, h], mode = mode_without_gpu)
rng = np.random.RandomState([2012, 10, 9]) batch_size = 5 rows = 10 cols = 9 channels = 3 filter_rows = 4 filter_cols = filter_rows + 1 num_filters = 6
rng = np.random.RandomState([2012, 10, 9]) batch_size = 5 rows = 10 cols = 9 channels = 3 filter_rows = 4 filter_cols = filter_rows num_filters = 6
continue theano.tests.unittest_tools.verify_grad(op, [a.get_value()])
cost_weights = rng.normal(size=(num_filters, rows - filter_rows + 1, cols - filter_cols + 1, batch_size)) cost = (constant(cost_weights) * output).sum()
theano_rng = MRG_RandomStreams(2013*5*4) cost_weights = theano_rng.normal(size=output_conv2d.shape, dtype=output_conv2d.dtype) cost = (cost_weights * output).sum()
images_grad, filters_grad = grad(cost, [images, filters]) reference_cost = (cost_weights * output_conv2d).sum() images_conv2d_grad, filters_conv2d_grad = grad(reference_cost, [images, filters])
p_shared = sharedX(zv[:,0:rows:pool_rows,0:cols:pool_cols,:]) h_shared = sharedX(zv) z_shared = sharedX(zv)
grad_shared = sharedX(zv) z_shared = sharedX(zv)
def make_thunk(self, node, storage_map, compute_map, no_recycling): if not convnet_available(): raise RuntimeError('Could not compile cuda_convnet')
theano.compile.debugmode.default_make_thunk.append( get_unbound_function(BaseActs.make_thunk))
if convnet_available.compiled: _logger.debug('already compiled') return True
if convnet_available.compile_error: _logger.debug('error last time') return False
if not cuda.cuda_available: convnet_available.compile_error = True _logger.debug('cuda unavailable') return False
success = convnet_compile() if success: convnet_available.compiled = True else: convnet_available.compile_error = False _logger.debug('compilation success: %s', success)
convnet_available.compiled = False convnet_available.compile_error = False
if should_recompile(): _logger.debug('recompiling')
open(libcuda_convnet_so).close()
nvcc_compiler.add_standard_rpath(cuda_convnet_loc)
return function([], T.cast(T.argmax(self.estimated_rewards), 'int32'))
for k in xrange(n_folds): this_blocks = [] for i, layer in enumerate(layers): this_blocks.append(layer[k]) this_stacked_blocks = StackedBlocks(this_blocks) stacked_blocks.append(this_stacked_blocks)
self._folds = stacked_blocks
this_algorithm = deepcopy(algorithm) this_algorithm._set_monitoring_dataset(datasets)
this_extensions = deepcopy(extensions)
for trainer in self.trainers: for extension in trainer.extensions: extension.on_save(trainer.model, trainer.dataset, trainer.algorithm)
for extension in self.cv_extensions: extension.on_save(self.trainers)
if valid_size < 1.0: valid_size /= 1.0 - np.true_divide(self.n_test, self.n) self.valid_size = valid_size
if valid_size < 1.0: valid_size /= 1.0 - np.true_divide(self.n_test, self.n) self.valid_size = valid_size
datasets = list(datasets[label] for label in data_subsets.keys()) if len(datasets) == 1: datasets, = datasets
os.remove(filename)
this_yaml = test_yaml_which_set % {'which_set': 'train'} trainer = yaml_parse.load(this_yaml) trainer.main_loop()
this_yaml = test_yaml_which_set % {'which_set': ['train', 'test']} trainer = yaml_parse.load(this_yaml) trainer.main_loop()
this_yaml = test_yaml_which_set % {'which_set': 'valid'} try: trainer = yaml_parse.load(this_yaml) trainer.main_loop() raise AssertionError except ValueError: pass
this_yaml = test_yaml_which_set % {'which_set': 'bogus'} try: yaml_parse.load(this_yaml) raise AssertionError except ValueError: pass
trainer = yaml_parse.load(test_yaml_layer0 % {'layer0_filename': layer0_filename}) trainer.main_loop()
trainer = yaml_parse.load(test_yaml_layer1 % {'layer0_filename': layer0_filename, 'layer1_filename': layer1_filename}) trainer.main_loop()
trainer = yaml_parse.load(test_yaml_layer2 % {'layer0_filename': layer0_filename, 'layer1_filename': layer1_filename, 'layer2_filename': layer2_filename}) trainer.main_loop()
trainer = yaml_parse.load(test_yaml_layer3 % {'layer0_filename': layer0_filename, 'layer1_filename': layer1_filename, 'layer2_filename': layer2_filename}) trainer.main_loop()
os.remove(layer0_filename) os.remove(layer1_filename)
sys.exit(0)
pythonpath = os.environ.get('PYTHONPATH', '') pythonpath = throot + ':' + pythonpath os.environ['PYTHONPATH'] = pythonpath
#sys.path.append(os.path.abspath('some/directory'))
extensions = ['sphinx.ext.autodoc', 'sphinx.ext.todo', 'numpydoc', 'sphinx.ext.autosummary'] #, 'ext']
templates_path = ['.templates']
source_suffix = '.txt'
master_doc = 'index'
project = 'Pylearn2' copyright = '2011-2015, LISA lab'
version = 'dev' release = 'dev'
#today = '' today_fmt = '%B %d, %Y'
#unused_docs = []
exclude_dirs = ['images', 'scripts', 'sandbox']
#default_role = None
#add_function_parentheses = True
#add_module_names = True
#show_authors = False
pygments_style = 'sphinx'
#html_style = 'default.css' html_theme = 'solar' html_theme_path = ["./themes"]
#html_title = None
#html_short_title = None
#html_logo = 'images/theano_logo-200x67.png' #html_logo = 'images/theano_logo_allblue_200x46.png'
#html_favicon = None
html_last_updated_fmt = '%b %d, %Y'
html_use_smartypants = True
#html_sidebars = {}
#html_additional_pages = {}
#html_use_modindex = True
#html_use_index = True
#html_split_index = False
#html_copy_source = True
#html_use_opensearch = ''
#html_file_suffix = ''
htmlhelp_basename = 'theanodoc'
#latex_paper_size = 'letter'
latex_font_size = '11pt'
latex_documents = [ ('index', 'pylearn2.tex', 'Pylearn2 Documentation', 'LISA lab, University of Montreal', 'manual'), ]
#latex_logo = 'images/snake_theta2-trans.png' latex_logo = None
#latex_use_parts = False
#latex_preamble = ''
#latex_appendices = []
#latex_use_modindex = True
return self._batch_size
return self._num_batches
return self.batch_size * self.num_batches
raise NotImplementedError()
product = self.batch_size * self.num_batches if product > self._dataset_size: return self.batch_size * (self.num_batches - 1) else: return product
return resolve_iterator_class(mode).uniform_batch_size
for stream in (self.stdout, self.stderr): stream.flush()
if expected != actual: raise AssertionError("values not equal, expected: %r, actual: %r" % (expected, actual))
if expected is not actual: raise AssertionError("values not identical, expected %r, actual %r" % (expected, actual))
if hasattr(variable, 'name') and variable.name is not None: return variable.name return anon
if dtype is None: dtype = theano.config.floatX return theano.shared(theano._asarray(value, dtype=dtype), name=name, borrow=borrow)
return theano.tensor.constant(np.asarray(value, dtype=theano.config.floatX))
result = {} for key in keys: if key in d: result[key] = d[key] return result
for key, val in six.iteritems(dict_from): if key in dict_to: raise KeyError(key) dict_to[key] = val return dict_to
assert all([len(arg) == len(args[0]) for arg in args]) return izip(*args)
global cuda if cuda is None: from theano.sandbox import cuda return cuda.mem_info()[0]/1024./1024
return [[False]]
return isinstance(op, _ElemwiseNoGradient)
return theano.function(*args, on_unused_input='ignore', **kwargs)
def __init__(cls, name, bases, dict): type.__init__(cls, name, bases, dict) cls.libv = LibVersion()
return self.str_versions
return os.path.realpath(module.__path__[0])
return os.path.dirname(self._get_module_path(module))
logger.info(self.__str__())
s = cPickle.dumps(obj, get_pickle_protocol()) return cPickle.loads(s)
return cPickle.dumps(obj, get_pickle_protocol())
return cPickle.loads(s)
global Image if Image is None: raise RuntimeError("You are trying to use PIL-dependent functionality" " but don't have PIL installed.")
assert len(image.shape) == 3 assert len(shape) == 2 shrunk = fit_inside(image, shape) letterboxed = letterbox(shrunk, shape) return letterboxed
pil_from_ndarray(ndarray).save(filepath)
ndar = ndar.copy() ndar -= ndar.min() ndar *= 1.0 / (ndar.max() + eps) return ndar
assert isinstance(error, MemoryError) if str(error): raise error else: raise TypicalMemoryError(msg)
try: return dict1[key] except KeyError: if default is None: return dict2[key] else: return dict2.get(key, default)
if conf.get('normalize', True): return sharedX(data_x, borrow=True) else: return theano.shared(theano._asarray(data_x), borrow=True)
return self.length
counter = [0, 0, 0] for chosen in self.permut: index = counter[chosen] counter[chosen] = (counter[chosen] + 1) % self.limit[chosen] yield chosen, index
try: raise TypicalMemoryError("test") except TypicalMemoryError as e: pass
arr = np.random.random(100) assert not contains_nan(arr) arr[0] = np.nan assert contains_nan(arr)
return six.next(six.iterkeys(obj))
stable_x = (x.T - x.max(axis=1)).T numer = np.exp(stable_x) return (numer.T / numer.sum(axis=1)).T
rval = np.log(x) rval -= rval.mean() return rval
assert not isinstance(x, theano.gof.Variable) return 1. / (1. + np.exp(-x))
return np.log(x / (1. - x))
precision = tp / T.maximum(1., tp + fp) return precision
recall = tp / T.maximum(1., y.sum()) return recall
X = T.matrix() Y = T.nnet.sigmoid(X) Z = arg_of_sigmoid(Y) assert X is Z
trainset = ToyDataset() testset = ToyDataset() return trainset, testset
train = yaml_parse.load(yaml_file) train.main_loop()
pv = get_weights_report.get_weights_report(model_path=model_path, rescale=rescale, border=border) if out is None: pv.show() else: pv.save(out)
if design_batch.ndim != 2: design_batch = dataset.get_design_matrix(design_batch) mapped_batch_design = dataset.mapback_for_viewer(design_batch) mapped_batch = dataset.get_topological_view(mapped_batch_design) return mapped_batch
print('Loading model...') model = serial.load(model_path) model.set_batch_size(m) return model
r = int(np.sqrt(m)) c = m // r while r * c < m: c += 1 return (r, c)
pv = PatchViewer(grid_shape, vis_chains.shape[1:3], is_color=vis_chains.shape[-1] == 3) for i in xrange(m): pv.add_patch(vis_chains[i, :], rescale=False) return pv
prod = np.dot(W1, W2) pv = make_viewer(prod.T) return pv
print('Loading model...') model = serial.load(model_path) model.set_batch_size(m) return model
return numpy.array(img.getdata()) / 255.
return Image.fromarray(arr.reshape(os, os) * 255.)
if os.path.isfile('dbm.pkl'): os.remove('dbm.pkl') control.pop_load_data()
negative_chains.show_negative_chains('dbm.pkl')
limited_epoch_train(os.path.join(pylearn2.__path__[0], "scripts/autoencoder_example/hcae.yaml"))
return T.maximum(0.0, x)
return x * (x > 0)
return T.switch(x < 0., 0., x)
return self._num_batches_seen
self._examples_seen += num_examples self._num_batches_seen += 1
self._epochs_seen += 1
for name in names: if name not in self.names_to_del: self.names_to_del.append(name)
self._layers.append(layer) if self._params is not None: self._params.update(layer._params)
return -self.ebm.free_energy(X) - self.logZ_driver * self.logZ_lr_scale
return self.ebm.free_energy(X)
self.mapbias = sharedX( numpy.zeros(self.nmap), name='mb', borrow=True )
self.visbiasX = sharedX( numpy.zeros(nvisx), name='vbX', borrow=True )
self.visbiasY = sharedX( numpy.zeros(nvisy), name='vbY', borrow=True )
return tensor.dot(inputs[0], self.wxf)
return tensor.dot(inputs[1], self.wyf)
return self.mapbias + tensor.dot( self._factorsX(inputs) * self._factorsY(inputs), self.whf_in.T)
if self.act_enc is None: act_enc = lambda x: x else: act_enc = self.act_enc return act_enc(self._mappings(inputs))
return tensor.dot(self._hidden_activation(inputs), self.whf)
return self._hidden_activation(inputs)
return len(self.aes) + 1
wb = trials - len(self.aes) if wb <= 0: return 0 else: return wb
return (self.get_input_space(), self.get_input_source())
return self.mean_h_given_v(v)
return self.mean_h_given_v(v)
raise NotImplementedError()
return self.conditional.get_weights()
return self.prior.get_params()
return self.conditional.get_params()
return self.posterior.get_params()
return self.posterior.encode_conditional_params(X)
return self.conditional.encode_conditional_params(z)
return self.prior.get_params()
return self.conditional.conditional_expectation(theta)
return self.log_p_x_given_z(X, theta)
return self.conditional.sample_from_conditional( conditional_params=theta, num_samples=num_samples )
return self.prior.sample_from_p_z(num_samples, **kwargs)
return self.posterior.sample_from_epsilon(shape)
return self.prior.log_p_z(z)
return self.conditional.log_conditional(X, theta)
if hasattr(self, 'vae'): return self.vae else: return None
raise NotImplementedError(str(self.__class__) + " does not implement " "initialize_parameters")
return OrderedDict()
raise NotImplementedError(str(self.__class__) + " does not implement " "sample_from_p_z.")
raise NotImplementedError(str(self.__class__) + " does not implement " "log_p_z.")
return self.mlp.get_weights()
return self.mlp.get_lr_scalers()
raise NotImplementedError(str(self.__class__) + " does not implement " "_get_default_output_layer")
raise NotImplementedError(str(self.__class__) + " does not implement " "_get_required_mlp_output_space")
return OrderedDict()
self.mlp.modify_updates(updates)
if hasattr(self, 'vae'): return self.vae else: return None
conditional_params = self.mlp.fprop(X) if not type(conditional_params) == tuple: conditional_params = (conditional_params, ) return conditional_params
raise NotImplementedError(str(self.__class__) + " does not implement " "conditional_expectation.")
raise NotImplementedError(str(self.__class__) + " does not implement " "log_conditional.")
raise NotImplementedError(str(self.__class__) + " does not " + "implement kl_divergence")
raise NotImplementedError('Not implemented in _PCABase. Use a ' + 'subclass (and implement it there).')
inputs = SparseType('csr', dtype=theano.config.floatX)() return theano.function([inputs], self(inputs), name=name)
inputs = SparseType('csr', dtype=theano.config.floatX)() return theano.function([inputs], self(inputs), name=name)
return "Maxout"
if hasattr(self, 'mlp'): return self.mlp return None
assert self.get_mlp() is None self.mlp = mlp
raise NotImplementedError( str(type(self)) + " does not implement fprop.")
self.freeze_set = self.freeze_set.union(parameter_set)
self._validate_layer_names(layers) total = 0 for layer in self.layers: if layer.layer_name in layers: total += layer.get_input_space().get_total_dimension() return total
self.cost_from_X_data_specs()[0].validate(data) X, Y = data Y_hat = self.fprop(X) return self.cost(Y, Y_hat)
space = CompositeSpace((self.get_input_space(), self.get_target_space())) source = (self.get_input_source(), self.get_target_source()) return (space, source)
p = linear_response return p
return T.sum(T.mean(T.sqr(Y-Y_hat), axis=batch_axis))
self.non_lin_name = "rectifier" self.left_slope = left_slope
p = linear_response * (linear_response > 0.) + self.left_slope *\ linear_response * (linear_response < 0.) return p
p = T.nnet.sigmoid(linear_response) return p
p = T.tanh(linear_response) return p
batch_axis = self.output_space.get_batch_axis() return self.nonlin.cost(Y=Y, Y_hat=Y_hat, batch_axis=batch_axis)
return beta_from_design(dataset.y, **kwargs)
return beta_from_design(dataset.X, **kwargs)
return dataset.y.mean(axis=0)
def __init__(self, estimator): raise RuntimeError("sklearn not available.")
super(DenseMulticlassSVM, self).fit(X, y) return self
return (self.get_input_space(), self.get_input_source())
rval = 1./ (self.model.alpha + self.model.w ) rval.name = 'var_s1' return rval
return self.encode(inputs)
if self.act_enc is None: act_enc = lambda x: x else: act_enc = self.act_enc return act_enc(self._hidden_input(x))
return self.hidbias + tensor.dot(x, self.weights)
return self.encode(inputs)
raise NotImplementedError(str(type(self)) + " does not implement get_default_cost.")
raise NotImplementedError(str(type(self)) + " does not implement continue_learning.")
pass
return type(self).censor_updates != Model.censor_updates
raise TypeError("Model.censor_updates has been replaced by " "Model.modify_updates.")
return self.input_space
return self.output_space
if hasattr(self, '_target_space'): return self._target_space else: return self.get_output_space()
if hasattr(self, 'input_source'): return self.input_source else: return 'features'
if hasattr(self, 'target_source'): return self.target_source else: return 'targets'
for param, value in zip(self.get_params(), values): param.set_value(value, borrow=borrow)
values = self.get_param_values() values = [value.reshape(value.size) for value in values] return np.concatenate(values, axis=0)
raise NotImplementedError()
raise NotImplementedError()
params = self.get_params() updates = OrderedDict(izip_no_length_check(params, params)) self.modify_updates(updates) f = function([], updates=updates) f()
self.dbm = dbm
self.dbm = dbm
return [self.visible_layer] + self.hidden_layers
self.setup_inference_procedure() return self.inference_procedure.mf(*args, **kwargs)
self.rng = make_np_rng(None, [2012, 10, 17], which_method="uniform")
if not hasattr(self, 'inference_procedure') or \ self.inference_procedure is None: self.inference_procedure = WeightDoubling() self.inference_procedure.set_dbm(self)
if not hasattr(self, 'sampling_procedure') or \ self.sampling_procedure is None: self.sampling_procedure = GibbsEvenOdd() self.sampling_procedure.set_dbm(self)
return (self.get_input_space(), self.get_input_source())
H = self.mf(V)[0] downward_state = self.hidden_layers[0].downward_state(H) recons = self.visible_layer.inpaint_update( layer_above=self.hidden_layers[0], state_above=downward_state, drop_mask=None, V=None) return recons
if hasattr(self, 'dbm'): return self.dbm return None
assert self.get_dbm() is None self.dbm = dbm
raise NotImplementedError(str(type(self))+" does not implement " +\ "get_total_state_space()")
raise NotImplementedError("%s doesn't implement make_state" % type(self))
raise NotImplementedError("%s doesn't implement make_symbolic_state" % type(self))
pass
return self.get_input_space()
return self.bias.get_value()
self.mu = sharedX(bias, name = 'mu')
return self.broadcast_beta(self.beta)
def downward_state(self, state): return state def downward_message(self, state): return state
def __init__(self, rng): self.rng = rng
def downward_state(self, state): return state def downward_message(self, state): return state
def downward_state(self, state): return state def downward_message(self, state): return state
global DenseMulticlassSVM skip_if_no_sklearn() skip_if_no_data() import pylearn2.models.svm DenseMulticlassSVM = pylearn2.models.svm.DenseMulticlassSVM
ae = Autoencoder(5, 7, act_enc='tanh', act_dec='cos', tied_weights=True) model = UntiedAutoencoder(ae) model._ensure_extensions()
_params = [sharedX(rng.randn(5)), sharedX(rng.randn(5, 3)), sharedX(rng.randn(4, 4, 4))]
prior = DummyPrior() vae = DummyVAE() prior.set_vae(vae) prior.set_vae(vae)
prior = DiagonalGaussianPrior() vae = DummyVAE() prior.set_vae(vae) prior.initialize_parameters(nhid=5)
prior = DiagonalGaussianPrior() vae = DummyVAE() prior.set_vae(vae) prior.initialize_parameters(nhid=5) prior.sample_from_p_z(10)
prior = DiagonalGaussianPrior() vae = DummyVAE() prior.set_vae(vae) prior.initialize_parameters(nhid=5) z = T.tensor3('z') prior.log_p_z(z)
mlp = MLP(nvis=10, layers=[Linear(layer_name='h', dim=10, irange=0.01)]) Conditional(mlp=mlp, name='conditional')
yaml_src_path = os.path.join(os.path.dirname(__file__), 'test_vae_cost_vae_criterion.yaml') train_object = yaml_parse.load_path(yaml_src_path) train_object.main_loop()
return hash(id(self))
if '.' not in tag_suffix: raise yaml.YAMLError("!import: tag suffix contains no '.'") return try_to_import(tag_suffix)
value = loader.construct_scalar(node) if '.' not in value: raise yaml.YAMLError("import tag suffix contains no '.'") return try_to_import(value)
value = loader.construct_scalar(node) return float(value)
f = open(json_file_path) lines = f.readlines() f.close() content = ''.join(lines) return yaml.load(content)
tag = get_tag(d) try: resolver = resolvers[tag] except KeyError: reraise_as(TypeError('config does not know of any object type "'+tag+'"')) return resolver(d)
pl2_path, = pylearn2.__path__ file_list = _list_files(pl2_path, suffix) return file_list
if not physical_line.rstrip() and line_number == len(lines): return 0, "W391 blank line at end of file"
if physical_line.rstrip() == physical_line: return len(physical_line), "W292 no newline at end of file"
pos = logical_line.find('.has_key(') if pos > -1 and not noqa: yield pos, "W601 .has_key() is deprecated, use 'in'"
pos = logical_line.find('`') if pos > -1: yield pos, "W604 backticks are deprecated, use 'repr()'"
if not patterns: return default return any(fnmatch(filename, pattern) for pattern in patterns)
mod = inspect.getmodule(register_check) for (name, function) in inspect.getmembers(mod, inspect.isfunction): register_check(function)
arguments = [] for name in argument_names: arguments.append(getattr(self, name)) return check(*arguments)
self._start_time = time.time()
self.elapsed = time.time() - self._start_time
self.counters['logical lines'] += 1
return self.file_errors
return sum([self.counters[key] for key in self.messages if key.startswith(prefix)])
for line in self.get_statistics(prefix): print(line)
print_filename = True
self._deferred_print = [] return super(StandardReport, self).init_file( filename, lines, expected, line_offset)
self.options.report = (reporter or self.options.reporter)(self.options) return self.options.report
if self.options.verbose: print('checking %s' % filename) fchecker = self.checker_class( filename, lines=lines, options=self.options) return fchecker.check_all(expected=expected, line_offset=line_offset)
try: verify_format_docstrings() except SkipTest as e: import traceback traceback.print_exc(e) raise AssertionError( "Some file raised SkipTest on import, and inadvertently" " canceled the documentation testing." )
return _is_batch_all(batch, lambda x: isinstance(x, theano.gof.Variable))
raise NotImplementedError("__eq__ not implemented in class %s." % type(self))
return 0
raise NotImplementedError()
return self.make_theano_batch(name=name, dtype=dtype, batch_size=batch_size)
self._check_is_symbolic(batch) self._validate(is_numeric=False, batch=batch)
return self._batch_size(is_numeric=False, batch=batch)
return self._batch_size(is_numeric=True, batch=batch)
raise NotImplementedError("%s does not implement batch_size" % type(self))
raise NotImplementedError(str(type(self)) + " does not implement " + "get_batch")
return ('%(classname)s(dim=%(dim)s, dtype=%(dtype)s)' % dict(classname=self.__class__.__name__, dim=self.dim, dtype=self.dtype))
return (isinstance(subspace, NullSpace) or (isinstance(subspace, CompositeSpace) and len(subspace.components) == 0))
if isinstance(space, CompositeSpace): return tuple(make_dtype_tree(dtype, component) for component in space.components) else: return super_self._clean_dtype_arg(dtype)
return CompositeSpace((CompositeSpace((image_space,) * 2), VectorSpace(dim=1)))
if isinstance(batch, np.ndarray): return batch.shape else: return tuple(get_shape(b) for b in batch)
if isinstance(space, CompositeSpace): return all(specifies_all_dtypes(subspace) for subspace in space.components) else: return space.dtype is not None
if isinstance(dtype, tuple): return tuple(replace_none_dtypes(d, fallback_dtype) for d in dtype) else: return fallback_dtype if dtype is None else dtype
return (arg.dtype is not None and str(arg.dtype).startswith('complex'))
self.get_data_specs(model)[0].validate(data) return None
return self._get_sampling_pos(model, X, Y), OrderedDict()
self.get_data_specs(model)[0].validate(data) return None
return - T.nnet.sigmoid(self.G(X, model))
return model.log_prob(X) - self.noise.log_prob(X)
raise NotImplementedError()
self.get_data_specs(model)[0].validate(data) X = data return self.cost(X, model.reconstruct(X))
return (NullSpace(), '')
self.variables = variables self.p = p
raise NotImplementedError( str(type(self)) + " does not implement " + "continue_learning.")
return self.best_params
if zmq is None: raise unittest.SkipTest
train(os.path.join( pylearn2.__path__[0], 'train_extensions/tests/live_monitor_test.yaml' ))
skip_if_no_sklearn() trainer = yaml_parse.load(test_yaml) trainer.main_loop()
skip_if_no_sklearn() trainer = yaml_parse.load(test_yaml_ovr) trainer.main_loop()
def __init__(self): pass
def __init__(self): self.val_record = []
raise NotImplementedError('get_response is not implemented.')
pass
@wraps(LiveMonitorMsg.get_response) def get_response(self): return ChannelListResponse()
raise NotImplementedError(str(type(self))+" does not implement setup.")
raise NotImplementedError(str(type(self))+" does not implement plot.")
if public: for filename in self.filenames: make_readable(filename)
return "file://"+urllib.pathname2url(os.path.abspath(filename))
return os.geteuid()==0
stats_a = os.stat(filename_a) stats_b = os.stat(filename_b) return stats_a.st_dev == stats_b.st_dev;
print("\r[dl] %6.2f%% %s" % (min(totalsize,blocks*blocksize)*100.0/totalsize, hook_download_filename), end='') sys.stdout.flush()
raise NotImplementedError()
raise NotImplementedError(str(type(self))+" does not implement " + "continue_learning.")
def __init__(self): super(NoBatchSizeError, self).__init__("Neither the " "TrainingAlgorithm nor the model were given a specification " "of the batch size.")
monitor.add_channel( name='momentum', ipt=None, val=self.momentum, data_specs=(NullSpace(), ''), dataset=monitoring_dataset)
if self.termination_criterion is None: return True else: return self.termination_criterion.continue_learning(self.model)
return self._base * min(1, self._anneal_start / self._count)
self._count += 1 self._apply_learning_rate(algorithm)
self.avg()
for i in xrange(int(numpy.sqrt(N)),0, -1): if 0 == N % i: return (i, N/i)
for a in args: if isinstance(a, theano.Variable): return True return False
raise NotImplementedError('override-me')
self.rows = shape[0] self.cols = shape[1]
def __init__(self, W): self.W = W super(ModelWithW, self).__init__()
return cls._categories[int(scalar_label)]
scalar_label = int(scalar_label) assert scalar_label >= 0 assert scalar_label < 9 return 30 + 5 * scalar_label
num_bytes = count * numpy.dtype(num_type).itemsize string = file_handle.read(num_bytes) return numpy.fromstring(string, dtype=num_type)
st = os.statvfs(path) total = st.f_blocks * st.f_frsize used = (st.f_blocks - st.f_bfree) * st.f_frsize return total, used
return tuple([alias if alias else source for alias, source in safe_zip(self._aliases, self._sources)])
space = [self.spaces[s] for s in self._get_sources] return space[0] if len(space) == 1 else tuple(space)
assert isinstance(key_or_alias, string_types) try: return super(alias_dict, self).__getitem__(key_or_alias) except KeyError: return super(alias_dict, self).__getitem__( self.__a2k__[key_or_alias])
return self.storage_space.np_format_as(batch, space)
return self.storage_space.np_format_as(design_mat, self.topo_space)
return self.design_mat_to_topo_view(design_mat)
return self.topo_space.np_format_as(topo_batch, self.storage_space)
return _get_array_element('category', label, ('animal', 'human', 'airplane', 'truck', 'car', 'blank'))
return _check_range_and_return('instance', label, -1, 9, -1)
name = 'elevation' _check_is_integral(name, label) _check_range(name, label, -1, 8) if label == -1: return None else: return label * 5 + 30
return _check_range_and_return('lighting', label, -1, 5, -1)
return _check_range_and_return('horizontal shift', label, -5, 5)
return _check_range_and_return('vertical shift', label, -5, 5)
return _check_range_and_return('lumination_change', label, -19, 19)
return _get_array_element('contrast change', label, (0.8, 1.3))
return _get_array_element('scale change', label, (0.78, 1.0))
return _check_range_and_return('rotation change', label, -4, 4)
return numpy.clip(X * 2. - 1., -1., 1.)
return self.adjust_for_viewer(X)
return self.data_specs
iterator = super(HDF5DatasetDeprecated, self).iterator(*args, **kwargs) iterator.__class__ = HDF5DatasetIterator return iterator
s = f.read(4) s_array = numpy.fromstring(s, dtype='int32') return s_array.item()
global tables if tables is None: import tables
if self.y is None: return self.X else: return (self.X, self.y)
axis = self.view_converter.axes.index('b') return axis
folds_iter = self.iterator(mode="sequential", num_batches=nfolds) folds = list(folds_iter) return folds
folds_iter = self.iterator(mode="random_slice", num_batches=nfolds, rng=rng) folds = list(folds_iter) return folds
return copy.copy(self.rng)
self.reset_RNG()
return self.data_specs
rows, cols, channels = self.shape self.topo_space = Conv2DSpace(shape=(rows, cols), num_channels=channels, axes=self.axes)
rval = X.copy() for i in xrange(rval.shape[0]): rval[i, :] /= np.abs(rval[i, :]).max() + 1e-12 return rval
self.iterator(mode='shuffled_sequential', batch_size=batch_size, num_batches=None) return self.next()
raise NotImplementedError('Not implemented for sparse dataset')
return self.data_specs
if self.y is None: return self.X else: return (self.X, self.y)
return self.iterator()
raise NotImplementedError()
train = TFD(which_set='train') topo = train.get_batch_topo(1) assert topo.ndim == 4
skip_if_no_data() data = stl10.STL10(which_set='train') data = stl10.STL10(which_set='test')
skip_if_no_data() self.train = OCR(which_set='train') self.valid = OCR(which_set='valid') self.test = OCR(which_set='test')
topo = self.train.get_batch_topo(1) assert topo.ndim == 4
train = CIFAR10(which_set='train') topo = train.get_batch_topo(1) assert topo.ndim == 4
self.train_set.adjust_for_viewer(self.train_set.X)
self.train_set.adjust_to_be_viewed_with( self.train_set.X, np.ones(self.train_set.X.shape))
topo = self.train_set.get_batch_topo(1) assert topo.ndim == 4
for X in [self.train.X, self.test.X]: assert X.min() == 0.0 assert X.max() == 1.0
topo = self.train.get_batch_topo(1) assert topo.ndim == 4
data_specs = (IndexSpace(max_labels=10, dim=1), 'targets') it = self.test.iterator(mode='sequential', data_specs=data_specs, batch_size=100) for y in it: pass
assert X.ndim == 2 return T.nnet.softmax(X * self.P)
raise NotImplementedError()
self._W = W
return [self._filters]
assert self.local.get_params() == [self.filters]
assert self.conv2d.get_params() == [self.filters]
assert self.conv2d.get_params() == [self.filters]
theano.config.floatX = self.orig_floatX
with self.assertRaises(AssertionError): Cudnn2D(filters=self.filters, batch_size=-1, input_space=self.input_space)
self.assertEqual(self.cudnn2d.get_params(), [self.filters])
for ext in self.extensions: ext.setup(self.model, self.dataset, self.algorithm)
self.params_on_monitor = np.asarray(model.get_param_values())
param += self.mlp.theano_rng.normal(size=param.shape, avg=0., std=self._std_dev, dtype=param.dtype) return param
if isinstance(input_space, CompositeSpace): return any(find_sequence_space(component) for component in input_space.components) if isinstance(input_space, SequenceDataSpace): return True return False
raise SkipTest('Sandbox RNNs are disabled.')
braces = '}' * num_braces + "\n" rval = (basic_setup + setup_nv_images + setup_nv_targets + setup_nv_denoms + do_normalize + braces) rval = rval % locals() return rval
braces = '}' * num_braces rval = basic_setup + \ setup_nv_images + \ setup_nv_filters + \ setup_nv_targets + \ do_convolution + \ braces rval = rval % locals() return rval
hid_acts, filters, output_shape = inputs out, = outputs assert hid_acts[0] == filters[3] flops = (hid_acts[3] * filters[0] * hid_acts[0] * filters[1] * filters[2] * hid_acts[1] * hid_acts[2] * 2) return flops
braces = '}' * num_braces rval = basic_setup + \ setup_nv_hid_acts + \ setup_nv_filters + \ setup_nv_targets + \ do_convolution + \ braces rval = rval % locals() return rval
try: open(libcuda_convnet_so).close() return True except IOError: return False
def rval(): X, y = self.dataset.get_batch_design(self.batch_size, include_labels=True) self.y_cache = y return X return rval
def rval(a): return (a * self.y_cache).sum(axis=1) return rval
return self._folds[k]
return self._folds[0][0].get_input_space()
return self._folds[0][-1].get_output_space()
self.setup_extensions()
for extension in self.cv_extensions: extension.setup(self.trainers)
trainer.main_loop(time_budget) return trainer
cv = list(super(ValidationKFold, self).__iter__()) for train, valid, test in get_k_fold_splits(cv): yield train, valid, test
cv = list(super(StratifiedValidationKFold, self).__iter__()) for train, valid, test in get_k_fold_splits(cv): yield train, valid, test
return self._folds[k]
return [fold.set_input_space(space) for fold in self._folds]
return self._folds[0].get_params()
return self._folds[0].get_input_space()
return self._folds[0].get_output_space()
skip_if_no_sklearn() mapping = {'dataset_iterator': 'DatasetKFold'} test_yaml = test_yaml_dataset_iterator % mapping trainer = yaml_parse.load(test_yaml) trainer.main_loop()
skip_if_no_sklearn() mapping = {'dataset_iterator': 'StratifiedDatasetKFold'} test_yaml = test_yaml_dataset_iterator % mapping trainer = yaml_parse.load(test_yaml) trainer.main_loop()
skip_if_no_sklearn() mapping = {'dataset_iterator': 'DatasetShuffleSplit'} test_yaml = test_yaml_dataset_iterator % mapping trainer = yaml_parse.load(test_yaml) trainer.main_loop()
skip_if_no_sklearn() mapping = {'dataset_iterator': 'StratifiedDatasetShuffleSplit'} test_yaml = test_yaml_dataset_iterator % mapping trainer = yaml_parse.load(test_yaml) trainer.main_loop()
skip_if_no_sklearn() mapping = {'dataset_iterator': 'DatasetValidationKFold'} test_yaml = test_yaml_dataset_iterator % mapping trainer = yaml_parse.load(test_yaml) trainer.main_loop()
skip_if_no_sklearn() mapping = {'dataset_iterator': 'StratifiedDatasetValidationKFold'} test_yaml = test_yaml_dataset_iterator % mapping trainer = yaml_parse.load(test_yaml) trainer.main_loop()
skip_if_no_sklearn() mapping = {'dataset_iterator': 'DatasetValidationShuffleSplit'} test_yaml = test_yaml_dataset_iterator % mapping trainer = yaml_parse.load(test_yaml) trainer.main_loop()
skip_if_no_sklearn() mapping = {'dataset_iterator': 'StratifiedDatasetValidationShuffleSplit'} test_yaml = test_yaml_dataset_iterator % mapping trainer = yaml_parse.load(test_yaml) trainer.main_loop()
from __future__ import absolute_import import copy import errno import fnmatch import glob import hashlib import logging import os import shutil from datetime import datetime from salt.exceptions import FileserverConfigError
import salt.ext.six as six try: import hglib HAS_HG = True except ImportError: HAS_HG = False
import salt.utils import salt.utils.url import salt.fileserver from salt.utils.event import tagify
__virtualname__ = 'hg'
return repo.branches()
pass
hglib.init(rp_) new_remote = True
try: shutil.rmtree(repo['lockfile']) except OSError as exc: _add_error(failed, repo, exc)
if not fnmatch.fnmatch(repo['url'], six.text_type(remote)): continue
if not fnmatch.fnmatch(repo['url'], six.text_type(remote)): continue
os.remove(destdir) os.makedirs(destdir)
os.remove(hashdir) os.makedirs(hashdir)
repo['repo'].close() continue
return []
if not relpath.startswith('../'): ret.add(os.path.join(repo['mountpoint'], relpath))
return []
from __future__ import absolute_import import logging
AUTH_PROVIDERS = ('pygit2',) AUTH_PARAMS = ('user', 'password', 'pubkey', 'privkey', 'passphrase', 'insecure_auth')
import salt.utils.gitfs from salt.exceptions import FileserverConfigError
__virtualname__ = 'git'
return __virtualname__
return []
import os import errno import logging
import salt.fileserver import salt.utils from salt.utils.event import tagify import salt.ext.six as six
return fnd
return fnd
pass
data = {'changed': False, 'files': {'changed': []}, 'backend': 'roots'}
new_mtime_map = salt.fileserver.generate_mtime_map(__opts__['file_roots'])
data['changed'] = salt.fileserver.diff_mtime_map(old_mtime_map, new_mtime_map)
if not path or not os.path.isfile(path): return ret
ret['hash_type'] = __opts__['hash_type']
try: os.unlink(cache_path) except OSError: pass return file_hash(load, fnd)
pass
return []
from __future__ import absolute_import import errno import fnmatch import logging import os import re import time
import salt.loader import salt.utils import salt.utils.locales
import salt.ext.six as six
time.sleep(1) if not os.path.isfile(dest): _unlock_cache(lk_fn) return False
wait_lock(w_lock, list_cache, 15 * 60)
cache_stat = os.stat(list_cache) age = time.time() - cache_stat.st_mtime
age = opts.get('fileserver_list_cache_time', 30) + 1
refresh_cache = True break
log.info( 'Failed to get mtime on {0}, ' 'dangling symlink ?'.format(file_path)) continue
if sorted(map1) != sorted(map2): #log.debug('diff_mtime_map: the maps are different') return True
#log.debug('diff_mtime_map: the maps are the same') return False
path = salt.utils.url.unescape(path)
continue
import os import logging
import salt.fileserver import salt.utils import salt.utils.url
__virtualname__ = 'minion'
pass
if not path or not os.path.isfile(path): return ret
ret['hash_type'] = __opts__['hash_type']
#def file_list_emptydirs(load):
from __future__ import absolute_import import datetime import os import time import pickle import logging
import salt.fileserver as fs import salt.modules import salt.utils import salt.utils.s3 as s3
import salt.ext.six as six from salt.ext.six.moves import filter from salt.ext.six.moves.urllib.parse import quote as _quote
metadata = _init() return list(metadata.keys())
_get_file_from_s3(metadata, saltenv, bucket, file_path, cached_file_path)
_get_file_from_s3(metadata, saltenv, fnd['bucket'], path, cached_file_path)
cached_file_path = _get_cached_file_name( fnd['bucket'], load['saltenv'], fnd['path'])
for dirs in six.itervalues(_find_dirs(metadata[saltenv])): dirs = _trim_env_off_path(dirs, saltenv, trim_slash=True) ret += [_f for _f in dirs if _f]
metadata = None try: if os.path.getmtime(cache_file) > exp: metadata = _read_buckets_cache_file(cache_file) except OSError: pass
metadata = _refresh_buckets_cache_file(cache_file)
return os.path.join(__opts__['cachedir'], 's3cache')
if not os.path.exists(os.path.dirname(file_path)): os.makedirs(os.path.dirname(file_path))
def __get_s3_meta(bucket, key=key, keyid=keyid): return s3.query( key=key, keyid=keyid, kms_keyid=keyid, bucket=bucket, service_url=service_url, verify_ssl=verify_ssl, location=location, return_bin=False)
for saltenv, buckets in six.iteritems(_get_buckets()): bucket_files = {} for bucket_name in buckets: s3_meta = __get_s3_meta(bucket_name)
if not s3_meta: continue
bucket_files[bucket_name] = [k for k in s3_meta if 'Key' in k]
for bucket_name in _get_buckets(): s3_meta = __get_s3_meta(bucket_name)
if not s3_meta: continue
files = [k for k in s3_meta if 'Key' in k]
for saltenv in environments: env_files = [k for k in files if k['Key'].startswith(saltenv)]
if os.path.isfile(cache_file): os.remove(cache_file)
ret[bucket_name] += [k for k in filePaths if not k.endswith('/')]
item_meta['ETag'] = item_meta['ETag'].strip('"')
if os.path.isfile(cached_file_path): file_meta = _find_file_meta(metadata, bucket_name, saltenv, path) if file_meta: file_etag = file_meta['ETag']
if cached_md5 == file_md5: return
s3.query( key=key, keyid=keyid, kms_keyid=keyid, bucket=bucket_name, service_url=service_url, verify_ssl=verify_ssl, location=location, path=_quote(path), local_file=cached_file_path )
from __future__ import absolute_import import copy import errno import fnmatch import hashlib import logging import os import shutil from datetime import datetime from salt.exceptions import FileserverConfigError
import salt.ext.six as six HAS_SVN = False try: import pysvn HAS_SVN = True CLIENT = pysvn.Client() except ImportError: pass
import salt.utils import salt.utils.url import salt.fileserver from salt.utils.event import tagify
__virtualname__ = 'svn'
pass
try: shutil.rmtree(repo['lockfile']) except OSError as exc: _add_error(failed, repo, exc)
if six.text_type(remote) not in repo['url']: continue
if not fnmatch.fnmatch(repo['url'], six.text_type(remote)): continue
continue
ret.add('base')
continue
if not path or not os.path.isfile(path): return ret
ret['hash_type'] = __opts__['hash_type']
continue
continue
for key in ret: ret[key] = sorted(ret[key]) if save_cache: salt.fileserver.write_file_list_cache( __opts__, ret, list_cache, w_lock ) return ret.get(form, [])
from __future__ import absolute_import import os import os.path import logging import time
HAS_FCNTL = False
import salt.fileserver import salt.utils import salt.syspaths
return fnd
return fnd
pass
envs = __opts__.get('azurefs_envs', []) for env in envs: storage_conn = azure.get_storage_conn(opts=envs[env]) result = azure.list_blobs( storage_conn=storage_conn, container=env, )
for blob in result: file_name = os.path.join(base_dir, blob)
comps = file_name.split('/') file_path = '/'.join(comps[:-1]) if not os.path.exists(file_path): os.makedirs(file_path)
azure.get_blob( storage_conn=storage_conn, container=env, name=blob, local_path=file_name, )
if not path or not os.path.isfile(path): return ret
ret['hash_type'] = __opts__['hash_type']
from __future__ import print_function from __future__ import absolute_import import os
file_root = os.path.abspath(self.options.file_root) self.config['file_roots'] = {'base': _expand_glob_path([file_root])}
pillar_root = os.path.abspath(self.options.pillar_root) self.config['pillar_roots'] = {'base': _expand_glob_path([pillar_root])}
states_dir = os.path.abspath(self.options.states_dir) self.config['states_dirs'] = [states_dir]
self.setup_logfile_logger() verify_log(self.config)
from __future__ import absolute_import import os import warnings from salt.utils.verify import verify_log
warnings.filterwarnings(
warnings.filterwarnings( 'ignore', 'With-statements now directly support multiple context managers', DeprecationWarning )
warnings.filterwarnings( 'ignore', '^Module backports was already imported from (.*), but (.*) is being added to sys.path$', UserWarning )
import salt.log.setup
from salt.utils import migrations from salt.utils import kinds
log = salt.log.setup.logging.getLogger(__name__)
current_umask = os.umask(0o027) verify_files([logfile], self.config['user']) os.umask(current_umask)
import salt.master self.master = salt.master.Master(self.config)
import salt.daemons.flo self.master = salt.daemons.flo.IofloMaster(self.config)
current_umask = os.umask(0o027) verify_files([logfile], self.config['user']) os.umask(current_umask)
if self.check_running(): self.action_log_info('An instance is already running. Exiting') self.shutdown(1)
self.config['id'] = self.values.proxyid
current_umask = os.umask(0o027) verify_files([logfile], self.config['user']) os.umask(current_umask)
current_umask = os.umask(0o027) verify_files([logfile], self.config['user']) os.umask(current_umask)
from __future__ import absolute_import, print_function
pass
import salt.ext.six as six
from salt.exceptions import ( SaltClientError, CommandNotFoundError, CommandExecutionError, SaltInvocationError, )
ttype = 'zeromq'
try: self.minion = salt.minion.SMinion(opts) except SaltClientError as exc: raise SystemExit(str(exc))
pass
return ret
self.process = MultiprocessingProcess(target=raet_minion_run, kwargs={'cleanup_protecteds': [self.stack.ha], }) self.process.start() self._wait_caller(opts)
from __future__ import absolute_import, print_function import math import time import copy from datetime import datetime, timedelta
import salt.client import salt.output import salt.exceptions from salt.utils import print_cli
import salt.ext.six as six from salt.ext.six.moves import range
bwait = self.opts.get('batch_wait', 0) wait = []
if queue in minion_tracker: minion_tracker[queue]['active'] = False
for minion in minion_tracker[queue]['minions']: if minion not in parts: parts[minion] = {} parts[minion]['ret'] = {}
ret[minion] = data yield {minion: data}
from __future__ import absolute_import, print_function import os import logging
import salt.client.netapi import salt.utils.parsers as parsers from salt.utils.verify import check_user, verify_files, verify_log
current_umask = os.umask(0o027) verify_files([logfile], self.config['user']) os.umask(current_umask)
from __future__ import print_function from __future__ import absolute_import
self.setup_logfile_logger() verify_log(self.config) profiling_enabled = self.options.profiling_enabled
from __future__ import print_function from __future__ import absolute_import
from __future__ import absolute_import
import salt.spm import salt.utils.parsers as parsers from salt.utils.verify import verify_log
from __future__ import print_function from __future__ import absolute_import import os import sys
import salt.client from salt.utils import parsers, print_cli from salt.utils.verify import verify_log import salt.output
self.setup_logfile_logger() verify_log(self.config)
from __future__ import absolute_import, print_function import os import sys
import salt.ext.six as six
self.setup_logfile_logger() verify_log(self.config)
skip_perm_errors = self.options.eauth != ''
if retcodes.count(0) < len(retcodes): sys.stderr.write('ERROR: Minions returned with non-zero exit code\n') sys.exit(11)
sys.exit(1)
import contextlib import errno import logging import os import shutil import subprocess import time
import salt.utils import salt.modules.selinux from salt.exceptions import CommandExecutionError, FileLockError, MinionError
from salt.ext import six
fh_ = os.open(lock_fn, open_flags)
with os.fdopen(fh_, 'w'): pass log.trace('Write lock %s obtained', lock_fn) obtained_lock = True yield break
from __future__ import absolute_import import sys import copy import types
import salt.loader
from __future__ import absolute_import
import os import urlparse
from salt.ext.six.moves.urllib.parse import urlparse
import salt.fileclient import salt.utils.url
from __future__ import absolute_import import json import pprint import logging from os import path from functools import wraps
import salt.ext.six as six from jinja2 import BaseLoader, Markup, TemplateNotFound, nodes from jinja2.environment import TemplateModule from jinja2.ext import Extension from jinja2.exceptions import TemplateRuntimeError import jinja2 import yaml
import salt import salt.utils import salt.utils.url import salt.fileclient from salt.utils.odict import OrderedDict
if '..' in template: log.warning( 'Discarded template path \'{0}\', relative paths are ' 'prohibited'.format(template) ) raise TemplateNotFound(template)
continue
raise TemplateNotFound(template)
output.append('\'{0}\': \'{1}\''.format(key, value))
output.append('\'{0}\': {1!s}'.format(key, value))
output.append('\'{0}\': {1!r}'.format(key, value))
{% load_yaml as var1 %} foo: it works {% endload %} {% load_yaml as var2 %} bar: for real {% endload %}
{% from "doc1.sls" import var1, var2 as local2 %} {{ var1.foo }} {{ local2.bar }}
from __future__ import absolute_import import os import threading
import salt.utils import salt.payload
pass
try: os.remove(path) except IOError: pass return None
return None
try: os.remove(path) except IOError: pass return None
from __future__ import absolute_import import re
yield val
import salt.utils from salt.exceptions import SaltException
@property def buffered(self): return self.__buffered
def __iter__(self): return self
def __enter__(self): return self
from __future__ import absolute_import from collections import Callable
import salt.ext.six as six
import collections
try: from salt.ext.six.moves._thread import get_ident as _get_ident except ImportError: from salt.ext.six.moves._dummy_thread import get_ident as _get_ident
if key not in self: root = self.__root last = root[0] last[1] = root[0] = self.__map[key] = [last, root, key] dict_setitem(self, key, value)
dict_delitem(self, key) link_prev, link_next, key = self.__map.pop(key) link_prev[1] = link_next link_next[0] = link_prev
from __future__ import absolute_import, print_function import os import re import time import logging try: import msgpack HAS_MSGPACK = True except ImportError: HAS_MSGPACK = False
import salt.config import salt.payload import salt.utils.dictupdate
self._write()
if __name__ == '__main__':
import logging
try: import requests
import salt.utils import salt.utils.aws import salt.utils.xmlutil as xml from salt._compat import ElementTree as ET
if not key: key = salt.utils.aws.IROLE_CODE
log.debug(' Response content: {0}'.format(response))
if return_bin: return response
from __future__ import absolute_import import copy import contextlib
import salt.ext.six as six
PER_REMOTE_ONLY = ('name',) SYMLINK_RECURSE_DEPTH = 100
AUTH_PROVIDERS = ('pygit2',) AUTH_PARAMS = ('user', 'password', 'pubkey', 'privkey', 'passphrase', 'insecure_auth')
try: import git import gitdb HAS_GITPYTHON = True except ImportError: HAS_GITPYTHON = False
if not isinstance(err, ImportError): log.error('Import pygit2 failed: {0}'.format(err))
GITPYTHON_MINVER = '0.3' PYGIT2_MINVER = '0.20.3' LIBGIT2_MINVER = '0.20.0' DULWICH_MINVER = (0, 9, 4)
per_remote_only = {} for param in PER_REMOTE_ONLY: if param in per_remote_conf: per_remote_only[param] = per_remote_conf.pop(param)
if 'root' not in repo_conf: repo_conf['root'] = ''
for key, val in six.iteritems(repo_conf): setattr(self, key, val)
self.mountpoint = ''
pass
try: shutil.rmtree(lock_file) except OSError as exc: _add_error(failed, exc)
return self._fetch()
os.write(fh_, str(os.getpid()))
pid = 0
head_sha = None
raise GitLockError( exc.errno, 'Checkout lock exists for {0} remote \'{1}\'' .format(self.role, self.id) )
self.repo.git.tag('-d', ref.name[10:])
self.repo = git.Repo.init(self.cachedir) new = True
pass
return files, symlinks
return None, None
break
if not self.env_is_exposed(tgt_env): return None
log.error( 'Unable to get SHA of HEAD for %s remote \'%s\'', self.role, self.id ) return None
self.repo.checkout(checkout_ref) if branch: self.repo.reset(oid, pygit2.GIT_RESET_HARD)
raise GitLockError( exc.errno, 'Checkout lock exists for {0} remote \'{1}\'' .format(self.role, self.id) )
oid = self.repo.lookup_reference(remote_ref).get_object().id if local_ref not in refs: self.repo.create_reference(local_ref, oid)
return self.check_root()
tag_sha = tag_obj.target.hex
log.error( 'Unable to resolve %s from %s remote \'%s\' ' 'to either an annotated or non-annotated tag', tag_ref, self.role, self.id ) return None
return self.check_root()
remote_refs.append( line.split()[-1].replace(b'refs/heads/', b'refs/remotes/origin/') )
continue
self.repo = pygit2.init_repository(self.cachedir) new = True
pass
continue
received_objects = fetch_results['received_objects']
received_objects = fetch_results.received_objects
continue
return files, symlinks
oid = tree[self.root].oid tree = self.repo[oid]
return None, None
link_tgt = self.repo[tree[path].oid].data path = os.path.normpath( os.path.join(os.path.dirname(path), link_tgt) )
if not self.env_is_exposed(tgt_env): return None try: commit = self.repo.revparse_single(tgt_ref) except (KeyError, TypeError): pass else: return commit.tree return None
return True
return True
transport = 'ssh' address = self.url
return True
return True
continue
log.warning( '{0} remote \'{1}\' is an empty repository and will ' 'be skipped.'.format(self.role, self.id) ) return False
for ref in self.get_env_refs(refs_post): self.repo[ref] = refs_post[ref] for ref in refs_pre: if ref not in refs_post: del self.repo[ref] return True
continue
return None, None
break
if not self.env_is_exposed(tgt_env): return None try: int(tgt_ref, 16) except ValueError: return None
return None
pass
self.url = 'git+' + self.url
self.repo = dulwich.repo.Repo.init(self.cachedir) new = True
for parent in path.split(os.path.sep): try: tree = self.repo.get_object(tree[parent][1]) except (KeyError, TypeError): return None return tree
try: if not fnmatch.fnmatch(repo.url, remote): continue except TypeError: if not fnmatch.fnmatch(repo.url, six.text_type(remote)): continue
try: if not fnmatch.fnmatch(repo.url, remote): continue except TypeError: if not fnmatch.fnmatch(repo.url, six.text_type(remote)): continue
data = {'changed': False, 'backend': 'gitfs'}
pygit2ver = distutils.version.LooseVersion(pygit2.__version__) pygit2_minver = distutils.version.LooseVersion(PYGIT2_MINVER)
os.remove(destdir) os.makedirs(destdir)
os.remove(hashdir) os.makedirs(hashdir)
return fnd
return []
GitBase.__init__(self, opts, valid_providers=('gitpython', 'pygit2'))
GitBase.__init__(self, opts, valid_providers=('gitpython', 'pygit2'), cache_root=winrepo_dir)
sls[ks_opts['lang']['lang']] = {'locale': ['system']}
sls[ks_opts['keyboard']['xlayouts']] = {'keyboard': ['system']}
if 'selinux' in ks_opts.keys(): for mode in ks_opts['selinux']: sls[mode] = {'selinux': ['mode']}
if 'nobase' not in ks_opts['packages']['options']: sls['base'] = {'pkg_group': ['installed']}
from __future__ import absolute_import import os import sys import stat import codecs import shutil import hashlib import socket import tempfile import time import subprocess import multiprocessing import logging import pipes import msgpack import traceback import copy import re import uuid
try: import pwd except ImportError: if not sys.platform.lower().startswith('win'): raise
import salt.cloud from salt.exceptions import ( SaltCloudConfigError, SaltCloudException, SaltCloudSystemExit, SaltCloudExecutionTimeout, SaltCloudExecutionFailure, SaltCloudPasswordError )
import salt.ext.six as six
log = logging.getLogger(__name__)
with salt.utils.fopen(path, 'r') as fp_: return fp_.read()
return __render_script(os_, vm_, opts, minion)
return __render_script('{0}.sh'.format(os_), vm_, opts, minion)
return ''
if keysize < 2048: keysize = 2048 tdir = tempfile.mkdtemp()
minion = { 'master': 'salt', 'log_level': 'info', 'hash_type': 'sha256', }
minion.setdefault('grains', {}).update( salt.config.get_cloud_config_value( 'grains', vm_, opts, default={}, search_global=True ) ) return minion
master = copy.deepcopy(salt.config.DEFAULT_MASTER_OPTS) master.update( log_level='info', log_level_logfile='info' )
master.update( salt.config.get_cloud_config_value( 'master', vm_, opts, default={}, search_global=True ) ) return master
if 'pub_key' not in vm_ and 'priv_key' not in vm_: log.debug('Generating keys for \'{0[name]}\''.format(vm_))
if 'gateway' in vm_: deploy_kwargs.update({'gateway': vm_['gateway']})
usernames = [x for x in usernames if x] initial = usernames[:]
test_ssh_host = host test_ssh_port = port
sock.shutdown(socket.SHUT_RDWR) sock.close() break
log.debug( 'Gateway {0} on port {1} is reachable.'.format( test_ssh_host, test_ssh_port ) )
kwargs = {'hostname': host, 'creds': creds}
raise DeprecationWarning( '`salt.utils.cloud.deploy_script now only accepts ' 'dictionaries for it\'s `minion_conf` parameter. ' 'Loading YAML...' )
raise DeprecationWarning( '`salt.utils.cloud.deploy_script now only accepts ' 'dictionaries for it\'s `master_conf` parameter. ' 'Loading from YAML ...' )
for minion_id, minion_key in six.iteritems(preseed_minion_keys): rpath = os.path.join( preseed_minion_keys_tempdir, minion_id ) ssh_file(opts, rpath, minion_key, ssh_kwargs)
time.sleep(0.025)
return 1
'-oStrictHostKeyChecking=no', '-oUserKnownHostsFile=/dev/null', '-oControlPath=none'
'-oStrictHostKeyChecking=no', '-oUserKnownHostsFile=/dev/null', '-oControlPath=none'
ssh_args.extend([ '-oPasswordAuthentication=no', '-oChallengeResponseAuthentication=no', '-oPubkeyAuthentication=yes', '-oIdentitiesOnly=yes', '-oKbdInteractiveAuthentication=no', '-oIdentityFile={0}'.format(kwargs['key_filename']) ])
return 1
ssh_args.extend(['-t', '-t'])
'-oStrictHostKeyChecking={0}'.format(host_key_checking), '-oUserKnownHostsFile={0}'.format(known_hosts_file), '-oControlPath=none'
if ip.startswith('fe80:'): return False return True
return False
return False
return False
else: script_content = url script_name = '{0}.sh'.format( hashlib.sha1(script_content).hexdigest() )
builtin_deploy_dir = os.path.join( os.path.dirname(__file__), 'deploy' )
deploy_d_from_conf_file = os.path.join( os.path.dirname(config['conf_file']), 'cloud.deploy.d' )
deploy_d_from_syspaths = os.path.join( syspaths.CONFIG_DIR, 'cloud.deploy.d' )
deploy_scripts_search_paths = [] for entry in config.get('deploy_scripts_search_path', []): if entry.startswith(builtin_deploy_dir): continue
deploy_scripts_search_paths.append((entry, True))
if deploy_d_from_conf_file not in deploy_scripts_search_paths: deploy_scripts_search_paths.append( (deploy_d_from_conf_file, True) ) if deploy_d_from_syspaths not in deploy_scripts_search_paths: deploy_scripts_search_paths.append( (deploy_d_from_syspaths, True) )
if entry in finished: continue else: finished.append(entry)
missing_node_cache(prov_dir, nodes, provider, opts)
u'\xa0': u' ', u'\u2013': u'-',
raise exc
raise RuntimeError('Invalid password provided.')
import re import inspect
import salt.ext.six as six
return arg
if (isinstance(original_arg, six.string_types) and not original_arg.startswith('{')): return original_arg else: return arg
return original_arg
return original_arg
salt.utils.compat.pack_dunder(__name__)
from __future__ import absolute_import import hashlib import logging import sys
try: import boto import boto.exception logging.getLogger('boto').setLevel(logging.CRITICAL) HAS_BOTO = True except ImportError: HAS_BOTO = False
from __future__ import absolute_import import atexit import logging import time
from salt.exceptions import SaltSystemExit import salt.modules.cmdmod import salt.utils
try: from pyVim.connect import GetSi, SmartConnect, Disconnect from pyVmomi import vim, vmodl HAS_PYVMOMI = True except ImportError: HAS_PYVMOMI = False
log = logging.getLogger(__name__)
if port is None: port = 443 if protocol is None: protocol = 'https'
esx_cmd += ' -s {0} -u {1} -p \'{2}\' ' \ '--protocol={3} --portnumber={4} {5}'.format(host, user, pwd, protocol, port, cmd)
if not container_ref: container_ref = service_instance.content.rootFolder
obj_view = service_instance.content.viewManager.CreateContainerView( container_ref, [obj_type], True)
traversal_spec = vmodl.query.PropertyCollector.TraversalSpec( name='traverseEntities', path='view', skip=False, type=vim.view.ContainerView )
property_spec = vmodl.query.PropertyCollector.PropertySpec( type=obj_type, all=True if not property_list else False, pathSet=property_list )
obj_spec = vmodl.query.PropertyCollector.ObjectSpec( obj=obj_view, skip=True, selectSet=[traversal_spec] )
filter_spec = vmodl.query.PropertyCollector.FilterSpec( objectSet=[obj_spec], propSet=[property_spec], reportMissingObjectsInResults=False )
content = service_instance.content.propertyCollector.RetrieveContents([filter_spec])
obj_view.Destroy()
object_list = get_mors_with_properties(service_instance, object_type, property_list=[property_name], container_ref=container_ref)
content = get_content(service_instance, object_type, property_list=property_list, container_ref=container_ref)
import json import logging import time import pprint from salt.ext.six.moves import range import salt.ext.six as six import salt.utils try: import requests
return str(key)
log.error('Failed to read region from instance metadata. Giving up.')
log.error('Failed to read metadata. Giving up on IAM credentials.')
from __future__ import absolute_import import warnings
import yaml from yaml.nodes import MappingNode, SequenceNode from yaml.constructor import ConstructorError try: yaml.Loader = yaml.CLoader yaml.Dumper = yaml.CDumper except Exception: pass
if node.value == '': node.value = '0'
existing_nodes = [name_node.value for name_node, value_node in node.value] mergeable_items = [x for x in merge if x[0].value not in existing_nodes]
from __future__ import absolute_import import os import fnmatch import re import logging
import salt.payload import salt.utils from salt.defaults import DEFAULT_TARGET_DELIM from salt.exceptions import CommandExecutionError import salt.auth.ldap import salt.ext.six as six
import salt.ext.six as six if six.PY3: import ipaddress else: import salt.ext.ipaddress as ipaddress HAS_RANGE = False try:
if expanded_nodegroup or not first_call: return ret else: log.debug('No nested nodegroups detected. ' 'Using original nodegroup definition: {0}' .format(nodegroups[nodegroup])) return nodegroups[nodegroup]
tgt = ipaddress.ip_address(tgt)
tgt = ipaddress.ip_network(tgt)
log.error('Detected nodegroup expansion failure of "{0}"'.format(word)) return []
log.error( 'Unrecognized target engine "{0}" for' ' target expression "{1}"'.format( target_info['engine'], word, ) ) return []
results.extend([')' for item in unmatched])
allowed_minions = set()
if len(minions - allowed_minions_from_auth_list) > 0: return False
if mismatch: return False
if self.match_check(ind, fun): return True
continue
continue
from __future__ import absolute_import import re import sys
resource = url.split('salt://', 1)[-1]
import os import sys import time import errno import signal import select import logging
from win32file import ReadFile, WriteFile from win32pipe import PeekNamedPipe import msvcrt import win32api import win32con import win32process
import salt.utils from salt.ext.six import string_types from salt.log.setup import LOG_LEVELS
pass
rows=None, cols=None,
log_stdin=None, log_stdin_level='debug', log_stdout=None, log_stdout_level='debug', log_stderr=None, log_stderr_level='debug',
stream_stdout=None, stream_stderr=None, ):
_cleanup()
self.pid = None self.stdin = None self.stdout = None self.stderr = None
self.status = None self.__irix_hack = 'irix' in sys.platform.lower()
try: self._spawn()
def _translate_newlines(self, data): if data is None or not data: return return data.replace('\r\n', os.linesep)
def __enter__(self): return self
if self.isalive(): self.wait()
if mswindows: def _execute(self): raise NotImplementedError
ecode = win32process.GetExitCodeProcess(self._handle) if ecode == win32con.STILL_ACTIVE: raise self.exitstatus = ecode
else: def _spawn(self): self.pid, self.child_fd, self.child_fde = self.__fork_ptys()
self.stdin = sys.stdin.fileno() self.stdout = sys.stdout.fileno() self.stderr = sys.stderr.fileno()
self.child_fd = self.stdin
max_fd = resource.getrlimit(resource.RLIMIT_NOFILE) try: os.closerange(pty.STDERR_FILENO + 1, max_fd[0]) except OSError: pass
self.closed = False self.terminated = False
os.close(stdout_parent_fd) os.close(stderr_parent_fd) salt.utils.reinit_crypto()
child_name = os.ttyname(stdout_child_fd) try: tty_fd = os.open('/dev/tty', os.O_RDWR | os.O_NOCTTY) if tty_fd >= 0: os.close(tty_fd)
pass
os.setsid()
pass
os.dup2(stdout_child_fd, pty.STDIN_FILENO) os.dup2(stdout_child_fd, pty.STDOUT_FILENO) os.dup2(stderr_child_fd, pty.STDERR_FILENO)
salt.utils.reinit_crypto() os.close(stdout_child_fd) os.close(stderr_child_fd)
if self.child_fd: fd_flags = fcntl.fcntl(self.child_fd, fcntl.F_GETFL) if self.child_fde: fde_flags = fcntl.fcntl(self.child_fde, fcntl.F_GETFL)
rlist, _, _ = select.select(rfds, [], [], 0)
if self.child_fde in rlist: try: stderr = self._translate_newlines( salt.utils.to_str( os.read(self.child_fde, maxsize) ) )
if self.child_fd in rlist: try: stdout = self._translate_newlines( salt.utils.to_str( os.read(self.child_fd, maxsize) ) )
return stdout, stderr
return 24, 80
waitpid_options = 0
time.sleep(0.1) if not self.isalive(): return True else: return False
return
if self.isalive() and _ACTIVE is not None: _ACTIVE.append(self)
import salt.ext.six as six
import salt.utils import salt.defaults.exitcodes from salt.utils.filebuffer import BufferedReader
log = logging.getLogger(__name__)
continue
self.criteria = criteria[_REQUIRES_PATH] + \ criteria[_REQUIRES_STAT] + \ criteria[_REQUIRES_CONTENTS]
from __future__ import absolute_import
import fnmatch import glob import logging
def __setstate__(self, state): self._is_child = True Reactor.__init__( self, state['opts'], log_queue=state['log_queue'])
for name in res: res[name]['__sls__'] = fn_
client_cache = None event_user = 'Reactor'
from __future__ import absolute_import import io
import yaml import salt.ext.six as six
import os
return True
return False
return False
parent_dir = os.path.dirname(path)
return False
return os.access(parent_dir, os.W_OK)
return True
return False
import re import socket
from salt.ext.six import string_types import salt.utils
if salt.utils.is_windows():
try: socket.inet_pton(address_family, ip) except socket.error: return False
try: mask = int(mask) except ValueError: return False else: if not 1 <= mask <= int(mask_max): return False
import re import logging from salt.ext.six import string_types
from __future__ import absolute_import from __future__ import print_function
from __future__ import absolute_import import logging import collections import salt.exceptions
raise salt.exceptions.CommandExecutionError(lazy_obj.missing_fun_string(fun))
return bool(self._dict or not self.loaded)
return self.__nonzero__()
self._dict = {}
self.loaded = False
if not self.loaded: self._load_all() return len(self._dict)
import os import logging import smtplib from email.utils import formatdate
from __future__ import absolute_import, division, print_function import contextlib import copy import collections import datetime
from salt.ext import six
from salt.ext.six.moves import range from salt.ext.six.moves import zip from salt.ext.six.moves import map from stat import S_IMODE
try: import Crypto.Random HAS_CRYPTO = True except ImportError: HAS_CRYPTO = False
HAS_FCNTL = False
HAS_GRP = False
HAS_PWD = False
return False
if use in colors: for color in colors: if color == 'ENDC': continue colors[color] = colors[use]
if line > num_template_lines: return template
buf = [to_str(i) if isinstance(i, six.text_type) else i for i in buf]
user_name = 'SYSTEM'
reinit_crypto() sys.exit(salt.defaults.exitcodes.EX_OK)
os.chdir('/') os.setsid() os.umask(18)
return (os.access(exe, os.X_OK) and (os.path.isfile(exe) or os.path.islink(exe)))
return exe
log.error(err)
os.makedirs(fn_)
stamp = time.strftime('%a_%b_%d_%H-%M-%S_%Y')
parts = [os.path.normpath(p) for p in parts]
finger += '{0}:'.format(pre[ind])
data = data.copy()
pass
for key, value in six.iteritems(data): if key in expected_extra_kws: continue ret['kwargs'][key] = value
return ret
extra = {} for key, value in six.iteritems(data): if key in expected_extra_kws: continue extra[key] = copy.deepcopy(value)
warn_until( 'Carbon', 'It\'s time to start raising `SaltInvocationError` instead of ' 'returning warnings', _dont_call_warnings=True )
ret.setdefault('context', {}).update(extra)
try: with fopen(fp_, 'rb') as fp2_: block = fp2_.read(blocksize) except IOError: return False
return False
return True
try:
return True
return True
return default
for embedded in (x for x in data if isinstance(x, dict)): try: data = embedded[each] embed_match = True break except KeyError: pass if not embed_match: return default
is_proxy = False try: if 'salt-proxy' in main.__file__: is_proxy = True except AttributeError: pass return is_proxy
if include_pat and not exclude_pat: ret = retchk_include elif exclude_pat and not include_pat: ret = retchk_exclude elif include_pat and exclude_pat: ret = retchk_include and retchk_exclude else: ret = True
elif result is _empty and isinstance(state_result, dict) and ret: ret = check_state_result(state_result, recurse=True)
try: value = int(value) except (ValueError, TypeError): pass try: value = float(value) except (ValueError, TypeError): pass
os.chmod(path, stat.S_IWUSR) func(path)
if status.st_ino != 0: node = (status.st_dev, status.st_ino) if node in _seen: return _seen.add(node)
for chunk in iter(lambda: ifile.read(chunk_size), b''): hash_obj.update(chunk) return hash_obj.hexdigest()
return True
try: if isinstance(date, six.string_types): try: if HAS_TIMELIB: return timelib.strtodatetime(to_bytes(date)) except ValueError: pass
if date.isdigit(): date = int(date) else: date = float(date)
stacklevel = 2
stacklevel = 3
if not isinstance(cmp_result, numbers.Integral): log.error('The version comparison function did not return an ' 'integer/long.') return False
if cmp_result < -1: cmp_result = -1 elif cmp_result > 1: cmp_result = 1
ret[key] = {'old': '', 'new': new[key]}
ret[key] = {'new': '', 'old': old[key]}
ret[key] = {'old': old[key], 'new': new[key]}
continue
raise ValueError
if len(nontext) / len(data) > 0.30: return True return False
return []
log.trace('Trying pysss.getgrouplist for \'{0}\''.format(user)) try:
try: default_group = grp.getgrgid(pwd.getpwnam(user).pw_gid).gr_name ugroups.remove(default_group) except KeyError: pass
return {}
return []
if not path: raise ValueError('no path specified')
i = len(os.path.commonprefix([start_list, path_list]))
from __future__ import absolute_import import logging import inspect
HAS_LIBS = False try: import azure HAS_LIBS = True except ImportError: pass
import salt.ext.six as six from salt.exceptions import SaltSystemExit
import sys import time import binascii from datetime import datetime import hashlib import hmac import logging import salt.config import re
import salt.utils.xmlutil as xml from salt._compat import ElementTree as ET
try: import requests
from salt.ext.six.moves import map, range, zip from salt.ext.six.moves.urllib.parse import urlencode, urlparse
global __AccessKeyId__, __SecretAccessKey__, __Token__, __Expiration__
if __Expiration__ != '': timenow = datetime.utcnow() timestamp = timenow.strftime('%Y-%m-%dT%H:%M:%SZ') if timestamp < __Expiration__: return __AccessKeyId__, __SecretAccessKey__, __Token__
access_key_id, secret_access_key, token = creds(provider)
if token != '': params_with_headers['SecurityToken'] = token
now = time.mktime(datetime.utcnow().timetuple())
if role_arn is None: access_key_id, secret_access_key, token = creds(prov_dict) else: access_key_id, secret_access_key, token = assumed_creds(prov_dict, role_arn, location=location)
if not payload_hash: payload_hash = hashlib.sha256(data).hexdigest()
canonical_request = '\n'.join(( method, uri, querystring, canonical_headers, signed_headers, payload_hash ))
signing_key = _sig_key( secret_access_key, datestamp, location, product )
signature = hmac.new( signing_key, string_to_sign.encode('utf-8'), hashlib.sha256).hexdigest()
authorization_header = ( '{0} Credential={1}/{2}, SignedHeaders={3}, Signature={4}' ).format( algorithm, access_key_id, credential_scope, signed_headers, signature, )
if token != '': new_headers['X-Amz-Security-Token'] = token
if __Location__ != '': return __Location__
result = requests.get( "http://169.254.169.254/latest/dynamic/instance-identity/document", proxies={'http': ''}, timeout=AWS_METADATA_TIMEOUT, )
__Location__ = 'do-not-get-from-metadata' return None
from __future__ import absolute_import
import jinja2 import yaml import msgpack import salt.ext.six as six import tornado
try: import certifi HAS_CERTIFI = True except ImportError: HAS_CERTIFI = False
HAS_MARKUPSAFE = False
from backports import ssl_match_hostname HAS_SSL_MATCH_HOSTNAME = True
try: from salt.ext import ssl_match_hostname HAS_SSL_MATCH_HOSTNAME = True except ImportError: HAS_SSL_MATCH_HOSTNAME = False
import salt import salt.utils import salt.exceptions
tops.append(os.path.dirname(xml.__file__))
raise salt.exceptions.SaltSystemExit( 'The minimum required python version to run salt-ssh is "2.6".' )
tempdir = tempfile.mkdtemp() egg = zipfile.ZipFile(top_dirname) egg.extractall(tempdir) top = os.path.join(tempdir, base) os.chdir(tempdir)
tfp.add(base, arcname=os.path.join('py{0}'.format(py_ver), base)) continue
raise salt.exceptions.SaltSystemExit( 'The minimum required python version to run salt-ssh is "2.6".' )
tempdir = tempfile.mkdtemp() egg = zipfile.ZipFile(top_dirname) egg.extractall(tempdir) top = os.path.join(tempdir, base) os.chdir(tempdir)
tfp.add(base, arcname=os.path.join('py{0}'.format(py_ver), base)) continue
import logging import os import re
from .vt import Terminal, TerminalException
raise TerminalException('Password authentication failed')
break
ret_stdout = [] ret_stderr = [] while self.conn.has_unread_data: stdout, stderr = self.conn.recv()
from __future__ import absolute_import import os import logging import json import salt.utils.http from salt.exceptions import CommandExecutionError
import salt.utils
try: import ntsecuritycon import psutil import pywintypes import win32api import win32net import win32security HAS_WIN32 = True except ImportError: HAS_WIN32 = False
groups = [name]
user_name = 'SYSTEM'
security_descriptor = win32security.GetFileSecurity( path, win32security.OWNER_SECURITY_INFORMATION) owner_sid = security_descriptor.GetSecurityDescriptorOwner()
memcached.host: 127.0.0.1 memcached.port: 11211
my_memcached_config: memcached.host: 127.0.0.1 memcached.port: 11211
from __future__ import absolute_import import logging
from salt.exceptions import CommandExecutionError, SaltInvocationError from salt.ext.six import integer_types
try: import memcache HAS_LIBS = True except ImportError: HAS_LIBS = False
log = logging.getLogger(__name__)
__func_alias__ = { 'set_': 'set' }
from __future__ import absolute_import
comps = name.split('}') name = comps[1]
if not isinstance(xmldict[name], list): xmldict[name] = [xmldict[name]] xmldict[name].append(to_dict(item))
from __future__ import absolute_import, print_function import os import sys import types import signal import getpass import logging import optparse import traceback import yaml from functools import partial
import salt.ext.six as six
_mixin_prio_ = 0
for func in dir(base): if not func.startswith('process_'): continue
continue
self.explicit = True return optparse.Option.take_action(self, action, dest, *args, **kwargs)
_mixin_prio_ = 100
_setup_mp_logging_listener_ = False
log.setup_temp_logger( getattr(self.options, 'log_level', 'error') )
process_option_funcs = [] for option_key in options.__dict__: process_option_func = getattr( self, 'process_{0}'.format(option_key), None ) if process_option_func is not None: process_option_funcs.append(process_option_func)
return options, args
log.shutdown_multiprocessing_logging_listener()
self._mixin_after_parsed_funcs.append(self.__merge_config_with_cli)
for option in self.option_list: if option.dest is None: continue
default = self.defaults.get(option.dest) value = getattr(self.options, option.dest, default)
if value is not None: self.config[option.dest] = value
self.config[option.dest] = value
setattr(self.options, option.dest, self.config[option.dest])
self.options.saltfile = os.environ.get('SALT_SALTFILE', None)
return
self.options.saltfile = os.path.abspath(self.options.saltfile)
logging.getLogger(__name__).info( 'Loading Saltfile from \'{0}\''.format(self.options.saltfile) )
return
return
cli_config = saltfile_config[self.get_prog_name()]
for option in self.option_list: if option.dest is None: continue
continue
default = self.defaults.get(option.dest) value = getattr(self.options, option.dest, default) if value != default: continue
setattr(self.options, option.dest, cli_config[option.dest]) option.explicit = True
for group in self.option_groups: for option in group.option_list: if option.dest is None: continue
continue
default = self.defaults.get(option.dest) value = getattr(self.options, option.dest, default) if value != default: continue
for key in cli_config: setattr(self.options, key, cli_config[key])
sys.stderr.write( 'WARNING: CONFIG \'{0}\' directory does not exist.\n'.format( self.options.config_dir ) )
self.options.config_dir = os.path.abspath(self.options.config_dir)
raise RuntimeError( 'Please set {0}._default_logging_logfile_'.format( self.__class__.__name__ ) )
self._mixin_after_parsed_funcs.append(self.__setup_extended_logging) self._mixin_after_parsed_funcs.append(self._setup_mp_logging_listener) self._mixin_after_parsed_funcs.append(self.__setup_console_logger)
self.options.log_file = self.config.get(cli_setting_name)
self.options.log_file = self.config.get( self._logfile_config_setting_name_ )
self.options.log_file = self._default_logging_logfile_
self.options.log_file_level = self.config.get(cli_setting_name)
self.options.log_file_level = self.config.get( self._logfile_loglevel_config_setting_name_ )
self.options.log_level = self._default_logging_level_
self.config.pop(self._logfile_loglevel_config_setting_name_)
self._loglevel_config_setting_name_, self.config['log_level']
self.config.pop(cli_log_path)
self.config.pop(self._logfile_config_setting_name_)
cli_log_path, self.config.get( self._logfile_config_setting_name_, self._default_logging_logfile_ )
self.config.pop(cli_log_file_fmt)
self.config.pop('log_fmt_logfile', None)
self.config.pop(cli_log_file_datefmt)
self.config.pop('log_datefmt_logfile', None)
self.config.pop('log_datefmt_console', None)
if getattr(self.options, 'daemon', False) is True: return
self.config.pop(cli_log_datefmt)
self.config.pop('log_datefmt_console', None)
if self.check_pidfile(): os.unlink(self.config['pidfile'])
log.shutdown_multiprocessing_logging_listener(daemonizing=True)
salt.utils.daemonize()
self._setup_mp_logging_listener()
if salt.utils.is_windows(): from salt.utils.win_functions import get_parent_pid ppid = get_parent_pid() else: ppid = os.getppid()
def _install_signal_handlers(self): signal.signal(signal.SIGTERM, self._handle_signals) signal.signal(signal.SIGINT, self._handle_signals)
pass
ofh.write('')
)
_config_filename_ = 'master' _default_logging_logfile_ = os.path.join(syspaths.LOGS_DIR, 'master') _setup_mp_logging_listener_ = True
_config_filename_ = 'minion' _default_logging_logfile_ = os.path.join(syspaths.LOGS_DIR, 'minion') _setup_mp_logging_listener_ = True
_config_filename_ = 'proxy' _default_logging_logfile_ = os.path.join(syspaths.LOGS_DIR, 'proxy')
_config_filename_ = 'master' _default_logging_logfile_ = os.path.join(syspaths.LOGS_DIR, 'master') _setup_mp_logging_listener_ = True
_config_filename_ = 'master'
sys.stdout.write('Invalid options passed. Please try -h for '
_config_filename_ = 'master'
_default_logging_level_ = 'warning' _default_logging_logfile_ = os.path.join(syspaths.LOGS_DIR, 'master') _loglevel_config_setting_name_ = 'cli_salt_cp_log_file'
if len(self.args) <= 1: self.print_help() self.exit(salt.defaults.exitcodes.EX_USAGE)
_config_filename_ = 'master'
_skip_console_logging_config_ = True _logfile_config_setting_name_ = 'key_logfile' _default_logging_logfile_ = os.path.join(syspaths.LOGS_DIR, 'key')
process_config_dir._mixin_prio_ = ConfigDirMixIn._mixin_prio_
keys_config['key_logfile'] = os.devnull keys_config['pki_dir'] = self.options.gen_keys_dir
self.config['loglevel'] = 'info'
_config_filename_ = 'minion'
_default_logging_level_ = 'info' _default_logging_logfile_ = os.path.join(syspaths.LOGS_DIR, 'minion')
_config_filename_ = 'master'
_default_logging_level_ = 'warning' _default_logging_logfile_ = os.path.join(syspaths.LOGS_DIR, 'master') _loglevel_config_setting_name_ = 'cli_salt_run_log_file'
_config_filename_ = 'master'
_default_logging_level_ = 'warning' _default_logging_logfile_ = os.path.join(syspaths.LOGS_DIR, 'ssh') _loglevel_config_setting_name_ = 'cli_salt_run_log_file'
_config_filename_ = 'cloud'
_default_logging_level_ = 'info' _logfile_config_setting_name_ = 'log_file' _loglevel_config_setting_name_ = 'log_level_logfile' _default_logging_logfile_ = os.path.join(syspaths.LOGS_DIR, 'cloud')
from salt.cloud import libcloudfuncs libcloudfuncs.check_libcloud_version()
_config_filename_ = 'spm' _default_logging_logfile_ = os.path.join(syspaths.LOGS_DIR, 'spm')
_config_filename_ = 'master' _default_logging_logfile_ = os.path.join(syspaths.LOGS_DIR, 'api')
APPL_KINDS = OrderedDict([('master', 0), ('minion', 1), ('syndic', 2), ('caller', 3)])
from __future__ import absolute_import import os import logging import signal import tempfile from threading import Thread, Event
import salt.ext.six as six try: import zmq HAS_ZMQ = True except ImportError: HAS_ZMQ = False
grains = {} pillars = {}
grains, pillars = self._get_cached_minion_data(*minion_ids)
os.remove(os.path.join(data_file))
os.remove(os.path.join(mine_file))
socket = context.socket(zmq.PUB) socket.setsockopt(zmq.LINGER, 100) socket.bind('ipc://' + self.timer_sock)
def __setstate__(self, state): self._is_child = True self.__init__(state['opts'], log_queue=state['log_queue'])
self.opts = opts
self.minions = []
self.timer_stop = Event() self.timer = CacheTimer(self.opts, self.timer_stop) self.timer.start() self.running = True
def __setstate__(self, state): self._is_child = True self.__init__(state['opts'], log_queue=state['log_queue'])
self.cleanup() if self.running: self.running = False self.timer_stop.set() self.timer.join()
creq_in = context.socket(zmq.REP) creq_in.setsockopt(zmq.LINGER, 100) creq_in.bind('ipc://' + self.cache_sock)
serial = salt.payload.Serial(self.opts.get('serial', ''))
signal.signal(signal.SIGINT, self.signal_handler)
self.secure()
if isinstance(msg, str): if msg == 'minions': reply = serial.dumps(self.minions) creq_in.send(reply)
if socks.get(cupd_in) == zmq.POLLIN: new_c_data = serial.loads(cupd_in.recv()) #cupd_in.send(serial.dumps('ACK'))
if not isinstance(new_c_data, list): log.error('ConCache Worker returned unusable result') del new_c_data continue
if socks.get(timer_in) == zmq.POLLIN: sec_event = serial.loads(timer_in.recv())
if int(sec_event % 30) == 0: cw = CacheWorker(self.opts) cw.start()
if __name__ == '__main__':
from __future__ import absolute_import import collections import logging
from salt._compat import subprocess
QUERYFORMAT = '%{NAME}_|-%{EPOCH}_|-%{VERSION}_|-%{RELEASE}_|-%{ARCH}_|-%{REPOID}'
except ValueError: return None
from salt.ext.six.moves.urllib.parse import urljoin as _urljoin import salt.ext.six.moves.http_client from salt.version import __version__ import salt.utils.http
from __future__ import print_function from __future__ import absolute_import from os.path import splitext, abspath from sys import modules
import win32serviceutil import win32service import win32event import win32api
def start(self): pass
def stop(self): pass
from sys import executable module_path = executable
import collections
from __future__ import absolute_import, print_function import sys import inspect import textwrap import functools
import salt.utils.args from salt.utils.odict import OrderedDict
#import yaml import salt.ext.six as six
def _failing_new(*args, **kwargs): raise TypeError('Can\'t create another NullSentinel instance')
attrs['__config__'] = True attrs['__flatten__'] = False attrs['__config_name__'] = None
instance.__flatten__ = True
instance.__allow_additional_items__ = True
title = None description = None _items = _sections = _order = None __flatten__ = False __allow_additional_items__ = False
properties[name] = serialized_section
if config.__flatten__ is True: serialized_config = config.serialize() cls.after_items_update.append(serialized_config) skip_order = True else: properties[item_name] = config.serialize()
required.append(item_name)
if item_name is not None: if item_name not in ordering: ordering.append(item_name) else: if name not in ordering: ordering.append(name)
serialized['required'] = required
serialized['x-ordering'] = ordering
__type__ = None __format__ = None _attributes = None __flatten__ = False
continue
if self.__serialize_attr_aliases__ and argname in self.__serialize_attr_aliases__: argname = self.__serialize_attr_aliases__[argname] serialized[argname] = argvalue
return self.items.serialize()
class PortItem(IntegerItem):
import os import logging
try: import win32con import win32api import win32process import win32security import win32pipe import win32event import win32profile import msvcrt import ctypes from ctypes import wintypes HAS_WIN32 = True except ImportError: HAS_WIN32 = False
import salt.utils
log = logging.getLogger(__name__)
kernel32 = ctypes.WinDLL('kernel32') advapi32 = ctypes.WinDLL('advapi32')
_win(kernel32.WaitForSingleObject, DWORD_IDV,
_win(kernel32.GetStdHandle, HANDLE_IHV,
_win(kernel32.CloseHandle, wintypes.BOOL,
_win(kernel32.SetHandleInformation, wintypes.BOOL,
_win(kernel32.DuplicateHandle, wintypes.BOOL,
_win(kernel32.GetCurrentProcess, wintypes.HANDLE)
_win(kernel32.GetExitCodeProcess, wintypes.BOOL,
_win(kernel32.CreatePipe, wintypes.BOOL,
_win(advapi32.CreateProcessWithLogonW, wintypes.BOOL,
token = win32security.LogonUser(username, domain, password, win32con.LOGON32_LOGON_INTERACTIVE, win32con.LOGON32_PROVIDER_DEFAULT)
security_attributes = win32security.SECURITY_ATTRIBUTES() security_attributes.bInheritHandle = 1
stdin_read, stdin_write = win32pipe.CreatePipe(security_attributes, 0) stdin_read = make_inheritable(stdin_read)
startup_info = win32process.STARTUPINFO() startup_info.dwFlags = win32con.STARTF_USESTDHANDLES startup_info.hStdInput = stdin_read startup_info.hStdOutput = stdout_write startup_info.hStdError = stderr_write
user_environment = win32profile.CreateEnvironmentBlock(token, False)
cmd = 'cmd /c {0}'.format(cmd)
procArgs = (None, cmd, security_attributes, security_attributes, 1, 0, user_environment, None, startup_info)
ret = {'pid': PId}
if win32event.WaitForSingleObject(hProcess, win32event.INFINITE) == win32con.WAIT_OBJECT_0: exitcode = win32process.GetExitCodeProcess(hProcess) ret['retcode'] = exitcode
win32api.CloseHandle(hProcess)
if win32api.GetUserName() == 'SYSTEM': return runas_system(cmd, username, password)
c2pread, c2pwrite = CreatePipe(inherit_read=False, inherit_write=True) errread, errwrite = CreatePipe(inherit_read=False, inherit_write=True)
stdin = kernel32.GetStdHandle(STD_INPUT_HANDLE) dupin = DuplicateHandle(srchandle=stdin, inherit=True)
startup_info = STARTUPINFO(dwFlags=win32con.STARTF_USESTDHANDLES, hStdInput=dupin, hStdOutput=c2pwrite, hStdError=errwrite)
cmd = 'cmd /c {0}'.format(cmd)
process_info = CreateProcessWithLogonW(username=username, domain=domain, password=password, logonflags=LOGON_WITH_PROFILE, commandline=cmd, startupinfo=startup_info, currentdirectory=cwd)
ret = {'pid': process_info.dwProcessId}
kernel32.CloseHandle(process_info.hProcess)
import logging
self.process.kill()
return StateRequisite(requisite, self.module, id_)
for attr in REQUISITES: if attr in kwargs: try: iter(kwargs[attr]) except TypeError: kwargs[attr] = [kwargs[attr]] self.kwargs = kwargs
for attr in REQUISITES: if attr in kwargs: kwargs[attr] = [ req() if isinstance(req, StateRequisite) else req for req in kwargs[attr] ]
return [ {k: kwargs[k]} for k in sorted(six.iterkeys(kwargs)) ]
for item in cls.__dict__: if item[0] == '_': continue
if not inspect.isclass(filt): continue
grain = getattr(filt, '__grain__', 'os_family') if grain not in match_groups: match_groups[grain] = OrderedDict([])
if hasattr(filt, '__match__'): match = filt.__match__ else: match = item
from __future__ import absolute_import import re import salt.ext.six as six
from __future__ import absolute_import import os import tempfile import sys import errno import time import random import shutil import salt.ext.six as six
_CreateTransaction = ctypes.windll.ktmw32.CreateTransaction _CommitTransaction = ctypes.windll.ktmw32.CommitTransaction _MoveFileTransacted = ctypes.windll.kernel32.MoveFileTransactedW _CloseHandle = ctypes.windll.kernel32.CloseHandle CAN_RENAME_OPEN_FILE = True
changes[namespace] = { 'new': config, 'old': update_config, }
changes[namespace] = { 'new': config, 'old': update_config, } return config
changes[namespace] = { 'new': config, 'old': update_config, }
changes[namespace] = { 'new': config, 'old': update_config, } return config
from __future__ import absolute_import import os import re import socket import logging from string import ascii_letters, digits
import salt.ext.six as six
try: import wmi import salt.utils.winapi except ImportError: pass
import salt.utils from salt._compat import subprocess, ipaddress
if ' ' in e: first = 1 else: first = -1
if e in punish: second = punish.index(e) else: second = -1
third = e.count(':')
fifth = -(e.count('.'))
sixth = -(len(e))
hosts = [] for name in h: name = name.strip() if len(name) > 0: hosts.append(name)
hosts = list(set(hosts)) return hosts
for addr in salt.utils.network.ip_addrs(): addr = ipaddress.ip_address(addr) if addr.is_loopback: continue possible_ids.append(str(addr))
if len(possible_ids) == 0: return 'noname'
addr = {'address': val.rstrip('(Preferred)'), 'prefixlen': None} iface['inet6'].append(addr)
iface['up'] = (val != 'Media disconnected')
if 'COMMAND' in chunks[1]:
if 'COMMAND' in chunks[1]:
log.warning('"lsof" returncode = 1, likely no active TCP sessions.') return remotes
from __future__ import absolute_import import inspect import logging import time from functools import wraps from collections import defaultdict
import salt.utils import salt.utils.args from salt.exceptions import CommandNotFoundError, CommandExecutionError from salt.version import SaltStackVersion, __saltstack_version__ from salt.log import LOG_LEVELS
import salt.ext.six as six
dependency_dict = defaultdict(lambda: defaultdict(set))
if frame: try: func_name = frame.f_globals['__func_alias__'][func.__name__] except (AttributeError, KeyError): func_name = func.__name__
if mod_key not in functions: continue
log.trace('{0} already removed, skipping'.format(mod_key)) continue
from __future__ import absolute_import import logging
import salt.minion import salt.utils.verify import salt.utils.jid from salt.utils.event import tagify
if not opts['job_cache'] or opts.get('ext_job_cache'): return
import logging from sys import stdout from os import makedirs from os.path import dirname, isdir from errno import EEXIST
import salt.utils
log = logging.getLogger(__name__)
HAS_SWIFT = False try: from swiftclient import client
from __future__ import absolute_import import pyrax
from __future__ import absolute_import
try: import pyrax from salt.utils.openstack.pyrax.authenticate import Authenticate from salt.utils.openstack.pyrax.queues import RackspaceQueues
import logging log = logging.getLogger(__name__)
import pyrax import pyrax.exceptions
from salt.utils.openstack.pyrax import authenticate
if self.conn.queue_exists(qname): return True return False
if not self.conn.queue_exists(qname): return {} for queue in self.conn.list(): if queue.name == qname: return queue
from __future__ import absolute_import, with_statement from distutils.version import LooseVersion import time import inspect import logging
import salt.utils from salt.exceptions import SaltCloudSystemExit
log = logging.getLogger(__name__)
NOVACLIENT_MINVER = '2.6.1'
class KwargsStruct(object): def __init__(self, **entries): self.__dict__.update(entries)
self.kwargs['os_auth_url'] = auth_url
self.kwargs['version'] = str(kwargs.get('version', 2))
from __future__ import absolute_import, with_statement import logging
import salt.ext.six as six HAS_NEUTRON = False try: from neutronclient.v2_0 import client from neutronclient.shell import NeutronShell
from salt import exceptions
log = logging.getLogger(__name__)
import os import sys import time import errno import select import logging import tempfile import subprocess
self.max_size_in_mem = kwargs.pop('max_size_in_mem', 512000)
#)
try: import exceptions except ImportError: pass
import salt.exceptions import salt.utils.event
from __future__ import absolute_import import logging from copy import copy
from salt.utils.odict import OrderedDict
import salt.ext.six as six
response = copy(obj_a)
response = copy(obj_b)
import os import time import fnmatch import hashlib import logging import datetime from collections import MutableMapping from multiprocessing.util import Finalize
import salt.ext.six as six import tornado.ioloop import tornado.iostream
SUB_EVENT = set([ 'state.highstate', 'state.sls', ])
TAGS = {
self.connect_pub()
cls.cache_regex = salt.utils.cache.CacheRegex(prepend='^')
self.cpub = True
self.cpush = True
wait = None
run_once = True
if not self.cpub and not self.connect_pub(timeout=wait): break
dump_data = self.serial.dumps(data, use_bin_type=True)
self.subscriber.read_async(event_handler)
try: self.destroy()
try: os.makedirs(minion_sock_dir, 0o755) except OSError as exc: log.error('Could not create SOCK_DIR: {0}'.format(exc)) if minion_sock_dir == default_minion_sock_dir: raise
raise
except Exception: log.critical('Unexpected error while polling minion events', exc_info=True) return None
def __setstate__(self, state): self._is_child = True self.__init__(state['opts'], log_queue=state['log_queue'])
Finalize(self, self.close, exitpriority=15)
except Exception: log.critical('Unexpected error while polling master events', exc_info=True) return None
def __setstate__(self, state): self._is_child = True self.__init__(state['opts'], log_queue=state['log_queue'])
if self.event_queue: self.flush_events() self.stop = True super(EventReturn, self)._handle_signals(signum, sigframe)
if log.level <= logging.DEBUG: log.debug('Event data that caused an exception: {0}'.format( self.event_queue))
from __future__ import absolute_import import time
import logging import os import shutil
import salt.fileclient import salt.utils.url
from salt.ext import six
while True: emptydirs = _list_emptydirs(mod_dir) if not emptydirs: break for emptydir in emptydirs: touched = True shutil.rmtree(emptydir, ignore_errors=True)
from __future__ import absolute_import, with_statement import os import time import signal import datetime import itertools import threading import logging import errno import random
import yaml import salt.ext.six as six
try: import dateutil.parser as dateutil_parser _WHEN_SUPPORTED = True _RANGE_SUPPORTED = True except ImportError: _WHEN_SUPPORTED = False _RANGE_SUPPORTED = False
self.loop_interval = six.MAXSIZE clean_proc_dir(opts)
if name in self.opts['schedule']: del self.opts['schedule'][name] schedule = self.opts['schedule']
if name in self.intervals: del self.intervals[name]
for job in data.keys(): if 'enabled' not in data[job]: data[job]['enabled'] = True
proc.start()
self.intervals = {}
evt = salt.utils.event.get_event('minion', opts=self.opts, listen=False) evt.fire_event({'complete': True}, tag='/salt/minion/minion_schedule_saved')
self.functions = salt.loader.minion_mods(self.opts) self.returners = salt.loader.returners(self.opts, self.functions)
log_setup.setup_multiprocessing_logging()
with salt.utils.fopen(proc_fn, 'w+b') as fp_: fp_.write(salt.payload.Serial(self.opts).dumps(ret))
if 'retcode' in self.functions.pack['__context__']: ret['retcode'] = self.functions.pack['__context__']['retcode']
pass
raise
exit(salt.defaults.exitcodes.EX_GENERIC)
schedule_keys = set(data.keys())
when = _when[0]
if '_when' in data and data['_when'] != when: data['_when_run'] = True data['_when'] = when seconds = when - now
if seconds < 0: continue
if '_when' not in data: data['_when'] = when
if when > data['_when']: data['_when'] = when data['_when_run'] = True
if seconds < 0: continue
if '_when' not in data: data['_when'] = when
if when > data['_when']: data['_when'] = when data['_when_run'] = True
if 'run_on_start' in data: if data['run_on_start']: run = True else: self.intervals[job] = int(time.time()) else: run = True
functions = self.functions self.functions = {} returners = self.returners self.returners = {}
proc.start()
self.functions = functions self.returners = returners
if salt.utils.is_windows(): fp_.close() try: os.unlink(fn_) continue except OSError: continue
if salt.utils.is_windows(): fp_.close() try: os.unlink(fn_) except OSError: pass
from __future__ import absolute_import
from salt.exceptions import SaltSystemExit
try: import zmq HAS_ZMQ = True except ImportError: HAS_ZMQ = False
import os import re import sys import stat import errno import socket import logging
if sys.platform.startswith('win'): import win32file else: import resource
from salt.log import is_console_configured from salt.exceptions import SaltClientError, SaltSystemExit, \ CommandExecutionError import salt.defaults.exitcodes import salt.utils
return True
match = re.match(r'^(\d+)\.(\d+)(?:\.(\d+))?', ver)
if point and point.isdigit(): point = int(point)
if os.getuid() == 0: os.chown(dir_, uid, gid) os.umask(cumask)
zmq_version()
if 'HOME' in os.environ: os.environ['HOME'] = pwuser.pw_dir
out = [head] (head, tail) = os.path.split(head)
out.insert(0, head) (head, tail) = os.path.split(head)
if user != current_user: msg += ' Try running as user {0}.'.format(user) else: msg += ' Please give {0} read permissions.'.format(user)
if skip_perm_errors: return raise SaltClientError(msg)
mof_s = mof_h = win32file._getmaxstdio()
return
msg += 'salt-master will crash pretty soon! ' level = logging.CRITICAL
level = logging.CRITICAL
elif (accepted_count * 4) >= mof_s: level = logging.INFO
path = dirs[0] while os.path.basename(path) not in ['salt', 'salt-tests-tmpdir']: path, base = os.path.split(path)
if not os.path.isdir(path): os.makedirs(path)
zmq_version()
import os.path import shutil
import salt.syspaths as syspaths
return
salt.utils.compat.pack_dunder(__name__)
from __future__ import absolute_import import hashlib import logging import sys
try: import boto import boto3 import boto.exception import boto3.session
logging.getLogger('boto3').setLevel(logging.CRITICAL) HAS_BOTO = True
import sys import os
from __future__ import absolute_import import re import string import random
try:
import crypt HAS_CRYPT = True
from salt.exceptions import SaltInvocationError
import codecs import os import imp import logging import tempfile import traceback import sys
import jinja2 import jinja2.ext
if 'salt' in kws: kws['salt'] = AliasedLoader(kws['salt'])
kws.update(context) context = kws assert 'opts' in context assert 'saltenv' in context
return dict(result=True, data=tmplsrc)
exc_info_on_loglevel=logging.DEBUG
output = os.linesep.join(output.splitlines())
return dict(result=True, data=outf.name)
tmplstr = tmplstr.decode(SLS_ENCODING)
loader = jinja2.FileSystemLoader( context, os.path.dirname(tmplpath))
raise SaltRenderError( 'Jinja variable {0}{1}'.format( exc, out), buf=tmplstr)
if newline: output += '\n'
from mako.lookup import TemplateLookup lookup = TemplateLookup(directories=[os.path.dirname(tmplpath)])
import gzip
from __future__ import absolute_import import collections
import copy import logging import salt.ext.six as six from salt.serializers.yamlex import merge_recursive as _yamlex_merge_recursive
for k in upd: dest[k] = upd[k]
from __future__ import absolute_import, print_function import logging
import salt.utils.http
from __future__ import absolute_import import sys
from salt.utils.winservice import Service, instart import salt import salt.defaults.exitcodes
import win32serviceutil import win32service import winerror
import copy import threading import collections from contextlib import contextmanager
if hasattr(func, 'im_func'): func = func.__func__
func_globals = func.__globals__ injected_func_globals = [] overridden_func_globals = {} for override in overrides: if override in func_globals: overridden_func_globals[override] = func_globals[override] else: injected_func_globals.append(override)
func_globals.update(overrides)
yield
func_globals.update(overridden_func_globals)
for injected in injected_func_globals: del func_globals[injected]
self._state = threading.local() self._state.data = None self.global_data = {}
for k, v in six.iteritems(self.parent.global_data): if k not in self._data: self._data[k] = copy.deepcopy(v)
from __future__ import absolute_import import json import salt.utils.http import logging
import salt.ext.six as six
from __future__ import absolute_import import logging import time
step = min(step or 1, timeout) * BLUR_FACTOR
step = min(step, max_time - time.time()) * BLUR_FACTOR
from __future__ import absolute_import import logging import os import subprocess
from salt.exceptions import SaltInvocationError import salt.utils
from __future__ import absolute_import import json import logging import os.path import pprint import socket import urllib import yaml
import tornado.httputil import tornado.simple_httpclient from tornado.httpclient import HTTPClient
url_full = tornado.httputil.url_concat(url, params)
log_url = sanitize_url(url_full, hide_fields)
sess_cookies = None
req_kwargs['prefetch'] = False
header_callback('HTTP/1.0 {0} MESSAGE'.format(result.status_code)) streaming_callback(result.content) return { 'handle': result, }
req_kwargs = {}
'/etc/ssl/certs/ca-certificates.crt', '/etc/pki/tls/certs/ca-bundle.crt', '/etc/pki/tls/certs/ca-bundle.trust.crt', '/etc/ssl/certs/ca-bundle.crt', '/var/lib/ca-certificates/ca-bundle.pem', '/etc/ssl/cert.pem',
cookies.append(cookie)
if 'expires' in cookie: cookie['expires'] = salt.ext.six.moves.http_cookiejar.http2time(cookie['expires'])
from __future__ import absolute_import from __future__ import print_function
from __future__ import absolute_import, with_statement import copy import os import sys import time import errno import types import signal import logging import threading import contextlib import subprocess import multiprocessing import multiprocessing.util
import salt.defaults.exitcodes import salt.utils import salt.log.setup import salt.defaults.exitcodes from salt.log.mixins import NewStyleClassMixIn
import salt.ext.six as six
HAS_PSUTIL = False try: import psutil HAS_PSUTIL = True except ImportError: pass
pass
return
if num_threads is None: num_threads = multiprocessing.cpu_count() self.num_threads = num_threads
self._job_queue = queue.Queue(queue_size)
for _ in range(num_threads): thread = threading.Thread(target=self._thread_target) thread.daemon = True thread.start() self._workers.append(thread)
try: try: func, args, kwargs = self._job_queue.get(timeout=1)
continue
self._process_map = {}
self._pid = os.getpid() self._sigterm_handler = signal.getsignal(signal.SIGTERM) self._restart_processes = True
if type(MultiprocessingProcess) is type(tgt) and ( issubclass(tgt, MultiprocessingProcess)): need_log_queue = True else: need_log_queue = False
self._process_map[pid]['Process'].join(1)
raise
self.check_children()
time.sleep(10)
if exc.errno != errno.EINTR: raise break
try: del self._process_map[pid] except KeyError: pass
instance._original_run = instance.run instance.run = instance._run return instance
salt.log.setup.set_multiprocessing_logging_queue(self.log_queue)
super(MultiprocessingProcess, self).__init__(*args, **kwargs)
def __setstate__(self, state): self._is_child = True args = state['args'] kwargs = state['kwargs'] self.__init__(*args, **kwargs)
del self._args_for_getstate del self._kwargs_for_getstate return {'args': args, 'kwargs': kwargs}
raise
raise
self.__setup_signals()
yield
for signum in old_signals: signal.signal(signum, old_signals[signum])
import logging import subprocess import os
import salt.utils import salt.utils.timed_subprocess import salt.grains.extra from salt.exceptions import CommandExecutionError, SaltInvocationError,\ TimedProcTimeoutError
log = logging.getLogger(__name__)
from __future__ import absolute_import import glob import sys import os
import salt.utils
from ctypes import cdll, c_char_p, c_int, c_void_p, pointer, create_string_buffer from ctypes.util import find_library
lib = glob.glob(os.path.join( '/opt/local/lib', 'libcrypto.so*')) lib = lib[0] if len(lib) > 0 else None
RSA_X931_PADDING = 5
from __future__ import absolute_import from uuid import uuid4 as _uuid
from salt.utils.odict import OrderedDict from salt.utils import warn_until from salt.state import HighState
import salt.ext.six as six
try: return self.get_all_decls()[id] except KeyError: self.get_all_decls()[id] = s = StateDeclaration(id) self.decls.append(s) return s
raise PyDslError( 'An error occurred while running highstate: {0}'.format( '; '.join(result) ) )
self.require_index = None
etcd.host: 127.0.0.1 etcd.port: 4001
my_etcd_config: etcd.host: 127.0.0.1 etcd.port: 4001
import logging
from salt.exceptions import CommandExecutionError
try: import etcd from urllib3.exceptions import ReadTimeoutError, MaxRetryError HAS_LIBS = True except ImportError: HAS_LIBS = False
log = logging.getLogger(__name__)
log.error("etcd: failed to perform 'watch' operation on key {0} due to connection error".format(key)) return {}
log.error("etcd: Could not connect") raise etcd.EtcdConnectionFailed("Could not connect to etcd server")
log.error("etcd: {0}".format(err)) raise
log.error("etcd: error. python-etcd does not fully support python 2.6, no error information available") raise
future = async.async_method()
with current_ioloop(self.io_loop): ret = attr(*args, **kwargs) if isinstance(ret, tornado.concurrent.Future): ret = self._block_future(ret) return ret
self.async.close()
del self.async
import os import time import logging
import salt.utils
from __future__ import absolute_import import re import os import logging
import salt.utils from salt.exceptions import CommandExecutionError
import salt.ext.six as six
return re.split(r'\s+', out['stdout'])[1][:-1] == 'inode/blockdevice'
import logging import pythoncom import threading
import logging import re
HAS_LIBS = False try: import vboxapi
reload(vboxapi) _virtualboxManager = vboxapi.VirtualBoxManager(None, None)
start_time = time.time() timeout_in_seconds = timeout / 1000 max_time = start_time + timeout_in_seconds
args = (machine, session) progress = wait_for(_start_machine, timeout=timeout_in_seconds, func_args=args) if not progress: progress = machine.launchVMProcess(session, "", "")
time_left = max_time - time.time() progress.waitForCompletion(time_left * 1000)
time_left = max_time - time.time() vb_wait_for_session_state(session, timeout=time_left) log.info("Started machine %s", name)
if "memorySize" in machine: del machine["memorySize"] return machine
from __future__ import absolute_import import os import logging import time from collections import MutableMapping
import salt.ext.six as six
continue
continue
return None
sls[ps_opts['d-i']['languagechooser']['language-name-fb']['argument']] = { 'locale': ['system'] }
sls[ps_opts['d-i']['kbd-chooser']['method']['argument']] = { 'keyboard': ['system'] }
import os import sys import time import signal import tempfile import traceback import inspect
import salt.utils
enable_sig_handler('SIGINFO', _handle_sigusr1)
import psutil
def boot_time(): return psutil.BOOT_TIME
if psutil.version_info < (1, 0, 1): net_io_counters = psutil.network_io_counters()
def cpu_affinity(self, *args, **kwargs): if args or kwargs: return self.set_cpu_affinity(*args, **kwargs) else: return self.get_cpu_affinity()
if os.name == 'nt': socket.inet_pton = inet_pton socket.inet_ntop = inet_ntop
parts = dn.split(r'.') leftmost = parts[0] remainder = parts[1:]
raise CertificateError( "too many wildcards in certificate DNS name: " + repr(dn))
if not wildcards: return dn.lower() == hostname.lower()
for frag in remainder: pats.append(re.escape(frag))
PY2 = sys.version_info[0] == 2 PY3 = sys.version_info[0] == 3
MAXSIZE = int((1 << 31) - 1)
delattr(obj.__class__, self.name)
_moved_attributes = []
return sys.modules[fullname]
int2byte = operator.methodcaller("to_bytes", 1, "big")
from itertools import imap as map range = xrange
def long_range(start, end): while start < end: yield start start += 1
bytes = bytearray
_builtin_isinstance = isinstance
if hasattr(int, 'bit_length'): _int_bit_length = lambda i: i.bit_length() else: _int_bit_length = lambda i: len(bin(abs(i))) - 2
return bits
ips = sorted(set(ips)) nets = sorted(set(nets))
try: ip_int = self._ip_int_from_string(ip_str) except AddressValueError: self._report_invalid_netmask(ip_str)
try: return self._prefix_from_ip_int(ip_int) except ValueError: pass
ip_int ^= self._ALL_ONES try: return self._prefix_from_ip_int(ip_int) except ValueError: self._report_invalid_netmask(ip_str)
def __add__(self, other): if not isinstance(other, int): return NotImplemented return self.__class__(int(self) + other)
msg = '%200s has no associated address class' % (type(self),) raise NotImplementedError(msg)
other = other.__class__('%s/%s' % (other.network_address, other.prefixlen))
raise AssertionError('Error performing exclusion: ' 's1: %s s2: %s other: %s' % (s1, s2, other))
raise AssertionError('Error performing exclusion: ' 's1: %s s2: %s other: %s' % (s1, s2, other))
_ALL_ONES = (2**IPV4LENGTH) - 1 _DECIMAL_DIGITS = frozenset('0123456789')
_valid_mask_octets = frozenset((255, 254, 252, 248, 240, 224, 192, 128, 0))
return False
if isinstance(address, int): self._check_int_address(address) self._ip = address return
if isinstance(address, bytes): self._check_packed_address(address, 4) self._ip = _int_from_bytes(address, 'big') return
addr_str = str(address) self._ip = self._ip_int_from_string(addr_str)
return False
return False
_address_class = IPv4Address
if isinstance(address, bytes): self.network_address = IPv4Address(address) self._prefixlen = self._max_prefixlen self.netmask = IPv4Address(self._ALL_ONES) #fixme: address/network test here return
addr = _split_optional_netmask(address) self.network_address = IPv4Address(self._ip_int_from_string(addr[0]))
self._prefixlen = self._prefix_from_prefix_string(addr[1])
self._prefixlen = self._prefix_from_ip_string(addr[1])
_min_parts = 3 if len(parts) < _min_parts: msg = "At least %d parts expected in %r" % (_min_parts, ip_str) raise AddressValueError(msg)
_max_parts = self._HEXTET_COUNT + 1 if len(parts) > _max_parts: msg = "At most %d colons permitted in %r" % (_max_parts-1, ip_str) raise AddressValueError(msg)
doublecolon_start = index
best_doublecolon_len = doublecolon_len best_doublecolon_start = doublecolon_start
if best_doublecolon_end == len(hextets): hextets += [''] hextets[best_doublecolon_start:best_doublecolon_end] = [''] if best_doublecolon_start == 0: hextets = [''] + hextets
if isinstance(address, int): self._check_int_address(address) self._ip = address return
if isinstance(address, bytes): self._check_packed_address(address, 16) self._ip = _int_from_bytes(address, 'big') return
addr_str = str(address) self._ip = self._ip_int_from_string(addr_str)
return False
return False
_address_class = IPv6Address
if isinstance(address, int): self.network_address = IPv6Address(address) self._prefixlen = self._max_prefixlen self.netmask = IPv6Address(self._ALL_ONES) return
if isinstance(address, bytes): self.network_address = IPv6Address(address) self._prefixlen = self._max_prefixlen self.netmask = IPv6Address(self._ALL_ONES) return
addr = _split_optional_netmask(address)
self._prefixlen = self._prefix_from_prefix_string(addr[1])
from __future__ import absolute_import, print_function, with_statement import os import re import sys import copy import time import types import signal import fnmatch import logging import threading import traceback import contextlib import multiprocessing from random import randint, shuffle from stat import S_IMODE
import salt.ext.six as six if six.PY3: import ipaddress else: import salt.ext.ipaddress as ipaddress from salt.ext.six.moves import range
ret['master'] = ip_port[0]
ret['master'] = ip_port[0] ret['master_port'] = ip_port[1]
os.makedirs(fn_, **mode)
uid = kwargs.pop('uid', -1) gid = kwargs.pop('gid', -1)
_args.append(arg)
_kwargs.update(string_kwarg)
for key, val in six.iteritems(string_kwarg): invalid_kwargs.append('{0}={1}'.format(key, val))
for key, val in six.iteritems(data): _kwargs['__pub_{0}'.format(key)] = val
if minion.schedule.loop_interval < loop_interval: loop_interval = minion.schedule.loop_interval log.debug( 'Overriding loop_interval because of scheduled jobs.' )
if isinstance(opts['master'], list): conn = False local_masters = copy.copy(opts['master']) last_exc = None
if 'master_list' not in opts: opts['master_list'] = local_masters
self.connected = False msg = ('No master could be reached or all masters ' 'denied the minions connection attempt.') log.error(msg)
opts['grains'] = salt.loader.grains(opts) super(SMinion, self).__init__(opts)
MINION_CONNECT_TIMEOUT = 5
self.minions = self._spawn_minions()
self.io_loop.start()
super(Minion, self).__init__(opts) self.timeout = timeout self.safe = safe
if HAS_ZMQ: try: zmq_version_info = zmq.zmq_version_info() except AttributeError: zmq_version_info = tuple(
if not salt.utils.is_proxy(): self.opts['grains'] = salt.loader.grains(opts)
if not salt.utils.is_proxy(): self.io_loop.spawn_callback(salt.engines.start_engines, self.opts, self.process_manager)
if signal.getsignal(signal.SIGINT) is signal.SIG_DFL: signal.signal(signal.SIGINT, self._handle_signals)
signal.signal(signal.SIGINT, self._handle_signals)
self.process_manager.stop_restarting() self.process_manager.send_signal_to_processes(signum) self.process_manager.kill_children() exit(0)
if hasattr(self, 'proxy'): proxy = self.proxy else: proxy = None
if modules_max_memory is True: resource.setrlimit(resource.RLIMIT_AS, old_mem_limit)
process.start()
salt.log.setup.shutdown_multiprocessing_logging()
salt.log.setup.setup_multiprocessing_logging()
pass
salt.utils.minion.cache_jobs(self.opts, load['jid'], ret)
log.error('Pillar data could not be refreshed. ' 'One or more masters may be down!')
try: master, self.pub_channel = yield self.eval_master( opts=self.opts, failed=True) except SaltClientError: pass
multiprocessing.active_children()
self.event_publisher = salt.utils.event.AsyncEventPublisher( self.opts, self.handle_event, io_loop=self.io_loop, )
enable_sigusr1_handler()
salt.utils.enable_ctrl_logoff_handler()
for periodic_cb in six.itervalues(self.periodic_callbacks): periodic_cb.start()
if 'tgt' not in load or 'jid' not in load or 'fun' not in load \ or 'arg' not in load: return False
if 'tgt_type' not in data: data['tgt_type'] = 'glob' kwargs = {}
self.pub_channel.on_recv(self._process_cmd_socket)
self._reset_event_aggregation() self.local.event.set_event_handler(self._process_event)
self.forward_events = tornado.ioloop.PeriodicCallback(self._forward_events, self.opts['syndic_event_forward_timeout'] * 1000, io_loop=self.io_loop) self.forward_events.start()
self._fire_master_syndic_start()
enable_sigusr1_handler()
self.local = salt.client.get_local_client( self.opts['_minion_conf_file'], io_loop=self.io_loop)
self.pub_channel.on_recv(self._process_cmd_socket)
return
jdict['__master_id__'] = event['data']['master_id']
if 'retcode' not in event['data']: self.raw_events.append(event)
master, self.pub_channel = yield self.eval_master(opts=self.opts)
super(Syndic, self).destroy() if hasattr(self, 'local'): del self.local
SYNDIC_CONNECT_TIMEOUT = 5 SYNDIC_EVENT_TIMEOUT = 5
self.syndic_mode = self.opts.get('syndic_mode', 'sync') self.syndic_failover = self.opts.get('syndic_failover', 'random')
syndic.tune_in_no_block() log.info('Syndic successfully connected to {0}'.format(opts['master'])) break
if self._syndics[master].done():
return False
continue
return False
self.local = salt.client.get_local_client( self.opts['_minion_conf_file'], io_loop=self.io_loop) self.local.event.subscribe('')
self._reset_event_aggregation() self.local.event.set_event_handler(self._process_event)
self.forward_events = tornado.ioloop.PeriodicCallback(self._forward_events, self.opts['syndic_event_forward_timeout'] * 1000, io_loop=self.io_loop) self.forward_events.start()
enable_sigusr1_handler()
return
jdict['__master_id__'] = master
return False
for member in val: if fnmatch.fnmatch(str(member).lower(), comps[1].lower()): return True return False
tgt = ipaddress.ip_address(tgt)
tgt = ipaddress.ip_network(tgt)
log.error('Detected nodegroup expansion failure of "{0}"'.format(word)) return False
log.error('Unrecognized target engine "{0}" for' ' target expression "{1}"'.format( target_info['engine'], word, ) ) return False
results.append(str(self.glob_match(word)))
self.functions, self.returners, self.function_errors, self.executors = self._load_modules()
self.functions['saltutil.sync_all'](saltenv='base')
self.proxy = salt.loader.proxy(self.opts)
if self.opts['add_proxymodule_to_opts']: self.opts['proxymodule'] = self.proxy
self.io_loop.spawn_callback(salt.engines.start_engines, self.opts, self.process_manager, proxy=self.proxy)
self.functions['saltutil.sync_grains'](saltenv='base') self.grains_cache = self.opts['grains']
from __future__ import absolute_import, print_function import logging
import salt.exceptions import salt.loader import salt.minion import salt.utils.args import salt.utils.event from salt.client import mixins from salt.output import display_output from salt.utils.lazy import verify_fun
async_pub = self._gen_async_pub() ret = self._proc_function(self.opts['fun'], low, user, async_pub['tag'], async_pub['jid'],
from __future__ import absolute_import import os import struct
import salt.utils
if not isinstance(config, dict): return False, ('Configuration for btmp beacon must ' 'be a list of dictionaries.') return True, 'Valid beacon configuration'
from __future__ import absolute_import import logging
from __future__ import absolute_import import time
import salt.utils import salt.utils.vt
if not isinstance(config, dict): return False, ('Configuration for sh beacon must be a dictionary.') return True, 'Valid beacon configuration'
from __future__ import absolute_import import logging
import salt.utils
try: from pyroute2.ipdb import IPDB HAS_PYROUTE2 = True except ImportError: HAS_PYROUTE2 = False
from __future__ import absolute_import import logging import copy import re
import salt.loader import salt.utils import salt.utils.minion from salt.ext.six.moves import map
if isinstance(config[mod], dict): del config[mod]['enabled'] else: self._remove_list_item(config[mod], 'enabled')
self.opts['beacons'][name]['enabled'] = enabled_value
from __future__ import absolute_import import logging
from __future__ import absolute_import
if not isinstance(config, dict): return False, ('Configuration for service beacon must be a dictionary.') return True, 'Valid beacon configuration'
if config[service] is None: defaults = { 'oncleanshutdown': False, 'emitatstartup': True, 'onchangeonly': False } config[service] = defaults
from __future__ import absolute_import
from __future__ import absolute_import import collections import fnmatch import os
import salt.ext.six
if notifier.check_events(1): notifier.read_events() notifier.process_events() queue = __context__['inotify.queue'] while queue: event = queue.popleft()
path = event.path while path != '/': if path in config: break path = os.path.dirname(path)
current = set() for wd in wm.watches: current.add(wm.watches[wd].path)
return ret
from __future__ import absolute_import
import salt.utils import salt.utils.locales import salt.utils.cloud import salt.ext.six
try: import systemd.journal HAS_SYSTEMD = True except ImportError: HAS_SYSTEMD = False
__context__['systemd.journald'].seek_tail() __context__['systemd.journald'].get_previous() return __context__['systemd.journald']
sub = salt.utils.cloud.simple_types_filter(cur) sub.update({'tag': name}) ret.append(sub)
from __future__ import absolute_import import logging import re
import salt.utils
try: import psutil HAS_PSUTIL = True except ImportError: HAS_PSUTIL = False
if not isinstance(config, dict): return False, ('Configuration for memusage ' 'beacon must be a dictionary.') return True, 'Valid beacon configuration'
from __future__ import absolute_import import os import struct
import salt.utils
if not isinstance(config, dict): return False, ('Configuration for wtmp beacon must be a dictionary.') return True, 'Valid beacon configuration'
from __future__ import absolute_import import logging
import salt.utils
from __future__ import absolute_import import logging
import salt.utils.http
__proxyenabled__ = ['*']
from __future__ import absolute_import import logging
try: import salt.utils.psutil_compat as psutil HAS_PSUTIL = True except ImportError: HAS_PSUTIL = False
if not isinstance(config, dict): return False, ('Configuration for ps beacon must be a dictionary.') return True, 'Valid beacon configuration'
from __future__ import absolute_import import logging
try: import salt.utils.psutil_compat as psutil HAS_PSUTIL = True except ImportError: HAS_PSUTIL = False
from __future__ import absolute_import import logging import re
try: import psutil HAS_PSUTIL = True except ImportError: HAS_PSUTIL = False
if not isinstance(config, dict): return False, ('Configuration for diskusage beacon ' 'must be a dictionary.') return True, 'Valid beacon configuration'
log.error('{0} is not a valid mount point, skipping.'.format(mount)) continue
from __future__ import absolute_import import logging
try: from twilio.rest import TwilioRestClient HAS_TWILIO = True except ImportError: HAS_TWILIO = False
if not isinstance(config, dict): return False, ('Configuration for twilio_txt_msg beacon ' 'must be a dictionary.') return True, 'Valid beacon configuration'
from __future__ import absolute_import import logging import os
import salt.utils
from salt.ext.six.moves import zip
if 'emitatstartup' not in config: config['emitatstartup'] = True if 'onchangeonly' not in config: config['onchangeonly'] = False
from __future__ import absolute_import, print_function, with_statement import signal import logging import weakref import traceback import collections
import tornado.stack_context
kwargs_keys = list(kwargs)
for kwargs_key in kwargs_keys: if kwargs_key.startswith('__pub_'): pub_data[kwargs_key] = kwargs.pop(kwargs_key)
if kwarg: kwarg['__kwarg__'] = True arglist.append(kwarg)
completed_funcs = []
if kwargs: salt.utils.warn_until( 'Carbon', 'kwargs must be passed inside the low under "kwargs"' )
with tornado.stack_context.StackContext(self.functions.context_dict.clone): data['return'] = self.functions[fun](*args, **kwargs) data['success'] = True
log.info('Runner completed: {0}'.format(data['jid'])) del event del namespaced_event return data['return']
salt.log.setup.shutdown_multiprocessing_logging()
salt.log.setup.setup_multiprocessing_logging()
low['__jid__'] = jid low['__user__'] = user low['__tag__'] = tag
proc.start()
if self.opts.get('quiet', False): return
if suffix in ('new',): return
import salt.ext.six as six try: import zmq HAS_ZMQ = True except ImportError: HAS_ZMQ = False
HAS_RANGE = False try: import seco.range HAS_RANGE = True except ImportError: pass
import salt.config opts = salt.config.client_config(c_path)
key_user = key_user.replace('\\', '_')
salt.utils.verify.check_path_traversal(self.opts['cachedir'], key_user, self.skip_perm_errors)
return ''
return self.opts['timeout']
raise SaltClientError( 'The salt master could not be contacted. Is master running?' )
raise SaltClientError(general_exception)
minions = set(minions)
event_iter = self.get_event_iter_returns(jid, minions, timeout=timeout)
ret = self.get_cache_returns(jid) if ret != {}: found.update(set(ret)) yield ret
if len(found.intersection(minions)) >= len(minions): raise StopIteration()
minion_timeouts = {}
if id_ not in minion_timeouts: minion_timeouts[id_] = time.time() + timeout
for raw in jinfo_iter: if raw is None: break
open_jids.add(jinfo['jid'])
if raw['data']['return'] == {}: continue
if block: time.sleep(0.01) else: yield
if open_jids: for jid in open_jids: self.event.unsubscribe(jid)
event_iter = self.get_event_iter_returns(jid, minions, timeout=timeout)
if len(set(ret).intersection(minions)) >= len(minions): return ret
if len(set(ret).intersection(minions)) >= len(minions): return ret
return ret
connected_minions = None return_count = 0
if expr_form == 'range' and HAS_RANGE: tgt = self._convert_range_to_list(tgt) expr_form = 'list'
if kwargs: payload_kwargs['kwargs'] = kwargs
if self.opts['order_masters']: payload_kwargs['to'] = timeout
if not self.event.connect_pub(timeout=timeout): raise SaltReqTimeoutError() payload = channel.send(payload_kwargs, timeout=timeout)
key = self.__read_master_key() if key == self.key: return payload self.key = key payload_kwargs['key'] = self.key payload = channel.send(payload_kwargs)
del channel
if listen and not self.event.connect_pub(timeout=timeout): raise SaltReqTimeoutError() payload = yield channel.send(payload_kwargs, timeout=timeout)
del channel
if hasattr(self, 'event'): del self.event
import os import time import logging
from raet import raeting, nacling from raet.lane.stacking import LaneStack from raet.lane.yarding import RemoteYard import salt.config import salt.client import salt.utils import salt.syspaths as syspaths from salt.utils import kinds
import os
import salt.config import salt.auth import salt.client import salt.runner import salt.wheel import salt.utils import salt.syspaths as syspaths from salt.utils.event import tagify from salt.exceptions import EauthAuthenticationError
funparts = cmd.get('fun', '').split('.')
from __future__ import absolute_import, print_function import base64 import copy import getpass import json import logging import multiprocessing import subprocess import hashlib import tarfile import os import re import sys import time import yaml import uuid import tempfile import binascii import sys
import salt.ext.six as six
DEFAULT_THIN_DIR = '/var/tmp/.%%USER%%_%%FQDNUUID%%_salt'
RSTR_RE = r'(?:^|\r?\n)' + RSTR + '(?:\r?\n|$)'
shim_file += "c"
stdout, stderr, retcode = single.shell.copy_id()
if len(running) >= self.opts.get('ssh_max_procs', 25) or len(self.targets) >= len(running): time.sleep(0.1)
argv = self.opts['argv']
self.returners['{0}.save_load'.format(self.opts['master_job_cache'])](jid, job_load)
argv = self.opts['argv']
final_exit = 1
opts_pkg['id'] = self.id
if isinstance(mine_args, dict): self.args = [] self.kwargs = mine_args elif isinstance(mine_args, list): self.args = mine_args self.kwargs = {}
with tempfile.NamedTemporaryFile(mode='w', prefix='shim_', delete=False) as shim_tmp_file: shim_tmp_file.write(cmd_str)
target_shim_file = '.{0}'.format(binascii.hexlify(os.urandom(6))) self.shell.send(shim_tmp_file.name, target_shim_file)
try: os.remove(shim_tmp_file.name) except IOError: pass
ret = self.shell.exec_cmd('/bin/sh \'$HOME/{0}\''.format(target_shim_file))
self.shell.exec_cmd('rm \'$HOME/{0}\''.format(target_shim_file))
return 'ERROR: Failure deploying thin, undefined state: {0}'.format(stdout), stderr, retcode
while re.search(RSTR_RE, stdout): stdout = re.split(RSTR_RE, stdout, 1)[1].strip()
while re.search(RSTR_RE, stderr): stderr = re.split(RSTR_RE, stderr, 1)[1].strip()
return None
return 'Undefined SHIM state'
return None
import collections
import salt.pillar import salt.utils from salt.defaults import DEFAULT_TARGET_DELIM
items = raw data = items
from __future__ import absolute_import import copy import logging
import salt.client.ssh import salt.runner
ssh = salt.client.ssh.SSH(opts)
rets = {} for ret in ssh.run_iter(): rets.update(ret)
runner = salt.runner.RunnerClient(__opts__['__master_opts__']) return runner.cmd(fun, arg)
from __future__ import absolute_import import collections import math
import salt.utils import salt.utils.dictupdate from salt.exceptions import SaltException
import salt.ext.six as six
__grains__ = {}
from __future__ import absolute_import import json import copy
import salt.loader import salt.utils import salt.client.ssh
import salt.ext.six as six
cmd = '{0}.{1}'.format(self.cmd_prefix, cmd)
raise KeyError('Cannot assign to module key {0} in the ' 'FunctionWrapper'.format(cmd))
cmd = '{0}.{1}'.format(self.cmd_prefix, cmd)
self.aliases[cmd] = value
from __future__ import absolute_import import re import os
import salt.utils import salt.syspaths as syspaths
import salt.ext.six as six
from __future__ import absolute_import import copy
import salt.client.ssh
ssh = salt.client.ssh.SSH(opts)
rets = {} for ret in ssh.run_iter(mine=True): rets.update(ret)
import os import copy import json import logging
try: os.remove(trans_tar) except (OSError, IOError): pass
return stdout
try: os.remove(trans_tar) except (OSError, IOError): pass
return stdout
try: os.remove(trans_tar) except (OSError, IOError): pass
return stdout
try: os.remove(trans_tar) except (OSError, IOError): pass
return stdout
try: os.remove(trans_tar) except (OSError, IOError): pass
return stdout
if errors: return errors return high_data
comps = fun.split('.') if len(comps) < 2: __context__['retcode'] = 1 return 'Invalid function passed'
kwargs.update({'state': comps[0], 'fun': comps[1], '__id__': name, 'name': name})
if salt.utils.test_mode(test=test, **kwargs): opts['test'] = True else: opts['test'] = __opts__.get('test', None)
__pillar__.update(kwargs.get('pillar', {}))
st_ = salt.client.ssh.state.SSHState(__opts__, __pillar__)
err = st_.verify_data(kwargs) if err: __context__['retcode'] = 1 return err
chunks = [kwargs]
trans_tar = salt.client.ssh.state.prep_trans_tar( __context__['fileclient'], chunks, file_refs, __pillar__, id_=st_kwargs['id_'])
trans_tar_sum = salt.utils.get_hash(trans_tar, __opts__['hash_type'])
cmd = 'state.pkg {0}/salt_state.tgz test={1} pkg_sum={2} hash_type={3}'.format( __opts__['thin_dir'], test, trans_tar_sum, __opts__['hash_type'])
single = salt.client.ssh.Single( __opts__, cmd, fsclient=__context__['fileclient'], minion_opts=__salt__.minion_opts, **st_kwargs)
single.shell.send( trans_tar, '{0}/salt_state.tgz'.format(__opts__['thin_dir']))
stdout, stderr, _ = single.cmd_block()
try: os.remove(trans_tar) except (OSError, IOError): pass
return stdout
import salt.client.ssh import logging import os from salt.exceptions import CommandExecutionError
if template not in salt.utils.templates.TEMPLATE_REGISTRY: raise CommandExecutionError( 'Attempted to render file paths with unavailable engine ' '{0}'.format(template) )
EX_THIN_DEPLOY = 11 EX_THIN_CHECKSUM = 12 EX_MOD_DEPLOY = 13 EX_SCP_NOT_FOUND = 14 EX_CANTCREAT = 73
#%%OPTS
sys.stdout.write("{0}\ndeploy\n".format(OPTIONS.delimiter)) sys.exit(EX_THIN_DEPLOY)
for chunk in iter(lambda: ifile.read(chunk_size), b''): hash_obj.update(chunk) return hash_obj.hexdigest()
if len(ARGS) == 1: argv_prepared = ARGS[0].split() else: argv_prepared = ARGS
from __future__ import absolute_import import os import copy import logging
import salt.config import salt.syspaths as syspaths
import os import tarfile import tempfile import json import shutil from contextlib import closing
import salt.client.ssh.shell import salt.client.ssh import salt.utils import salt.utils.thin import salt.utils.url import salt.roster import salt.state import salt.loader import salt.minion
cwd = os.getcwd()
import re import os import json import time import logging import subprocess
import salt.defaults.exitcodes import salt.utils import salt.utils.nb_popen import salt.utils.vt
RSTR = '_edbc7885e4f9aac9b83b35999b68d015148caf467b78fa39c05f669c0ff89878' RSTR_RE = re.compile(r'(?:^|\r?\n)' + RSTR + r'(?:\r?\n|$)')
send_password = False
return '', 'Password authentication failed', 254
from __future__ import absolute_import import signal import logging
import salt.loader import salt.utils.process
if signal.getsignal(signal.SIGINT) is signal.SIG_DFL: signal.signal(signal.SIGINT, self._handle_signals)
signal.signal(signal.SIGINT, self._handle_signals)
self.process_manager.stop_restarting() self.process_manager.send_signal_to_processes(signum) self.process_manager.kill_children()
from __future__ import absolute_import import os import imp import sys import salt import time import logging import inspect import tempfile import functools from collections import MutableMapping from zipimport import zipimporter
import salt.modules.cmdmod
import salt.ext.six as six try: import pkg_resources HAS_PKG_RESOURCES = True except ImportError: HAS_PKG_RESOURCES = False
pyximport = None
if name not in loader.file_mapping: return {}
try:
self.suffix_map = {}
for (suffix, mode, kind) in SUFFIXES: self.suffix_map[suffix] = (suffix, mode, kind) suffix_order.append(suffix)
self.suffix_map['.pyx'] = tuple()
self.file_mapping = salt.utils.odict.OrderedDict()
if ext not in self.suffix_map:
if ext == '': subfiles = os.listdir(fpath) for suffix in suffix_order: if '' == suffix:
self.file_mapping[f_noext] = (fpath, ext)
if hasattr(self, 'opts'): self.refresh_file_mapping() self.initial_load = False
if mod_name in self.file_mapping: yield mod_name
for k in self.file_mapping: if mod_name in k: yield k
for k in self.file_mapping: if mod_name not in k: yield k
for submodule in submodules: if submodule.__name__.startswith(mod.__name__ + '.'): reload(submodule) self._reload_submodules(submodule)
for p_name, p_value in six.iteritems(self.pack): setattr(mod, p_name, p_value)
if virtual_ret is not True: self.missing_modules[module_name] = virtual_err self.missing_modules[name] = virtual_err return False
if module_name in self.loaded_modules: mod_dict = self.loaded_modules[module_name] else: mod_dict = self.mod_dict_class()
continue
if full_funcname not in self._dict: self._dict[full_funcname] = func if funcname not in mod_dict: setattr(mod_dict, funcname, func) mod_dict[funcname] = func self._apply_outputter(func, mod)
if self._load_module(name) and key in self._dict: return True
if virtual is not True and module_name != virtual: log.trace('Loaded {0} as virtual {1}'.format( module_name, virtual ))
elif virtual is True and virtualname != module_name: if virtualname is not True: module_name = virtualname
log.debug( 'KeyError when loading {0}'.format(module_name), exc_info=True )
log.error( 'Failed to read the virtual function for ' '{0}: {1}'.format( self.tag, module_name ), exc_info=True ) return (False, module_name, error_reason)
from __future__ import absolute_import
import salt.utils
import salt.ext.six as six
import salt.output.highstate
from numbers import Number
import salt.output from salt.ext.six import string_types from salt.utils import get_colors import salt.utils.locales
import pprint
ret += '{0}\n'.format(pprint.pformat(data))
from __future__ import absolute_import
try: import progressbar HAS_PROGRESSBAR = True except ImportError: HAS_PROGRESSBAR = False
from __future__ import print_function from __future__ import absolute_import import os import sys import errno import logging import traceback from salt.ext.six import string_types
import salt.loader import salt.utils from salt.utils import print_cli import salt.ext.six as six
try: progress_outputter = salt.loader.outputters(opts)[out]
if exc.errno != errno.EPIPE: raise exc
out = opts['output']
if out != 'grains': log.error('Invalid outputter {0} specified, fall back to nested'.format(out)) return outputters['nested']
import yaml
from salt.utils.yamldumper import OrderedDumper
__virtualname__ = 'yaml'
params.update(default_flow_style=False)
params.update(default_flow_style=False, indent=__opts__['output_indent'])
__virtualname__ = 'quiet'
from __future__ import absolute_import
import salt.utils.locales
import salt.utils
import pprint
__virtualname__ = 'pprint'
from __future__ import absolute_import import pprint import textwrap
import salt.utils import salt.output from salt.utils.locales import sdecode
import salt.ext.six as six
nchanges = 1 hstrs.append((u'{0} {1}{2[ENDC]}' .format(hcolor, data, colors)))
if __opts__.get('state_output_diff', False) and \ ret['result'] and not schanged: continue
if not __opts__.get('state_verbose', False) and \ ret['result'] and not schanged: continue
msg = _format_terse(tcolor, comps, ret, colors, tabular) hstrs.append(msg) continue
if ret['result'] is not False: msg = _format_terse(tcolor, comps, ret, colors, tabular) hstrs.append(msg) continue
if ret['result'] and not schanged: msg = _format_terse(tcolor, comps, ret, colors, tabular) hstrs.append(msg) continue
comment = str(ret['comment']) comment = comment.strip().replace( u'\n', u'\n' + u' ' * 14)
'colors': colors
import json import logging
__virtualname__ = 'json'
return json.dumps({})
import salt.utils import salt.output from salt.utils.locales import sdecode
from __future__ import absolute_import
import salt.ext.six as six
from __future__ import absolute_import
import salt.ext.six as six
from __future__ import absolute_import import os import subprocess
import salt.utils.locales
import os.path import msgpack
import salt.loader import salt.utils import salt.utils.cloud import salt.utils.validate.net from salt import syspaths
from __future__ import absolute_import import os.path
import msgpack
import salt.loader import salt.utils import salt.utils.cloud import salt.utils.validate.net import salt.config from salt import syspaths from salt.ext.six import string_types
HAS_RANGE = False try: import seco.range HAS_RANGE = True except ImportError: log.error('Unable to load range library')
tgt_func = { 'range': target_range, 'glob': target_range, }
import salt.loader import salt.syspaths
from __future__ import absolute_import import socket
ports = list(map(int, str(ports).split(',')))
from __future__ import absolute_import import socket import logging
import salt.utils.network from salt._compat import ipaddress
ports = list(map(int, str(ports).split(',')))
import fnmatch import re
HAS_RANGE = False try: import seco.range HAS_RANGE = True except ImportError: pass
import salt.loader from salt.template import compile_template from salt.ext.six import string_types from salt.roster import get_roster_file
from __future__ import absolute_import import os import re import fnmatch import json import subprocess
import salt.utils from salt.roster import get_roster_file
import salt.ext.six as six
import logging import copy
try: import confidant.client import confidant.formatter HAS_LIBS = True except ImportError: HAS_LIBS = False
log = logging.getLogger(__name__)
import logging
import yaml
log = logging.getLogger(__name__)
import logging
try: import salt.utils.etcd_util HAS_LIBS = True except ImportError: HAS_LIBS = False
log = logging.getLogger(__name__)
path %= { 'minion_id': minion_id }
import logging
try: import consul HAS_CONSUL = True except ImportError: HAS_CONSUL = False
log = logging.getLogger(__name__)
path %= { 'minion_id': minion_id, 'role': role }
if value is None: return ret
pillar_value = yaml.load(value)
sorted_mappings = sorted(mappings, key=lambda m: (-len(m[0]), m[0]))
from __future__ import absolute_import import logging import os import time import pickle from copy import deepcopy
import salt.ext.six as six from salt.ext.six.moves import filter from salt.ext.six.moves.urllib.parse import quote as _quote
from salt.pillar import Pillar import salt.utils import salt.utils.s3 as s3
log = logging.getLogger(__name__)
opts['ext_pillar'] = [x for x in opts['ext_pillar'] if 's3' not in x]
if os.path.isfile(cache_file): cache_file_mtime = os.path.getmtime(cache_file) else: cache_file_mtime = 0
if not os.path.exists(os.path.dirname(file_path)): os.makedirs(os.path.dirname(file_path))
def __get_pillar_files_from_s3_meta(s3_meta): return [k for k in s3_meta if 'Key' in k]
log.debug('Single environment per bucket mode')
if s3_meta: bucket_files[bucket] = __get_pillar_files_from_s3_meta(s3_meta)
log.debug('Multiple environment per bucket mode') s3_meta = __get_s3_meta()
if s3_meta: files = __get_pillar_files_from_s3_meta(s3_meta) environments = __get_pillar_environments(files)
for saltenv in environments: env_files = [k for k in files if k['Key'].startswith(saltenv)]
if os.path.isfile(cache_file): os.remove(cache_file)
filePaths = [k['Key'] for k in data] ret[bucket] += [k for k in filePaths if not k.endswith('/')]
log.debug("Cached file: path={0}, md5={1}, etag={2}".format(cached_file_path, cached_md5, file_md5)) if cached_md5 == file_md5: return
s3.query( key=creds.key, keyid=creds.keyid, kms_keyid=creds.kms_keyid, bucket=bucket, service_url=creds.service_url, path=_quote(path), local_file=cached_file_path, verify_ssl=creds.verify_ssl, location=creds.location )
import logging import re
try: import pymongo HAS_PYMONGO = True except ImportError: HAS_PYMONGO = False
log = logging.getLogger(__name__)
if re_pattern: minion_id = re.sub(re_pattern, re_replace, minion_id)
result['_id'] = str(result['_id'])
log.debug( 'ext_pillar.mongo: no document found in collection {0}'.format( collection ) ) return {}
import logging
from salt.serializers.yamlex import deserialize
log = logging.getLogger(__name__)
stack_k = stack[k] stack[k] = _cleanup(v) v = stack_k
import logging
import salt.utils.virt
log = logging.getLogger(__name__)
import logging
from salt.utils.odict import OrderedDict from salt.ext.six.moves import range from salt.ext import six
log = logging.getLogger(__name__)
def __virtual__(): return False
qbuffer = []
qbuffer.extend([[None, s] for s in args])
klist = list(kwargs.keys()) klist.sort() qbuffer.extend([[k, kwargs[k]] for k in klist])
if root: self.result[root] = self.focus = {} else: self.focus = self.result
if self.depth == self.num_fields - 1:
from __future__ import absolute_import import copy import os import collections import logging import tornado.gen
import salt.ext.six as six
raise SaltClientError(msg)
self.opts = opts self.grains = grains self.minion_id = minion_id self.ext = ext self.functions = functions self.pillar = pillar self.pillarenv = pillarenv
self.cache = salt.utils.cache.CacheFactory.factory( self.opts['pillar_cache_backend'], self.opts['pillar_cache_ttl'], minion_cache_path=self._minion_cache_path(minion_id))
fresh_pillar = self.fetch_pillar() self.cache[self.minion_id] = {self.saltenv: fresh_pillar} log.debug('Pillar cache miss for minion {0}'.format(self.minion_id))
for saltenv, targets in six.iteritems(top): sorted_targets = sorted(targets, key=lambda target: orders[saltenv][target]) for target in sorted_targets: sorted_top[saltenv][target] = targets[target] return sorted_top
return None, mods, errors
mopts['file_roots'] = self.actual_file_roots mopts['saltversion'] = __version__ pillar['master'] = mopts
from __future__ import absolute_import import re import logging
try: import boto.ec2 import boto.utils import boto.exception HAS_BOTO = True except ImportError: HAS_BOTO = False
log = logging.getLogger(__name__)
(instance_id, region) = _get_instance_info()
from __future__ import absolute_import
from salt.exceptions import SaltInvocationError from salt.utils.reclass import ( prepend_reclass_source_path, filter_out_source_path_option, set_inventory_base_uri_default )
import salt.ext.six as six
__virtualname__ = 'reclass'
opts = next(six.itervalues(pillar)) prepend_reclass_source_path(opts) break
from reclass.adapters.salt import ext_pillar as reclass_ext_pillar from reclass.errors import ReclassException
filter_out_source_path_option(kwargs)
set_inventory_base_uri_default(__opts__, kwargs)
return reclass_ext_pillar(minion_id, pillar, **kwargs)
from __future__ import absolute_import import logging import salt.utils.vault
from contextlib import contextmanager import logging
from salt.pillar.sql_base import SqlBaseExtPillar
log = logging.getLogger(__name__)
try: import MySQLdb HAS_MYSQL = True except ImportError: HAS_MYSQL = False
import logging
log = logging.getLogger(__name__)
__virtualname__ = 'foreman'
if api != 2: log.error('Foreman API v2 is supported only, please specify' 'version 2 in your Salt master config') raise Exception
import logging
log = logging.getLogger(__name__)
import logging
import yaml
log = logging.getLogger(__name__)
import logging
log = logging.getLogger(__name__)
__virtualname__ = 'varstack'
import fnmatch import logging import os
import salt.utils import salt.utils.dictupdate import salt.utils.minions
log = logging.getLogger(__name__)
for dir_name in dir_names: pillar_node[dir_name] = {}
del pillar
return ngroup_pillar
import logging import json
log = logging.getLogger(__name__)
# Copyright (C) 2014 Floris Bruynooghe <flub@devork.be>
from __future__ import absolute_import import copy import hashlib import logging import os
import salt.pillar
try: import hglib except ImportError: hglib = None
__opts__ = {}
from contextlib import contextmanager import logging import sqlite3
import salt.utils from salt.pillar.sql_base import SqlBaseExtPillar
log = logging.getLogger(__name__)
_opts = __opts__.get('sqlite3', {})
from __future__ import print_function from __future__ import absolute_import import os import logging
from salt.exceptions import SaltInvocationError
import yaml from jinja2 import Environment, FileSystemLoader try:
log = logging.getLogger(__name__)
import json
return globals()[function](minion_id, pillar, **kwargs)
if not key_data: return {}
if isinstance(data, dict) and not pillar_key: return data elif not pillar_key: return {'redis_pillar': data} else: return {pillar_key: data}
from __future__ import absolute_import
from salt.minion import Matcher
import salt.ext.six as six
import os import subprocess
import salt.utils
gen_hyper_keys(minion_id)
from __future__ import absolute_import import logging
import salt.utils
import yaml import salt.ext.six as six
log = logging.getLogger(__name__)
import logging
log = logging.getLogger(__name__)
from __future__ import absolute_import import logging
try: import salt.utils.openstack.neutron as suoneu HAS_NEUTRON = True except NameError as exc: HAS_NEUTRON = False
log = logging.getLogger(__name__)
from copy import deepcopy import logging import os import hashlib
HAS_SVN = False try: import pysvn HAS_SVN = True CLIENT = pysvn.Client() except ImportError: pass
from salt.pillar import Pillar
log = logging.getLogger(__name__)
__virtualname__ = 'svn'
options = repo_string.strip().split() branch = options[0] repo_location = options[1] root = ''
branch = (branch == 'trunk' and 'base' or branch)
env: /path/to/virtualenv/
my_application.clients:
Client:
name: shortname
filter: {'kw': 'args'}
fields: - field_1 - field_2
sys.path.insert(0, os.path.join( virtualenv.path_locations(env)[1], 'site-packages'))
sys.path.append(project_path)
if name_field not in model: raise salt.exceptions.SaltException( "Name '{0}' not found in returned fields.".format( name_field))
import logging
log = logging.getLogger(__name__)
#pepa_delimiter: ..
#pepa_grains:
#pepa_pillars:
#log_level: debug
from __future__ import absolute_import, print_function
import logging import sys import glob import yaml import jinja2 import re from os.path import isfile, join
import salt.ext.six as six
import salt.utils
log = None if __name__ == '__main__':
__opts__ = { 'pepa_roots': { 'base': '/srv/salt' }, 'pepa_delimiter': '..', 'pepa_validate': False }
inp = {} inp['default'] = 'default' inp['hostname'] = minion_id
output = inp output['pepa_templates'] = [] immutable = {}
with salt.utils.fopen(args.config) as fh_: __opts__.update(yaml.load(fh_.read()))
__grains__ = {} if 'pepa_grains' in __opts__: __grains__ = __opts__['pepa_grains'] if args.grains: __grains__.update(yaml.load(args.grains))
__pillar__ = {} if 'pepa_pillar' in __opts__: __pillar__ = __opts__['pepa_pillar'] if args.pillar: __pillar__.update(yaml.load(args.pillar))
if args.validate: __opts__['pepa_validate'] = True
from contextlib import contextmanager import logging
from salt.pillar.sql_base import SqlBaseExtPillar
log = logging.getLogger(__name__)
try: from pysqlcipher import dbapi2 as sqlcipher HAS_SQLCIPHER = True except ImportError: HAS_SQLCIPHER = False
import copy import logging import hashlib import os
import salt.utils.gitfs import salt.utils.dictupdate from salt.exceptions import FileserverConfigError from salt.pillar import Pillar
import salt.ext.six as six try: import git HAS_GITPYTHON = True except ImportError: HAS_GITPYTHON = False
log = logging.getLogger(__name__)
__virtualname__ = 'git'
return False
try: salt.utils.gitfs.GitPillar(__opts__) return __virtualname__ except FileserverConfigError: pass
pillar.fetch_remotes()
pillar_roots = [pillar_dir] pillar_roots.extend([x for x in all_dirs if x != pillar_dir]) opts['pillar_roots'] = {env: pillar_roots}
self.working_dir = rp_
pass
options = repo_string.strip().split() branch_env = options[0] repo_location = options[1] root = ''
cfg_branch, _, environment = branch_env.partition(':')
pillar_dir = os.path.normpath(os.path.join(gitpil.working_dir, root))
if __opts__['pillar_roots'].get(branch, []) == [pillar_dir]: return {}
from __future__ import absolute_import, print_function import re import sys import platform
if sys.version_info[0] == 3: MAX_SIZE = sys.maxsize string_types = (str,) else: MAX_SIZE = sys.maxint string_types = (basestring,) from itertools import imap as map
return 0 < self.major < 2014
return method(self.noc_info, other.noc_info)
if self.rc > 0 and other.rc <= 0: noc_info = list(self.noc_info) noc_info[3] = -1 return method(tuple(noc_info), other.noc_info)
def __discover_version(saltstack_version): import os import subprocess
return saltstack_version
return saltstack_version
kwargs['close_fds'] = True
saltstack_version.sha = out.strip() saltstack_version.noc = -1
raise
__saltstack_version__ = __get_version(__saltstack_version__) del __get_version
__version_info__ = __saltstack_version__.info __version__ = __saltstack_version__.string
from __future__ import absolute_import, print_function import os import sys import time import logging import threading import traceback from random import randint
from salt import cloud, defaults
if os.getuid() == 0 and not salt.utils.is_windows(): os.kill(parent_pid, 0)
log.error('Minion process encountered exception: {0}'.format(exc)) os._exit(salt.defaults.exitcodes.EX_GENERIC)
os.kill(pid, signum)
minion = salt.cli.daemons.Minion() minion.start() break
signal.signal(signal.SIGINT, prev_sigint_handler) signal.signal(signal.SIGTERM, prev_sigterm_handler)
time.sleep(2 + randint(1, 10)) rlogger = logging.getLogger() for handler in rlogger.handlers: rlogger.removeHandler(handler) logging.basicConfig()
os.kill(parent_pid, 0)
os._exit(999)
queue.put(random_delay)
has_saltcloud = False
from __future__ import absolute_import import warnings
warnings.filterwarnings(
warnings.filterwarnings( 'ignore', 'With-statements now directly support multiple context managers', DeprecationWarning )
warnings.filterwarnings( 'ignore', '^Module backports was already imported from (.*), but (.*) is being added to sys.path$', UserWarning )
del locale if not encoding: encoding = sys.getdefaultencoding() or 'ascii'
if sys.version_info[0] < 3: import __builtin__ as builtins else:
setattr(builtins, '__salt_system_encoding__', encoding)
del sys del builtins del encoding
del __define_global_system_encoding_variable__
from __future__ import absolute_import import os import re import sys import time import types import socket import logging import logging.handlers import traceback import multiprocessing
import salt.ext.six as six
PROFILE = logging.PROFILE = 15 TRACE = logging.TRACE = 5 GARBAGE = logging.GARBAGE = 1 QUIET = logging.QUIET = 1000
from salt.textformat import TextFormat from salt.log.handlers import (TemporaryLoggingHandler, StreamHandler, SysLogHandler, FileHandler, WatchedFileHandler, QueueHandler) from salt.log.mixins import LoggingMixInMeta, NewStyleClassMixIn
SORTED_LEVEL_NAMES = [ l[0] for l in sorted(six.iteritems(LOG_LEVELS), key=lambda x: x[1]) ]
LOGGING_LOGGER_CLASS = logging.getLoggerClass()
LOGGING_NULL_HANDLER = TemporaryLoggingHandler(logging.WARNING)
LOGGING_TEMP_HANDLER = StreamHandler(sys.stderr)
LOGGING_STORE_HANDLER = TemporaryLoggingHandler()
self.bracketname = '[%-17s]' % self.name self.bracketlevel = '[%-8s]' % self.levelname self.bracketprocess = '[%5s]' % self.process
handler.release() return instance
handler.release() return instance
handler.release() return instance
pass
if logging.getLoggerClass() is not SaltLoggingClass:
logging.root.setLevel(GARBAGE)
logging.root.addHandler(LOGGING_NULL_HANDLER)
logging.root.addHandler(LOGGING_STORE_HANDLER)
continue
break
__remove_null_logging_handler()
__remove_temp_logging_handler()
continue
break
if not log_format: log_format = '[%(levelname)-8s] %(message)s' if not date_format: date_format = '%H:%M:%S'
__remove_temp_logging_handler()
syslog_opts['address'] = os.sep.join( parsed_log_path.path.split(os.sep)[:-1] )
raise RuntimeError( 'The syslog facility \'{0}\' is not known'.format( facility_name ) )
syslog_opts.pop('socktype', None)
handler = SysLogHandler(**syslog_opts)
handler = WatchedFileHandler(log_path, mode='a', encoding='utf-8', delay=0)
return
return
import salt.loader
initial_handlers = logging.root.handlers[:]
providers = salt.loader.log_handlers(opts)
additional_handlers = []
initial_handlers_count = len(logging.root.handlers)
handlers = [handlers]
__remove_queue_logging_handler()
__remove_null_logging_handler()
return __MP_LOGGING_QUEUE
return
return
__MP_LOGGING_CONFIGURED = True
__remove_null_logging_handler() __remove_queue_logging_handler()
return
logging.root.removeHandler(__MP_LOGGING_QUEUE_HANDLER) __MP_LOGGING_QUEUE_HANDLER = None __MP_LOGGING_CONFIGURED = False
return
pass
__MP_LOGGING_QUEUE_PROCESS.terminate()
setup_temp_logger() setup_extended_logging(opts)
break
return
LOGGING_NULL_HANDLER = None break
return
LOGGING_STORE_HANDLER = None break
return
__remove_null_logging_handler()
LOGGING_TEMP_HANDLER = None break
logging.captureWarnings(True)
if is_mp_logging_listener_configured(): shutdown_multiprocessing_logging_listener()
sys.excepthook = __global_logging_exception_handler
import sys import logging
if self.level > exc_info_on_loglevel: return formatted_record
if not record.exc_info_on_loglevel_instance and not exc_info_on_loglevel_formatted: return formatted_record
if formatted_record[-1:] != '\n': formatted_record += '\n'
from salt.log.setup import ( LOG_LEVELS, SORTED_LEVEL_NAMES, is_console_configured, is_logfile_configured, is_logging_configured, is_temp_logging_configured, setup_temp_logger, setup_console_logger, setup_logfile_logger, set_logger_level, )
from __future__ import absolute_import import socket import logging
from salt.log.mixins import NewStyleClassMixIn from salt.log.setup import LOG_LEVELS
try: from log4mongo.handlers import MongoHandler, MongoFormatter HAS_MONGO = True except ImportError: HAS_MONGO = False
from __future__ import absolute_import, print_function import logging import logging.handlers import time import datetime import socket import threading
from salt.log.setup import LOG_LEVELS from salt.log.mixins import NewStyleClassMixIn import salt.utils.network
import salt.ext.six as six
import msgpack if msgpack.loads(msgpack.dumps([1, 2, 3]), use_list=True) is None: raise ImportError
__virtualname__ = 'fluent'
__opts__.get( 'log_level', 'error' )
self._close()
if self.pendings: self.pendings += bytes_ bytes_ = self.pendings
self._reconnect()
self.socket.sendall(bytes_)
self.pendings = None
import sys import atexit import logging import threading import logging.handlers
from salt.log.mixins import NewStyleClassMixIn, ExcInfoOnLogLevelFormatMixIn
self.__messages.pop(0)
continue
from __future__ import absolute_import import os import json import logging import logging.handlers import datetime
from salt.log.setup import LOG_LEVELS from salt.log.mixins import NewStyleClassMixIn import salt.utils.network
import salt.ext.six as six try: import zmq except ImportError: pass
__virtualname__ = 'logstash'
__opts__.get( 'log_level', 'error' )
__opts__.get( 'log_level', 'error' )
import logging
import salt.loader from salt.log import LOG_LEVELS
try: import raven from raven.handlers.logging import SentryHandler HAS_RAVEN = True except ImportError: HAS_RAVEN = False
__virtualname__ = 'sentry'
options.update({ 'site': get_config_value('site'),
'name': get_config_value('name'),
'exclude_paths': get_config_value('exclude_paths', ()),
'include_paths': get_config_value('include_paths', ()),
'list_max_length': get_config_value('list_max_length'),
'string_max_length': get_config_value('string_max_length'),
'auto_log_stacks': get_config_value('auto_log_stacks'),
'timeout': get_config_value('timeout', 1),
'processors': get_config_value('processors'),
'dsn': dsn
import logging
pass
import salt.ext.six as six
class Channel(object): @staticmethod def factory(opts, **kwargs): ttype = 'zeromq'
from __future__ import absolute_import import logging import socket import msgpack import weakref import time
import tornado import tornado.gen import tornado.netutil import tornado.concurrent from tornado.ioloop import IOLoop from tornado.iostream import IOStream
import salt.transport.client import salt.transport.frame import salt.ext.six as six
def future_with_timeout_callback(future): if future._future_with_timeout is not None: future._future_with_timeout._done_callback(future)
self._future._future_with_timeout = self if self._future.done(): future_with_timeout_callback(self._future)
self._future._future_with_timeout = None self.set_exception(tornado.ioloop.TimeoutError())
self.sock = None self.io_loop = io_loop or IOLoop.current() self._closing = False
instance_map = weakref.WeakKeyDictionary()
key = str(socket_path)
new_client.__singleton_init__(io_loop=io_loop, socket_path=socket_path) loop_instance_map[key] = new_client
pass
import tornado.ioloop
import salt.config import salt.transport.ipc
ipc_client.connect()
import tornado.ioloop
import salt.transport.ipc import salt.config
ipc_server.start(ipc_server_socket_path)
io_loop.start()
def print_to_console(payload): print(payload)
self.sock = None self.io_loop = io_loop or IOLoop.current() self._closing = False self.streams = set()
import tornado.ioloop
import salt.config import salt.transport.ipc
io_loop = tornado.ioloop.IOLoop()
io_loop.run_sync(ipc_subscriber.connect)
timeout = None
break
ret = None
self.io_loop.spawn_callback(self.io_loop.stop)
if self._read_sync_future is not None: self._read_sync_future.exc_info() if self._read_stream_future is not None: self._read_stream_future.exc_info()
from __future__ import absolute_import import msgpack import salt.ext.six as six
from __future__ import absolute_import
ttype = 'zeromq'
ttype = 'zeromq'
if ttype == 'zeromq': import salt.transport.zeromq return salt.transport.zeromq.ZeroMQPubServerChannel(opts, **kwargs)
from __future__ import absolute_import, print_function import logging
import salt.utils from salt.transport.client import ReqChannel
ret = { 'data': None, 'dest': None, }
from __future__ import absolute_import import os import copy import errno import signal import hashlib import logging import weakref from random import randint
import tornado import tornado.gen import tornado.concurrent
import salt.ext.six as six from Crypto.Cipher import PKCS1_OAEP
instance_map = weakref.WeakKeyDictionary()
def __init__(self, opts, **kwargs): pass
def __singleton_init__(self, opts, **kwargs): self.opts = dict(opts) self.ttype = 'zeromq'
self.crypt = kwargs.get('crypt', 'aes')
self.auth = salt.crypt.AsyncAuth(self.opts, io_loop=self._io_loop)
yield self.auth.authenticate()
yield self.auth.authenticate() ret = yield self.message_client.send( self._package_load(self.auth.crypticle.dumps(load)), timeout=timeout, tries=tries, )
yield self.auth.authenticate()
ret = yield _do_transfer()
yield self.auth.authenticate() ret = yield _do_transfer()
self._socket.setsockopt(zmq.IPV4ONLY, 0)
import threading self._w_monitor = ZeroMQSocketMonitor(self._socket) t = threading.Thread(target=self._w_monitor.start_poll) t.start()
stream.send('Server-side exception handling payload')
log.info('Starting the Salt Publisher on {0}'.format(pub_uri)) pub_sock.bind(pub_uri)
if load['tgt_type'] == 'list': int_payload['topic_lst'] = load['tgt']
int_payload['topic_lst'] = match_ids
self._init_socket()
self.send_future_map = {}
if hasattr(zmq, 'RECONNECT_IVL_MAX'): self.socket.setsockopt( zmq.RECONNECT_IVL_MAX, 5000 )
del self.send_queue[0] continue
self.io_loop.remove_timeout(timeout)
message = self.serial.dumps(message)
self.send_future_map[message] = future
from __future__ import absolute_import import logging import msgpack import socket import os import weakref import time import traceback
import tornado import tornado.tcpserver import tornado.gen import tornado.concurrent import tornado.tcpclient import tornado.netutil
if six.PY2: import urlparse else: import urllib.parse as urlparse
from Crypto.Cipher import PKCS1_OAEP
def __setstate__(self, state): self._is_child = True self.__init__( state['opts'], state['socket_queue'], log_queue=state['log_queue'] )
connection, address = self._socket.accept() self.socket_queue.put((connection, address), True, None)
if tornado.util.errno_from_exception(e) == errno.ECONNABORTED: continue raise
instance_map = weakref.WeakKeyDictionary()
new_obj = object.__new__(cls) new_obj.__singleton_init__(opts, **kwargs) loop_instance_map[key] = new_obj
def __init__(self, opts, **kwargs): pass
def __singleton_init__(self, opts, **kwargs): self.opts = dict(opts)
self.crypt = kwargs.get('crypt', 'aes')
raise SaltClientError('Connection to master lost')
stream.write('Server-side exception handling payload') stream.close()
self.io_loop.spawn_callback( self._handle_connection, client_socket, address)
if self.connect_callback is not None: def handle_future(future): response = future.result() self.io_loop.add_callback(self.connect_callback, response) future.add_done_callback(handle_future)
if self._connecting_future.done(): self._connecting_future = self.connect() yield self._connecting_future
if self._connecting_future.done(): self._connecting_future = self.connect() yield self._connecting_future
raise Exception('Unable to find available messageid')
self.send_future_map[message_id] = future
self._read_until_future.exc_info()
self.presence_events = True
return
return
continue
f = client.stream.write(payload) self.io_loop.add_future(f, lambda f: True)
if self.io_loop is None: self.io_loop = tornado.ioloop.IOLoop.current()
try: self.io_loop.start() except (KeyboardInterrupt, SystemExit): salt.log.setup.shutdown_multiprocessing_logging()
if load['tgt_type'] == 'list': int_payload['topic_lst'] = load['tgt'] pub_sock.send(int_payload)
from __future__ import absolute_import
from salt.utils.async import SyncWrapper
sync = SyncWrapper(AsyncReqChannel.factory, (opts,), kwargs) return sync
_resolver_configured = False
ttype = 'zeromq'
ttype = 'zeromq'
if ttype == 'zeromq': import salt.transport.zeromq return salt.transport.zeromq.AsyncZeroMQPubChannel(opts, **kwargs)
import salt.transport.ipc return salt.transport.ipc.IPCMessageClient(opts, **kwargs)
from __future__ import absolute_import import multiprocessing import ctypes import logging import os import hashlib import shutil import binascii
import salt.crypt import salt.payload import salt.master import salt.transport.frame import salt.utils.event import salt.ext.six as six from salt.utils.cache import CacheCli
import tornado.gen from Crypto.Cipher import PKCS1_OAEP from Crypto.PublicKey import RSA
auto_reject = self.auto_key.check_autoreject(load['id']) auto_sign = self.auto_key.check_autosign(load['id'])
pass
key_path = None
if self.cache_cli: self.cache_cli.put_cache([load['id']])
pass
pass
from __future__ import absolute_import, with_statement import copy import ctypes import os import re import sys import time import errno import signal import stat import logging import multiprocessing import tempfile import traceback
from Crypto.PublicKey import RSA import salt.ext.six as six from salt.ext.six.moves import range
HAS_RESOURCE = False
try:
self.loop_interval = int(self.opts['loop_interval']) self.rotate = int(time.time())
def __setstate__(self, state): self._is_child = True self.__init__(state['opts'], log_queue=state['log_queue'])
self.presence_events = True
self._post_fork_init()
last = int(time.time()) salt.daemons.masterapi.clean_fsbackend(self.opts) salt.daemons.masterapi.clean_pub_auth(self.opts)
log.debug('Pinging all connected minions ' 'due to key rotation') salt.utils.master.ping_all_connected_minions(self.opts)
if self.schedule.loop_interval < self.loop_interval: self.loop_interval = self.schedule.loop_interval
self.event.fire_event(data, tagify('present', 'presence'), timeout=3) old_present.clear() old_present.update(present)
try: fileserver.init() except FileserverConfigError as exc: critical_errors.append('{0}'.format(exc))
salt.utils.gitfs.GitPillar(new_opts)
def run_reqserver(self, **kwargs): secrets = kwargs.pop('secrets', None) if secrets is not None: SMaster.secrets = secrets
with default_signals(signal.SIGINT, signal.SIGTERM):
log.info('Creating master maintenance process') self.process_manager.add_process(Maintenance, args=(self.opts,))
self.process_manager.add_process(self.run_reqserver, kwargs=kwargs, name='ReqServer')
if signal.getsignal(signal.SIGINT) is signal.SIG_DFL: signal.signal(signal.SIGINT, self._handle_signals)
signal.signal(signal.SIGINT, self._handle_signals)
self.process_manager.stop_restarting() self.process_manager.send_signal_to_processes(signum) self.process_manager.kill_children()
def __setstate__(self, state): self._is_child = True self.__init__(state['hopts'], log_queue=state['log_queue'])
self.key = key
os.chmod(dfn, stat.S_IRUSR | stat.S_IWUSR)
if hasattr(self, 'process_manager'): self.process_manager.stop_restarting() self.process_manager.send_signal_to_processes(signum) self.process_manager.kill_children()
if HAS_ZMQ: zmq.eventloop.ioloop.install() self.io_loop = LOOP_CLASS() for req_channel in self.req_channels:
log.warning( 'Minion id {0} is not who it says it is!'.format( load['id'] ) ) return False
return False
log.warning( 'Minion id {0} is not who it says it is!'.format( load['id'] ) ) return {}
self.masterapi._minion_event(load) self._handle_minion_event(load)
log.warning('Authentication failure of type "eauth" occurred.') return ''
if not token or token['eauth'] not in self.opts['external_auth']: log.warning('Authentication failure of type "token" occurred.') return ''
if clear_load['fun'] != 'saltutil.find_job': log.warning( 'Authentication failure of type "token" occurred.' ) return ''
log.warning( 'Authentication failure of type "eauth" occurred.' ) return ''
group_auth_match = False for group_config in group_perm_keys: group_config = group_config.rstrip('%') for group in groups: if group == group_config: group_auth_match = True
#[group for groups in ['external_auth'][extra['eauth']]]):
if not self.loadauth.time_auth(extra): log.warning( 'Authentication failure of type "eauth" occurred.' ) return ''
if clear_load['fun'] != 'saltutil.find_job': log.warning( 'Authentication failure of type "eauth" occurred.' ) return ''
self._send_pub(payload)
passed_jid = clear_load['jid'] if clear_load.get('jid') else None nocache = extra.get('nocache', False)
self.event.fire_event(new_job_load, tagify([clear_load['jid'], 'new'], 'job'))
if isinstance(exc, zmq.ZMQError) and exc.errno == errno.EINTR: return
from __future__ import absolute_import import logging
from salt.exceptions import SaltSystemExit
__proxyenabled__ = ['cisconso']
GRAINS_CACHE = {} DETAILS = {}
log = logging.getLogger(__file__)
__virtualname__ = 'cisconso'
ret = client.get_datastore(DatastoreType.RUNNING) GRAINS_CACHE.update(ret) return GRAINS_CACHE
from __future__ import absolute_import import salt.ext.six.moves.http_client as http_client
import logging import time import json from salt.exceptions import (CommandExecutionError, MinionError)
return True
return True
state = devices[str(dev_id)]['state']['on'] and Const.LAMP_OFF or Const.LAMP_ON
import json import logging
from salt.utils.vt_helper import SSHConnection from salt.utils.vt import TerminalException
__proxyenabled__ = ['ssh_sample']
log = logging.getLogger(__file__)
out, err = DETAILS['server'].sendline(cmd)
DETAILS['grains_cache'] = parse(out)
out, err = DETAILS['server'].sendline('pkg_list\n')
return parse(out)
out, err = DETAILS['server'].sendline(cmd)
return parse(out)
out, err = DETAILS['server'].sendline(cmd)
return parse(out)
out, err = DETAILS['server'].sendline(cmd)
return parse(out)
out, err = DETAILS['server'].sendline(cmd)
return parse(out)
out, err = DETAILS['server'].sendline(cmd)
return parse(out)
out, err = DETAILS['server'].sendline(cmd)
return parse(out)
import logging import salt.utils import salt.utils.http
__proxyenabled__ = ['fx2']
GRAINS_CACHE = {} DETAILS = {}
log = logging.getLogger(__file__)
from __future__ import absolute_import import logging
from salt.exceptions import SaltSystemExit
__proxyenabled__ = ['esxi']
GRAINS_CACHE = {} DETAILS = {}
log = logging.getLogger(__file__)
__virtualname__ = 'esxi'
try: username, password = find_credentials(host) except SaltSystemExit as err: log.critical('Error: {0}'.format(err)) return False
ret = __salt__['vsphere.system_info'](host=host, username=user, password=password)
continue
raise SaltSystemExit('Cannot complete login due to an incorrect user name or password.')
import logging log = logging.getLogger(__file__)
import napalm
NETWORK_DEVICE['UP'] = True
from __future__ import absolute_import from __future__ import print_function import logging
try: HAS_JUNOS = True import jnpr.junos import jnpr.junos.utils import jnpr.junos.utils.config import jnpr.junos.utils.sw except ImportError: HAS_JUNOS = False
__virtualname__ = 'junos'
crypt_salt = secure_password(8, use_random=False)
import logging import salt.utils.http
__proxyenabled__ = ['rest_sample']
GRAINS_CACHE = {} DETAILS = {}
log = logging.getLogger(__file__)
DETAILS['url'] = opts['proxy']['url']
if not DETAILS['url'].endswith('/'): DETAILS['url'] += '/'
from __future__ import absolute_import, print_function import os import sys import copy import time import hmac import base64 import hashlib import logging import stat import traceback import binascii import weakref import getpass
import salt.ext.six as six
pass
return priv
pass
if opts['master_sign_pubkey']:
def __setstate__(self, state): self.__init__(state['opts'])
instance_map = weakref.WeakKeyDictionary()
creds_map = {}
io_loop = io_loop or tornado.ioloop.IOLoop.current() if io_loop not in AsyncAuth.instance_map: AsyncAuth.instance_map[io_loop] = weakref.WeakValueDictionary() loop_instance_map = AsyncAuth.instance_map[io_loop]
new_auth = object.__new__(cls) new_auth.__singleton_init__(opts, io_loop=io_loop) loop_instance_map[key] = new_auth
def __init__(self, opts, io_loop=None): pass
continue
user = self.opts.get('user', 'root') salt.utils.verify.check_path_traversal(self.opts['pki_dir'], user)
if 'pub_sig' in payload and self.opts['verify_master_pubkey_sign']: return True elif 'pub_sig' not in payload and not self.opts['verify_master_pubkey_sign']: return True
log.error('The master key has changed, the salt master could ' 'have been subverted, verify salt master\'s public ' 'key') return ''
instances = weakref.WeakValueDictionary()
def __init__(self, opts, io_loop=None): super(SAuth, self).__init__(opts, io_loop=io_loop)
if not data.startswith(self.PICKLE_PAD): return {} load = self.serial.loads(data[len(self.PICKLE_PAD):], raw=raw) return load
from __future__ import absolute_import
from salt.utils.schema import (Schema, IPv4Item, ) from salt.config.schemas.common import (MinionDefaultInclude, IncludeConfig )
__allow_additional_items__ = True
from __future__ import absolute_import
from salt.utils.schema import (Schema, StringItem, IntegerItem, SecretItem, PortItem, BooleanItem, RequirementsItem, DictItem, AnyOfItem ) from salt.config.schemas.minion import MinionConfiguration
pattern=r'^((\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})|([A-Za-z0-9][A-Za-z0-9\.\-]{1,255}))$', min_length=1, required=True)
from __future__ import absolute_import
from salt.utils.schema import (Schema, StringItem, ArrayItem, OneOfItem)
from __future__ import absolute_import from __future__ import generators import os import re import sys import glob import time import codecs import logging from copy import deepcopy import types
import yaml try: yaml.Loader = yaml.CLoader yaml.Dumper = yaml.CDumper except Exception: pass
import salt.ext.six as six from salt.ext.six import string_types, text_type from salt.ext.six.moves.urllib.parse import urlparse
_DFLT_IPC_MODE = 'tcp' _MASTER_TRIES = -1
'master': (string_types, list),
'master_port': int,
'master_uri_format': str,
'master_finger': str,
'master_shuffle': bool,
'master_alive_interval': int,
'master_failback': bool,
'master_failback_interval': int,
'master_sign_key_name': str,
'master_sign_pubkey': bool,
'verify_master_pubkey_sign': bool,
'always_verify_signature': bool,
'master_pubkey_signature': str,
'master_use_pubkey_signature': bool,
'syndic_finger': str,
'user': str,
'root_dir': str,
'pki_dir': str,
'id': str,
'cachedir': str,
'cache_jobs': bool,
'conf_file': str,
'sock_dir': str,
'backup_mode': str,
'renderer': str,
'renderer_whitelist': list,
'renderer_blacklist': list,
'failhard': bool,
'autoload_dynamic_modules': bool,
'environment': str,
'pillarenv': str,
'state_top': str,
'startup_states': str,
'sls_list': list,
'top_file': str,
'file_client': str,
'use_master_when_local': bool,
'file_roots': dict,
'pillar_roots': dict,
'hash_type': str,
'disable_modules': list,
'disable_returners': list,
'whitelist_modules': list,
'module_dirs': list,
'returner_dirs': list,
'states_dirs': list,
'grains_dirs': list,
'render_dirs': list,
'outputter_dirs': list,
'utils_dirs': list,
'providers': dict,
'clean_dynamic_modules': bool,
'open_mode': bool,
'multiprocessing': bool,
'mine_enabled': bool,
'mine_return_job': bool,
'mine_interval': int,
'ipc_mode': str,
'ipv6': bool,
'file_buffer_size': int,
'tcp_pub_port': int,
'tcp_pull_port': int,
'tcp_master_pub_port': int,
'tcp_master_pull_port': int,
'tcp_master_publish_pull': int,
'tcp_master_workers': int,
'log_file': str,
'log_level': str,
'log_level_logfile': str,
'log_datefmt': str,
'log_datefmt_logfile': str,
'log_fmt_console': str,
'log_fmt_logfile': (tuple, str),
'log_granular_levels': dict,
'max_event_size': int,
'test': bool,
'cython_enable': bool,
'enable_zip_modules': bool,
'show_timeout': bool,
'show_jid': bool,
'state_verbose': bool,
'state_output': str,
'state_output_diff': bool,
'state_auto_order': bool,
'state_events': bool,
'acceptance_wait_time': float,
'acceptance_wait_time_max': float,
'rejected_retry': bool,
'loop_interval': float,
'verify_env': bool,
'grains': dict,
'permissive_pki_access': bool,
'default_include': str,
'update_url': (bool, string_types),
'update_restart_services': list,
'retry_dns': float,
'recon_max': float,
'recon_default': float,
'recon_randomize': bool,
'event_return': str,
'event_return_queue': int,
'event_return_whitelist': list,
'event_return_blacklist': list,
'event_match_type': str,
'pidfile': str,
'range_server': str,
'tcp_keepalive': bool,
'tcp_keepalive_idle': float,
'tcp_keepalive_cnt': float,
'tcp_keepalive_intvl': float,
'interface': str,
'publish_port': int,
'pub_hwm': int,
'salt_event_pub_hwm': int, 'event_publisher_pub_hwm': int,
'worker_threads': int,
'ret_port': int,
'keep_jobs': int,
'master_roots': dict,
'ext_pillar': list,
'pillar_version': int,
'pillar_opts': bool,
'pillar_cache': bool,
'pillar_cache_ttl': int,
'pillar_cache_backend': str,
'pillar_source_merging_strategy': str,
'pillar_merge_lists': bool,
'top_file_merging_strategy': str,
'env_order': list,
'default_top': str,
'max_open_files': int,
'auto_accept': bool, 'autosign_timeout': int,
'master_tops': dict,
'order_masters': bool,
'job_cache': bool,
'ext_job_cache': str,
'master_job_cache': str,
'job_cache_store_endtime': bool,
'publish_session': int,
'reactor': list,
'reactor_refresh_interval': int,
'reactor_worker_threads': int,
'reactor_worker_hwm': int,
'engines': list,
'search_index_interval': int,
'nodegroups': dict,
'ssh_list_nodegroups': dict,
'key_logfile': str,
'winrepo_source_dir': str,
'modules_max_memory': int,
'grains_refresh_every': int,
'enable_lspci': bool,
'syndic_wait': int,
'jinja_lstrip_blocks': bool,
'jinja_trim_blocks': bool,
'minion_id_caching': bool,
'sign_pub_messages': bool,
'keysize': int,
'transport': str,
'gather_job_timeout': int,
'auth_timeout': int,
'auth_tries': int,
'auth_safemode': bool,
'random_reauth_delay': int,
'syndic_event_forward_timeout': float,
'syndic_max_event_process_time': float,
'syndic_jid_forward_cache_hwm': int,
'ioflo_verbose': int,
'ioflo_realtime': bool,
'ioflo_console_logdir': str,
'ping_interval': int,
'cli_summary': bool,
'max_minions': int,
'zmq_filtering': bool,
'con_cache': bool, 'rotate_aes_key': bool,
'cache_sreqs': bool,
'cmd_safe': bool,
'dummy_publisher': bool,
'rest_timeout': int,
'sudo_user': str,
'http_request_timeout': float,
'http_max_body': int,
'bootstrap_delay': int,
'minion_restart_command': list,
'pub_ret': bool,
'salt_event_pub_hwm': 2000, 'event_publisher_pub_hwm': 1000, 'event_match_type': 'startswith', 'minion_restart_command': [], 'pub_ret': True,
'pidfile': '/var/run/salt-api.pid', 'logfile': '/var/log/salt/api', 'rest_timeout': 300,
return valid_type.__name__
return {}
env_path = os.environ.get(env_var, path) if not env_path or not os.path.isfile(env_path): env_path = path if path != default_path: env_path = path
return {}
'syndic_master_port', opts.get( 'master_port', minion_defaults.get( 'master_port', DEFAULT_MINION_OPTS['master_port'] ) )
master_config_path = _absolute_path(master_config_path, config_dir)
providers_config_path = overrides['providers_config']
providers_config_path = _absolute_path(providers_config_path, config_dir)
profiles_config_path = overrides['profiles_config']
profiles_config_path = _absolute_path(profiles_config_path, config_dir)
deploy_scripts_search_path = overrides.get( 'deploy_scripts_search_path', defaults.get('deploy_scripts_search_path', 'cloud.deploy.d') ) if isinstance(deploy_scripts_search_path, string_types): deploy_scripts_search_path = [deploy_scripts_search_path]
deploy_scripts_search_path[idx] = entry continue
deploy_scripts_search_path.pop(idx)
overrides.update( deploy_scripts_search_path=tuple(deploy_scripts_search_path) )
master_config.update(overrides) overrides = master_config
'providers_config', os.path.join(salt.syspaths.CONFIG_DIR, 'cloud.providers')
'profiles_config', os.path.join(salt.syspaths.CONFIG_DIR, 'cloud.profiles')
opts = apply_cloud_config(overrides, defaults)
providers_config = opts['providers']
providers_config = cloud_providers_config(providers_config_path)
opts['providers'] = providers_config
if profiles_config is None: profiles_config = vm_profiles_config(profiles_config_path, providers_config) opts['profiles'] = profiles_config
apply_sdb(opts)
return opts
alias, driver = driver.split(':')
alias, driver = driver.split(':')
config = old_to_new(config)
if 'provider' in provider_config: provider_config['driver'] = provider_config.pop('provider')
opts['providers'][lprovider] = {} opts['providers'][lprovider][lprovider] = provider_config
vms[profile] = extended
for name, settings in six.iteritems(config.copy()): if '.' in name: log.warning( 'Please switch to the new providers configuration syntax' )
config = old_to_new(config)
for prov_name, prov_settings in six.iteritems(config.pop('providers')): config[prov_name] = prov_settings break
if 'provider' in details: details['driver'] = details.pop('provider')
providers[provider_alias][driver]['profiles'] = {}
details['driver'] = provider
details['extends'] = extends keep_looping = True
keep_looping = False for alias, entries in six.iteritems(providers.copy()): for driver, details in six.iteritems(entries):
continue
keep_looping = True continue
value = default
value = deepcopy(opts[name])
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
return provider_details
return False
required_keys = ['provider'] alias, driver = provider.split(':')
non_image_drivers = ['nova', 'virtualbox']
if driver == 'linode' and profile_key.get('clonefrom', False): non_image_drivers.append('linode') non_size_drivers.append('linode')
if driver == 'vmware' and profile_key.get('image', True): non_image_drivers.append('vmware')
for item in list(required_keys): if item in provider_key: required_keys.remove(item)
if vm_: for item in list(required_keys): if item in vm_: required_keys.remove(item)
id_cache = os.path.join(root_dir, config_dir.lstrip(os.path.sep), 'minion_id')
using_ip_for_id = False if not opts.get('id'): opts['id'], using_ip_for_id = get_id( opts, cache_minion_id=cache_minion_id)
if not using_ip_for_id and 'append_domain' in opts: opts['id'] = _append_domain(opts)
opts['open_mode'] = opts['open_mode'] is True
opts['utils_dirs'] = ( opts.get('utils_dirs') or [os.path.join(opts['extension_modules'], 'utils')] )
insert_system_path(opts, opts['utils_dirs'])
prepend_root_dirs = [ 'pki_dir', 'cachedir', 'sock_dir', 'extension_modules', 'pidfile', ]
for config_key in ('log_file', 'key_logfile'): if urlparse(opts.get(config_key, '')).scheme == '': prepend_root_dirs.append(config_key)
if 'beacons' not in opts: opts['beacons'] = {}
if 'schedule' not in opts: opts['schedule'] = {}
opts['hash_type'] = opts['hash_type'].lower()
if not using_ip_for_id and 'append_domain' in opts: opts['id'] = _append_domain(opts) if append_master: opts['id'] += '_master'
for config_key in ('log_file', 'key_logfile'): log_setting = opts.get(config_key, '') if log_setting is None: continue
opts['open_mode'] = opts['open_mode'] is True opts['auto_accept'] = opts['auto_accept'] is True opts['file_roots'] = _validate_file_roots(opts)
if isinstance(opts['file_ignore_regex'], str): ignore_regex = [opts['file_ignore_regex']] elif isinstance(opts['file_ignore_regex'], list): ignore_regex = opts['file_ignore_regex']
re.compile(regex) opts['file_ignore_regex'].append(regex)
if isinstance(opts['file_ignore_glob'], str): opts['file_ignore_glob'] = [opts['file_ignore_glob']]
opts['hash_type'] = opts['hash_type'].lower()
_validate_opts(opts) return opts
defaults = DEFAULT_MASTER_OPTS defaults.update(DEFAULT_API_OPTS)
defaults = DEFAULT_MASTER_OPTS defaults.update(DEFAULT_SPM_OPTS)
prepend_root_dirs = [ 'formula_path', 'pillar_path', 'reactor_path', 'spm_cache_dir', 'spm_build_dir' ]
for config_key in ('spm_logfile',): log_setting = opts.get(config_key, '') if log_setting is None: continue
from __future__ import absolute_import import time import logging
bitmask = 0xffffffff h = 0
salt --async '*' splay.splay pkg.install cowsay version=3.03-8.el6
from __future__ import absolute_import import json try:
from salt.executors import ModuleExecutorBase import salt.utils import salt.syspaths
from __future__ import absolute_import import sys import os.path
if 'SETUP_DIRNAME' in globals():
if __PLATFORM.startswith('win'): ROOT_DIR = r'c:\salt' else: ROOT_DIR = '/'
import time import os import codecs import logging
import salt.utils from salt.utils.odict import OrderedDict from salt._compat import string_io from salt.ext.six import string_types
ret = {}
input_data = ifile.read() if not input_data.strip(): log.error('Template is nothing but whitespace: {0}'.format(template)) return ret
render_pipe = template_shebang(template, renderers, default, blacklist, whitelist, input_data)
time.sleep(0.01) ret = render(input_data, saltenv, sls, **render_kwargs)
pass
render_pipe = check_render_pipe_str(line.strip()[2:], renderers, blacklist, whitelist)
OLD_STYLE_RENDERERS = {}
from StringIO import StringIO
import salt.utils.templates from salt.exceptions import SaltRenderError
{% from 'lib.sls' import pythonpkg with context %}
/etc/redis/redis.conf: file.managed: - source: salt://redis.conf - template: jinja - context: bind: 127.0.0.1
{% set port = 6379 %}
{% from 'lib.sls' import port with context %} port {{ port }} bind {{ bind }}
{{ salt['cmd.run']('whoami') }} {{ salt.cmd.run('whoami') }}
from __future__ import absolute_import import logging
from salt.exceptions import SaltRenderError import salt.utils.templates
import salt.ext.six as six
mod_dict[mod] = lambda: None
from StringIO import StringIO
from salt.exceptions import SaltRenderError import salt.utils.templates
try: import hjson as hjson HAS_LIBS = True except ImportError: HAS_LIBS = False
from salt.ext.six import string_types
import logging import warnings from yaml.scanner import ScannerError from yaml.parser import ParserError from yaml.constructor import ConstructorError
data = data.encode('utf-8')
from __future__ import absolute_import
import logging import warnings
import salt.utils.url from salt.serializers.yamlex import deserialize
import os
from salt.exceptions import SaltRenderError import salt.utils.templates
try: from genshi.template import MarkupTemplate from genshi.template import NewTextTemplate from genshi.template import OldTextTemplate HAS_LIBS = True except ImportError: HAS_LIBS = False
from salt.ext.six import string_types
import dson import logging
from salt.ext import six
Pkg.installed("nginx", require=Pkg("some-other-package"))
from __future__ import absolute_import import logging import os import re
from salt.ext.six import exec_ import salt.utils import salt.loader from salt.fileclient import get_file_client from salt.utils.pyobjects import Registry, StateFactory, SaltObject, Map import salt.ext.six as six
_globals = {}
_globals['include'] = Registry.include _globals['extend'] = Registry.make_extend
Map.__salt__ = __salt__ _globals['Map'] = Map
'__salt__': __salt__, '__pillar__': __pillar__, '__grains__': __grains__
if not salt_data: return _globals
client = get_file_client(__opts__)
imports = None
final_template, final_locals = process_template(template, _globals) _globals.update(final_locals)
Registry.enabled = True
exec_(final_template, _globals)
import salt.utils json = salt.utils.import_json()
from salt.ext.six import string_types
apache2.service.running() \\ .require(apache2.pkg, pkg='libapache2-mod-wsgi') \\ .watch(file='/etc/apache2/httpd.conf')
apache2.service.require(state('libapache2-mod-wsgi').pkg, pkg='apache2') \\ .watch(file='/etc/apache2/httpd.conf')
apache2.service.running()
_, mod = include('a-non-pydsl-sls', 'a-pydsl-sls')
mod = include('a-pydsl-sls')
mod.myfunc(1, 2, "three")
s.cmd.run('echo at render time', cwd='/') s.file.managed('target.txt', source='salt://source.txt')
extend(state('.start').stateconf.require(stateconf='xxx::goal'))
extend(state('.goal').stateconf.require_in(stateconf='yyy::start'))
mod.__deepcopy__ = lambda x: mod
from __future__ import absolute_import import os import re import logging from subprocess import Popen, PIPE
import salt.utils import salt.syspaths from salt.exceptions import SaltRenderError
import salt.ext.six as six
from __future__ import absolute_import
import msgpack
from salt.ext.six import string_types
from __future__ import absolute_import import logging import re import getopt import copy from os import path as ospath
import salt.utils from salt.exceptions import SaltRenderError
import salt.ext.six as six
data = copy.deepcopy(high) try: rewrite_single_shorthand_state_decl(data) rewrite_sls_includes_excludes(data, sls, saltenv)
extract_state_confs(data)
match = re.search(__opts__['stateconf_end_marker'], sls_templ) if match: process_sls_data(sls_templ[:match.start()], extract=True)
if STATE_CONF: tmplctx = STATE_CONF.copy() if tmplctx: prefix = sls + '::'
data = process_sls_data(sls_templ, tmplctx)
def add_implicit_requires(data):
if prev_state[0] is not None: try: next(nvlist(args, ['require']))[2].insert(0, dict([prev_state]))
continue
class Bunch(dict): def __getattr__(self, name): return self[name]
try: from Cheetah.Template import Template HAS_LIBS = True except ImportError: HAS_LIBS = False
from salt.ext.six import string_types
import logging try: import json5 as json HAS_JSON5 = True except ImportError: HAS_JSON5 = False
from salt.ext.six import string_types
__virtualname__ = 'json5'
from __future__ import absolute_import import os
import salt.minion import salt.loader import salt.utils
import salt.ext.six as six
import os
import salt.search import salt.ext.six as six
HAS_WHOOSH = False try: import whoosh.index import whoosh.fields import whoosh.store import whoosh.qparser HAS_WHOOSH = True except ImportError: pass
__virtualname__ = 'whoosh'
from __future__ import absolute_import import sys import types import subprocess
from salt.ext.six import binary_type, string_types, text_type from salt.ext.six.moves import cStringIO, StringIO
import xml.etree.cElementTree as ElementTree
import xml.etree.ElementTree as ElementTree
import elementtree.cElementTree as ElementTree
import elementtree.ElementTree as ElementTree
PY3 = sys.version_info[0] == 3
from __future__ import absolute_import
from __future__ import absolute_import import salt.runner
from __future__ import absolute_import import os import time import logging import traceback
import salt.state import salt.payload from salt.exceptions import SaltRenderError
from __future__ import absolute_import
import salt.client
from __future__ import absolute_import import salt.wheel
from __future__ import absolute_import import os import json
import salt.utils
from __future__ import absolute_import, division import fnmatch
import logging
import salt.utils import salt.utils.decorators as decorators from salt.utils.odict import OrderedDict
__func_alias__ = { 'list_records': 'list', }
__virtualname__ = 'fmadm'
header = [field for field in output[0].lower().split(" ") if field] del output[0]
fault = OrderedDict() for field in header: fault[field] = entry[header.index(field)]
header = [field for field in output[0].lower().split(" ") if field] del output[0]
for entry in output: entry = [item for item in entry.split(" ") if item] entry = entry[0:3] + [" ".join(entry[3:])]
component = OrderedDict() for field in header: component[field] = entry[header.index(field)]
keyed_result = OrderedDict() for component in result: keyed_result[component['module']] = component del keyed_result[component['module']]['module']
if line.startswith('-'): if summary and summary_data and fault_data: result.update(_merge_data(summary_data, fault_data))
continue
if not summary: summary.append(line) continue
if summary and not summary_data: summary.append(line) summary_data = _parse_fmdump("\n".join(summary))[0] continue
result.update(_merge_data(summary_data, fault_data))
import salt.utils from salt.ext.six import string_types from salt.exceptions import CommandExecutionError import logging
__virtualname__ = 'user'
if purge: try: sid = getUserSid(name) win32profile.DeleteProfile(sid) except pywintypes.error as exc: (number, context, message) = exc
current_info = info(name) if not current_info: raise CommandExecutionError('User \'{0}\' does not exist'.format(name))
new_info = info(new_name) if new_info: raise CommandExecutionError( 'User \'{0}\' already exists'.format(new_name) )
pythoncom.CoInitialize() c = wmi.WMI(find_classes=0)
try: user = c.Win32_UserAccount(Name=name)[0] except IndexError: raise CommandExecutionError('User \'{0}\' does not exist'.format(name))
result = user.Rename(new_name)[0]
import os import re import glob import hashlib import tempfile import logging
import salt.utils
__valid_configs = { 'user': [ 'tomcat-manager.user', 'tomcat-manager:user' ], 'passwd': [ 'tomcat-manager.passwd', 'tomcat-manager:passwd' ] }
auth = _auth(url) if auth is False: ret['res'] = False ret['msg'] = 'missing username and password settings (grain/pillar)' return ret
_install_opener(auth)
ret['msg'] = _urlopen(url, timeout=timeout).read().splitlines()
ret['msg'] = _urlopen(url6, timeout=timeout).read().splitlines()
deployed = _wget('deploy', opts, url, timeout=timeout) res = '\n'.join(deployed['msg'])
if cache: __salt__['file.remove'](tfile)
import logging
import salt.utils.systemd import salt.utils.odict as odict
log = logging.getLogger(__name__)
__virtualname__ = 'service'
if service[1]: if include_enabled: enabled_services.update({service[0]: sorted(service[1].split())}) continue if include_disabled: disabled_services.update({service[0]: []})
from __future__ import absolute_import import logging import os import datetime
import salt.utils from salt.exceptions import CommandExecutionError
import yaml import salt.ext.six as six from salt.ext.six.moves import range log = logging.getLogger(__name__)
args = args and list(args) or []
if self.subcmd == 'apply': self.subcmd_args = [args[0]] del args[0]
args.extend([ 'test' ])
self.args = args
for line in output.splitlines(): if not line: continue fact, value = _format_fact(line) if not fact: continue ret[fact] = value return ret
from __future__ import absolute_import import os
import salt.utils
__func_alias__ = { 'list_': 'list' }
__virtualname__ = 'autoruns'
import salt.utils.sdb
import os import glob import logging import time
from salt.exceptions import CommandExecutionError import salt.utils
__func_alias__ = { 'reload_': 'reload' }
VALID_SERVICE_DIRS = [ '/service', '/var/service', '/etc/service', ] SERVICE_DIR = None for service_dir in VALID_SERVICE_DIRS: if os.path.exists(service_dir): SERVICE_DIR = service_dir break
AVAIL_SVR_DIRS = []
__virtualname__ = 'service'
return False
ret = ava.difference(ena)
ret = ava.union(ena)
return name in _get_svc_list(name, 'ENABLED')
return name not in _get_svc_list(name, 'ENABLED')
if not available(name): return False
alias = get_svc_alias() if name in alias: log.error('This service is aliased, enable its alias instead') return False
svc_realpath = _get_svc_path(name)[0] down_file = os.path.join(svc_realpath, 'down')
try: os.symlink(svc_realpath, _service_path(name))
log.error('Unable to create symlink {0}'.format(down_file)) if not start: os.unlink(down_file) return False
if retcode_sv != 0: os.unlink(os.path.join([_service_path(name), name])) return False return True
if not enabled(name): return False
svc_realpath = _get_svc_path(name)[0] down_file = os.path.join(svc_realpath, 'down')
from __future__ import absolute_import, print_function import json import logging
from salt.exceptions import SaltInvocationError import salt.utils.http
import salt.utils
import logging import re import socket
return False
return [x for x in cmd['stdout'].split('\n') if check_ip(x)]
return [x for x in cmd['stdout'].split('\n') if check_ip(x)]
return SPF(domain, 'TXT', nameserver)
return SPF(sections[1][9:], 'SPF', nameserver)
continue
a = A aaaa = AAAA ns = NS spf = SPF mx = MX
if salt.utils.is_proxy() and 'proxy' in __opts__: return True return (False, 'The marathon execution module cannot be loaded: this only works in proxy minions.')
import os try: import spwd HAS_SPWD = True except ImportError: HAS_SPWD = False try: import pwd except ImportError:
import salt.utils from salt.exceptions import CommandExecutionError try: import salt.utils.pycrypto HAS_CRYPT = True except ImportError: HAS_CRYPT = False
__virtualname__ = 'shadow'
return ret
import copy import logging import json import os
import salt.utils from salt.exceptions import CommandExecutionError, SaltInvocationError
log = logging.getLogger(__name__)
__virtualname__ = 'dsc'
cmd = 'Install-Module -name "{0}" -Force'.format(name) no_ret = _pshell(cmd) return name in list_modules()
cmd = 'Uninstall-Module "{0}"'.format(name) no_ret = _pshell(cmd) return name not in list_modules()
if not os.path.exists(path): error = '"{0} not found.'.format(path) log.error(error) raise CommandExecutionError(error)
config = os.path.splitext(os.path.basename(path))[0]
cmd = '{0} '.format(path) cmd += '| Select-Object -Property FullName, Extension, Exists, ' \ '@{Name="LastWriteTime";Expression={Get-Date ($_.LastWriteTime) -Format g}}'
if ret.get('Exists'): log.info('DSC Compile Config: {0}'.format(ret)) return ret
if ret.get('Exists'): log.info('DSC Compile Config: {0}'.format(ret)) return ret
if not os.path.exists(config): error = '{0} not found.'.format(config) log.error(error) raise CommandExecutionError(error)
_pshell(cmd)
from __future__ import absolute_import import logging import re
import salt.ext.six as six
from salt.exceptions import SaltInvocationError import salt.utils
__virtualname__ = 'win_smtp_server'
for obj in objs: name = str(obj.Name).replace(prefix, '', 1) ret[name] = str(obj.LogModuleId)
settings = _normalize_server_settings(**settings)
new_settings = get_server_setting(settings=settings.keys(), server=server) failed_settings = dict()
for key in log_format_types: if str(format_id) == log_format_types[key]: return key _LOG.warning('Unable to determine log format.') return None
if not addresses: addresses = dict() _LOG.debug('Empty %s specified.', setting)
for address in addresses: formatted_addresses.append('{0}, {1}'.format(address.strip(), addresses[address].strip()))
if set(formatted_addresses) == set(current_addresses): _LOG.debug('%s already contains the provided addresses.', setting) return True
current_grant_by_default = _get_wmi_setting('IIsIPSecuritySetting', 'GrantByDefault', server)
import logging
import salt.utils.mac_utils from salt.exceptions import CommandExecutionError
import salt.utils
#pylint: disable=E0602
from __future__ import absolute_import import logging
import salt.utils.boto3 import salt.utils.compat import salt.utils
import os import stat import string import logging
import salt.utils from salt.exceptions import CommandExecutionError
__virtualname__ = 'partition'
__func_alias__ = { 'set_': 'set', 'list_': 'list', }
mode = 'partitions'
import logging import os import plistlib import re
import salt.utils import salt.utils.decorators as decorators import salt.ext.six as six
log = logging.getLogger(__name__)
__virtualname__ = 'service'
true_path = os.path.realpath(file_path) if not os.path.exists(true_path): continue
with salt.utils.fopen(file_path): plist = plistlib.readPlist(true_path)
return services[name]
return service
return service
return None
import os import logging
import salt.utils
import os import sys import time
import salt.utils import salt.key
import salt.ext.six as six
__func_alias__ = { 'list_': 'list' }
pki_dir = pki_dir.replace('minion', 'master')
if transport in ('zeromq', 'tcp'): key_dirs = _check_minions_directories(pki_dir) else: key_dirs = _check_minions_directories_raetkey(pki_dir)
continue
ret['retcode'] = int(not __salt__['ps.kill_pid'](pid))
import logging
from salt.exceptions import CommandExecutionError import salt.utils
__virtualname__ = 'virt'
#pylint: disable=E0602
import logging import json import salt.ext.six as six
try: import boto import boto.sqs logging.getLogger('boto').setLevel(logging.CRITICAL) HAS_BOTO = True except ImportError: HAS_BOTO = False
from __future__ import absolute_import import os import re
import salt.utils import salt.utils.decorators as decorators from salt.exceptions import CommandExecutionError
import salt.ext.six as six
import functools import glob import json import logging import os import shutil import subprocess import sys import time import traceback import base64 from salt.utils import vt
try: import pwd except ImportError: pass
__virtualname__ = 'cmd'
log = logging.getLogger(__name__)
if __pub_jid and python_shell is None: return True elif __opts__.get('cmd_safe', True) is False and python_shell is None: return True
if template not in salt.utils.templates.TEMPLATE_REGISTRY: raise CommandExecutionError( 'Attempted to render file paths with unavailable engine ' '{0}'.format(template) )
if not cwd: cwd = os.path.expanduser('~{0}'.format('' if not runas else runas))
if not os.access(cwd, os.R_OK): cwd = '/' if salt.utils.is_windows(): cwd = os.tempnam()[:3]
cwd = str(cwd)
stack = traceback.extract_stack(limit=2)
(cmd, cwd) = _render_cmd(cmd, cwd, template, saltenv, pillarenv, pillar_override)
if python_shell: cmd = 'chcp 437 > nul & ' + cmd
if kwargs['shell'] is True: kwargs['executable'] = shell kwargs['close_fds'] = True
ret['retcode'] = 1 return ret
err = ''
pass
with salt.utils.fopen(jid_file, 'w+b') as fn_: fn_.write(serial.dumps(jid_dict))
pass
sh_ = '/bin/sh' if os.path.isfile(os.path.join(root, 'bin/bash')): sh_ = '/bin/bash'
return None
cmd = '{0} | ConvertTo-Json -Depth 32'.format(cmd)
saltenv=saltenv, pillarenv=kwargs.get('pillarenv'), pillar_override=kwargs.get('pillar'), )
import json import logging
import salt.utils from salt.exceptions import CommandExecutionError
__func_alias__ = { 'list_': 'list' }
stdout = json.loads(result['stdout']) return stdout != {}
stdout = json.loads(result['stdout']) return stdout != {}
__virtualname__ = 'system'
import salt.utils from salt.ext.six import string_types from salt.exceptions import CommandExecutionError
from __future__ import absolute_import try: import pwd except ImportError: pass
import salt.ext.six as six import salt.utils from salt.exceptions import SaltInvocationError
__virtualname__ = 'shadow'
import os import re import stat import tempfile
import salt.utils from salt.utils import which as _which from salt.exceptions import SaltInvocationError
newaliases = _which('newaliases') if newaliases is not None: __salt__['cmd.run'](newaliases)
import os import logging
import salt.utils
__salt__['file.write']('{0}-make.conf'.format(os.path.join(cdir, jname)), 'WITH_PKGNG=yes')
_check_config_exists()
if is_jail(name): return '{0} already exists'.format(name)
make_pkgng_aware(name)
if is_jail(name): return 'Created jail {0}'.format(name)
if is_jail(name): return 'Looks like there was an issue deleteing jail \ {0}'.format(name)
return 'Looks like jail {0} has not been created'.format(name)
import inspect import logging import sys
import salt.minion import salt.utils from salt.defaults import DEFAULT_TARGET_DELIM from salt.ext.six import string_types
from __future__ import absolute_import import logging
import salt.utils
from __future__ import absolute_import import re
import salt.utils
result = __salt__['cmd.retcode']('tuned-adm off') if int(result) != 0: return False return True
import copy import os import re import logging from salt.ext import six try:
import salt.utils import salt.utils.itertools from salt.utils.decorators import which as _which
__virtualname__ = 'pkg'
if refresh: refresh_db()
if len(names) == 1: return ret[names[0]] return ret
available_version = latest_version
ret[pkg] = {'old': oldstate, 'new': state} return ret
if any([salt.utils.is_true(kwargs.get(x)) for x in ('removed', 'purge_desired')]): return {}
if repo['uri'] not in repos: repos[repo['uri']] = [repo]
refresh_db() return ret
import logging
import salt.utils import salt.utils.s3
from __future__ import absolute_import
import datetime import json import logging import os import re import traceback import shutil import types
from salt.modules import cmdmod from salt.exceptions import CommandExecutionError, SaltInvocationError import salt.utils import salt.utils.odict
import salt.ext.six as six
__virtualname__ = 'docker'
kwargs['timeout'] = timeout
kwargs['version'] = 'auto'
kwargs['base_url'] = os.environ.get('DOCKER_HOST')
if inspect: for container in containers: container_id = container.get('Id') if container_id: inspect = _get_container_infos(container_id) container['detail'] = inspect.copy()
byte = response.read(4096) fic.write(byte)
_valid(status, comment='Kill signal \'{0}\' successfully' ' sent to the container \'{1}\''.format(signal, container), id_=container)
valid_states = [ 'Download complete', 'Already exists', ]
import salt.utils import socket
import logging import time
import logging
import salt.ext.six as six if six.PY3: import ipaddress else: import salt.ext.ipaddress as ipaddress
def long_range(start, end): while start < end: yield start start += 1
import salt.utils
log = logging.getLogger(__name__)
for item in _CREATE_OPTIONS_REQUIRED[set_type]: if item not in kwargs: return 'Error: {0} is a required argument'.format(item)
if 'family' in _CREATE_OPTIONS[set_type]: cmd = '{0} family {1}'.format(cmd, ipset_family)
cmd = '{0} add -exist {1} {2}'.format(_ipset_cmd(), set, cmd) out = __salt__['cmd.run'](cmd, python_shell=False)
return False
return False
return False
if ':' in item: key, value = item.split(':', 1) setinfo[key] = value[1:]
from __future__ import absolute_import, generators, with_statement import time import logging import salt import os import os.path
import salt.utils from salt.exceptions import CommandExecutionError
import salt.ext.six as six
__func_alias__ = { 'reload_': 'reload' }
_current_statement = None _current_option = None _current_parameter = None _current_parameter_value = None
for i in params: if _is_simple_type(i): _current_parameter = SimpleParameter(i) else: _current_parameter = TypedParameter() _parse_typed_parameter(i) _current_option.add_parameter(_current_parameter)
return full_version[:3]
version_line_index = 0 version_column_index = 1 line = lines[version_line_index].split()[version_column_index] return _format_return_data(0, stdout=line)
import collections
import os import yaml import salt.ext.six as six
import salt.pillar import salt.utils from salt.defaults import DEFAULT_TARGET_DELIM from salt.exceptions import CommandExecutionError
if args: return item(*args)
data = salt.utils.alias_function(items, 'data')
fetch = get
import logging
import salt.utils import salt.utils.decorators as decorators
__func_alias__ = { 'list_nictags': 'list' }
__virtualname__ = 'nictagadm'
import logging
import salt.utils
import salt.utils import logging import re
__virtualname__ = 'timezone'
return False
return 'localtime'
return False
from __future__ import absolute_import import logging import pipes
import salt.utils
from __future__ import absolute_import import os import logging import copy
try: import salt.cloud HAS_SALTCLOUD = True except ImportError: HAS_SALTCLOUD = False
import salt.ext.six as six
info = next(six.itervalues(next(six.itervalues(next(six.itervalues(info))))))
from __future__ import absolute_import import fnmatch import logging import os import pprint
import yaml import salt.ext.six as six
import salt.utils from salt.exceptions import SaltInvocationError
srcinfo.append(__salt__['cp.cache_file'](pkg_src, saltenv))
if len(ret) == 1 and not pkg_glob: try: return next(six.itervalues(ret)) except StopIteration: return '' return ret
import os import re import logging
log = logging.getLogger(__name__)
uninstall_python(python, runas=runas) return False
import time import logging
import salt.crypt import salt.payload import salt.transport import salt.utils.args from salt.exceptions import SaltReqTimeoutError
if len(returned_minions) < 1: return {} end_loop = True
from __future__ import absolute_import import re import logging
import sys import contextlib import os from salt.ext.six.moves import range from salt.ext.six.moves import map
HAS_IMPORTLIB = False
from salt.exceptions import CommandExecutionError import salt.utils import salt.modules.cmdmod
__virtualname__ = 'virt'
xapi_uri = 'httpu:///var/run/xend/xen-api.sock'
except Exception: return __salt__['cmd.run']( '{0} vcpu-pin {1} {2} {3}'.format(_get_xtool(), vm_, vcpu, cpus), python_shell=False)
return False
return 'xenstore' in __salt__['cmd.run'](__grains__['ps'])
cputime_percent = (1.0e-7 * cputime / host_cpus) / vcpus
from __future__ import absolute_import, print_function import errno import logging import os import shutil import tempfile import time import re import traceback import functools
import salt.utils from salt.exceptions import SaltInvocationError
cmd = 'rpm --import {0}'.format(pkg_pub_key_file) __salt__['cmd.run'](cmd, runas=runas, use_vt=True)
proc.sendline(phrase)
time.sleep(0.5)
urllib3_logger = logging.getLogger('urllib3') urllib3_logger.setLevel(logging.WARNING)
dev['hash'] = all_devices['hash'] log.info('Found device %s in Zenoss', device) return dev
from __future__ import absolute_import import re import logging
import salt.utils
from __future__ import absolute_import, print_function import os import copy import math import random import logging import operator import collections import json from functools import reduce
import salt.utils.compat from salt.utils.odict import OrderedDict import yaml import salt.ext.six as six
import salt.utils import salt.utils.dictupdate from salt.defaults import DEFAULT_TARGET_DELIM from salt.exceptions import SaltException
__grains__ = {}
__outputter__ = { 'items': 'nested', 'item': 'nested', 'setval': 'nested', }
_infinitedict = lambda: collections.defaultdict(_infinitedict)
_new_value_type = 'simple' if isinstance(val, dict): _new_value_type = 'complex' elif isinstance(val, list): _new_value_type = 'complex'
fetch = get
import logging
import salt.utils try: import wmi except ImportError: pass
interface = interface.split('\\') interface = ''.join(interface)
if servers is False: return False
try: if servers[index - 1] == ip: return True except IndexError: pass
if ip in servers: rm_dns(ip, interface)
interface = interface.split('\\') interface = ''.join(interface)
import os import re import logging
import salt.utils
from __future__ import absolute_import import json import logging
if not channel.startswith('#'): channel = '#{0}'.format(channel)
result = salt.utils.slack.query(function='message', api_key=api_key, method='POST', header_dict={'Content-Type': 'application/x-www-form-urlencoded'}, data=_urlencode(parameters), opts=__opts__)
database: image: mongo:3.0 command: mongod --smallfiles --quiet --logpath=/dev/null '
from __future__ import print_function from __future__ import absolute_import import re import json from salt.utils.odict import OrderedDict from salt.utils import fopen as _fopen
if not hasattr(value, 'iteritems'): self._uncomment_if_commented(key)
import re import logging
import salt.utils
import salt.ext.six as six
__virtualname__ = 'pecl'
import os
import salt.utils
__virtualname__ = 'pam'
from __future__ import absolute_import import copy import logging import os import re
import salt.utils import salt.utils.decorators as decorators from salt.exceptions import CommandExecutionError, MinionError
import salt.ext.six as six
__virtualname__ = 'pkg'
return False
if refresh: refresh_db()
continue
available_version = salt.utils.alias_function(latest_version, 'available_version')
if any([salt.utils.is_true(kwargs.get(x)) for x in ('removed', 'purge_desired')]): return {}
pkg, ver = re.split('[; ]', line, 1)[0].rsplit('-', 1)
repo = kwargs.get('repo', '') if not fromrepo and repo: fromrepo = repo
return {}
import logging import socket import time
import salt.utils import salt.utils.network import salt.utils.validate.net from salt.exceptions import ( CommandExecutionError, SaltInvocationError ) from salt.ext.six.moves import range
log = logging.getLogger(__name__)
__virtualname__ = 'ip'
import binascii import hashlib import logging import os import re import subprocess
import salt.utils import salt.utils.files import salt.utils.decorators as decorators from salt.exceptions import ( SaltInvocationError, CommandExecutionError, ) from salt.ext.six.moves import range
if enc in ['e', 'ecdsa']: return 'ecdsa-sha2-nistp256' return enc
continue
search = re.search(linere, line) if not search: continue
continue
options = opts.split(',')
if 'Key not removed' in rval: return 'Key not removed' elif 'Key removed' in rval: return 'Key removed' else: return 'Key not present'
full = _get_config_file(user, config)
if not os.path.isfile(full): return 'Authorized keys file {0} not present'.format(full)
with salt.utils.fopen(full, 'r') as _fh: for line in _fh: if line.startswith('#'): lines.append(line) continue
search = re.search(linere, line) if not search: continue
lines.append(line) continue
if pkey == key: continue
with salt.utils.fopen(full, 'w') as _fh: _fh.writelines(lines)
rcon = salt.utils.which('restorecon') if rcon: cmd = [rcon, fconfig] subprocess.call(cmd)
need_dash_t = ('CentOS-5',)
rm_known_host(user, hostname, config=config)
ssh_dir = os.path.dirname(full) if user: uinfo = __salt__['user.info'](user)
if user: os.chown(ssh_dir, uinfo['uid'], uinfo['gid']) os.chmod(ssh_dir, 0o700)
user = [user]
continue
userKeys += ['id_rsa.pub', 'id_dsa.pub', 'id_ecdsa.pub', 'id_ed25519.pub']
userKeys += ['id_rsa', 'id_dsa', 'id_ecdsa', 'id_ed25519']
keyname = key fn_ = '{0}/.ssh/{1}'.format(userinfo['home'], key)
_keys = {} for key in keys: if keys[key]: _keys[key] = keys[key] return _keys
salt.utils.files.process_read_exception(exc, key)
from __future__ import absolute_import, print_function import datetime import copy import textwrap import difflib import logging import tempfile import os import pipes import time import shutil import re import random
import salt import salt.utils.odict import salt.utils import salt.utils.dictupdate import salt.utils.network from salt.exceptions import CommandExecutionError, SaltInvocationError import salt.utils.cloud import salt.config
import salt.ext.six as six
log = logging.getLogger(__name__)
__func_alias__ = { 'list_': 'list', 'ls_': 'ls' }
lxc_init_interface['clone_from'] = _cloud_get(clone_from, None) if lxc_init_interface['clone_from'] is not None: break
overrides = salt.utils.clean_kwargs(**copy.deepcopy(kwargs)) profile_match = salt.utils.dictupdate.update( copy.deepcopy(profile_match), overrides ) return profile_match
nic_opts = {}
if nic and isinstance(nic, (six.string_types, dict)): nicp = get_network_profile(nic) else: nicp = {} if DEFAULT_NIC not in nicp: nicp[DEFAULT_NIC] = {}
gateway_set = True
for idx in ['lxc.cgroup.memory.limit_in_bytes']: if not default_data.get(idx): self._filter_data(idx)
with salt.utils.fopen(self.path, 'w') as fic: fic.write(content) fic.flush()
ntf = tempfile.NamedTemporaryFile() ntf.write(self.as_string()) ntf.flush() return ntf
if kw_overrides_match is _marker: return profile_match return kw_overrides_match
for param in ('path', 'image', 'vgname', 'template'): kwargs.pop(param, None)
changes_dict = {'init': []} changes = changes_dict.get('init')
if kw_overrides_match is _marker: return profile_match return kw_overrides_match
download_template_deps = ('dist', 'release', 'arch')
if kw_overrides_match is None: return profile_match return kw_overrides_match
if backing in ('aufs', 'dir', 'overlayfs', 'btrfs'): lvname = vgname = None
return _after_ignition_network_profile(cmd, ret, name, network_profile, path, nic_opts)
if kw_overrides_match is None: return profile_match return kw_overrides_match
return _after_ignition_network_profile(cmd, ret, name, network_profile, path, nic_opts)
return start(name, path=path)
unfreeze(name, path=path)
remove = salt.utils.alias_function(destroy, 'remove')
if not _exists: _exists = name in ls_(cache=False, path=path) return _exists
ret['size'] = size.splitlines()[-1].split()[1]
ret['changes'] = {}
if result['retcode'] in (0, 2): __context__[k] = ret = not result['retcode']
time.sleep(5)
if ret: run(name, 'touch \'{0}\''.format(SEED_MARKER), path=path, python_shell=False)
__context__['cmd.run_chroot.func'] = __salt__['cmd.run'] ret = __salt__['cmd.run_chroot'](rootfs, cmd, stdin=stdin, python_shell=python_shell, output_loglevel=output_loglevel, ignore_retcode=ignore_retcode)
return None
if kw_overrides_match is _marker: return profile_match return kw_overrides_match
import logging
from salt.ext import six
try: import win32com.client import pythoncom
import salt.utils
search_string = '' search_params = []
pythoncom.CoInitialize()
wua_session = win32com.client.Dispatch('Microsoft.Update.Session')
wua_searcher = wua_session.CreateUpdateSearcher()
if categories is None: category_match = True else: for category in update.Categories: if category.Name in categories: category_match = True
if severities is None: severity_match = True else: if update.MsrcSeverity in severities: severity_match = True
results['Total'] += 1
if not update.IsDownloaded and not update.IsInstalled: results['Available'] += 1
if update.IsDownloaded and not update.IsInstalled: results['Downloaded'] += 1
if update.IsInstalled: results['Installed'] += 1
for category in update.Categories: if category.Name in results['Categories']: results['Categories'][category.Name] += 1 else: results['Categories'][category.Name] = 1
if update.MsrcSeverity: if update.MsrcSeverity in results['Severity']: results['Severity'][update.MsrcSeverity] += 1 else: results['Severity'][update.MsrcSeverity] = 1
results[guid]['Severity'] = str(update.MsrcSeverity)
results[guid]['NeedsReboot'] = str(update.RebootRequired)
rb = {0: 'Never Requires Reboot', 1: 'Always Requires Reboot', 2: 'Can Require Reboot'} results[guid]['RebootBehavior'] = rb[update.InstallationBehavior.RebootBehavior]
results[guid]['Categories'] = [] for category in update.Categories: results[guid]['Categories'].append(category.Name)
salt '*' win_wua.list_update 12345678-abcd-1234-abcd-1234567890ab
salt '*' win_wua.list_update KB3030298
salt '*' win_wua.list_update 'Microsoft Camera Codec Pack'
pythoncom.CoInitialize()
wua_session = win32com.client.Dispatch('Microsoft.Update.Session')
wua_searcher = wua_session.CreateUpdateSearcher()
wua_found = win32com.client.Dispatch('Microsoft.Update.UpdateColl')
search_string = 'UpdateID=\'{0}\''.format(name)
if found_using_guid: for update in wua_search_result.Updates: wua_found.Add(update) else: for update in wua_search_result.Updates: if name in update.Title: wua_found.Add(update)
salt '*' win_wua.list_updates
salt '*' win_wua.list_updates categories=['Critical Updates','Drivers']
salt '*' win_wua.list_updates categories=['Security Updates'] severities=['Critical']
salt '*' win_wua.list_updates severities=['Critical']
salt '*' win_wua.list_updates summary=True
salt '*' win_wua.list_updates categories=['Feature Packs','Windows 8.1'] summary=True
updates = _wua_search(software_updates=software, driver_updates=drivers, skip_installed=not installed)
updates = _filter_list_by_category(updates=updates, categories=categories)
if not updates: return 'No updates found. Check software and drivers parameters. One must be true.'
if guid is None: return "No GUID Specified"
pythoncom.CoInitialize()
wua_session = win32com.client.Dispatch('Microsoft.Update.Session') wua_session.ClientApplicationID = 'Salt: Install Update'
wua_searcher = wua_session.CreateUpdateSearcher() wua_download_list = win32com.client.Dispatch('Microsoft.Update.UpdateColl') wua_downloader = wua_session.CreateUpdateDownloader()
if wua_download_list.Count == 0: log.debug('No updates to download') ret['Success'] = False ret['Message'] = 'No updates to download' return ret
log.debug('Downloading...') wua_downloader.Updates = wua_download_list
if guid is None: return 'No GUID Specified'
pythoncom.CoInitialize()
wua_session = win32com.client.Dispatch('Microsoft.Update.Session') wua_session.ClientApplicationID = 'Salt: Install Update'
if wua_download_list.Count == 0: log.debug('No updates to download') else: log.debug('Downloading...') wua_downloader.Updates = wua_download_list
for update in wua_search_result.Updates: if update.IsDownloaded: log.debug(u'To be installed: {0}'.format(update.Title)) wua_install_list.Add(update)
log.debug('No updates to install') ret['Success'] = False ret['Message'] = 'No Updates to install' return ret
try: result = wua_installer.Install()
ret['Success'] = False ret['Result'] = format(error)
pythoncom.CoInitialize()
obj_au = win32com.client.Dispatch('Microsoft.Update.AutoUpdate')
obj_au_settings = obj_au.Settings
obj_sm = win32com.client.Dispatch('Microsoft.Update.ServiceManager')
obj_sm.ClientApplicationID = "My App"
try: obj_sm.AddService2('7971f918-a847-4430-9279-4a52d1efe18d', 7, '') ret['msupdate'] = msupdate except Exception as error:
ret['Comment'] = "Failed with failure code: {0}".format(exc[5]) ret['Success'] = False
if _get_msupdate_status(): try: obj_sm.RemoveService('7971f918-a847-4430-9279-4a52d1efe18d') ret['msupdate'] = msupdate except Exception as error:
ret['Comment'] = "Failed with failure code: {0}".format(exc[5]) ret['Success'] = False
pythoncom.CoInitialize()
obj_au = win32com.client.Dispatch('Microsoft.Update.AutoUpdate')
obj_au_settings = obj_au.Settings
obj_sm = win32com.client.Dispatch('Microsoft.Update.ServiceManager')
col_services = obj_sm.Services
for service in col_services: if service.name == 'Microsoft Update': return True
pythoncom.CoInitialize()
obj_sys = win32com.client.Dispatch('Microsoft.Update.SystemInfo')
import logging import re
from salt.utils.decorators import depends import salt.utils
from __future__ import absolute_import import logging
import salt.utils
import os
import salt.utils import salt.utils.decorators as decorators from salt.exceptions import CommandExecutionError
__virtualname__ = 'grub'
return '/boot/grub/menu.lst'
from __future__ import absolute_import, generators, print_function, with_statement import re import logging
import salt.utils
url += '?auto' try: response = _urlopen(url, timeout=timeout).read().splitlines() except URLError: return 'error'
for line in response: splt = line.split(':', 1) splt[0] = splt[0].strip() splt[1] = splt[1].strip()
return ret
from __future__ import absolute_import import logging
import salt.utils
log = logging.getLogger(__name__)
mnt_image = salt.utils.alias_function(mount_image, 'mnt_image')
import logging
from salt.ext.six import string_types from salt.exceptions import get_error_message as _get_error_message
try: import pymongo HAS_MONGODB = True except ImportError: HAS_MONGODB = False
from __future__ import absolute_import import os import re import logging
import salt.utils from salt.utils import which as _which from salt.exceptions import CommandNotFoundError, CommandExecutionError
import salt.ext.six as six
log = logging.getLogger(__name__)
__virtualname__ = 'mount'
if salt.utils.is_windows(): return (False, 'The mount module cannot be loaded: not a POSIX-like system.') return True
compatibility_keys = ('device', 'name', 'fstype', 'opts', 'dump', 'pass')
return True
if isinstance(opts, list): opts = ','.join(opts)
entry = _fstab_entry(**entry_args) try: criteria = entry.pick(match_on)
if not os.path.isfile(config): raise CommandExecutionError('Bad config file "{0}"'.format(config))
ret = 'present' if entry.match(line): lines.append(line) else: ret = 'change' lines.append(str(entry))
if ret is None: lines.append(str(entry)) ret = 'new'
ofile.writelines(lines)
__salt__['cmd.run']('automount -cv') return True
if isinstance(opts, list): opts = ','.join(opts) lines = [] change = False present = False automaster_file = "/etc/auto_master"
lines.append(line) continue
lines.append(line) continue
lines.append(line) continue
ofile.writelines(lines)
return 'present'
newline = ( '{0}\t{1}\t{2}\n'.format( name, type_opts, device_fmt)
ofile.writelines(lines)
continue
continue
continue
if 'defaults' in opts and __grains__['os'] in ['MacOS', 'Darwin']: opts = None
if not cmd_path: return False elif not _which('ldd'): raise CommandNotFoundError('ldd')
from __future__ import absolute_import import logging import salt.utils
__virtualname__ = 'service'
try: import netaddr HAS_NETADDR = True except ImportError as e: HAS_NETADDR = False
from __future__ import absolute_import import sqlite3 import os
self.init_queries.append("CREATE TABLE inspector_ignored (path CHAR(4096))") self.init_queries.append("CREATE TABLE inspector_allowed (path CHAR(4096))")
from __future__ import absolute_import, print_function import os import sys from subprocess import Popen, PIPE, STDOUT
from salt.modules.inspectlib.dbhandle import DBHandle from salt.modules.inspectlib.exceptions import (InspectorSnapshotException) import salt.utils from salt.utils import fsutils from salt.utils import reinit_crypto
data = dict()
if not data[pkg_name]: data.pop(pkg_name)
try: os.kill(int(open(pidfile).read().strip()), 0) sys.exit(1) except Exception as ex: pass
try: if os.fork() > 0: reinit_crypto() sys.exit(0) else: reinit_crypto() except OSError as ex: sys.exit(1)
from __future__ import absolute_import import os import time import logging
import salt.utils.network from salt.modules.inspectlib.dbhandle import DBHandle from salt.modules.inspectlib.exceptions import (InspectorQueryException, SIException)
return users
if os_family == 'suse': PATTERNS = 'pkg.list_installed_patterns' elif os_family == 'redhat': PATTERNS = 'pkg.group_list' else: PATTERNS = None
if 'packages' not in excludes: data['packages'] = __salt__['pkg.list_pkgs']()
if 'repositories' not in excludes: repos = __salt__['pkg.list_repos']() if repos: data['repositories'] = repos
import salt.utils
(status, ring, pending, node) = line.split()
import salt.utils import salt.utils.decorators as decorators from salt.ext import six from salt.exceptions import CommandExecutionError from salt.utils import locales
__virtualname__ = 'user'
break
break
pass
return True
if __grains__['os_family'] not in ('Debian',): return False
log.debug( 'While the userdel exited with code 12, this is a known bug on ' 'debian based distributions. See http://goo.gl/HH3FzT' ) return True
gecos_field = data.pw_gecos.split(',', 3) while len(gecos_field) < 4: gecos_field.append('')
import os import logging
import salt.utils
__func_alias__ = { 'set_': 'set' }
if value == 'True': new_line = key elif value == 'False': new_line = '' else: new_line = '{0} {1}'.format(key, value)
import hashlib import random
import salt.utils.pycrypto from salt.exceptions import SaltInvocationError
__virtualname__ = 'random'
#pylint: disable=E0602
from __future__ import absolute_import import logging
import salt.utils.boto3 import salt.utils.compat import salt.utils
if IdentityPoolName is not None and IdentityPoolName != request_params.get('IdentityPoolName'): request_params['IdentityPoolName'] = IdentityPoolName
current_val = request_params.pop('DeveloperProviderName', None) if current_val is None and DeveloperProviderName is not None: request_params['DeveloperProviderName'] = DeveloperProviderName
from __future__ import absolute_import import logging
__virtualname__ = 'group'
else: retcode = 0
from __future__ import absolute_import import difflib import os import yaml
ret = {} ret['result'] = False ret['comment'] = 'Event module not available. Beacon add failed.' return ret
ret['comment'] = 'Event module not available. Beacon add failed.'
ret['comment'] = 'Event module not available. Beacon add failed.'
ret['comment'] = 'Event module not available. Beacon add failed.'
ret['comment'] = 'Event module not available. Beacons enable job failed.'
ret['comment'] = 'Event module not available. Beacons enable job failed.'
ret['comment'] = 'Event module not available. Beacon enable job failed.'
ret['comment'] = 'Event module not available. Beacon disable job failed.'
from __future__ import absolute_import import os import re
import salt.utils from salt._compat import subprocess
__virtualname__ = 'jail'
continue
import salt.utils
if module not in get_modules(): log.error('Module {0} not available'.format(module)) return False
if self._session_expired: raise ForceRetryError("Retry on session loss at top")
if self.cancelled: raise CancelledError("Semaphore cancelled")
children = self.client.get_children(self.path, self._watch_lease_change)
if len(children) < self.max_leases: self.client.create(self.create_path, self.data, ephemeral=self.ephemeral_lease)
if self.client.exists(self.create_path): self.is_acquired = True else: self.is_acquired = False
return self.is_acquired
if force: SEMAPHORE_MAP[path].assured_path = True
if zk_hosts is not None and path not in SEMAPHORE_MAP: zk = _get_zk_conn(zk_hosts) SEMAPHORE_MAP[path] = _Semaphore(zk, path, identifier, max_leases=max_concurrency, ephemeral_lease=ephemeral_lease)
import logging
from salt.exceptions import SaltInvocationError
try: from celery import Celery from celery.exceptions import TimeoutError HAS_CELERY = True except ImportError: HAS_CELERY = False
import glob import shutil import logging import os
import salt.utils from salt.exceptions import CommandExecutionError, SaltInvocationError from salt.ext.six import string_types
__virtualname__ = 'virtualenv'
if clear is True: cmd.append('--clear') if system_site_packages is True: cmd.append('--system-site-packages')
cmd.append(path)
ret = __salt__['cmd.run_all'](cmd, runas=user, python_shell=False) if ret['retcode'] != 0: return ret
if (pip or distribute) and not os.path.exists(venv_setuptools): _install_script( 'https://bitbucket.org/pypa/setuptools/raw/default/ez_setup.py', path, venv_python, user, saltenv=saltenv, use_vt=use_vt )
for fpath in glob.glob(os.path.join(path, 'distribute-*.tar.gz*')): os.unlink(fpath)
return ret
from __future__ import absolute_import import salt.utils from datetime import datetime import logging import time
try: import pythoncom import win32com.client HAS_DEPENDENCIES = True except ImportError: HAS_DEPENDENCIES = False from salt.ext.six.moves import range
__virtualname__ = 'task'
TASK_ACTION_EXEC = 0 TASK_ACTION_COM_HANDLER = 5 TASK_ACTION_SEND_EMAIL = 6 TASK_ACTION_SHOW_MESSAGE = 7
TASK_COMPATIBILITY_AT = 0 TASK_COMPATIBILITY_V1 = 1 TASK_COMPATIBILITY_V2 = 2 TASK_COMPATIBILITY_V3 = 3
TASK_VALIDATE_ONLY = 0x1 TASK_CREATE = 0x2 TASK_UPDATE = 0x4 TASK_CREATE_OR_UPDATE = 0x6 TASK_DISABLE = 0x8 TASK_DONT_ADD_PRINCIPAL_ACE = 0x10 TASK_IGNORE_REGISTRATION_TRIGGERS = 0x20
TASK_INSTANCES_PARALLEL = 0 TASK_INSTANCES_QUEUE = 1 TASK_INSTANCES_IGNORE_NEW = 2 TASK_INSTANCES_STOP_EXISTING = 3
TASK_LOGON_NONE = 0 TASK_LOGON_PASSWORD = 1 TASK_LOGON_S4U = 2 TASK_LOGON_INTERACTIVE_TOKEN = 3 TASK_LOGON_GROUP = 4 TASK_LOGON_SERVICE_ACCOUNT = 5 TASK_LOGON_INTERACTIVE_TOKEN_OR_PASSWORD = 6
TASK_RUNLEVEL_LUA = 0 TASK_RUNLEVEL_HIGHEST = 1
TASK_STATE_UNKNOWN = 0 TASK_STATE_DISABLED = 1 TASK_STATE_QUEUED = 2 TASK_STATE_READY = 3 TASK_STATE_RUNNING = 4
TASK_TRIGGER_EVENT = 0 TASK_TRIGGER_TIME = 1 TASK_TRIGGER_DAILY = 2 TASK_TRIGGER_WEEKLY = 3 TASK_TRIGGER_MONTHLY = 4 TASK_TRIGGER_MONTHLYDOW = 5 TASK_TRIGGER_IDLE = 6 TASK_TRIGGER_REGISTRATION = 7 TASK_TRIGGER_BOOT = 8 TASK_TRIGGER_LOGON = 9 TASK_TRIGGER_SESSION_STATE_CHANGE = 11
pythoncom.CoInitialize() task_service = win32com.client.Dispatch("Schedule.Service") task_service.Connect()
task_folder = task_service.GetFolder(location) tasks = task_folder.GetTasks(0)
pythoncom.CoInitialize() task_service = win32com.client.Dispatch("Schedule.Service") task_service.Connect()
task_folder = task_service.GetFolder(location) folders = task_folder.GetFolders(0)
pythoncom.CoInitialize() task_service = win32com.client.Dispatch("Schedule.Service") task_service.Connect()
task_folder = task_service.GetFolder(location) task_definition = task_folder.GetTask(name).Definition triggers = task_definition.Triggers
pythoncom.CoInitialize() task_service = win32com.client.Dispatch("Schedule.Service") task_service.Connect()
task_folder = task_service.GetFolder(location) task_definition = task_folder.GetTask(name).Definition actions = task_definition.Actions
if name in list_tasks(location) and not force: return '{0} already exists'.format(name)
pythoncom.CoInitialize() task_service = win32com.client.Dispatch("Schedule.Service") task_service.Connect()
task_definition = task_service.NewTask(0)
edit_task(task_definition=task_definition, user_name=user_name, password=password)
add_action(task_definition=task_definition, **kwargs)
add_trigger(task_definition=task_definition, **kwargs)
task_folder = task_service.GetFolder(location)
_save_task_definition(name=name, task_folder=task_folder, task_definition=task_definition, user_name=task_definition.Principal.UserID, password=password, logon_type=task_definition.Principal.LogonType)
if name in list_tasks(location): return True else: return False
if name in list_tasks(location): return '{0} already exists'.format(name)
pythoncom.CoInitialize() task_service = win32com.client.Dispatch("Schedule.Service") task_service.Connect()
if xml_path: xml_text = xml_path
task_folder = task_service.GetFolder(location)
try: task_folder.RegisterTask(name, xml_text, TASK_CREATE, user_name, password, logon_type)
if name in list_tasks(location): return True else: return False
if name in list_folders(location): return '{0} already exists'.format(name)
pythoncom.CoInitialize() task_service = win32com.client.Dispatch("Schedule.Service") task_service.Connect()
task_folder = task_service.GetFolder(location) task_folder.CreateFolder(name)
if name in list_folders(location): return True else: return False
save_definition = False if kwargs.get('task_definition', False): task_definition = kwargs.get('task_definition') else: save_definition = True
if not name: return 'Required parameter "name" not passed'
if name in list_tasks(location):
pythoncom.CoInitialize() task_service = win32com.client.Dispatch("Schedule.Service") task_service.Connect()
task_folder = task_service.GetFolder(location)
task_definition = task_folder.GetTask(name).Definition
return '{0} not found'.format(name)
if save_definition: task_definition.RegistrationInfo.Author = 'Salt Minion' task_definition.RegistrationInfo.Source = "Salt Minion Daemon"
if enabled is not None: task_definition.Settings.Enabled = enabled if hidden is not None: task_definition.Settings.Hidden = hidden
if run_if_idle is not None: task_definition.Settings.RunOnlyIfIdle = run_if_idle
if ac_only is not None: task_definition.Settings.DisallowStartIfOnBatteries = ac_only if stop_if_on_batteries is not None: task_definition.Settings.StopIfGoingOnBatteries = stop_if_on_batteries if wake_to_run is not None: task_definition.Settings.WakeToRun = wake_to_run
if save_definition: return _save_task_definition(name=name, task_folder=task_folder, task_definition=task_definition, user_name=user_name, password=password, logon_type=task_definition.Principal.LogonType)
if name not in list_tasks(location): return '{0} not found in {1}'.format(name, location)
pythoncom.CoInitialize() task_service = win32com.client.Dispatch("Schedule.Service") task_service.Connect()
task_folder = task_service.GetFolder(location)
if name not in list_tasks(location): return True else: return False
if name not in list_folders(location): return '{0} not found in {1}'.format(name, location)
pythoncom.CoInitialize() task_service = win32com.client.Dispatch("Schedule.Service") task_service.Connect()
task_folder = task_service.GetFolder(location)
task_folder.DeleteFolder(name, 0)
if name not in list_folders(location): return True else: return False
if name not in list_tasks(location): return '{0} not found in {1}'.format(name, location)
pythoncom.CoInitialize() task_service = win32com.client.Dispatch("Schedule.Service") task_service.Connect()
task_folder = task_service.GetFolder(location) task = task_folder.GetTask(name)
if name not in list_tasks(location): return '{0} not found in {1}'.format(name, location)
pythoncom.CoInitialize() task_service = win32com.client.Dispatch("Schedule.Service") task_service.Connect()
task_folder = task_service.GetFolder(location) task = task_folder.GetTask(name)
if task.State == TASK_STATE_RUNNING: return 'Task already running'
if name not in list_tasks(location): return '{0} not found in {1}'.format(name, location)
pythoncom.CoInitialize() task_service = win32com.client.Dispatch("Schedule.Service") task_service.Connect()
task_folder = task_service.GetFolder(location) task = task_folder.GetTask(name)
if name not in list_tasks(location): return '{0} not found in {1}'.format(name, location)
pythoncom.CoInitialize() task_service = win32com.client.Dispatch("Schedule.Service") task_service.Connect()
task_folder = task_service.GetFolder(location) task = task_folder.GetTask(name)
if name not in list_tasks(location): return '{0} not found in {1}'.format(name, location)
pythoncom.CoInitialize() task_service = win32com.client.Dispatch("Schedule.Service") task_service.Connect()
task_folder = task_service.GetFolder(location) task = task_folder.GetTask(name)
if not name: return 'Required parameter "name" not passed'
if name in list_tasks(location):
pythoncom.CoInitialize() task_service = win32com.client.Dispatch("Schedule.Service") task_service.Connect()
task_folder = task_service.GetFolder(location)
task_definition = task_folder.GetTask(name).Definition
return '{0} not found'.format(name)
if kwargs.get('server', False): task_action.Server = kwargs.get('server') else: return 'Required parameter "server" not found'
if save_definition: return _save_task_definition(name=name, task_folder=task_folder, task_definition=task_definition, user_name=task_definition.Principal.UserID, password=None, logon_type=task_definition.Principal.LogonType)
pythoncom.CoInitialize() task_service = win32com.client.Dispatch("Schedule.Service") task_service.Connect()
task_folder = task_service.GetFolder(location) task_definition = task_folder.GetTask(name).Definition actions = task_definition.Actions
return _save_task_definition(name=name, task_folder=task_folder, task_definition=task_definition, user_name=task_definition.Principal.UserID, password=None, logon_type=task_definition.Principal.LogonType)
if start_date: date_format = _get_date_time_format(start_date) if date_format: dt_obj = datetime.strptime(start_date, date_format) else: return 'Invalid start_date' else: dt_obj = datetime.now()
if not name: return 'Required parameter "name" not passed'
if name in list_tasks(location):
pythoncom.CoInitialize() task_service = win32com.client.Dispatch("Schedule.Service") task_service.Connect()
task_folder = task_service.GetFolder(location)
task_definition = task_folder.GetTask(name).Definition
return '{0} not found'.format(name)
trigger = task_definition.Triggers.Create(trigger_types[trigger_type])
elif trigger_types[trigger_type] == TASK_TRIGGER_DAILY: trigger.Id = 'Daily_ID1' trigger.DaysInterval = kwargs.get('days_interval', 1)
elif trigger_types[trigger_type] == TASK_TRIGGER_IDLE: trigger.Id = 'OnIdle_ID1'
elif trigger_types[trigger_type] == TASK_TRIGGER_REGISTRATION: trigger.Id = 'OnTaskCreation_ID1'
elif trigger_types[trigger_type] == TASK_TRIGGER_BOOT: trigger.Id = 'OnBoot_ID1'
elif trigger_types[trigger_type] == TASK_TRIGGER_LOGON: trigger.Id = 'OnLogon_ID1'
if save_definition: return _save_task_definition(name=name, task_folder=task_folder, task_definition=task_definition, user_name=task_definition.Principal.UserID, password=None, logon_type=task_definition.Principal.LogonType)
if name not in list_tasks(location): return '{0} not found in {1}'.format(name, location)
pythoncom.CoInitialize() task_service = win32com.client.Dispatch("Schedule.Service") task_service.Connect()
task_folder = task_service.GetFolder(location) task_definition = task_folder.GetTask(name).Definition triggers = task_definition.Triggers
return _save_task_definition(name=name, task_folder=task_folder, task_definition=task_definition, user_name=task_definition.Principal.UserID, password=None, logon_type=task_definition.Principal.LogonType)
#pylint: disable=E0602
import logging
exists = conn.describe_stacks(name) log.debug('Stack {0} exists.'.format(name)) return True
return conn.validate_template(template_body, template_url)
proxyfile = '/etc/salt/proxy' status_file, msg_new, msg_old = _proxy_conf_file(proxyfile, test) changes_new.extend(msg_new) changes_old.extend(msg_old) status_proc = False
import copy import os import logging
import salt.utils from salt.exceptions import CommandExecutionError, MinionError
__virtualname__ = 'pkg'
fd_, adminfile = salt.utils.mkstemp(prefix="salt-", close_fd=False)
if any([salt.utils.is_true(kwargs.get(x)) for x in ('removed', 'purge_desired')]): return {}
if len(names) == 1: return ret[names[0]] return ret
available_version = salt.utils.alias_function(latest_version, 'available_version')
salt 'global_zone' pkg.install sources='[{"SMClgcc346": "/var/spool/pkg/gcc-3.4.6-sol10-sparc-local.pkg"}]' current_zone_only=True
salt '*' pkg.install sources='[{"<pkg name>": "salt://pkgs/<pkg filename>"}]' instance="overwrite"
if kwargs.get('current_zone_only') == 'True': cmd_prefix += '-G '
out = __salt__['cmd.run_all'](cmd, output_loglevel='trace', python_shell=False)
if 'admin_source' not in kwargs: os.unlink(adminfile)
fd_, adminfile = salt.utils.mkstemp(prefix="salt-", close_fd=False)
cmd = ['/usr/sbin/pkgrm', '-n', '-a', adminfile] + targets out = __salt__['cmd.run_all'](cmd, python_shell=False, output_loglevel='trace')
if 'admin_source' not in kwargs: os.unlink(adminfile)
import os import random
import salt.utils from salt.ext.six.moves import range
if old == '*': return True
line = line[10:] commented_cron_job = True
if SALT_CRON_IDENTIFIER in comment_line: parts = comment_line.split(SALT_CRON_IDENTIFIER) comment_line = parts[0].rstrip() if len(parts[1]) > 1: identifier = parts[1][1:]
ls = salt.utils.alias_function(list_tab, 'ls')
return comdat['stderr']
daymonth_max = 28
return comdat['stderr']
return comdat['stderr']
rm_ = ind
return comdat['stderr']
return comdat['stderr']
return comdat['stderr']
import logging
import salt.utils
import logging import os import os.path import re
import salt.utils from salt.exceptions import CommandExecutionError
__func_alias__ = { 'reload_': 'reload' }
BINS = frozenset(('svc', 'supervise', 'svok')) return all(salt.utils.which(b) for b in BINS)
from __future__ import absolute_import import json import logging import os import yaml
continue
if ':' in key: namespace, key = key.split(':', 1) else: namespace, key = key, None
defaults = _load(namespace)
if key: return salt.utils.traverse_dict_and_list(defaults, key, default) else: return defaults
import os
from __future__ import absolute_import import salt.utils import time import logging from salt.exceptions import CommandExecutionError
try: import win32security import win32service import win32serviceutil import pywintypes HAS_WIN32_MODS = True except ImportError: HAS_WIN32_MODS = False
__virtualname__ = 'service'
if name in get_all(): raise CommandExecutionError('Service Already Exists: {0}'.format(name))
bin_path = bin_path.strip('"') if exe_args is not None: bin_path = '{0} {1}'.format(bin_path, exe_args)
handle_scm = win32service.OpenSCManager( None, None, win32service.SC_MANAGER_ALL_ACCESS)
handle_svc = win32service.CreateService(handle_scm, name, display_name, win32service.SERVICE_ALL_ACCESS, service_type, start_type, error_control, bin_path, load_order_group, 0, dependencies, account_name, account_password)
if start_type == 2: win32service.ChangeServiceConfig2( handle_svc, win32service.SERVICE_CONFIG_DELAYED_AUTO_START_INFO, start_delayed)
import salt.utils
from __future__ import absolute_import import logging
from salt.exceptions import SaltInvocationError import salt.utils
__virtualname__ = 'win_snmp'
for service, bitmask in sorted_types: if current_bitmask > 0: remaining_bitmask = current_bitmask - bitmask
services = sorted(set(services))
vdata = sum(_SERVICE_TYPES[service] for service in services)
new_settings = get_agent_settings() failed_settings = dict()
for current_value in current_values: permissions = str() for permission_name in _PERMISSION_TYPES: if current_value['vdata'] == _PERMISSION_TYPES[permission_name]: permissions = permission_name break ret[current_value['vname']] = permissions
for vname in values: if vname not in current_communities: __salt__['reg.set_value'](_HKEY, _COMMUNITIES_KEY, vname, values[vname], 'REG_DWORD')
new_communities = get_community_names() failed_communities = dict()
from __future__ import absolute_import import copy import re import os import logging
import salt.config import salt.utils try: import salt.utils.cloud HAS_CLOUD = True except ImportError: HAS_CLOUD = False
import salt.ext.six as six
mode = str(mode)
return '0{0}'.format(ret)
from __future__ import absolute_import from distutils.version import LooseVersion import re import logging
import salt.utils from salt.exceptions import CommandExecutionError
import re import logging
import salt.utils
from __future__ import absolute_import try: import iptc IPTC_IMPORTED = True except ImportError: IPTC_IMPORTED = False
from salt.exceptions import CommandExecutionError import salt.utils
from __future__ import absolute_import, print_function import yaml import logging
import salt.utils.http import salt.ext.six as six from salt._compat import ElementTree as ET
from __future__ import absolute_import import os
import salt.utils import salt.utils.odict as odict
import salt.ext.six as six
return dict(_list_hosts())
for addr in hosts: if host in hosts[addr]: return addr return ''
if not alias.strip(): line_to_add = ''
if lines and not lines[-1].endswith(os.linesep): lines[-1] += os.linesep line = line_to_add lines.append(line)
lines[ind] = ''
lines[ind] = newline + os.linesep
ofile.write(line.strip() + os.linesep)
import salt.loader
import logging
import salt.utils import salt.utils.decorators as decorators
__func_alias__ = { 'list_': 'list', 'get_': 'get', 'put_': 'put', 'delete_': 'delete', }
__virtualname__ = 'mdata'
__virtualname__ = 'shadow'
import logging
import salt.utils from salt.exceptions import CommandExecutionError, SaltInvocationError from salt.ext.six import integer_types
try: import memcache HAS_MEMCACHE = True except ImportError: HAS_MEMCACHE = False
__func_alias__ = { 'set_': 'set' }
import logging
from salt.ext.six.moves import shlex_quote as _cmd_quote
import salt.utils.validate.net from salt.exceptions import CommandExecutionError
__virtualname__ = 'bluetooth'
import logging log = logging.getLogger(__file__)
from napalm import get_network_driver HAS_NAPALM = True
from __future__ import absolute_import try: import grp except ImportError: pass
import salt.utils import salt.utils.itertools from salt.exceptions import CommandExecutionError, SaltInvocationError from salt.modules.mac_user import _dscl, _flush_dscl_cache
__virtualname__ = 'group'
gid_list = _list_gids() if str(gid) in gid_list: raise CommandExecutionError( 'gid \'{0}\' already exists'.format(gid) )
else: retcode = 0
grinfo = next(iter(x for x in grp.getgrall() if x.gr_name == name))
import re import logging
import salt.utils.itertools from salt.exceptions import CommandExecutionError
import salt.utils
if gem_bin is None: if __salt__['rvm.is_installed'](runas=runas): return __salt__['rvm.do'](ruby, cmdline, runas=runas)
import logging import os.path import re import tempfile
import salt.utils from salt.exceptions import CommandExecutionError, CommandNotFoundError
try: choc_path = _find_chocolatey(__context__, __salt__) except CommandExecutionError: choc_path = None if choc_path and not force: return 'Chocolatey found at {0}'.format(choc_path)
net4_url = 'http://download.microsoft.com/download/1/B/E/1BE39E79-7E39-46A3-96FF-047F95396215/dotNetFx40_Full_setup.exe'
ps_path = 'C:\\Windows\\SYSTEM32\\WindowsPowerShell\\v1.0\\powershell.exe'
import os import stat
import salt.utils from salt.ext.six import string_types
if '.' in val: val = float(val) else: val = int(val) data[plugin][key] = val
from __future__ import absolute_import
from salt.ext.six import string_types, text_type from salt.ext.six.moves import range from salt.ext.six.moves.urllib.request import urlopen as _urlopen
import salt.utils from salt.exceptions import CommandExecutionError
__virtualname__ = 'buildout'
return _merge_statuses([boot_ret, buildout_ret])
import salt.utils import logging import salt.utils.mac_utils from salt.exceptions import CommandExecutionError
if not salt.utils.is_darwin(): return False, 'Not Darwin'
cmd = "dscl . -create /Users/{0} Password '*'".format(name) salt.utils.mac_utils.execute_return_success(cmd)
import os import logging
__func_alias__ = { 'set_': 'set' }
#pylint: disable=E0602
import logging import json import yaml
def ordered_dict_presenter(dumper, data): return dumper.represent_dict(list(data.items()))
from __future__ import absolute_import import logging
HAS_LIBS = False try: from servicenow_rest.api import Client
from __future__ import absolute_import import os
import salt.utils import salt.exceptions
import salt.ext.six as six
__virtualname__ = 'django'
from __future__ import absolute_import import logging import yaml import urllib
import salt.ext.six as six HAS_LIBS = False try: import splunklib.client import requests HAS_LIBS = True except ImportError: pass
from salt.utils.odict import OrderedDict
__func_alias__ = { 'list_': 'list' }
try: search = client.saved_searches[name] except KeyError: pass return search
def ordered_dict_presenter(dumper, data): return dumper.represent_dict(six.iteritems(data)) yaml.add_representer( OrderedDict, ordered_dict_presenter, Dumper=yaml.dumper.SafeDumper)
import logging import os.path import os
import jinja2 import jinja2.exceptions
import salt.utils import salt.utils.templates import salt.utils.validate.net import salt.ext.six as six
log = logging.getLogger(__name__)
JINJA = jinja2.Environment( loader=jinja2.FileSystemLoader( os.path.join(salt.utils.templates.TEMPLATE_DIRNAME, 'rh_ip') ) )
__virtualname__ = 'ip'
retain_settings = opts.get('retain_settings', False) result = current if retain_settings else {}
lines = contents.read().splitlines() try: lines.remove('') except ValueError: pass return lines
if iface_type not in ['slave']: return __salt__['cmd.run']('ifdown {0}'.format(iface)) return None
if iface_type not in ['slave']: return __salt__['cmd.run']('ifup {0}'.format(iface)) return None
current_network_settings = _parse_rh_config(_RH_NETWORK_FILE)
_write_file_network(network, _RH_NETWORK_FILE)
from __future__ import absolute_import import time import logging import re import sys import shlex
import salt.utils
import salt.ext.six as six
import MySQLdb import MySQLdb.cursors import MySQLdb.converters from MySQLdb.constants import FIELD_TYPE, FLAG HAS_MYSQLDB = True
import pymysql pymysql.install_as_MySQLdb() import MySQLdb import MySQLdb.cursors import MySQLdb.converters from MySQLdb.constants import FIELD_TYPE, FLAG HAS_MYSQLDB = True
HAS_MYSQLDB = False
qry = 'CHECK TABLE {0}.{1}'.format(s_name, s_table) _execute(cur, qry) results = cur.fetchall() log.debug(results) return results
qry = 'REPAIR TABLE {0}.{1}'.format(s_name, s_table) _execute(cur, qry) results = cur.fetchall() log.debug(results) return results
qry = 'OPTIMIZE TABLE {0}.{1}'.format(s_name, s_table) _execute(cur, qry) results = cur.fetchall() log.debug(results) return results
if 'connection_default_file' in kwargs: get_opts = False else: get_opts = True
database += token try: if exploded_grant[position_tracker + 1] == '.': phrase = 'tables' except IndexError: break
if exploded_grant[position_tracker + 1] == '@': phrase = 'pre-host'
return -1
return -2
if db_exists(name, **connection_args): log.info('DB \'{0}\' already exists'.format(name)) return False
if not db_exists(name, **connection_args): log.info('DB \'{0}\' does not exist'.format(name)) return False
def __grant_normalize(grant): if grant == 'ALL': grant = 'ALL PRIVILEGES'
for opt in ssl_option: key = next(six.iterkeys(opt))
new_ssl_option.append("{0} '{1}'".format(normal_key, opt[key].replace("'", '')))
dbc = quote_identifier(dbc, for_grants=(table is '*'))
s_database = quote_identifier(dbc, for_grants=(table is '*'))
s_database = dbc
if len(rtnv) == 0: rtnv.append([])
if len(rtnv) == 0: rtnv.append([])
from __future__ import absolute_import import os import logging
import salt.utils
python_shell = False if '*.' in cmd: python_shell = True
temp_dir = __salt__['temp.dir'](prefix='pkg-')
cmd = 'xar -x -f {0} {1}'.format(pkg, ' '.join(files)) __salt__['cmd.run'](cmd, cwd=temp_dir, output_loglevel='quiet')
for f in files: i = _get_pkg_id_from_pkginfo(os.path.join(temp_dir, f)) if len(i): package_ids.extend(i)
__salt__['file.remove'](temp_dir)
cmd = 'find {0} -name *.pkg'.format(base_path) out = __salt__['cmd.run'](cmd, python_shell=True)
python_shell = False if '*.' in cmd: python_shell = True
from salt import utils
import logging log = logging.getLogger(__name__)
import salt.utils
import json
import salt.utils
from __future__ import absolute_import import logging
import salt.utils import salt.utils.locales
self.categories = categories
self.foundCategories = []
self.download_collection = win32com.client.Dispatch('Microsoft.Update.UpdateColl')
self.install_collection = win32com.client.Dispatch('Microsoft.Update.UpdateColl')
self.win_downloader = self.update_session.CreateUpdateDownloader() self.win_downloader.Updates = self.download_collection
self.win_installer = self.update_session.CreateUpdateInstaller() self.win_installer.Updates = self.install_collection
self.download_results = None
self.install_results = None
self.search_results = None
for update in self.search_results.Updates: if update.InstallationBehavior.CanRequestUserInput: log.debug(U'Skipped update {0} - requests user input'.format(update.title)) continue
if self.skipDownloaded and update.IsDownloaded: log.debug(u'Skipped update {0} - already downloaded'.format(update.title)) continue
self.foundCategories = _gather_update_categories(self.download_collection) log.debug('found categories: {0}'.format(str(self.foundCategories))) return True
log.debug('generated search string: {0}'.format(search_string)) return self.Search(search_string)
log.debug('blugger has {0} updates in it'.format(self.install_collection.Count)) if self.install_collection.Count == 0: return {}
updates.append('{0}: {1}'.format( self.install_results.GetUpdateResult(i).ResultCode, self.install_collection.Item(i).Title))
for i, update in enumerate(updates): results['update {0}'.format(i)] = update
return [update['Title'] for update in updates_verbose]
comment += 'Search was done without error.\n'
salt '*' win_update.list_updates
salt '*' win_update.list_updates fields="['Title', 'Description']"
salt '*' win_update.list_updates categories="['Critical Updates']" verbose=True
salt '*' win_update.download_updates
salt '*' win_update.download_updates categories="['Critical Updates']"
comment, passed, retries = _search(quidditch, retries) if not passed: return (comment, str(passed))
comment, passed, retries = _download(quidditch, retries) if not passed: return (comment, str(passed))
salt '*' win_update.install_updates
salt '*' win_update.install_updates categories="['Critical Updates']"
comment, passed, retries = _search(quidditch, retries) if not passed: return (comment, str(passed))
comment, passed, retries = _download(quidditch, retries) if not passed: return (comment, str(passed))
comment, passed, retries = _install(quidditch, retries) if not passed: return (comment, str(passed))
from __future__ import absolute_import import logging
import salt.utils import salt.ext.six as six import salt.utils.event from salt._compat import subprocess from salt.utils.network import host_to_ip as _host_to_ip
__virtualname__ = 'status'
cmd = list2cmdline(['wmic', 'cpu']) info = __salt__['cmd.run'](cmd).split('\r\n')
column = info[0].index('LoadPercentage')
end = info[1].index(' ', column+1)
return int(info[1][column:end])
cmd = list2cmdline(['wmic', 'os', 'get', 'lastbootuptime']) outs = __salt__['cmd.run'](cmd)
stats_line = '' stats_line = outs.split('\r\n')[1]
startup_time = stats_line[:14] startup_time = time.strptime(startup_time, '%Y%m%d%H%M%S') startup_time = datetime.datetime(*startup_time[:6])
uptime = datetime.datetime.now() - startup_time
owner['user'] = 'SYSTEM' owner['user_domain'] = 'NT AUTHORITY'
port = 4505 master_ip = None
if master is not None: tmp_ip = _host_to_ip(master) if tmp_ip is not None: master_ip = tmp_ip
#_connarg('connection_useSSL', 'useSSL')
from __future__ import absolute_import import copy import logging import re import os.path
import salt.utils import salt.utils.itertools from salt.exceptions import CommandExecutionError, MinionError
import salt.ext.six as six
__virtualname__ = 'pkg'
if refresh: refresh_db()
for name in names: ret[name] = '' cmd = ['pacman', '-Sp', '--needed', '--print-format', '%n %v'] cmd.extend(names)
if name in names: ret[name] = version_num
if len(names) == 1: return ret[names[0]] return ret
available_version = salt.utils.alias_function(latest_version, 'available_version')
if any([salt.utils.is_true(kwargs.get(x)) for x in ('removed', 'purge_desired')]): return {}
pkg_params = {name: version_num}
prefix = prefix or '=' targets.append('{0}{1}{2}'.format(param, prefix, verstr))
from __future__ import absolute_import import os import re import logging import glob
import salt.utils import salt.utils.decorators as decorators from salt.exceptions import CommandExecutionError, MinionError
__virtualname__ = 'pkg'
return False
if any([salt.utils.is_true(kwargs.get(x)) for x in ('removed', 'purge_desired')]): return {}
pkg, ver = line.split(None)[1].rsplit('-', 1)
if refresh: refresh_db()
if refresh: refresh_db()
ret = {} for name in names: ret[name] = ''
if len(names) == 1: return ret[names[0]] return ret
available_version = latest_version
targets = [x for x in pkg_params if x in old] if not targets: return {}
regex = re.compile(r'\s*repository\s*=\s*'+repo+r'/?\s*(#.*)?$')
from __future__ import absolute_import import logging import datetime import os
import salt.utils
import logging import struct
import logging import re
import salt.utils from salt.state import STATE_INTERNAL_KEYWORDS as _STATE_INTERNAL_KEYWORDS from salt.exceptions import ( CommandExecutionError )
log = logging.getLogger(__name__)
after_jump = []
rule = rule.strip()
_fh.writelines(rules)
return 'Error: table_type hook and priority required'
if not position: position = get_rule_handle(table, chain, rule, family)
import os import logging
import salt.utils
from __future__ import absolute_import
import bz2 import copy
from salt.exceptions import CommandExecutionError, SaltInvocationError
import salt.ext.six as six
try: import docker import docker.utils HAS_DOCKER_PY = True except ImportError: HAS_DOCKER_PY = False
log = logging.getLogger(__name__)
MIN_DOCKER = (1, 4, 0) MIN_DOCKER_PY = (1, 4, 0)
CLIENT_TIMEOUT = 60
STOP_TIMEOUT = 10
__virtualname__ = 'dockerng'
post = None
client_kwargs['base_url'] = os.environ.get('DOCKER_HOST')
client_kwargs['version'] = 'auto'
if timeout is not None and __context__['docker.client'].timeout != timeout: __context__['docker.client'].timeout = timeout
return None
__context__[contextkey] = 'docker-exec'
if from_config is not None: __context__[contextkey] = from_config return from_config
log.warning( 'Assuming tag \'{0}\' for repo \'{1}\'' .format(default_tag, image) ) r_tag = default_tag
raise CommandExecutionError( 'Error {0}: {1}'.format(exc.response.status_code, exc.explanation) )
raise
raise CommandExecutionError( 'Error {0}: {1}'.format(exc.response.status_code, exc.explanation) )
raise
data['Image'] = '{0}:{1}'.format(repo_name, repo_tag) data['Id'] = status
already_pushed = data.setdefault('Layers', {}).setdefault( 'Already_Pushed', []) already_pushed.append(item['id'])
pushed = data.setdefault('Layers', {}).setdefault( 'Pushed', []) pushed.append(item['id'])
return
try: kwargs['command'] = salt.utils.shlex_split(kwargs['command']) except AttributeError: pass
return
kwargs['user'] = str(kwargs['user']) return
return
return
return
return
try: kwargs['entrypoint'] = salt.utils.shlex_split(kwargs['entrypoint']) except AttributeError: pass
return
return
return
return
return
return
link_name = '/' + link_name
return
return
kwargs['volumes_from'] = str(kwargs['volumes_from'])
return
return
log.info( 'Assuming network_mode \'{0}\' is a network.'.format( kwargs['network_mode']) )
return
return
continue
raise SaltInvocationError(kwarg + ' cannot be None')
validator = kwarg validation_arg = ()
_locals[key](*validation_arg)
for key in list(__context__): try: if key.startswith('validation.docker.'): __context__.pop(key) except AttributeError: pass
image_id = inspect_image(name)['Id']
if verbose: for img_id in ret: ret[img_id]['Info'] = inspect_image(img_id)
mappings = inspect_container(name).get('NetworkSettings', {}).get( 'Ports', {}) if not mappings: return {}
if kwargs.get('verbose', False): for c_id in ret: ret[c_id]['Info'] = inspect_container(c_id)
columns = {} for idx, col_name in enumerate(response['Titles']): columns[idx] = col_name
ret = [] for process in response['Processes']: cur_proc = {} for idx, val in enumerate(process): cur_proc[columns[idx]] = val ret.append(cur_proc) return ret
inspect_image(image)
cp = salt.utils.alias_function(copy_from, 'cp')
out = salt.utils.fopen(path, 'wb')
data = compressor.flush() if data: out.write(data)
if kwargs.get(push, False): ret['Push'] = __salt__['cp.push'](path)
out = salt.utils.fopen(path, 'wb')
data = compressor.flush() if data: out.write(data)
os.remove(saved_path)
if kwargs.get(push, False): ret['Push'] = __salt__['cp.push'](path)
return response
return response
return response
return response
return response
return response
return response
return response
return response
return response
return response
return {'result': ignore_already_stopped, 'comment': 'Container \'{0}\' absent'.format(name)}
post = None
from __future__ import absolute_import import functools import logging import os.path import os import re import time
import jinja2 import jinja2.exceptions import salt.ext.six as six
import salt.utils import salt.utils.templates import salt.utils.validate.net import salt.utils.odict
log = logging.getLogger(__name__)
JINJA = jinja2.Environment( loader=jinja2.FileSystemLoader( os.path.join(salt.utils.templates.TEMPLATE_DIRNAME, 'debian_ip') ) )
__virtualname__ = 'ip'
'address': __ipv4_quad, 'netmask': __ipv4_netmask, 'broadcast': __ipv4_quad, 'metric': __int,
if os.path.exists(_DEB_NETWORK_DIR): interface_files += ['{0}/{1}'.format(_DEB_NETWORK_DIR, dir) for dir in os.listdir(_DEB_NETWORK_DIR)]
if line.lstrip().startswith('#') or line.isspace(): continue if line.startswith('iface'): sline = line.split()
if iface_name not in adapters: adapters[iface_name] = salt.utils.odict.OrderedDict()
if 'data' not in adapters[iface_name]: adapters[iface_name]['data'] = salt.utils.odict.OrderedDict()
elif line[0].isspace(): sline = line.split()
iface_data['inet']['ethtool_keys'] = sorted(ethtool)
iface_data['inet6'] = {} iface_data['inet6']['addrfam'] = 'inet6'
if optname == 'proto' and valuestr == 'none': valuestr = 'static'
_optname = _optname.replace('-', '_') iface_data[addrfam][_optname] = value
return [item + '\n' for item in ifcfg.split('\n')]
adapters = _parse_interfaces() adapters[iface] = data
return saved_ifcfg.split('\n')
return filename
__salt__['kmod.load']('bonding')
__salt__['pkg.install']('ifenslave-2.6')
return [item + '\n' for item in ifcfg]
if iface_type not in ['slave', 'source']: return __salt__['cmd.run'](['ifdown', iface]) return None
return [item + '\n' for item in ifcfg.split('\n')]
if iface_type not in ('slave', 'source'): return __salt__['cmd.run'](['ifup', iface]) return None
current_network_settings = _parse_current_network_settings()
opts = _parse_network_settings(settings, current_network_settings)
_write_file_network(network, _DEB_NETWORKING_FILE, True)
if not found_domain: new_contents.insert(0, 'domain {0}\n' . format(domainname))
if not ('test' in settings and settings['test']): _write_file_network(new_resolv, _DEB_RESOLV_FILE)
from __future__ import absolute_import import os import shutil
import salt.utils
relative_path = parts.repo or "gentoo"
tmp.sort(cmp=lambda x, y: cmp(x.lstrip('-'), y.lstrip('-'))) return tmp
new_flags.sort(cmp=lambda x, y: cmp(x.lstrip('-'), y.lstrip('-')))
if has_wildcard: match_list = set(atom) else: match_list = set(_porttree().dbapi.xmatch("match-all", atom))
dirty_flags = _porttree().dbapi.aux_get(cpv, ["IUSE"])[0].split() return list(set(dirty_flags))
_porttree().dbapi.settings.reset() _porttree().dbapi.settings.lock() return use, use_expand_hidden, usemask, useforce
import os import re
from salt.exceptions import CommandExecutionError
#pylint: disable=E0602
import logging import time import salt.ext.six as six
try: import boto import boto.elasticache import boto.utils logging.getLogger('boto').setLevel(logging.CRITICAL) HAS_BOTO = True except ImportError: HAS_BOTO = False
log = logger.getLogger(__name__)
apiserver_url = "http://127.0.0.1:8080"
node = _guess_node_id(node) apiserver_url = _guess_apiserver(apiserver_url) if apiserver_url is None: return False
ret = _get_labels(node, apiserver_url)
node = _guess_node_id(node) apiserver_url = _guess_apiserver(apiserver_url) if apiserver_url is None: return False
labels = _get_labels(node, apiserver_url)
ret['comment'] = "Label {0} already set".format(name)
node = _guess_node_id(node) apiserver_url = _guess_apiserver(apiserver_url) if apiserver_url is None: return False
node = _guess_node_id(node) apiserver_url = _guess_apiserver(apiserver_url) if apiserver_url is None: return False
apiserver_url = _guess_apiserver(apiserver_url) if apiserver_url is None: return False
_create_namespace(name, apiserver_url) ret['changes'] = name ret['comment'] = "Namespace {0} created".format(name)
apiserver_url = _guess_apiserver(apiserver_url) if apiserver_url is None: return False
ret = _get_namespaces(apiserver_url, namespace) return ret
apiserver_url = _guess_apiserver(apiserver_url) if apiserver_url is None: return False
if not decode: ret = _get_secrets(namespace, name, apiserver_url) else: ret = _decode_secrets(_get_secrets(namespace, name, apiserver_url)) return ret
apiserver_url = _guess_apiserver(apiserver_url) if apiserver_url is None: return False
import logging log = logging.getLogger(__name__)
from salt.ext import six
from napalm import get_network_driver HAS_NAPALM = True
return proxy_output
import os import tempfile import hashlib import logging
import salt.utils
pass
from __future__ import absolute_import import os import re import logging
import salt.utils from salt.exceptions import SaltInvocationError
import salt.ext.six as six
log = logging.getLogger(__name__)
if s is None: ret = salt.utils.shlex_split('') else: ret = salt.utils.shlex_split(s)
uninstall_ruby(ruby, runas=runas) return False
raise SaltInvocationError('Command must be specified')
raise SaltInvocationError('Command must be specified')
import logging import os import re
import salt.utils
__virtualname__ = 'debconf'
from __future__ import absolute_import, print_function import errno import logging import os import tempfile import shutil
import salt.utils from salt.exceptions import SaltInvocationError, CommandExecutionError
if isinstance(sources, str): sources = sources.split(',') for src in sources: _get_src(tree_base, src, saltenv)
for dsc in dscs: afile = os.path.basename(dsc) adist = os.path.join(dest_dir, afile) shutil.copy(dsc, adist)
gpg_info_file = '{0}/gpg-agent-info-salt'.format(gnupghome) with salt.utils.fopen(gpg_info_file, 'r') as fow: gpg_raw_info = fow.readlines()
import logging
import salt.utils
import os import uuid import pprint import logging try:
import salt.utils import salt.utils.yast import salt.utils.preseed import salt.utils.kickstart import salt.syspaths from salt.exceptions import SaltInvocationError
import copy import logging import re
import salt.utils from salt.exceptions import CommandExecutionError, MinionError import salt.ext.six as six
__virtualname__ = 'pkg'
available_version = salt.utils.alias_function(latest_version, 'available_version')
if any([salt.utils.is_true(kwargs.get(x)) for x in ('removed', 'purge_desired')]): return {}
delete = salt.utils.alias_function(remove, 'delete') purge = salt.utils.alias_function(remove, 'purge')
import logging import os.path
import salt.utils from salt.exceptions import ( CommandExecutionError, CommandNotFoundError, SaltInvocationError )
__func_alias__ = { 'list_': 'list' }
if not _valid_composer(composer): raise CommandNotFoundError( '\'composer.{0}\' is not available. Couldn\'t find \'{1}\'.' .format(action, composer) )
if directory is None and action != 'selfupdate': raise SaltInvocationError( 'The \'directory\' argument is required for composer.{0}'.format(action) )
cmd = [composer, action, '--no-interaction', '--no-ansi']
if php is not None: cmd = [php] + cmd
if directory is not None: cmd.extend(['--working-dir', directory])
if quiet is True: cmd.append('--quiet')
import salt.utils from salt.exceptions import CommandExecutionError
__virtualname__ = 'sysctl'
from __future__ import absolute_import import json import logging as logger
try: import requests import requests.exceptions HAS_LIBS = True except ImportError: HAS_LIBS = False
import salt.ext.six as six
import salt.utils import salt.output import salt.exceptions
log = logger.getLogger(__name__)
__virtualname__ = 'bigip'
if item_kind is None: items.append(value) else: items.append({'kind': item_kind, 'name': value})
if isinstance(member, dict):
else:
from __future__ import absolute_import import logging
import salt.ext.six.moves.http_client from salt.exceptions import CommandExecutionError
my-minion: - esxi-1.example.com - esxi-2.example.com
<vcenter-password> esxi_hosts='[esxi-1.example.com, esxi-2.example.com]'
from __future__ import absolute_import import datetime import logging
import salt.ext.six as six import salt.utils import salt.utils.vmware import salt.utils.http from salt.utils import dictupdate from salt.exceptions import CommandExecutionError
try: from pyVmomi import vim, vmodl HAS_PYVMOMI = True except ImportError: HAS_PYVMOMI = False
salt '*' vsphere.esxcli_cmd my.esxi.host root bad-password \ 'system coredump network get'
salt '*' vsphere.get_coredump_network_config my.esxi.host root bad-password
salt '*' vsphere.get_coredump_network_config my.vcenter.location root bad-password \ esxi_hosts='[esxi-1.host.com, esxi-2.host.com]'
ret.update({esxi_host: {'Coredump Config': _format_coredump_stdout(response)}})
salt '*' vsphere.coredump_network_enable my.esxi.host root bad-password True
salt '*' vsphere.set_coredump_network_config my.esxi.host root bad-password 'dump_ip.host.com'
ret.update({esxi_host: response})
salt '*' vsphere.get_firewall_status my.esxi.host root bad-password
ret.update({esxi_host: _format_firewall_stdout(response)})
salt '*' vsphere.enable_firewall_ruleset my.esxi.host root bad-password True 'syslog'
response = salt.utils.vmware.esxcli(host, username, password, cmd, protocol=protocol, port=port) ret.update({host: response})
salt '*' vsphere.syslog_service_reload my.esxi.host root bad-password
response = salt.utils.vmware.esxcli(host, username, password, cmd, protocol=protocol, port=port) ret.update({host: response})
salt '*' vsphere.set_syslog_config my.esxi.host root bad-password \ loghost ssl://localhost:5432,tcp://10.1.0.1:1514
if firewall and syslog_config == 'loghost': if esxi_hosts: if not isinstance(esxi_hosts, list): raise CommandExecutionError('\'esxi_hosts\' must be a list.')
if esxi_hosts: if not isinstance(esxi_hosts, list): raise CommandExecutionError('\'esxi_hosts\' must be a list.')
if ret.get(esxi_host) is None: ret.update({esxi_host: {}}) ret[esxi_host].update(response)
salt '*' vsphere.get_syslog_config my.esxi.host root bad-password
ret.update({esxi_host: _format_syslog_config(response)})
response = salt.utils.vmware.esxcli(host, username, password, cmd, protocol=protocol, port=port) ret.update({host: _format_syslog_config(response)})
salt '*' vsphere.reset_syslog_config my.esxi.host root bad-password \ syslog_config='logdir,loghost'
response_dict = _reset_syslog_config_params(host, username, password, cmd, resets, valid_resets, protocol=protocol, port=port) ret.update({host: response_dict})
salt '*' vsphere.get_host_datetime my.esxi.host root bad-password
salt '*' vsphere.get_ntp_config my.esxi.host root bad-password
salt '*' vsphere.get_service_policy my.esxi.host root bad-password 'ssh'
if service_name not in valid_services: ret.update({host_name: {'Error': '{0} is not a valid service name.'.format(service_name)}}) return ret
if service_name == 'SSH' or service_name == 'ssh': temp_service_name = 'TSM-SSH' else: temp_service_name = service_name
salt '*' vsphere.get_service_running my.esxi.host root bad-password 'ssh'
if service_name not in valid_services: ret.update({host_name: {'Error': '{0} is not a valid service name.'.format(service_name)}}) return ret
if service_name == 'SSH' or service_name == 'ssh': temp_service_name = 'TSM-SSH' else: temp_service_name = service_name
salt '*' vsphere.get_vmotion_enabled my.esxi.host root bad-password
salt '*' vsphere.get_vsan_enabled my.esxi.host root bad-password
salt '*' vsphere.get_vsan_eligible_disks my.esxi.host root bad-password
salt '*' vsphere.list_ssds my.esxi.host root bad-password
salt '*' vsphere.list_non_ssds my.esxi.host root bad-password
salt '*' vsphere.ntp_configure my.esxi.host root bad-password '[192.174.1.100, 192.174.1.200]'
ntp_config = vim.HostNtpConfig(server=ntp_servers)
date_config = vim.HostDateTimeConfig(ntpConfig=ntp_config)
salt '*' vsphere.service_start my.esxi.host root bad-password 'ntpd'
if service_name == 'SSH' or service_name == 'ssh': temp_service_name = 'TSM-SSH' else: temp_service_name = service_name
if service_name not in valid_services: ret.update({host_name: {'Error': '{0} is not a valid service name.'.format(service_name)}}) return ret
salt '*' vsphere.service_stop my.esxi.host root bad-password 'ssh'
if service_name == 'SSH' or service_name == 'ssh': temp_service_name = 'TSM-SSH' else: temp_service_name = service_name
if service_name not in valid_services: ret.update({host_name: {'Error': '{0} is not a valid service name.'.format(service_name)}}) return ret
salt '*' vsphere.service_restart my.esxi.host root bad-password 'ntpd'
if service_name == 'SSH' or service_name == 'ssh': temp_service_name = 'TSM-SSH' else: temp_service_name = service_name
if service_name not in valid_services: ret.update({host_name: {'Error': '{0} is not a valid service name.'.format(service_name)}}) return ret
salt '*' vsphere.set_service_policy my.esxi.host root bad-password 'ntpd' 'automatic'
if service_name not in valid_services: ret.update({host_name: {'Error': '{0} is not a valid service name.'.format(service_name)}}) return ret
for service in services: service_key = None
if service.key == service_name: service_key = service.key elif service_name == 'ssh' or service_name == 'SSH': if service.key == 'TSM-SSH': service_key = 'TSM-SSH'
salt '*' vsphere.update_date_time my.esxi.host root bad-password
account_manager = salt.utils.vmware.get_inventory(service_instance).accountManager
user_account = vim.host.LocalAccountManager.AccountSpecification() user_account.id = username user_account.password = new_password
salt '*' vsphere.vmotion_disable my.esxi.host root bad-password
salt '*' vsphere.vmotion_enable my.esxi.host root bad-password
salt '*' vsphere.vsan_add_disks my.esxi.host root bad-password
disk_names = [] for disk in eligible: disk_names.append(disk.canonicalName) ret.update({host_name: {'Disks Added': disk_names}})
ret.update({host_name: {'Disks Added': eligible}})
ret.update({host_name: {'Error': error}})
ret.update({host_name: {'Disks Added': 'No new VSAN-eligible disks were found to add.'}})
salt '*' vsphere.vsan_disable my.esxi.host root bad-password
vsan_config = vim.vsan.host.ConfigInfo() vsan_config.enabled = False
salt '*' vsphere.vsan_enable my.esxi.host root bad-password
vsan_config = vim.vsan.host.ConfigInfo() vsan_config.enabled = True
if host_name: host_ref = search_index.FindByDnsName(dnsName=host_name, vmSearch=False) else: host_ref = search_index.FindByDnsName(dnsName=host, vmSearch=False)
if host_ref is None: host_ref = search_index.FindByIp(ip=host, vmSearch=False)
suitable_disks = [] query = vsan_system.QueryDisksForVsan() for item in query: if item.state == 'eligible': suitable_disks.append(item)
disks = _get_host_ssds(host_ref) + _get_host_non_ssds(host_ref)
matching = [] for disk in disks: for suitable_disk in suitable_disks: if disk.canonicalName == suitable_disk.disk.canonicalName: matching.append(disk)
ret[host_name] = {}
import salt.utils import salt.utils.decorators as decorators from salt.exceptions import SaltException
@decorators.memoize def __detect_os(): return salt.utils.which('ipvsadm')
if out['retcode']: ret = out['stderr'].strip() else: ret = True return ret
if out['retcode']: ret = out['stderr'].strip() else: ret = True return ret
if out['retcode']: ret = out['stderr'].strip() else: ret = True return ret
if out['retcode']: ret = out['stderr'].strip() else: ret = True return ret
if out['retcode']: ret = out['stderr'].strip() else: ret = True return ret
if out['retcode']: ret = out['stderr'].strip() else: ret = True return ret
if out['retcode']: ret = out['stderr'].strip() else: ret = True return ret
if out['retcode']: ret = out['stderr'].strip() else: ret = out['stdout'].strip()
if out['retcode']: ret = out['stderr'].strip() else: ret = True return ret
if not kwargs: cmd += ' '
if not kwargs: cmd += ' '
#pylint: disable=E0602
from __future__ import absolute_import import logging
import salt.utils.compat import salt.utils from salt.ext.six import string_types
#pylint: disable=E0602
from __future__ import absolute_import import logging import json
import salt.utils.boto3 import salt.utils.compat import salt.utils from salt.ext.six import string_types
import fnmatch import logging
import salt.loader import salt.runner import salt.state import salt.utils import salt.utils.schema as S from salt.utils.doc import strip_rst as _strip_rst from salt.ext.six.moves import zip
import salt.ext.six as six
__virtualname__ = 'sys'
target_mod = module + '.' if not module.endswith('.') else module
target_mod = module + '.' if not module.endswith('.') else module
target_mod = module + '.' if not module.endswith('.') else module
target_mod = module + '.' if not module.endswith('.') else module
return sorted(__salt__)
module = module + '.' if not module.endswith('.') else module
return True
return sorted(st_.states)
module = module + '.' if not module.endswith('.') else module
return sorted(run_.functions)
module = module + '.' if not module.endswith('.') else module
return sorted(returners_)
module = module + '.' if not module.endswith('.') else module
))
))
import copy import logging import os
import salt.utils from salt.exceptions import CommandExecutionError, MinionError import salt.ext.six as six
__virtualname__ = 'pkg'
info = salt.utils.alias_function(version, 'info')
update = salt.utils.alias_function(refresh_db, 'update')
if len(names) == 1: return ret[names[0]] return ret
available_version = salt.utils.alias_function(latest_version, 'available_version')
if any([salt.utils.is_true(kwargs.get(x)) for x in ('removed', 'purge_desired')]): return {}
return True
opts = ''.join([opt for opt in opts if opt in 'AfIMq']) targets = pkg_params
pkg_params = {name: kwargs.get('version')}
return ' '.join(cmd)
if pkg[0].find("/") > 0: origin = pkg[0] pkg = [k for k, v in old.iteritems() if v['origin'] == origin][0]
delete = salt.utils.alias_function(remove, 'delete') purge = salt.utils.alias_function(remove, 'purge')
import base64 import hashlib import hmac import StringIO
import salt.exceptions import salt.ext.six as six import salt.utils
import re import os import logging
import salt.utils from salt.exceptions import CommandExecutionError
__func_alias__ = { 'list_': 'list' }
from __future__ import absolute_import import json import logging import os import tempfile
import salt.ext.six as six
if params: endpoint = 'resources'
for key, val in six.iteritems(params): params[key] = str(val)
from __future__ import absolute_import
__virtualname__ = 'service'
if __grains__['kernelrelease'] == "5.9": return (False, 'The smf execution module failed to load: SMF not available on Solaris 9.') return __virtualname__
clear_cmd = '/usr/sbin/svcadm clear {0}'.format(name) __salt__['cmd.retcode'](clear_cmd, python_shell=False) return not __salt__['cmd.retcode'](cmd, python_shell=False)
return start(name)
return start(name)
return _get_enabled_disabled("true")
return _get_enabled_disabled("false")
import yaml import json
import salt.utils.pagerduty from salt.ext.six import string_types
list_maintenance_windows = salt.utils.alias_function(list_windows, 'list_maintenance_windows')
list_escalation_policies = salt.utils.alias_function(list_policies, 'list_escalation_policies')
from __future__ import absolute_import try: import pwd except ImportError: pass import logging import time
import salt.utils import salt.utils.decorators as decorators from salt.utils.locales import sdecode as _sdecode from salt.exceptions import CommandExecutionError, SaltInvocationError
__virtualname__ = 'user'
if createhome: __salt__['file.mkdir'](home, user=uid, group=gid)
time.sleep(1) if groups: chgroups(name, groups) return True
if force: log.warn('force option is unsupported on MacOS, ignoring')
if remove: __salt__['file.remove'](info(name)['home'])
chgroups(name, ()) return _dscl(['/Users/{0}'.format(name)], ctype='delete')['retcode'] == 0
time.sleep(1) return info(name).get('uid') == uid
time.sleep(1) return info(name).get('gid') == gid
time.sleep(1) return info(name).get('shell') == shell
time.sleep(1) return info(name).get('home') == home
ctype='create'
time.sleep(1)
time.sleep(1) return info(new_name).get('RecordName') == new_name
try: import requests HAS_REQUESTS = True except ImportError: HAS_REQUESTS = False
if not HAS_REQUESTS: return False return __virtualname__
auth = _auth(profile=profile)
return {'err_code': response.status_code, 'err_msg': json.loads(response.text).get('err', '')}
alert_ids = get_alert_config(deployment_id, api_key=api_key, profile=profile)
from __future__ import absolute_import import re import logging
from salt.exceptions import SaltInvocationError from salt.utils import dictdiffer
import salt.ext.six as six
from __future__ import absolute_import import json import logging import os
from salt.exceptions import SaltInvocationError import salt.utils
__virtualname__ = 'win_iis'
current_bindings = list_bindings(site)
current_name = None
certs = _list_certs()
current_cert_bindings = list_cert_bindings(site)
cmd_ret = _srvmgr(func=str().join(pscmd_validate), as_json=True)
for setting in settings: settings[setting] = str(settings[setting])
try: complex(settings[setting]) value = settings[setting] except ValueError: value = "'{0}'".format(settings[setting])
new_settings = get_container_setting(name=name, container=container, settings=settings.keys()) failed_settings = dict()
if not os.path.isdir(sourcepath): _LOG.error('Path is not present: %s', sourcepath) return False
if not os.path.isdir(sourcepath): _LOG.error('Path is not present: %s', sourcepath) return False
import salt.utils import hashlib import datetime import socket import salt.utils.network import salt.utils.validate.net
try:
__virtualname__ = 'network'
addresses.append(line.strip()) continue
hwaddr = salt.utils.alias_function(hw_addr, 'hwaddr')
import logging
import salt.utils
__virtualname__ = 'pkg'
comps[1] = comps[1].lstrip('"').rstrip('"')
if iface_type not in ['slave']: return __salt__['cmd.run']('ip link set {0} down'.format(iface)) return None
if iface_type not in ['slave']: return __salt__['cmd.run']('ip link set {0} up'.format(iface)) return None
import logging import json
keystone.auth_key: 203802934809284k2j34lkj2l3kj43k
keystone.auth_key: 203802934809284k2j34lkj2l3kj43k
keystone.auth_key: 303802934809284k2j34lkj2l3kj43k
import logging
import salt.utils.openstack.swift as suos
log = logging.getLogger(__name__)
from __future__ import absolute_import import logging
from salt.exceptions import SaltInvocationError
#pylint: disable=E0602
from __future__ import absolute_import import logging import time import json from boto.ec2.blockdevicemapping import BlockDeviceMapping, BlockDeviceType
import salt.utils import salt.utils.compat import salt.ext.six as six from salt.exceptions import SaltInvocationError, CommandExecutionError
try: import boto import boto.ec2 HAS_BOTO = True except ImportError: HAS_BOTO = False
from __future__ import print_function from __future__ import absolute_import import copy import logging
import salt.utils from salt.exceptions import CommandExecutionError
__virtualname__ = 'pkg' log = logging.getLogger(__name__)
lines = __salt__['cmd.run_stdout']("/bin/pkg list -Huv").splitlines() for line in lines: upgrades[_ips_get_pkgname(line)] = _ips_get_pkgversion(line) return upgrades
old = list_pkgs()
cmd = ['pkg', 'update', '-v', '--accept'] out = __salt__['cmd.run_all'](cmd, output_loglevel='trace', python_shell=False)
if any([salt.utils.is_true(kwargs.get(x)) for x in ('removed', 'purge_desired')]): return {}
for line in lines: name = _ips_get_pkgname(line) version = _ips_get_pkgversion(line) __salt__['pkg_resource.add_pkg'](ret, name, version)
available_version = salt.utils.alias_function(latest_version, 'available_version')
return name
return name
return {}
old = list_pkgs()
cmd += '{0}'.format(pkg2inst)
__context__.pop('pkg.list_pkgs', None) new = list_pkgs() ret = salt.utils.compare_dicts(old, new)
if test: return 'Test succeeded.'
old = list_pkgs()
cmd = '/bin/pkg uninstall -v {0}'.format(pkg2rm) out = __salt__['cmd.run_all'](cmd, output_loglevel='trace')
__context__.pop('pkg.list_pkgs', None) new = list_pkgs() ret = salt.utils.compare_dicts(old, new)
import logging import salt.utils
__virtualname__ = 'pkg'
#pylint: disable=E0602
from __future__ import absolute_import import logging import json
import salt.utils.boto3 import salt.utils.compat import salt.utils from salt.exceptions import SaltInvocationError from salt.ext.six import string_types
time.sleep((2 ** (RoleRetries - retry)) + (random.randint(0, 1000) / 1000)) continue
time.sleep((2 ** (RoleRetries - retry)) + (random.randint(0, 1000) / 1000)) continue
#pylint: disable=E0602
import logging import re
try: import boto import boto.ec2 logging.getLogger('boto').setLevel(logging.CRITICAL) HAS_BOTO = True except ImportError: HAS_BOTO = False
from __future__ import absolute_import import logging import string import json import datetime
import salt.utils.boto3 import salt.utils.compat
from __future__ import absolute_import
__virtualname__ = 'influxdb'
if hasattr(client, 'create_user'): client.create_user(name, passwd) return True
if database: return client.add_database_user(name, passwd) return client.add_cluster_admin(name, passwd)
import logging import os import subprocess import re import collections import decimal
from salt.ext import six from salt.ext.six.moves import zip
import salt.utils import salt.utils.decorators as decorators from salt.utils.decorators import depends from salt.exceptions import CommandExecutionError
if not comps: continue
if len(stats) < h_len: h_len = len(stats) dev_stats[disk].append(stats)
if h_len < len(dev_header): sys_header = dev_header[h_len:] dev_header = dev_header[0:h_len]
from __future__ import absolute_import import re
#import salt.ext.six as six
from salt.exceptions import ( #CommandExecutionError, SaltInvocationError ) from salt.utils import warn_until
_version_ary = __version__.split('.') CUR_VER = SaltStackVersion(_version_ary[0], _version_ary[1]) BORON = SaltStackVersion.from_name('Boron')
HAS_GLANCE = False try: from glanceclient import client from glanceclient import exc HAS_GLANCE = True except ImportError: pass
HAS_KEYSTONE = False try: from keystoneclient.v2_0 import client as kstone #import keystoneclient.apiclient.exceptions as kstone_exc HAS_KEYSTONE = True except ImportError: pass
g_endpoint_url = re.sub('/v2', '', g_endpoint_url['internalurl'])
raise SaltInvocationError('Only can use keystone admin token ' + 'with Glance API v1')
return ret
import salt.utils import salt.utils.mac_utils from salt.exceptions import SaltInvocationError
import logging
import salt.utils
__virtualname__ = 'group'
retcode = __salt__['cmd.retcode']('pw groupmod {0} -m {1}'.format( name, username), python_shell=False)
retcode = __salt__['cmd.retcode']('pw groupmod {0} -d {1}'.format( name, username), python_shell=False)
import glob import logging import os import stat
import salt.utils
__virtualname__ = 'service'
if 'unknown' in out: return '3' else: return out.split()[1]
return bool(os.stat( os.path.join('/etc/init.d', name)).st_mode & stat.S_IXUSR)
result = _chkconfig_is_enabled(name, runlevel) if result: return True
return [x for x in _services if _service_is_sysv(x)]
import re import logging
import salt.utils
new_conf.append(new_line)
new_conf.append(_format_master(**line))
new_conf.append(line)
new_conf.append(new_line)
#========================================================================== #smtp inet n - n - - smtpd if private == 'y': private = '-'
return pairs, conf_list
continue
import copy import logging import json
import salt.utils
log = logging.getLogger(__name__)
__virtualname__ = 'psget'
flags = [('Name', name)]
flags = [('Name', name)]
cmd = 'Uninstall-Module "{0}"'.format(name) no_ret = _pshell(cmd) return name not in list_modules()
flags = [('Name', name)]
cmd = 'Get-PSRepository "{0}"'.format(name) no_ret = _pshell(cmd) return name not in list_modules()
import logging import hmac import base64 import subprocess
HAS_LIBS = False try: import splunklib.client from splunklib.client import AuthenticationError from splunklib.binding import HTTPError HAS_LIBS = True except ImportError: pass
return True
kwargs = {} roles = [role.name for role in user.role_entities]
__salt__['file.remove'](tmpfilename)
import logging
try: import elasticsearch logging.getLogger('elasticsearch').setLevel(logging.CRITICAL) HAS_ELASTICSEARCH = True except ImportError: HAS_ELASTICSEARCH = False
import os import re import sys import uuid import string
import salt.utils from salt.state import STATE_INTERNAL_KEYWORDS as _STATE_INTERNAL_KEYWORDS from salt.exceptions import SaltException from salt.ext import six
return '/etc/sysconfig/scripts/SuSEfirewall2-custom'
kwargs.pop('name', None) kwargs.pop('state', None)
after_jump = [] after_jump_arguments = (
'to-port',
add_arg('-m', '--match', dest='match', action='append')
import os import datetime try: import spwd except ImportError: pass
import salt.utils from salt.exceptions import CommandExecutionError try: import salt.utils.pycrypto HAS_CRYPT = True except ImportError: HAS_CRYPT = False
import logging import salt.utils
from __future__ import absolute_import import logging
from salt.ext.six.moves.urllib.parse import urlencode as _urlencode
from salt.exceptions import SaltInvocationError import salt.utils.pushover
import re
import salt.utils from salt import utils, exceptions
import salt.utils
__virtualname__ = 'group'
from __future__ import absolute_import import os import re import uuid import logging
import salt.utils import salt.utils.fsutils from salt.exceptions import CommandExecutionError
import salt.ext.six as six
image_path = "{0}/ext2_saved".format(mountpoint) orig_fstype = ret['before']['type']
import os import errno import logging import re import string
import salt.utils import salt.utils.itertools from salt.exceptions import SaltInvocationError, CommandExecutionError
return 'UTC'
from __future__ import absolute_import import datetime import hashlib import logging import re import os import socket
import salt.utils import salt.utils.decorators as decorators import salt.utils.network import salt.utils.validate.net from salt.exceptions import CommandExecutionError
import salt.ext.six as six
if salt.utils.is_windows(): return (False, 'The network execution module cannot be loaded on Windows: use win_network instead.') return True
continue
local_addr = ''.join(x for x in local_addr if x not in '[]')
if salt.utils.is_sunos(): traceroute_version = [0, 0, 0] else: cmd2 = 'traceroute --version' out2 = __salt__['cmd.run'](cmd2) try:
hwaddr = salt.utils.alias_function(hw_addr, 'hwaddr')
host_c = salt.utils.fopen('/etc/hosts', 'r').readlines()
host[host.index(o_hostname.split('.')[0])] = hostname.split('.')[0]
if __grains__['os_family'] == 'RedHat': network_c = salt.utils.fopen('/etc/sysconfig/network', 'r').readlines()
if parts[0].endswith('sh:'): out = ' '.join(parts[1:]) ret['comment'] = out
import salt.utils
import os import re import plistlib from distutils.version import LooseVersion
import salt.utils import salt.utils.decorators as decorators from salt.exceptions import CommandExecutionError
import salt.ext.six as six
__virtualname__ = 'service'
true_path = os.path.realpath(file_path) if not os.path.exists(true_path): continue
with salt.utils.fopen(file_path): plist = plistlib.readPlist(true_path)
return services[name]
return service
return service
raise CommandExecutionError('Service not found: {0}'.format(name))
return_stdout = kwargs.pop('return_stdout', False)
cmd = ['launchctl', sub_cmd] cmd.extend(args)
kwargs['python_shell'] = False ret = __salt__['cmd.run_all'](cmd, **kwargs)
service = _get_service(name) label = service['plist']['Label']
return launchctl('list', label, return_stdout=True, output_loglevel='trace', runas=runas)
return launchctl('list', return_stdout=True, output_loglevel='trace', runas=runas)
service = _get_service(name) label = service['plist']['Label']
return launchctl('enable', 'system/{0}'.format(label), runas=runas)
service = _get_service(name) label = service['plist']['Label']
return launchctl('disable', 'system/{0}'.format(label), runas=runas)
service = _get_service(name) path = service['file_path']
return launchctl('load', path, runas=runas)
service = _get_service(name) path = service['file_path']
return launchctl('unload', path, runas=runas)
if enabled(name): stop(name, runas=runas) start(name, runas=runas)
if sig: return __salt__['status.pid'](sig)
try: list_(name=name, runas=runas) return True except CommandExecutionError: return False
return not enabled(name, runas=runas)
enabled = get_enabled(runas=runas)
available = list(_available_services().keys())
return sorted(set(enabled + available))
stdout = list_(runas=runas) service_lines = [line for line in stdout.splitlines()]
enabled = [] for line in service_lines: if line.startswith('PID'): continue
#pylint: disable=E0602
import logging
import salt.utils.compat
import logging import re import os from salt.ext.six.moves import map
try: import win32gui import win32con HAS_WIN32 = True except ImportError: HAS_WIN32 = False
import salt.utils
log = logging.getLogger(__name__)
return list(map(_normalize_dir, ret))
salt '*' win_path.add 'c:\\python27' 0
if index < 0: index = len(sysPath) + index + 1 if index > len(sysPath): index = len(sysPath)
try: currIndex = sysPath.index(path) if currIndex != index: sysPath.pop(currIndex) else: return True except ValueError: pass
if regedit: return rehash() else: return False
import json import logging import random import string
import salt.utils import salt.utils.itertools import salt.ext.six as six from salt.exceptions import SaltInvocationError from salt.ext.six.moves import range from salt.exceptions import CommandExecutionError
data_rows = _strip_listing_to_done(cmdoutput.splitlines())
clear_pw = True password = ''.join(random.SystemRandom().choice( string.ascii_uppercase + string.digits) for x in range(15))
res2 = clear_password(name, runas)
delete_user(name, runas) msg = 'Error' return _format_response(res2, msg)
import logging import os import re import datetime
import salt.utils from salt.exceptions import CommandExecutionError, SaltInvocationError
__virtualname__ = 'lowpkg'
dselect_pkg_avail = _get_pkg_ds_avail()
from __future__ import absolute_import import salt.utils
__virtualname__ = 'service'
import salt.utils
import os import logging
import os import ast import logging
import salt.utils import salt.payload
import salt.ext.six as six
import json
import salt.utils
from __future__ import absolute_import import os import logging
import salt.utils from salt.exceptions import CommandExecutionError from salt.ext.six import string_types
try:
__virtualname__ = 'win_dacl'
from __future__ import absolute_import import os import glob import tempfile import time import logging
import salt.utils import salt.crypt
import salt.ext.six as six
log = logging.getLogger(__name__)
import os
import salt.utils
from __future__ import absolute_import import copy import logging import re
import salt.utils import salt.utils.mac_utils from salt.exceptions import CommandExecutionError
import salt.ext.six as six
if any([salt.utils.is_true(kwargs.get(x)) for x in ('removed', 'purge_desired')]): return {}
available_version = salt.utils.alias_function(latest_version, 'available_version')
if pkgs is None: version_num = kwargs.get('version') variant_spec = kwargs.get('variant') spec = None
import logging
import salt.utils
__virtualname__ = 'ntp'
import json import logging
import salt.utils import salt.modules.cmdmod from salt.exceptions import CommandExecutionError
__func_alias__ = { 'list_': 'list' }
npm_output = result['stdout'] or result['stderr'] try: return json.loads(npm_output) except ValueError: pass
while not lines[0].startswith('{') and not lines[0].startswith('['): lines = lines[1:]
return npm_output
if pkg: pkg = _cmd_quote(pkg)
if pkg: pkg = _cmd_quote(pkg)
if result['retcode'] != 0 and result['stderr']: raise CommandExecutionError(result['stderr'])
#pylint: disable=E0602
from __future__ import absolute_import import logging import socket
subnets = conn.get_all_subnets(subnet_ids=subnets)
from __future__ import absolute_import import logging import json
from salt.exceptions import CommandExecutionError from salt.ext import six
from cassandra.cluster import Cluster from cassandra.cluster import NoHostAvailable from cassandra.connection import ConnectionException, ConnectionShutdown from cassandra.auth import PlainTextAuthProvider from cassandra.query import dict_factory HAS_DRIVER = True
if not isinstance(value, six.text_type): if not isinstance(value, (set, list, dict)): value = str(value) values[key] = value
replication_map = { 'class': replication_strategy }
import logging
import salt.utils
import logging import glob import re
from salt.ext.six.moves import shlex_quote as _cmd_quote
import salt.utils.systemd
__virtualname__ = 'service'
if service != 'README': ret.add(service)
from __future__ import absolute_import import os
from salt.ext.six import string_types
import salt.utils from salt.exceptions import CommandExecutionError, CommandNotFoundError
return True
if isinstance(val, string_types): if val.lower() == 'true': val = True elif val.lower() == 'false': val = False ret[key] = val
from __future__ import absolute_import import time import datetime
from salt.exceptions import SaltInvocationError, CommandExecutionError
import salt.ext.six as six try: import salt.utils.psutil_compat as psutil
return psutil.TOTAL_PHYMEM
return psutil.NUM_CPUS
b_time = int(psutil.boot_time())
try:
#pylint: disable=E0602
from __future__ import absolute_import import logging import json import yaml
import salt.utils.compat import salt.utils.odict as odict import salt.utils.boto
from salt.ext.six import string_types
conn.get_instance_profile(name) return True
conn.create_instance_profile(name) log.info('Created {0} instance profile.'.format(name))
_policy = _policy.get_role_policy_response.policy_document _policy = _unquote(_policy) _policy = json.loads(_policy, object_pairs_hook=odict.OrderedDict) return _policy
arn = ret['get_user_response']['get_user_result']['user']['arn']
def ordered_dict_presenter(dumper, data): return dumper.represent_dict(data.items())
from __future__ import absolute_import import logging import os import time import re
import salt.utils
return list(os.walk('/sys/fs/bcache/'))[0][1][0]
return os.path.basename(_bcsys(dev, 'cache'))
cache = uuid()
if bucket_size is None: bucket_size = _size_map(_fssys('bucket_size'))
cache = uuid() if cache: if not force: log.error('BCache cache {0} is already on the system'.format(cache)) return False cache = _bdev()
if cache: if not stop(): return False elif not _wipe(cache): return False
if not _wipe(dev): return False
if not _run_all(cmd, 'error', 'Error creating bcache partitions on {0}: {{0}}'.format(dev)): return False dev = '{0}2'.format(dev)
cmd = 'make-bcache --cache /dev/{0} --block {1} --wipe-bcache'.format(dev, block_size)
if bucket_size: cmd += ' --bucket {0}'.format(bucket_size)
updates = dict([(key, val) for key, val in kwargs.items() if not key.startswith('__')])
if statii[dev]['cache'] == cuuid: count += 1
result['uuid'] = uuid() base_attr = ['block_size', 'bucket_size', 'cache_available_percent', 'cache_replacement_policy', 'congested']
result.update(_sysfs_parse(_bcpath(dev), base_attr, stats, config, internals)) result.update(_sysfs_parse(_fspath(), base_attr, stats, config, internals))
back_uuid = uuid(dev) if back_uuid is not None: result['cache'] = back_uuid
if superblock: result['superblock'] = super_(dev)
return os.path.join('/sys/block/', dev)
intfs = __salt__['sysfs.interfaces'](path)
del intfs['w']
bintflist = [intf for iflist in bintf.values() for intf in iflist] result.update(__salt__['sysfs.read'](bintflist, path))
for boolkey in ('running', 'writeback_running', 'congested'): if boolkey in result: result[boolkey] = bool(result[boolkey])
block_sizes = ('hw_sector_size', 'minimum_io_size', 'physical_block_size', 'logical_block_size') discard_sizes = ('discard_max_bytes', 'discard_max_hw_bytes', )
cmd += ' seek={0}'.format((size/1024**2) - blocks) endres += _run_all(cmd, 'warn', wipe_failmsg)
import ctypes import string
import salt.ext.six as six import salt.utils
__virtualname__ = 'disk'
import logging import re import os HAS_DBUS = False try: import dbus HAS_DBUS = True except ImportError: pass
import salt.utils import salt.utils.locales import salt.utils.systemd import salt.ext.six as six from salt.exceptions import CommandExecutionError
__virtualname__ = 'locale'
cmd = 'grep "^LANG=" /etc/default/locale'
update_locale = salt.utils.which('update-locale') if update_locale is None: raise CommandExecutionError( 'Cannot set locale: "update-locale" was not found.')
__salt__['file.replace']( '/etc/default/locale', '^LANG=.*', 'LANG="{0}"'.format(locale), append_if_not_found=True )
if not locale_info['charmap'] and not on_ubuntu: locale_info['charmap'] = locale_info['codeset'] locale = salt.utils.locales.join_locale(locale_info)
import logging import json from lxml import etree
log = logging.getLogger(__name__)
__virtualname__ = 'junos'
import logging
import salt.utils
from __future__ import absolute_import import json import logging
import logging
import salt.utils import salt.utils.decorators as decorators
import json
import salt.utils HAS_CLOUD = False try:
__virtualname__ = 'saltcloud'
import salt.utils as utils
import os import logging
import salt.ext.six as six
to_unset = [key for key in os.environ if key not in environ] for key in to_unset: ret[key] = setval(key, False, false_unsets, permanent=permanent)
import logging import time from datetime import datetime
try: import pythoncom import wmi import win32net import win32api import win32con import pywintypes from ctypes import windll HAS_WIN32NET_MODS = True except ImportError: HAS_WIN32NET_MODS = False
import salt.utils import salt.utils.locales from salt.modules.reg import read_value
log = logging.getLogger(__name__)
__virtualname__ = 'system'
system_info = win32net.NetServerGetInfo(None, 101)
if desc: system_info['comment'] = desc.decode('utf-8') else: return False
if isinstance(account_ou, str): account_ou = account_ou.split('\\') account_ou = ''.join(account_ou)
dt_obj = salt.utils.date_cast(newtime)
return set_system_date_time(hours=int(dt_obj.strftime('%H')), minutes=int(dt_obj.strftime('%M')), seconds=int(dt_obj.strftime('%S')))
time_tuple = (years, months, days, hours, minutes, seconds, 0)
dt_obj = salt.utils.date_cast(newdate)
return set_system_date_time(years=int(dt_obj.strftime('%Y')), months=int(dt_obj.strftime('%m')), days=int(dt_obj.strftime('%d')))
checks = (get_pending_update, get_pending_file_rename, get_pending_servermanager, get_pending_component_servicing, get_pending_computer_name, get_pending_domain_join)
import salt.utils import salt.ext.six as six
return True
import errno import logging
import salt.utils from salt.exceptions import CommandExecutionError, SaltInvocationError
import os import sys import time import traceback import random
import salt import salt.utils import salt.version import salt.loader import salt.ext.six as six from salt.utils.decorators import depends
__func_alias__ = { 'true_': 'true', 'false_': 'false' }
for argument in args: ret['args'].append(str(type(argument)))
for key, val in six.iteritems(kwargs): ret['kwargs'][key] = str(type(val))
from __future__ import absolute_import import logging
import salt.ext.six as six
HAS_KEYSTONE = False try: from keystoneclient.v2_0 import client import keystoneclient.exceptions HAS_KEYSTONE = True except ImportError: pass
def get(key, default=None): return connection_args.get('connection_' + key, __salt__['config.get'](prefix + key, default))
if insecure: kwargs['insecure'] = True
return ret
import salt.utils import salt.utils.decorators as decorators
@decorators.memoize def __detect_os(): return salt.utils.which('nginx')
from __future__ import absolute_import import logging import re
from salt.exceptions import CommandExecutionError import salt.utils
cmd = '--{0}-{1}={2} --permanent'.format(action, _type, name)
import collections import logging import os import sys import traceback
import salt.crypt import salt.utils.event import salt.payload import salt.transport import salt.ext.six as six
log.warning('Local mode detected. Event with tag {0} will NOT be sent.'.format(tag)) return False
if 'master_uri' not in __opts__: __opts__['master_uri'] = 'tcp://{ip}:{port}'.format( ip=salt.utils.ip_bracket(__opts__['interface']),
if isinstance(data, collections.Mapping): data_dict.update(data)
import os import glob
__virtualname__ = 'service'
import logging import salt.utils
from __future__ import absolute_import import logging import re import pprint import time
from salt.exceptions import CommandExecutionError
import salt.ext.six as six
import salt.utils import salt.utils.fsutils from salt.exceptions import CommandExecutionError from salt.exceptions import get_error_message as _get_error_message
import salt.utils from salt.exceptions import CommandExecutionError
__virtualname__ = 'sysctl'
return salt.utils.is_proxy() and 'proxy' in __opts__
import time import logging
import salt.payload import salt.transport import salt.utils.args from salt.exceptions import SaltReqTimeoutError
import salt.ext.six as six from salt.ext.six.moves import range, zip from salt.ext.six.moves.urllib.parse import urlparse as _urlparse
if salt.utils.is_windows(): return (False, 'The file execution module cannot be loaded: only available on non-Windows systems - use win_file instead.') return True
return os.path.join(__salt__['config.get']('cachedir'), 'file_backup')
gid = group_to_gid(gid)
return ''
return gid
return uid
return os.lchown(path, uid, gid)
path = os.path.expanduser(path)
path = os.path.expanduser(path)
path = os.path.expanduser(path)
#after = _sed_esc(after, escape_all) limit = _sed_esc(limit, escape_all)
path = os.path.realpath(os.path.expanduser(path))
if not os.path.isfile(path): raise SaltInvocationError('File not found: {0}'.format(path))
if not salt.utils.istextfile(path): raise SaltInvocationError( 'Cannot perform string replacements on a binary file: {0}'.format(path))
if not found: return False
try: temp_file = _mkstemp_copy(path=path, preserve_inode=False) except (OSError, IOError) as exc: raise CommandExecutionError("Exception: {0}".format(exc))
return ''.join(difflib.unified_diff(orig_file, new_file))
if before is None and after is None and not match: match = content
if (idx < len(lines) and _starts_till(lines[idx + 1], cnd) < 0) or idx + 1 == len(lines): out.append(cnd)
has_changes = False
repl = str(repl)
if nrepl > 0: found = True has_changes = True if pattern != repl else has_changes
if re.search('^{0}$'.format(re.escape(content)), r_data, flags=flags_num): found = True
try: temp_file = _mkstemp_copy(path=path, preserve_inode=preserve_inode) except (OSError, IOError) as exc: raise CommandExecutionError("Exception: {0}".format(exc))
if 0 != len(new_file): if not new_file[-1].endswith('\n'): new_file[-1] += '\n' new_file.append(not_found_content + '\n')
temp_file = _mkstemp_copy(path=path, preserve_inode=preserve_inode)
try: fh_ = salt.utils.atomicfile.atomic_open(path, 'w') for line in new_file: fh_.write(line) finally: fh_.close()
in_block = True
in_block = False
if content and content[-1] == '\n': content = content[:-1]
for cline in content.split('\n'): new_file.append(cline + '\n')
old_content += line result = None
raise CommandExecutionError( 'Unterminated marked block. End of file reached before marker_end.' )
new_file.insert(0, marker_end + '\n') new_file.insert(0, content) new_file.insert(0, marker_start + '\n') done = True
if backup is not False: shutil.copy2(path, '{0}{1}'.format(path, backup))
try: fh_ = salt.utils.atomicfile.atomic_open(path, 'w') for line in new_file: fh_.write(line) finally: fh_.close()
check_perms(path, None, perms['user'], perms['group'], perms['mode'])
return replace(path, pattern, '', flags=flags, bufsize=bufsize, dry_run=True, search_only=True, show_changes=False, ignore_if_missing=ignore_if_missing)
if '-N' not in cmd and '--forward' not in cmd: cmd.append('--forward')
if not has_rejectfile_option: cmd.append('--reject-file=-')
pstat = os.lstat(path)
return ret
raise CommandExecutionError( 'none of the specified sources were found' )
sfn = '' source_sum = {}
if source and not (not follow_symlinks and os.path.islink(real_name)): name_sum = get_hash(real_name, source_sum['hash_type']) else: name_sum = None
ret['changes'].pop('diff', None) return _error(ret, 'Parent directory not present')
if mode: current_umask = os.umask(0o77)
makedirs_perms(directory, user, group, mode)
dirname = os.path.normpath(os.path.dirname(path))
directories_to_create.reverse() for directory_to_create in directories_to_create: log.debug('Creating directory: %s', directory_to_create) mkdir(directory_to_create, user=user, group=group, mode=mode)
if exc.errno != errno.EEXIST: raise
return False
if exc.errno != errno.EEXIST: raise else: ret['comment'] = 'File {0} exists and cannot be overwritten'.format(name)
return False
if exc.errno != errno.EEXIST: raise else: ret['comment'] = 'File {0} exists and cannot be overwritten'.format(name)
return False
if exc.errno != errno.EEXIST: raise else: ret['comment'] = 'File {0} exists and cannot be overwritten'.format(name)
src_dir = parent_dir.replace(':', '_')
bkdir = os.path.join(bkroot, src_dir)
strpfmt = '{0}_%a_%b_%d_%H-%M-%S_%f_%Y'.format(basename)
continue
bkdir = os.path.join(bkroot, parent_dir[1:])
continue
pids = {} procfs = os.listdir('/proc/') for pfile in procfs: try: pids[int(pfile)] = [] except ValueError: pass
files = {} for pid in pids: ppath = '/proc/{0}'.format(pid) try: tids = os.listdir('{0}/task'.format(ppath)) except OSError: continue
fd_ = []
#except:
for fdpath in fd_: try: name = os.path.realpath(fdpath) os.stat(name) except OSError: continue
files[name].append(pid) files[name] = sorted(set(files[name]))
from __future__ import absolute_import import datetime
import salt.utils import salt.utils.itertools from salt.exceptions import SaltInvocationError
import salt.ext.six as six
return line
query = 'CREATE DATABASE "{0}"'.format(name)
ret = _psql_prepare_and_run(['-c', query], user=user, host=host, port=port, maintenance_db=maintenance_db, password=password, runas=runas) return ret['retcode'] == 0
ret = _psql_prepare_and_run(['-c', query], user=user, host=host, port=port, maintenance_db=maintenance_db, password=password, runas=runas) return ret['retcode'] == 0
_x = lambda s: s if return_password else ''
if not bool(role): log.info( '{0} \'{1}\' could not be found'.format(typ_.capitalize(), name) ) return False
sub_cmd = 'DROP ROLE "{0}"'.format(name) _psql_prepare_and_run( ['-c', sub_cmd], runas=runas, host=host, user=user, port=port, maintenance_db=maintenance_db, password=password)
cmdret = _psql_prepare_and_run(['-f', sqlfile.name], user=user, runas=runas, host=host, port=port, password=password, maintenance_db=dbname) return cmdret
sub_cmd = 'DROP SCHEMA "{0}"'.format(name) _psql_prepare_and_run( ['-c', sub_cmd], runas=user, maintenance_db=dbname, host=db_host, user=db_user, port=db_port, password=db_password)
from __future__ import absolute_import import datetime import os import re import fnmatch import collections import copy import time
import salt.ext.six as six
__func_alias__ = { 'time_': 'time' }
get_version = { 'Linux': linux_cpustats, 'FreeBSD': freebsd_cpustats, }
get_version = { 'Linux': linux_cpuinfo, 'FreeBSD': freebsd_cpuinfo, }
get_version = { 'Linux': linux_diskstats, 'FreeBSD': freebsd_diskstats, }
fstypes.add('*')
selected.add(arg)
fstypes.add(arg)
get_version = { 'Linux': linux_vmstats, 'FreeBSD': freebsd_vmstats, }
get_version = { 'Linux': linux_netstats, 'FreeBSD': freebsd_netstats, }
get_version = { 'Linux': linux_netdev, 'FreeBSD': freebsd_netdev, }
get_version = { 'Linux': linux_version, 'FreeBSD': lambda: __salt__['cmd.run']('sysctl -n kern.version'), }
port = 4505 master_ip = None
if master is not None: tmp_ip = _host_to_ip(master) if tmp_ip is not None: master_ip = tmp_ip
from __future__ import absolute_import import logging
import salt.utils
import salt.ext.six as six
if not __execute_cmd('config -g cfgUserAdmin -o \ cfgUserAdminUserName -i {0} {1}'.format(uid, username)): delete_user(username, uid) return False
if not set_permissions(username, permissions, uid): log.warning('unable to set user permissions') delete_user(username, uid) return False
if not change_password(username, password, uid): log.warning('unable to set user password') delete_user(username, uid) return False
if not __execute_cmd('config -g cfgUserAdmin -o \ cfgUserAdminEnable -i {0} 1'.format(uid)): delete_user(username, uid) return False
if uid is None: user = list_users() uid = user[username]['index']
for i in permissions.split(','): perm = i.strip()
from __future__ import absolute_import import copy import logging try: import pwd HAS_PWD = True except ImportError: HAS_PWD = False
import salt.ext.six as six
import salt.utils from salt.exceptions import CommandExecutionError from salt.utils import locales
__virtualname__ = 'user'
from __future__ import absolute_import import copy as pycopy import difflib import os import yaml
import salt.utils import salt.utils.odict
import salt.ext.six as six
salt '*' schedule.list show_all=True
salt '*' schedule.list show_disabled=False
if job.startswith('__') and not show_all: del schedule[job] continue
if 'enabled' not in schedule[job]: schedule[job]['enabled'] = True
if not show_disabled and not schedule[job]['enabled']: del schedule[job] continue
if schedule[job]['_seconds'] > 0: schedule[job]['seconds'] = schedule[job]['_seconds'] elif 'seconds' in schedule[job]: del schedule[job]['seconds']
del schedule[job]['_seconds']
ret['comment'] = 'Event module not available. Schedule add failed.' ret['result'] = True
ret['comment'] = 'Event module not available. Schedule add failed.'
if 'enabled' not in kwargs: schedule[name]['enabled'] = True
ret['comment'] = 'Event module not available. Schedule add failed.'
ret['comment'] = 'Event module not available. Schedule enable job failed.'
ret['comment'] = 'Event module not available. Schedule enable job failed.'
ret['comment'] = 'Event module not available. Schedule save failed.'
ret['comment'] = 'Event module not available. Schedule enable job failed.'
ret['comment'] = 'Event module not available. Schedule enable job failed.'
errors = [] minions = [] for minion in response: minions.append(minion) if not response[minion]: errors.append(minion)
errors = [] minions = [] for minion in response: minions.append(minion) if not response[minion]: errors.append(minion)
try: import sqlite3 HAS_SQLITE3 = True except ImportError: HAS_SQLITE3 = False
def __virtual__(): if not HAS_SQLITE3: return (False, 'The sqlite3 execution module failed to load: the sqlite3 python library is not available.') return True
import re import logging import shlex import yaml
import salt.utils from salt.utils.locales import sdecode as _sdecode from salt.exceptions import SaltInvocationError
import salt.ext.six as six
GUID_REGEX = re.compile(r'{?([0-9A-F]{8}-[0-9A-F]{4}-[0-9A-F]{4}-[0-9A-F]{4}-[0-9A-F]{12})}?', re.I)
cmd = ['prlctl', sub_cmd] if args: cmd.extend(_normalize_args(args))
return __salt__['cmd.run'](cmd, runas=runas)
if args is None: args = [] else: args = _normalize_args(args)
return prlctl('list', args, runas=runas)
args = [_sdecode(name)] if kill: args.append('--kill')
return prlctl('stop', args, runas=runas)
args = [_sdecode(name)] args.extend(_normalize_args(command))
return prlctl('exec', args, runas=runas)
info = prlctl('snapshot-list', [name, '--id', snap_id], runas=runas)
if not len(info): raise SaltInvocationError( u'No snapshots for VM "{0}" have ID "{1}"'.format(name, snap_id) )
name = _sdecode(name) snap_name = _sdecode(snap_name)
info = prlctl('snapshot-list', name, runas=runas)
snap_ids = _find_guids(info)
named_ids = [] for snap_id in snap_ids: if snapshot_id_to_name(name, snap_id, runas=runas) == snap_name: named_ids.append(snap_id)
if re.match(GUID_REGEX, snap_name): return snap_name.strip('{}') else: return snapshot_name_to_id(name, snap_name, strict=True, runas=runas)
name = _sdecode(name) if snap_name: snap_name = _validate_snap_name(name, snap_name, runas=runas)
args = [name] if tree: args.append('--tree') if snap_name: args.extend(['--id', snap_name])
res = prlctl('snapshot-list', args, runas=runas)
if names: snap_ids = _find_guids(res)
else: return res
name = _sdecode(name) if snap_name: snap_name = _sdecode(snap_name)
args = [name] if snap_name: args.extend(['--name', snap_name]) if desc: args.extend(['--description', desc])
return prlctl('snapshot', args, runas=runas)
name = _sdecode(name) snap_name = _validate_snap_name(name, snap_name, runas=runas)
args = [name, '--id', snap_name]
return prlctl('snapshot-delete', args, runas=runas)
name = _sdecode(name) snap_name = _validate_snap_name(name, snap_name, runas=runas)
args = [name, '--id', snap_name]
return prlctl('snapshot-switch', args, runas=runas)
import logging import socket import json from distutils.version import LooseVersion
import salt.utils
__virtualname__ = 'zabbix'
for key in kwargs.keys(): if not key.startswith('_'): params.setdefault(key, kwargs[key])
if not isinstance(usrgrps, list): usrgrps = [usrgrps] for usrgrp in usrgrps: params['usrgrps'].append({"usrgrpid": usrgrp})
for k, v in result_json.items(): if isinstance(v, list): result_json[k] += next_page_results[k]
resource = None for field in identifier_fields: if field in data: resource = get_resource(resource_name, data[field], identifier_fields, profile, subdomain, api_key) if resource is not None: break
del __context__['pagerduty_util.resource_cache'][resource_name] return _query(method='POST', action=resource_name, data=data, profile=profile, subdomain=subdomain, api_key=api_key)
import os import shutil import logging import tempfile
import salt.crypt import salt.utils import salt.utils.cloud import salt.config import salt.syspaths import uuid
log = logging.getLogger(__name__)
__func_alias__ = { 'apply_': 'apply' }
pass
import hashlib import logging import os.path import random import signal
import salt.utils from salt.ext.six.moves import range
if hasher == 'sha256': h = hashlib.sha256(password) elif hasher == 'md5': h = hashlib.md5(password) else: return NotImplemented
h.update(r['Salt']) r['Hash'] = h.hexdigest()
#pylint: disable=E0602
from __future__ import absolute_import import logging import json
import salt.utils.boto3 import salt.utils.compat import salt.utils from salt.exceptions import SaltInvocationError from salt.ext.six import string_types
import copy import os import re import logging import json
from salt.modules.cmdmod import _parse_env import salt.utils from salt.exceptions import ( CommandExecutionError, MinionError, SaltInvocationError )
try: import apt.cache import apt.debfile from aptsources import sourceslist HAS_APT = True except ImportError: HAS_APT = False
LP_SRC_FORMAT = 'deb http://ppa.launchpad.net/{0}/{1}/ubuntu {2} main' LP_PVT_SRC_FORMAT = 'deb https://{0}private-ppa.launchpad.net/{1}/{2}/ubuntu' \ ' {3} main'
__virtualname__ = 'pkg'
os.environ.update(DPKG_ENV_VARS)
for name in names: ret[name] = '' pkgs = list_pkgs(versions_as_list=True) repo = ['-o', 'APT::Default-Release={0}'.format(fromrepo)] \ if fromrepo else None
if refresh: refresh_db()
if name in all_virt and name not in pkgs: candidate = '1' else: candidate = ''
if not any( (salt.utils.compare_versions(ver1=x, oper='>=', ver2=candidate, cmp_func=version_cmp) for x in installed) ): ret[name] = candidate
if len(names) == 1: return ret[names[0]] return ret
available_version = salt.utils.alias_function(latest_version, 'available_version')
ident = re.sub(r' \[.+B\]$', '', ident) ret[ident] = True
if not _latest_version == _version: _refresh_db = True
if not _latest_version == _version: _refresh_db = True
_refresh_db = True
repo = kwargs.get('repo', '') if not fromrepo and repo: fromrepo = repo
version_num = kwargs['version']
if pkg_type == 'repository': pkgstr = '{0}={1}'.format(pkgname, version_num) else: pkgstr = pkgpath
cmd.insert(-1, '--force-yes')
rexp = re.compile('(?m)^Conf '
apt_pkg.init_system()
pass
if (is_ppa and repo_type == 'deb' and source.type == 'deb-src' and source.uri == repo_uri and source.dist == repo_dist):
refresh_db() return ret
from __future__ import absolute_import import copy import fnmatch import itertools import logging import os import re import string
from salt.ext import six from salt.ext.six.moves import zip
import salt.utils import salt.utils.itertools import salt.utils.decorators as decorators import salt.utils.pkg.rpm from salt.exceptions import ( CommandExecutionError, MinionError, SaltInvocationError )
__virtualname__ = 'pkg'
value = value.rstrip('-')
value = value.lstrip('@')
pkginfo = salt.utils.pkg.rpm.pkginfo(**cur) cur = {} if pkginfo is not None: yield pkginfo
if repo and not fromrepo: fromrepo = repo
conf = { 'reposdir': ['/etc/yum/repos.d', '/etc/yum.repos.d'], }
fn = None paths = ('/etc/yum/yum.conf', '/etc/yum.conf') for path in paths: if os.path.exists(path): fn = path break
conf[opt] = [x.strip() for x in cp.get('main', opt).split(',')]
if isinstance(basedir, six.string_types): basedir = [x.strip() for x in basedir.split(',')]
if not basedir: basedir = _get_yum_config_value('reposdir')
if refresh: refresh_db(**kwargs)
break
if len(names) == 1: return ret[names[0]] return ret
available_version = salt.utils.alias_function(latest_version, 'available_version')
if any([salt.utils.is_true(kwargs.get(x)) for x in ('removed', 'purge_desired')]): return {}
repos = tuple( x for x, y in six.iteritems(list_repos()) if str(y.get('enabled', '1')) == '1' )
cmd.extend(args)
for pkgname in ret[reponame]: sorted_versions = sorted( [_LooseVersion(x) for x in ret[reponame][pkgname]], reverse=True ) ret[reponame][pkgname] = [x.vstring for x in sorted_versions]
list_updates = salt.utils.alias_function(list_upgrades, 'list_updates')
for key, value in pkg_nfo.items(): if key == 'source_rpm': t_nfo['source'] = value else: t_nfo[key] = value
pkg_params = {name: version_num}
current_locks = list_holds(full=_yum() == 'yum')
if key is None: continue
break
target_found = True
for p_type in pkgtypes: ret[p_type].update(set(expanded[p_type]))
pkgs = [x for x in targets if x not in list_pkgs()] if not pkgs: return {}
repofile = '' for repo in repos: if repo == name: repofile = repos[repo]['file']
filerepos = _parse_repo_file(repofile)[1] return filerepos[name]
basedirs = _normalize_basedir(basedir) repos = list_repos(basedirs)
repofile = '' for arepo in repos: if arepo == repo: repofile = repos[arepo]['file']
onlyrepo = True for arepo in six.iterkeys(repos): if arepo == repo: continue if repos[arepo]['file'] == repofile: onlyrepo = False
if onlyrepo: os.remove(repofile) return 'File {0} containing repo {1} has been removed'.format( repofile, repo)
repo_opts = dict( (x, kwargs[x]) for x in kwargs if not x.startswith('__') and x not in ('saltenv',) )
todelete = [] for key in repo_opts: if repo_opts[key] != 0 and not repo_opts[key]: del repo_opts[key] todelete.append(key)
if 'enabled' not in repo_opts: repo_opts['enabled'] = int(str(repo_opts.pop('disabled', False)).lower() != 'true')
if 'mirrorlist' in repo_opts: todelete.append('baseurl') elif 'baseurl' in repo_opts: todelete.append('mirrorlist')
if 'name' in todelete: raise SaltInvocationError('The repo name cannot be deleted')
repos = {} basedirs = _normalize_basedir(basedir) repos = list_repos(basedirs)
repofile = repos[repo]['file'] header, filerepos = _parse_repo_file(repofile)
for key in todelete: if key in six.iterkeys(filerepos[repo].copy()): del filerepos[repo][key]
from __future__ import absolute_import import copy import logging import re import os import time import datetime
import salt.utils from salt.exceptions import ( CommandExecutionError, MinionError)
__virtualname__ = 'pkg'
if not salt.utils.which('zypper'): return (False, "Module zypper: zypper package manager not found") return __virtualname__
self.__xml = False self.__no_lock = False self.__no_raise = False self.__refresh = False
if self.__called: self._reset() self.__called = False
if self.__no_lock: self.__no_lock = not self.__refresh
list_updates = salt.utils.alias_function(list_upgrades, 'list_updates')
if kwargs.get('refresh', True): refresh_db()
if len(names) == 1 and len(ret): return ret[names[0]]
available_version = salt.utils.alias_function(latest_version, 'available_version')
if any([salt.utils.is_true(kwargs.get(x)) for x in ('removed', 'purge_desired')]): return {}
for alias in repos_cfg.sections(): repo_meta = _get_repo_info(alias, repos_cfg=repos_cfg)
new_url = _urlparse(url) if not new_url.path:
__zypper__.xml.call('ar', url, repo)
cmd_opt = []
if not added and not cmd_opt: raise CommandExecutionError( 'Specified arguments did not result in modification of repo' )
pkg_params = {name: version_num}
from __future__ import absolute_import import logging import os import re import datetime
import salt.utils import salt.utils.itertools import salt.utils.decorators as decorators import salt.utils.pkg.rpm from salt.ext.six.moves import zip from salt.ext import six
from salt.exceptions import CommandExecutionError, SaltInvocationError
__virtualname__ = 'lowpkg'
cmd.extend(packages)
msg = 'Failed to verify package(s)' if out['stderr']: msg += ': {0}'.format(out['stderr']) raise CommandExecutionError(msg)
if ret['retcode'] > 1: del ret['stdout'] return ret elif not ret['retcode']: return data
cmd.extend(packages)
from __future__ import absolute_import import os import logging
import salt.ext.six as six
import salt.utils
__virtualname__ = 'service'
if krel[0] > 5 or (krel[0] == 5 and krel[1] > 0): if not os.path.exists('/usr/sbin/rcctl'): return __virtualname__
with salt.utils.fopen('/etc/rc', 'r') as handle: lines = handle.readlines()
line = line[len(match.group(1)):] for daemon in start_daemon_parameter_regex.findall(line): daemons_flags[daemon] = True
if available(service): services.append(service)
from __future__ import absolute_import import logging import re
import salt.utils
import stat import os import logging
from __future__ import absolute_import, print_function import logging import os
import salt.output import salt.utils import salt.loader import salt.template from salt.exceptions import CommandExecutionError, SaltRenderError
from salt.runners.winrepo import ( genrepo as _genrepo, update_git_repos as _update_git_repos, PER_REMOTE_OVERRIDES ) from salt.ext import six try: import msgpack except ImportError:
__virtualname__ = 'winrepo'
repo = _get_local_repo_dir(saltenv)
repo = repo.split('\\') definition = name.split('.') repo.extend(definition)
sls_file = '{0}.sls'.format(os.sep.join(repo)) if not os.path.exists(sls_file):
sls_file = '{0}\\init.sls'.format(os.sep.join(repo)) if not os.path.exists(sls_file):
return 'Software definition {0} not found'.format(name)
renderers = salt.loader.render(__opts__, __salt__) config = {}
try: config = salt.template.compile_template( sls_file, renderers, __opts__['renderer'], __opts__['renderer_blacklist'], __opts__['renderer_whitelist'])
from __future__ import absolute_import, print_function import json import logging
import salt.utils.http
__func_alias__ = { 'list_': 'list' }
import os
import salt.utils
return salt.utils.pem_finger(os.path.join(__opts__['pki_dir'], 'minion.pub'), sum_type=__opts__.get('hash_type', 'md5'))
return salt.utils.pem_finger(os.path.join(__opts__['pki_dir'], 'minion_master.pub'), sum_type=__opts__.get('hash_type', 'md5'))
import logging import json
import salt.utils import salt.utils.decorators as decorators
__func_alias__ = { 'list_installed': 'list', 'update_installed': 'update', 'import_image': 'import' }
__virtualname__ = 'imgadm'
continue
result = [] for image in res['stdout'].splitlines(): image = [var for var in image.split(" ") if var] result.append(image[2])
import logging
import salt.utils from salt.exceptions import CommandExecutionError, SaltInvocationError
__func_alias__ = { 'set_': 'set' }
from __future__ import absolute_import import logging import os import stat
import salt.ext.six as six
import salt.utils
from salt.exceptions import SaltInvocationError, CommandExecutionError from salt.ext.six import string_types, integer_types import salt.utils
__func_alias__ = { 'zip_': 'zip' }
os.setegid(uinfo['gid']) os.seteuid(uinfo['uid'])
if runas: os.seteuid(euid) os.setegid(egid) if exc is not None: raise CommandExecutionError( 'Exception encountered creating zipfile: {0}'.format(exc) )
os.setegid(uinfo['gid']) os.seteuid(uinfo['uid'])
cleaned_files = [] with contextlib.closing(zipfile.ZipFile(zip_file, "r")) as zfile: files = zfile.namelist()
if info.external_attr == 2716663808: source = zfile.read(target) os.symlink(source, os.path.join(dest, target)) continue
if runas: os.seteuid(euid) os.setegid(egid) if exc is not None: raise CommandExecutionError( 'Exception encountered unpacking zipfile: {0}'.format(exc) )
if template not in salt.utils.templates.TEMPLATE_REGISTRY: raise CommandExecutionError( 'Attempted to render file paths with unavailable engine ' '{0}'.format(template) )
count = 100 if not isinstance(trim_output, bool): count = trim_output
SUPPORTED_BSD_LIKE = ['FreeBSD', 'NetBSD', 'OpenBSD']
if line.startswith('bridge name'): continue vals = line.split() if not vals: continue
if len(vals) > 1: brname = vals[0]
import re import os
import salt.utils import salt.utils.mac_utils from salt.exceptions import CommandExecutionError, SaltInvocationError
rexp = re.compile('(?m)^ [*|-] ' r'([^ ].*)[\r\n].*\(([^\)]+)')
rexp = re.compile('(?m)^ [*] ' r'([^ ].*)[\r\n].*\(([^\)]+)')
rexp1 = re.compile('(?m)^ [*|-] ' r'([^ ].*)[\r\n].*restart*')
to_ignore = name.rsplit('-', 1)[0]
rexp = re.compile('(?m)^ ["]?' r'([^,|\s].*[^"|\n|,])[,|"]?')
cmd = ['softwareupdate', '--set-catalog', url]
cmd = ['softwareupdate', '--clear-catalog']
import logging import os import re
import salt.ext.six as six import salt.utils from salt.ext.six import string_types from salt.exceptions import CommandExecutionError import salt.utils.systemd import string
__virtualname__ = 'sysctl'
regex = re.compile(r'^{0}\s+=\s+{1}$'.format(re.escape(name), re.escape(value)))
if not os.path.isfile(config): try: with salt.utils.fopen(config, 'w+') as _fh:
comps = [i.strip() for i in line.split('=', 1)]
if isinstance(comps[1], string_types) and ' ' in comps[1]: comps[1] = re.sub(r'\s+', '\t', comps[1])
if isinstance(value, string_types) and ' ' in value: value = re.sub(r'\s+', '\t', value)
import logging
import salt.utils
__virtualname__ = 'group'
from __future__ import absolute_import import errno import functools import logging import os import re import shutil import time import tempfile
import salt.defaults.exitcodes import salt.utils import salt.utils.systemd from salt.exceptions import CommandExecutionError, SaltInvocationError from salt.ext import six
if preserve_state \ and orig_state == 'stopped' \ and state(name) != 'stopped': stop(name)
time.sleep(5)
if ret: run(name, 'touch \'{0}\''.format(SEED_MARKER), python_shell=False)
list_ = salt.utils.alias_function(list_running, 'list_')
return start(name)
destroy = salt.utils.alias_function(remove, 'destroy')
if 'index' in kwargs: pull_opts.append('--dkr-index-url={0}'.format(kwargs['index']))
from __future__ import absolute_import import os import base64 import logging
import salt.utils
try: from salt._compat import ElementTree as ET HAS_ELEMENT_TREE = True except ImportError: HAS_ELEMENT_TREE = False
file_name = '{artifact_id}-{version}{classifier}.{packaging}'.format( artifact_id=artifact_id, version=version, packaging=packaging, classifier=__get_classifier_url(classifier))
artifact_metadata_url = '{artifactory_url}/{repository}/{group_url}/{artifact_id}/maven-metadata.xml'.format( artifactory_url=artifactory_url, repository=repository, group_url=group_url, artifact_id=artifact_id) log.debug('artifact_metadata_url=%s', artifact_metadata_url) return artifact_metadata_url
latest_version_url = '{artifactory_url}/api/search/latestVersion?g={group_url}&a={artifact_id}&repos={repository}'.format( artifactory_url=artifactory_url, repository=repository, group_url=group_url, artifact_id=artifact_id) log.debug('latest_version_url=%s', latest_version_url) return latest_version_url
import os import os.path import logging
import salt.utils
__func_alias__ = { 'list_': 'list' }
info.insert(2, '')
ret['error'] = 'This package does not seem to exist' return ret
import os import re import logging from salt.ext.six.moves import zip import salt.ext.six as six
HAS_AUGEAS = False try: from augeas import Augeas as _Augeas HAS_AUGEAS = True except ImportError: pass
import salt.utils from salt.exceptions import SaltInvocationError
__virtualname__ = 'augeas'
cmd, arg = command.split(' ', 1)
repo_name: my_repo
from __future__ import absolute_import import logging
from salt.exceptions import CommandExecutionError import salt.utils.http
HAS_LIBS = False try: import github import github.PaginatedList import github.NamedUser from github.GithubException import UnknownObjectException
return True
if issue.get('pull_request'): continue issue_id = issue.get('id') if output == 'full': ret[issue_id] = issue else: ret[issue_id] = _format_issue(issue)
return result['dict']
next_page = False continue
page_number = link_info.split('>')[0].split('&page=')[1]
next_page = False
import logging
import salt.utils
import logging from salt.serializers import json
import salt.utils.compat import salt.utils.odict as odict
from __future__ import absolute_import
from salt.ext.six.moves.urllib.parse import urljoin as _urljoin import salt.ext.six.moves.http_client
import salt.utils.http
__func_alias__ = { 'list_': 'list' }
if not key: query_params['recurse'] = 'True' function = 'kv/' else: function = 'kv/{0}'.format(key)
from __future__ import absolute_import import os import re import time import logging
import salt.utils from salt.exceptions import CommandExecutionError
import salt.ext.six as six
if serialized.startswith("="): serialized = serialized[1:].strip()
return [tuple(items.split("=")) for items in opt]
if len(out) == 1 and 'restore status' in out[0].lower(): return {'restore_status': out[0]}
#pylint: disable=E0602
import logging
from salt.ext.six import string_types import salt.utils.odict as odict
if isinstance(instances, str) or isinstance(instances, six.text_type): instances = [instances] conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)
if isinstance(instances, str) or isinstance(instances, six.text_type): instances = [instances] conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)
import copy import logging import re
import salt.utils from salt.exceptions import CommandExecutionError, MinionError import salt.ext.six as six
__virtualname__ = 'pkg'
if blocked and unsatisfied: ret['blocked'] = blocked
if refresh: refresh_db()
if len(names) == 1: return ret[names[0]] return ret
available_version = salt.utils.alias_function(latest_version, 'available_version')
if any([salt.utils.is_true(kwargs.get(x)) for x in ('removed', 'purge_desired')]): return {}
cmd = 'emerge-webrsync -q' if salt.utils.which('emerge-delta-webrsync'): cmd = 'emerge-delta-webrsync -q' return __salt__['cmd.retcode'](cmd, python_shell=False) == 0
cmd = 'emerge-webrsync -q' if salt.utils.which('emerge-delta-webrsync'): cmd = 'emerge-delta-webrsync -q' return __salt__['cmd.retcode'](cmd, python_shell=False) == 0
from __future__ import absolute_import import re import logging
import salt.utils
from __future__ import absolute_import import re import logging try: import pwd HAS_PWD = True except ImportError: HAS_PWD = False
try:
__virtualname__ = 'gnome'
__func_alias__ = { 'set_': 'set' }
from __future__ import absolute_import import json import os
import salt.utils
elif isinstance(value, six.string_types): if value.lower() == 'none': return None return value else: return None
data = slave if core is None else {core: {'data': slave}}
if not _is_master() and _get_none_or_value(host) is None: err = [ 'solr.pre_indexing_check can only be called by "master" minions'] return _get_return_dict(False, err)
import logging import re
import salt.utils
__virtualname__ = 'varnish'
break
#pylint: disable=E0602
from __future__ import absolute_import import datetime import logging import json import sys import email.mime.multipart
import salt.utils.odict as odict
from __future__ import absolute_import
from salt.exceptions import CommandExecutionError import salt.utils
from salt.exceptions import CommandExecutionError from salt.exceptions import SaltInvocationError import logging
import os import re
from __future__ import absolute_import import os import logging import hashlib import glob import random import ctypes import tempfile import yaml import re import datetime import ast
import salt.utils import salt.exceptions import salt.ext.six as six from salt.utils.odict import OrderedDict
try: import M2Crypto HAS_M2 = True except ImportError: HAS_M2 = False
text = _match.group(0) break
pem_body = ''.join(pem_body.split())
ret = pem_header+'\n' for i in range(0, len(pem_body), 64): ret += pem_body[i:i+64]+'\n' ret += pem_footer+'\n'
kwargs['public_key'] = get_public_key(kwargs['public_key']).replace('\n', '')
for ignore in list(_STATE_INTERNAL_KEYWORDS) + ['listen_in', 'preqrequired']: kwargs.pop(ignore, None)
kwargs.update(signing_policy)
cert.set_version(kwargs['version'] - 1)
if 'public_key' not in kwargs and 'csr' not in kwargs: kwargs['public_key'] = kwargs['signing_private_key']
extval = kwargs.get(extname) or kwargs.get(extlongname) or \ csrexts.get(extname) or csrexts.get(extlongname)
from __future__ import absolute_import import functools import copy import logging import os import pipes import time import traceback
import salt.utils from salt.exceptions import CommandExecutionError, SaltInvocationError from salt.utils import vt
return None
state = __salt__['{0}.state'.format(container_type)]
from __future__ import absolute_import from salt.ext.six.moves import zip
try: import redis HAS_REDIS = True except ImportError: HAS_REDIS = False
conn_args = {} for arg in ['host', 'port', 'db', 'password']: if arg in connection_args: conn_args[arg] = connection_args[arg]
server.ping()
server.ping()
from __future__ import absolute_import import copy import logging import time import traceback
import salt.crypt import salt.payload import salt.utils import salt.utils.network import salt.utils.event from salt.exceptions import SaltClientError
import salt.ext.six as six
time.sleep(0.5) return event_ret
if not m_data: return
pass
cmd = 'dockerng.ps' docker_hosts = get('*', cmd)
for containers in six.itervalues(docker_hosts): host = containers.pop('host') host_ips = []
import logging import sys import xml.etree.ElementTree as ET
import salt.utils import salt.utils.cloud as suc from salt.exceptions import SaltInvocationError
if isinstance(bricks, str): bricks = [bricks]
if device_vg and len(bricks) > 1: raise SaltInvocationError('Block device backend volume does not ' + 'support multiple bricks')
root = _gluster_xml('volume status {0}'.format(name)) if not _gluster_ok(root): return None
running = (volinfo[target]['status'] == '1')
log.error('Volume {0} must be stopped before deletion'.format(target)) return False
from __future__ import absolute_import from __future__ import unicode_literals import sys import logging
try:
import salt.utils from salt.exceptions import CommandExecutionError
__virtualname__ = 'reg'
return instr.encode('mbcs')
return instr
return obj
_, res = SendMessageTimeout(HWND_BROADCAST, WM_SETTINGCHANGE, 0, 0, SMTO_ABORTIFHUNG, 5000) return not bool(res)
registry = Registry() hkey = registry.hkeys[local_hive] key_path = local_key access_mask = registry.registry_32[use_32bit_registry]
key_list = [] key_list = _traverse_registry_tree(hkey, key_path, key_list, access_mask) key_list.append(r'{0}'.format(key_path))
from __future__ import absolute_import, print_function
import salt.utils
if old_value is not None: __salt__['file.sed'](makeconf, '^{0}=.*'.format(var), '')
if old_value is not None: __salt__['file.sed'](makeconf, value, '', limit=var)
value = value.replace('\\', '') if setval is None: return False return value in setval.split()
from __future__ import absolute_import, print_function import copy import fnmatch import json import logging import os import shutil import sys import tarfile import tempfile import time
import salt.config import salt.payload import salt.state import salt.utils import salt.utils.jid import salt.utils.url from salt.exceptions import SaltInvocationError
import salt.ext.six as six
__context__['retcode'] = 0
__salt__['cmd.run']('attrib -R "{0}"'.format(notify_path))
__salt__['cmd.run']('attrib -R "{0}"'.format(notify_path))
__opts__['test'] = orig_test return ret
if queue: _wait(kwargs.get('__pub_jid')) else: conflict = running(concurrent) if conflict: __context__['retcode'] = 1 return conflict
__opts__['environment'] = saltenv __opts__['pillarenv'] = pillarenv
__salt__['cmd.run'](['attrib', '-R', cache_file], python_shell=False)
__opts__['test'] = orig_test
pass
__opts__['test'] = orig_test return ret
__opts__['test'] = orig_test return ret
__opts__['test'] = orig_test if errors: __context__['retcode'] = 1 return errors return high_
__opts__['test'] = orig_test return ret
__salt__['saltutil.refresh_modules']()
__salt__['saltutil.refresh_modules']()
import os
import salt.utils import salt.utils.decorators as decorators from salt.exceptions import CommandNotFoundError
__virtualname__ = 'service'
from __future__ import absolute_import from json import JSONEncoder, loads
return loads(_MssqlEncoder().encode({'resultset': cur.fetchall()}))['resultset']
return (('Could not run the query', ), (str(e), ))
return len(tsql_query("SELECT database_id FROM sys.databases WHERE NAME='{0}'".format(database_name), **kwargs)) == 1
return len(tsql_query(query='sp_helprole "{0}"'.format(role), as_dict=True, **kwargs)) == 1
return len(tsql_query(query="SELECT name FROM sys.syslogins WHERE name='{0}'".format(login), **kwargs)) == 1
if 'database' not in kwargs: return False
return len(tsql_query(query="SELECT name FROM sysusers WHERE name='{0}'".format(username), **kwargs)) == 1
if 'database' not in kwargs: return False if user_exists(username, **kwargs): return False
from __future__ import absolute_import import copy import errno import glob import logging import os import re import shlex
import salt.utils.itertools import salt.utils.systemd from salt.exceptions import CommandExecutionError from salt.ext import six
__virtualname__ = 'service'
__context__[contextkey] = True
ret = _default_runlevel()
ret.update(set( [x for x in _get_sysv_services() if _sysv_enabled(x)] )) return sorted(ret)
ret.update(set( [x for x in _get_sysv_services() if not _sysv_enabled(x)] )) return sorted(ret)
return sorted(ret)
import os.path
import salt.utils import salt.ext.six as six from salt.exceptions import CommandExecutionError
__virtualname__ = 'lvm'
if not pvdisplay(device): cmd.append(device) elif not override: raise CommandExecutionError('Device "{0}" is already an LVM physical volume.'.format(device))
return True
for device in devices: if not pvdisplay(device): raise CommandExecutionError('Device "{0}" was not affected.'.format(device))
for device in devices: if pvdisplay(device): raise CommandExecutionError('Device "{0}" was not affected.'.format(device))
import logging log = logging.getLogger(__file__)
from napalm import get_network_driver HAS_NAPALM = True
import os import re import subprocess import sys
import salt.utils
try: for link in listdir: path = dirpath + link readlink = os.readlink(path) filenames = []
kernel_current = __salt__['cmd.run']('uname -a') for kernel in kernel_versions: if kernel in kernel_current: kernel_restart = False break
is_oneshot = True
from __future__ import absolute_import import logging import uuid import re
import salt.utils
import salt.modules.cmdmod
val = '\n'.join([v for v in val.split('\n') if not v.startswith('#')])
record = { 'handle': handle, 'description': dmi_raw.pop(0).strip(), 'type': int(htype) }
if not clean: dmi.append(record) continue
dmi_data = _dmi_data(dmi_raw, clean, fields) if len(dmi_data): record['data'] = dmi_data dmi.append(record) elif not clean: dmi.append(record)
val = _dmi_cast(key, line.strip(), clean) if val is not None: key_data[1].append(val)
except: pass
return False
return not re.search(r'manufacturer|to be filled|available|asset|^no(ne|t)', val, flags=re.IGNORECASE)
from __future__ import absolute_import
super(SendMsgBot, self).__init__(jid, password)
for handler in logging.root.handlers: handler.addFilter(SleekXMPPMUC())
import logging
import salt.utils import salt.modules.cmdmod import salt.utils.decorators as decorators from salt.utils.odict import OrderedDict
__func_alias__ = { 'list_': 'list', }
return salt.utils.which('zfs')
man = salt.utils.which('man') if not man: return False
cmd = 'which zfs'
if properties: optlist = [] for prop in properties.keys():
cmd = '{0} {1}'.format(cmd, name)
res = __salt__['cmd.run_all'](cmd)
if res['retcode'] != 0: ret[name] = res['stderr'] if 'stderr' in res else res['stdout'] else: ret[name] = 'created'
if recursive:
if ltype: cmd = '{0} -t {1}'.format(cmd, ltype)
if recursive: cmd = '{0} -r'.format(cmd) if depth: cmd = '{0} -d {1}'.format(cmd, depth)
properties = properties.split(',')
if name: cmd = '{0} {1}'.format(cmd, name)
if properties: optlist = [] for prop in properties.keys():
if not _check_features(): ret['error'] = 'bookmarks are not supported' return ret
if not snapshot: ret['error'] = 'one or more snapshots must be specified'
if err == 'usage:': break ret[csnap][ctag] = res['stderr']
if not snapshot: ret['error'] = 'one or more snapshots must be specified'
if err == 'usage:': break ret[csnap][ctag] = res['stderr']
if not snapshot: ret['error'] = 'one or more snapshots must be specified'
if properties: optlist = [] for prop in properties.keys():
if err == 'usage:': break ret[csnap] = res['stderr']
if not dataset: ret['error'] = 'one or more snapshots must be specified'
for ds in dataset: for prop in properties.keys():
if depth: cmd = '{0} -d {1}'.format(cmd, depth) elif recursive: cmd = '{0} -r'.format(cmd)
fields = fields.split(',')
if source: cmd = '{0} -s {1}'.format(cmd, source)
if ltype: cmd = '{0} -t {1}'.format(cmd, ltype)
cmd = '{0} {1}'.format(cmd, properties)
cmd = '{0} {1}'.format(cmd, ' '.join(dataset))
from __future__ import absolute_import import os import stat import logging
import salt.ext.six as six
if key_name is None: key_name = _format_dict_key(args, plugin)
if isinstance(command, dict): plugin = next(six.iterkeys(command)) args = command[plugin] else: plugin = command args = ''
stat_f = os.path.join(PLUGINDIR, plugin) execute_bit = stat.S_IXUSR & os.stat(stat_f)[stat.ST_MODE] if execute_bit: ret.append(plugin)
import salt.utils.http
from __future__ import absolute_import import os
import salt.ext.six as six
keystone.region_name: 'RegionOne'
import logging
import salt.utils.openstack.nova as suon
log = logging.getLogger(__name__)
__func_alias__ = { 'list_': 'list' }
__virtualname__ = 'nova'
import os import re import shutil import logging
import salt.utils import tempfile import salt.utils.locales import salt.utils.url from salt.ext.six import string_types from salt.exceptions import CommandExecutionError, CommandNotFoundError
__func_alias__ = { 'list_': 'list' }
return False
cached_requirements = __salt__['cp.cache_file']( requirements, saltenv )
raise ValueError('Timeout cannot be a float')
pip_version = version(pip_bin)
if salt.utils.compare_versions(ver1=pip_version, oper='>=', ver2='1.4'): cmd.append('--pre')
if not (entry == '.' or entry.startswith(('file://', '/'))): match = egg_match.search(entry)
raise CommandExecutionError( 'You must specify an egg for this editable' )
raise ValueError('Timeout cannot be a float')
continue
continue
from napalm import get_network_driver HAS_NAPALM = True
import os
import salt.utils from salt.exceptions import CommandExecutionError
__virtualname__ = 'sysctl'
if not os.path.isfile(config): try: with salt.utils.fopen(config, 'w+') as _fh:
if apply_change is True: assign(name, value) return 'Updated and applied' return 'Updated'
from __future__ import absolute_import import logging
try: import salt.utils.openstack.neutron as suoneu HAS_NEUTRON = True except NameError as exc: HAS_NEUTRON = False
log = logging.getLogger(__name__)
__func_alias__ = { 'list_': 'list' }
from __future__ import absolute_import import logging import os import re
from salt.exceptions import CommandExecutionError import salt.utils
import salt.ext.six as six
cmd = __salt__['cmd.run_all']('racadm {0} {1}'.format(command, modswitch))
cmd = __salt__['cmd.run_all']('racadm {0} {1}'.format(command, modswitch))
if not __execute_cmd('config -g cfgUserAdmin -o ' 'cfgUserAdminUserName -i {0} {1}' .format(uid, username), host=host, admin_username=admin_username, admin_password=admin_password): delete_user(username, uid) return False
if not set_permissions(username, permissions, uid): log.warning('unable to set user permissions') delete_user(username, uid) return False
if not change_password(username, password, uid): log.warning('unable to set user password') delete_user(username, uid) return False
if not __execute_cmd('config -g cfgUserAdmin -o ' 'cfgUserAdminEnable -i {0} 1'.format(uid)): delete_user(username, uid) return False
if uid is None: user = list_users() uid = user[username]['index']
for i in permissions.split(','): perm = i.strip()
slot = str(slot) return slots[slot]['slotname']
import salt.utils from salt.exceptions import CommandExecutionError
__virtualname__ = 'acl'
vals['type'] = 'acl' if comps[0] == 'default': vals['type'] = 'default' comps.pop(0)
if comps[0] == 'user' and not comps[1]: comps[1] = user elif comps[0] == 'group' and not comps[1]: comps[1] = group vals[comps[0]] = comps[1]
import logging import re import os import bz2
import salt.utils from salt.exceptions import SaltInvocationError
__virtualname__ = 'cyg'
if not os.path.exists(cyg_cache_dir): os.mkdir(cyg_cache_dir) elif os.path.exists(cyg_setup_path): os.remove(cyg_setup_path)
if packages is not None: args.append('--packages {pkgs}'.format(pkgs=packages)) if not _check_cygwin_installed(cyg_arch): _run_silent_cygwin(cyg_arch=cyg_arch)
if not _check_cygwin_installed(cyg_arch): LOG.debug('Cygwin ({0}) not installed,\ could not update'.format(cyg_arch)) return False
from __future__ import absolute_import
self.o.maxtimeout = config['api_login_timeout'] self.o.wait_for_rsp(timeout=1)
from __future__ import absolute_import import logging
import json import salt.ext.six import salt.ext.six.moves.http_client from salt.ext.six.moves.urllib.parse import urljoin as _urljoin
import salt.utils.http
#pylint: disable=E0602
from __future__ import absolute_import import logging import time
import salt.ext.six as six
MAX_ATTEMPTS = 30 for i in range(MAX_ATTEMPTS): if exists( table_name, region, key, keyid, profile ): return True else:
MAX_ATTEMPTS = 30 for i in range(MAX_ATTEMPTS): if not exists(table_name, region, key, keyid, profile): return True else:
from __future__ import absolute_import, print_function import datetime import json import logging import time
from salt.exceptions import SaltInvocationError import salt.utils.http
log.error('Wrong type, skipping {0}'.format(kwarg))
days=5 \ CN='My Little CA' \ C=US \ ST=Utah \ L=Salt Lake City \ O=Saltstack \ emailAddress=pleasedontemail@example.com
Created Private Key: "/etc/pki/my_little/certs/www.example.com.key Created CSR for "www.example.com": "/etc/pki/my_little/certs/www.example.com.csr"
Created Certificate for "www.example.com": /etc/pki/my_little/certs/www.example.com.crt"
Created Private Key: "/etc/pki/my_little/certs//DBReplica_No.1.key." Created CSR for "DBReplica_No.1": "/etc/pki/my_little/certs/DBReplica_No.1.csr."
Created Certificate for "DBReplica_No.1": "/etc/pki/my_little/certs/DBReplica_No.1.crt"
cert_type=client
Created Certificate for "DBReplica_No.1": "/etc/pki/my_little/certs/DBReplica_No.1.crt"
cert_type=server
cert_type=server type_ext=True
Certificate "MasterDBReplica_No.2" already exists
cert_type=server type_ext=True
cert_type=server type_ext=True
cert_type=server cert_filename="something_completely_different"
import os import time import calendar import logging import math import binascii import salt.utils from salt._compat import string_types from salt.ext.six.moves import range as _range from datetime import datetime
return True
subject = '/'
key = OpenSSL.crypto.load_privatekey( OpenSSL.crypto.FILETYPE_PEM, fic2.read()) bits = key.bits()
cert = OpenSSL.crypto.X509() cert.set_version(2)
date_fmt = '%Y%m%d%H%M%SZ'
import copy
import salt.utils from salt.exceptions import CommandExecutionError, MinionError import salt.ext.six as six
__virtualname__ = 'pkgutil'
if salt.utils.is_true(kwargs.get('removed')): return {}
for name in names: ret[name] = ''
if refresh: refresh_db()
ret[name] = version_rev
if len(names) == 1: return ret[names[0]] return ret
available_version = salt.utils.alias_function(latest_version, 'available_version')
pkg_params = __salt__['pkg_resource.parse_targets'](name, pkgs, **kwargs)[0]
from __future__ import absolute_import
import salt.utils import salt.syspaths from salt.exceptions import SaltInvocationError
import salt.ext.six as six
log = logging.getLogger(__name__)
__virtualname__ = 'gpg'
return salt.utils.which('gpg')
ret['message'] = 'Secret key for {0} deleted\n'.format(fingerprint)
with salt.utils.flopen(filename, 'rb') as _fp: _contents = _fp.read() result = gpg.encrypt(_contents, recipients, passphrase=gpg_passphrase, output=output)
import salt.utils import salt.utils.mac_utils from salt.exceptions import SaltInvocationError from salt.ext.six.moves import range
import logging import os
import salt.utils import salt.utils.decorators as decorators from salt.exceptions import CommandNotFoundError
__virtualname__ = 'service'
if __grains__['os'] == 'FreeBSD': return __virtualname__ return (False, 'The freebsdservice execution module cannot be loaded: only available on FreeBSD systems.')
return False
try: from gentoolkit.eclean import search, clean, cli, exclude as excludemod HAS_GENTOOLKIT = True except ImportError: pass
__virtualname__ = 'gentoolkit'
clean_me = search.findPackages(None, destructive=destructive, package_names=package_names, time_limit=time_limit, exclude=exclude, pkgdir=search.pkgdir)
import salt.utils import salt.exceptions
__func_alias__ = { 'set_': 'set' }
import time import logging
from salt.exceptions import CommandExecutionError
try: import ldap import ldap.modlist HAS_LDAP = True except ImportError: HAS_LDAP = False
__virtualname__ = 'ldap'
if HAS_LDAP: return __virtualname__ return (False, 'The ldapmod execution module cannot be loaded: ' 'ldap config not present.')
import fnmatch import os import re import logging
import salt.utils from salt.ext.six import string_types from salt.exceptions import SaltInvocationError, CommandExecutionError import salt.ext.six as six
__virtualname__ = 'ports'
ret = {name: {'old': old.get(name, ''), 'new': new.get(name, '')}}
pkg = next(iter(configuration)) conf_ptr = configuration[pkg]
from distutils.version import LooseVersion try:
import salt.utils
command = 'Add-WindowsFeature' management_tools = '' if LooseVersion(__grains__['osversion']) >= LooseVersion('6.2'): command = 'Install-WindowsFeature' management_tools = '-IncludeManagementTools'
import logging
try: import ethtool HAS_ETHTOOL = True except ImportError: HAS_ETHTOOL = False
__virtualname__ = 'ethtool'
from __future__ import absolute_import import os import logging import fnmatch
import salt.minion import salt.fileclient import salt.utils import salt.utils.url import salt.crypt import salt.transport from salt.exceptions import CommandExecutionError
import salt.ext.six as six
if template not in salt.utils.templates.TEMPLATE_REGISTRY: raise CommandExecutionError( 'Attempted to render file paths with unavailable engine ' '{0}'.format(template) )
saltenv = env
saltenv = env
saltenv = env
saltenv = env
saltenv = env
saltenv = env
__context__[contextkey] = result
saltenv = env
saltenv = env
saltenv = env
if path_cached: path_hash = hash_file(path) path_cached_hash = hash_file(path_cached)
return _client().cache_local_file(path)
saltenv = env
saltenv = env
saltenv = env
saltenv = env
saltenv = env
saltenv = env
saltenv = env
import copy import fnmatch import logging import os import signal import sys
try: import esky from esky import EskyVersionError HAS_ESKY = True except ImportError: HAS_ESKY = False from salt.ext import six from salt.ext.six.moves.urllib.error import URLError
try: salt_SIGKILL = signal.SIGKILL except AttributeError: salt_SIGKILL = signal.SIGTERM
ret = __salt__['event.fire']({}, 'module_refresh')
log.trace('refresh_modules waiting for module refresh to complete') eventer.get_event(tag='/salt/minion/minion_mod_complete', wait=30)
my-minion: arg: - 30 fun: test.sleep jid: 20160503150049487736 pid: 9601 ret: tgt: my-minion tgt_type: glob user: root
return
return
return signal_job(jid, salt_SIGKILL)
ret = [] for data in running(): ret.append(signal_job(data['jid'], salt_SIGKILL)) return ret
if expr_form == 'list' and len(tgt) == seen: break
__grains__ = grains
m.opts['grains'] = grains
import copy import re import logging
import salt.utils from salt.exceptions import CommandExecutionError, MinionError
__virtualname__ = 'pkg'
if any([salt.utils.is_true(kwargs.get(x)) for x in ('removed', 'purge_desired')]): return {}
for name in names: ret[name] = ''
if len(names) == 1: return ret[names[0]] return ret
available_version = salt.utils.alias_function(latest_version, 'available_version')
import os import logging import re
import salt.utils from salt.exceptions import CommandExecutionError, SaltInvocationError
log = logging.getLogger(__name__)
__func_alias__ = { 'list_': 'list' }
__virtualname__ = 'raid'
if not os.path.exists(device): msg = "Device {0} doesn't exist!" raise CommandExecutionError(msg.format(device))
if __grains__.get('os_family') == 'Debian': cfg_file = '/etc/mdadm/mdadm.conf' else: cfg_file = '/etc/mdadm.conf'
if isinstance(devices, str): devices = devices.split(',')
import os import stat import os.path import logging import struct
try: import win32api import win32file import win32security import win32con from pywintypes import error as pywinerror HAS_WINDOWS_MODULES = True except ImportError: HAS_WINDOWS_MODULES = False
import salt.utils
__virtualname__ = 'file'
if token_privileges[privilege] == privilege_attrs: log.debug( 'The requested privilege {0} is already in the ' 'requested state.'.format(privilege_name) ) return True
if exc.winerror == 1332: return '' else: raise
if exc.winerror == 1332: return '' else: raise
try: userSID, domainName, objectType = win32security.LookupAccountName(None, user) except pywinerror: err += 'User does not exist\n'
try: groupSID, domainName, objectType = win32security.LookupAccountName(None, pgroup) except pywinerror: err += 'Group does not exist\n'
win32security.SetNamedSecurityInfo( path, win32security.SE_FILE_OBJECT, win32security.OWNER_SECURITY_INFORMATION + win32security.GROUP_SECURITY_INFORMATION, userSID, groupSID, None, None )
win32security.SetNamedSecurityInfo( path, win32security.SE_FILE_OBJECT, win32security.OWNER_SECURITY_INFORMATION, userSID, None, None, None )
try: groupSID, domainName, objectType = win32security.LookupAccountName(None, group) except pywinerror: err += 'Group does not exist\n'
attributes = {}
intAttributes = win32file.GetFileAttributes(path)
if not os.path.exists(path): return 'File/Folder not found: {0}'.format(path)
if force: file_attributes = win32api.GetFileAttributes(path) win32api.SetFileAttributes(path, win32con.FILE_ATTRIBUTE_NORMAL)
os.remove(path)
os.rmdir(path)
remove(item, force)
os.rmdir(path)
win32api.SetFileAttributes(path, file_attributes)
if sys.getwindowsversion().major < 6: raise SaltInvocationError('Symlinks are only supported on Windows Vista or later.')
src = os.path.normpath(src) link = os.path.normpath(link)
reparse_data = _get_reparse_data(path)
if not reparse_data: return False
header_parser = struct.Struct('L') ReparseTag, = header_parser.unpack(reparse_data[:header_parser.size]) if not ReparseTag & 0xA000FFFF == 0xA000000C: return False else: return True
path = os.path.normpath(path)
data_parser = struct.Struct('LHHHHHHL') ReparseTag, ReparseDataLength, Reserved, SubstituteNameOffset, \ SubstituteNameLength, PrintNameOffset, \ PrintNameLength, Flags = data_parser.unpack(reparse_data[:data_parser.size])
target = win32file.GetLongPathName(target)
if exc.winerror == 2: return target raise
#pylint: disable=E0602
import logging
import salt.utils.compat import salt.utils.odict as odict from salt.exceptions import SaltInvocationError
if retry_on_rate_limit and 'Throttling' == e.code: log.debug('Throttled by AWS API.') time.sleep(2) rate_limit_retries -= 1
if retry_on_rate_limit and 'Throttling' == e.code: log.debug('Throttled by AWS API.') time.sleep(2) rate_limit_retries -= 1
if retry_on_rate_limit and 'Throttling' == e.code: log.debug('Throttled by AWS API.') time.sleep(2) rate_limit_retries -= 1
if ttl is None: ttl = 60 status = _zone.add_record(_type, name, _value, ttl, identifier) return _wait_for_sync(status.id, conn, wait_for_sync)
if retry_on_rate_limit and 'Throttling' == e.code: log.debug('Throttled by AWS API.') time.sleep(2) rate_limit_retries -= 1
if retry_on_rate_limit and 'Throttling' == e.code: log.debug('Throttled by AWS API.') time.sleep(2) rate_limit_retries -= 1
if retry_on_rate_limit and 'Throttling' == e.code: log.debug('Throttled by AWS API.') time.sleep(2) rate_limit_retries -= 1
import re
import salt.utils
__func_alias__ = { 'id_': 'id', 'reload_': 'reload', }
return True
from __future__ import absolute_import import os import re import sys import shutil import subprocess
import yaml import jinja2 import jinja2.exceptions from xml.dom import minidom import salt.ext.six as six
import salt.utils import salt.utils.files import salt.utils.templates import salt.utils.validate.net from salt.exceptions import CommandExecutionError, SaltInvocationError
JINJA = jinja2.Environment( loader=jinja2.FileSystemLoader( os.path.join(salt.utils.templates.TEMPLATE_DIRNAME, 'virt') ) )
config_data = __salt__['config.option']('virt.nic', {}).get( profile_name, None )
if isinstance(config_data, dict): append_dict_profile_to_interface_list(config_data)
attributes['source'] = attributes.pop(type_)
disk_name = next(six.iterkeys(diskp[0])) disk_type = diskp[0][disk_name]['format'] disk_file_name = '{0}.{1}'.format(disk_name, disk_type)
mode = (0o0777 ^ mask) & 0o0666 os.chmod(img_dest, mode)
pass
flags = libvirt.VIR_DOMAIN_MEM_MAXIMUM if config: flags = flags | libvirt.VIR_DOMAIN_AFFECT_CONFIG
return ret1 == ret2 == 0
flags = libvirt.VIR_DOMAIN_VCPU_MAXIMUM if config: flags = flags | libvirt.VIR_DOMAIN_AFFECT_CONFIG
mem -= 256 for vm_ in list_domains(): dom = _get_domain(vm_) if dom.ID() > 0: mem -= dom.info()[2] / 1024 return mem
return dom.reset(0) == 0
return False
pass
return False
return False
return False
cputime_percent = (1.0e-7 * cputime / host_cpus) / vcpus
for vm_ in list_active_vms(): info[vm_] = _info(vm_)
if getattr(libvirt, 'VIR_CONNECT_BASELINE_CPU_MIGRATABLE', False): flags += libvirt.VIR_CONNECT_BASELINE_CPU_MIGRATABLE else: raise ValueError
flags += libvirt.VIR_CONNECT_BASELINE_CPU_EXPAND_FEATURES
with salt.utils.fopen('/usr/share/libvirt/cpu_map.xml', 'r') as cpu_map: cpu_map = minidom.parse(cpu_map)
import copy import logging import os import re from distutils.version import LooseVersion as _LooseVersion
import salt.utils import salt.utils.files import salt.utils.itertools import salt.utils.url from salt.exceptions import SaltInvocationError, CommandExecutionError from salt.ext import six
value_regex = None
to_expand = '~' + str(user) if user else '~'
if not isinstance(identity, list): identity = [identity]
return ['--local']
return ['--file', _git_config(cwd, user)]
kwargs = salt.utils.clean_kwargs(**kwargs) format_ = kwargs.pop('format', None) if kwargs: salt.utils.invalid_kwargs(kwargs)
return True
return _git_run(command, cwd=cwd, runas=user, ignore_retcode=ignore_retcode, redirect_stderr=True)['stdout']
command.extend(['--', filename])
all_ = kwargs.pop('all', False)
if result['retcode'] == 1: return None ret = result['stdout'].splitlines() if all_: return ret else: try: return ret[-1] except IndexError: return ''
ref_key = 'new tags' \ if new_ref_type == 'tag' \ else 'new branches' ret.setdefault(ref_key, []).append(ref_name)
ret.setdefault('updated branches', {})[ref_name] = \ {'old': old_sha, 'new': new_sha}
ret.setdefault('updated tags', []).append(ref_name)
shared = str(shared).lower()
return False
return False
worktree_data = dict([(x, '') for x in tracked_data_points])
tags_found = _git_tag_points_at(cwd, wt_ptr['HEAD'], user) if tags_found: wt_ptr['tags'] = tags_found
break
salt.utils.files.process_read_exception(exc, path)
wt_loc = toplevel
if wt_detached: tags_found = _git_tag_points_at(cwd, wt_head, user) if tags_found: wt_ptr['tags'] = tags_found
action = str(action)
salt myminion git.submodule /path/to/repo/sub/repo init=True salt myminion git.submodule /path/to/repo/sub/repo update opts='--init'
salt myminion git.submodule /path/to/repo/sub/repo update opts='--rebase'
salt myminion git.submodule /path/to/repo/sub/repo add opts='https://mydomain.tld/repo.git'
log.error('Running \'git --version\' returned no stdout') __context__[contextkey] = 'unknown'
return _git_run(command, cwd=cwd, runas=user, ignore_retcode=ignore_retcode, redirect_stderr=True)['stdout']
import salt.utils from salt.exceptions import CommandExecutionError
__virtualname__ = 'sysctl'
max_primes = [500, 1000, 2500, 5000]
test_command = 'sysbench --test=cpu --cpu-max-prime={0} run' result = None ret_val = {}
thread_yields = [100, 200, 500, 1000] thread_locks = [2, 4, 8, 16]
test_command = 'sysbench --num-threads=64 --test=threads ' test_command += '--thread-yields={0} --thread-locks={1} run ' result = None ret_val = {}
test_command = 'sysbench --num-threads=250 --test=mutex ' test_command += '--mutex-num={0} --mutex-locks={1} --mutex-loops={2} run ' result = None ret_val = {}
memory_oper = ['read', 'write'] memory_scope = ['local', 'global']
test_command = 'sysbench --num-threads=64 --test=memory ' test_command += '--memory-oper={0} --memory-scope={1} ' test_command += '--memory-block-size=1K --memory-total-size=32G run ' result = None ret_val = {}
test_modes = ['seqwr', 'seqrewr', 'seqrd', 'rndrd', 'rndwr', 'rndrw']
test_command = 'sysbench --num-threads=16 --test=fileio ' test_command += '--file-num=32 --file-total-size=1G --file-test-mode={0} ' result = None ret_val = {}
for mode in test_modes: key = 'Mode: {0}'.format(mode)
run_command = (test_command + 'prepare').format(mode) __salt__['cmd.run'](run_command)
run_command = (test_command + 'run').format(mode) result = __salt__['cmd.run'](run_command) ret_val[key] = _parser(result)
run_command = (test_command + 'cleanup').format(mode) __salt__['cmd.run'](run_command)
from __future__ import absolute_import import logging
import logging
try:
log = logging.getLogger(__name__)
__func_alias__ = { 'get_': 'get', 'set_': 'set', 'rm_': 'rm', 'ls_': 'ls' }
from datetime import datetime
import salt.utils import salt.utils.mac_utils from salt.exceptions import SaltInvocationError
time_format = _get_date_time_format(time) dt_obj = datetime.strptime(time, time_format)
import contextlib import functools import glob import logging import os import re import tempfile
import copy import json import logging
import salt.utils from salt.exceptions import CommandExecutionError, MinionError import salt.ext.six as six from salt.ext.six.moves import zip
__virtualname__ = 'pkg'
if any([salt.utils.is_true(kwargs.get(x)) for x in ('removed', 'purge_desired')]): return {}
return pkg_info['versions']['stable'] or pkg_info['versions']['devel']
available_version = salt.utils.alias_function(latest_version, 'available_version')
if taps: if not isinstance(taps, list): taps = [taps]
from __future__ import absolute_import import errno import os import locale import logging from distutils.version import LooseVersion
import salt.ext.six as six from salt.ext.six.moves.urllib.parse import urlparse as _urlparse try: import msgpack except ImportError: import msgpack_pure as msgpack
from salt.exceptions import (CommandExecutionError, SaltInvocationError, SaltRenderError) import salt.utils import salt.syspaths from salt.exceptions import MinionError
__virtualname__ = 'pkg'
ret = {} for name in names: ret[name] = ''
if salt.utils.is_true(kwargs.get('refresh', True)): refresh_db(saltenv)
for name in names: latest_installed = '0' latest_available = '0'
available_version = salt.utils.alias_function(latest_version, 'available_version')
if any([salt.utils.is_true(kwargs.get(x)) for x in ('removed', 'purge_desired')]): return {}
pkg_info = _get_package_info(key, saltenv=saltenv) if not pkg_info: continue for pkg_ver in pkg_info.keys(): if pkg_info[pkg_ver]['full_name'] == pkg_name: val = pkg_ver
reg_software.update({d_name: d_vers})
cached_files = __salt__['cp.cache_dir']( winrepo_source_dir, saltenv, include_pat='*.sls' ) genrepo(saltenv=saltenv) return cached_files
cached_hash_file = __salt__['cp.cache_file'](source_hash, saltenv)
items = source_hash.split('=', 1)
if not name and not pkgs: return 'Must pass a single package or a list of packages'
pkg_params = __salt__['pkg_resource.parse_targets'](name, pkgs, **kwargs)[0]
pkg_params = { name: { 'version': kwargs.get('version'), 'extra_install_flags': kwargs.get('extra_install_flags') } }
old = list_pkgs(saltenv=saltenv)
changed = [] latest = [] for pkg_name, options in six.iteritems(pkg_params):
pkginfo = _get_package_info(pkg_name, saltenv=saltenv)
if not pkginfo: log.error('Unable to locate package {0}'.format(pkg_name)) ret[pkg_name] = 'Unable to locate package {0}'.format(pkg_name) continue
version_num = '' if options: version_num = options.get('version', False)
if version_num == old.get(pkg_name) \ or (pkg_name in old and old[pkg_name] == 'Not Found'): ret[pkg_name] = {'current': version_num} continue
elif version_num not in pkginfo: log.error('Version {0} not found for package ' '{1}'.format(version_num, pkg_name)) ret[pkg_name] = {'not found': version_num} continue
installer = pkginfo[version_num].get('installer', False) cache_dir = pkginfo[version_num].get('cache_dir', False) cache_file = pkginfo[version_num].get('cache_file', False)
if not installer: log.error('No installer configured for version {0} of package ' '{1}'.format(version_num, pkg_name)) ret[pkg_name] = {'no installer': version_num} continue
if installer.startswith(('salt:', 'http:', 'https:', 'ftp:')):
if cache_file and cache_file.startswith('salt:'):
cached_file = __salt__['cp.is_cached'](cache_file, saltenv) if not cached_file: cached_file = __salt__['cp.cache_file'](cache_file, saltenv)
if not cached_file: log.error('Unable to cache {0}'.format(cache_file)) ret[pkg_name] = { 'failed to cache cache_file': cache_file } continue
cached_pkg = __salt__['cp.is_cached'](installer, saltenv) if not cached_pkg: cached_pkg = __salt__['cp.cache_file'](installer, saltenv)
if not cached_pkg: log.error('Unable to cache {0}'.format(installer)) ret[pkg_name] = {'unable to cache': installer} continue
cached_pkg = installer
cached_pkg = cached_pkg.replace('/', '\\') cache_path, _ = os.path.split(cached_pkg)
if pkginfo[version_num].get('use_scheduler', False):
new = list_pkgs(saltenv=saltenv)
if latest: for pkg_name in latest: if old.get(pkg_name, 'old') == new.get(pkg_name, 'new'): ret[pkg_name] = {'current': new[pkg_name]}
difference = salt.utils.compare_dicts(old, new)
ret.update(difference)
return {}
if not name and not pkgs: return 'Must pass a single package or a list of packages'
pkg_params = __salt__['pkg_resource.parse_targets'](name, pkgs, **kwargs)[0]
old = list_pkgs(saltenv=saltenv)
changed = [] for target in pkg_params:
pkginfo = _get_package_info(target, saltenv=saltenv)
if not pkginfo: log.error('Unable to locate package {0}'.format(name)) ret[target] = 'Unable to locate package {0}'.format(target) continue
if not version: version_num = _get_latest_pkg_version(pkginfo) else: version_num = version
uninstaller = pkginfo[version_num].get('uninstaller')
if not uninstaller: uninstaller = pkginfo[version_num].get('installer')
if not uninstaller: log.error('Error: No installer or uninstaller configured ' 'for package {0}'.format(name)) ret[target] = {'no uninstaller': version_num} continue
if uninstaller.startswith(('salt:', 'http:', 'https:', 'ftp:')):
cached_pkg = __salt__['cp.is_cached'](uninstaller) if not cached_pkg: cached_pkg = __salt__['cp.cache_file'](uninstaller)
if not cached_pkg: log.error('Unable to cache {0}'.format(uninstaller)) ret[target] = {'unable to cache': uninstaller} continue
cached_pkg = uninstaller
cached_pkg = cached_pkg.replace('/', '\\') cache_path, _ = os.path.split(cached_pkg)
expanded_cached_pkg = str(os.path.expandvars(cached_pkg))
if pkginfo[version_num].get('use_scheduler', False):
new = list_pkgs(saltenv=saltenv) tries = 0 difference = salt.utils.compare_dicts(old, new)
ret.update(difference)
from __future__ import absolute_import import os.path
import salt.utils import salt.utils.itertools import salt.utils.mac_utils from salt.exceptions import SaltInvocationError
__func_alias__ = {'list_': 'list'}
__virtualname__ = 'pkgutil'
#pylint: disable=E0602
import logging from salt.exceptions import SaltInvocationError from time import time, sleep
import salt.ext.six as six try: import boto import boto.rds2 logging.getLogger('boto').setLevel(logging.CRITICAL) HAS_BOTO = True except ImportError: HAS_BOTO = False
import os import re
import salt.utils
__virtualname__ = 'kmod'
ret.append('.'.join(comps[:comps.index('ko')]))
return [None]
import logging import os
import salt.utils from salt.ext.six.moves import range
log = logging.getLogger(__name__)
if old == '*': return True
comps = line.split() path = comps[0] mask = comps[1]
ls = salt.utils.alias_function(list_tab, 'ls')
mask = str(mask).upper()
for item in mask.split(','): if item not in _MASK_TYPES: return 'Invalid mask type: {0}' . format(item)
return comdat['stderr']
mask = str(mask).upper()
for item in mask.split(','): if item not in _MASK_TYPES: return 'Invalid mask type: {0}' . format(item)
return comdat['stderr']
#pylint: disable=E0602
try: #pylint: disable=unused-import import boto import boto.sns #pylint: enable=unused-import logging.getLogger('boto').setLevel(logging.CRITICAL) HAS_BOTO = True except ImportError: HAS_BOTO = False
import salt.utils import salt.ext.six as six
if item in ['constraint']: cmd += [item_type]
if item in ['constraint']: if not isinstance(extra_args, (list, tuple)) or '--full' not in extra_args: cmd += ['--full']
if item in ['constraint']: if isinstance(item_type, six.string_types): cmd += [item_type]
if item not in ['constraint']: cmd += [item_id] if isinstance(item_type, six.string_types): cmd += [item_type]
if item in ['constraint']: extra_args = extra_args + ['id={0}'.format(item_id)] cmd += extra_args
from __future__ import absolute_import, print_function import logging
import logging import json import os try:
import salt.utils import salt.utils.decorators as decorators from salt.utils.odict import OrderedDict
__func_alias__ = { 'list_vms': 'list' }
__virtualname__ = 'vmadm'
if res['stderr'].startswith('Successfully created VM'): return res['stderr'][24:]
vmcfg = {} kwargs = salt.utils.clean_kwargs(**kwargs) for k, v in kwargs.iteritems(): vmcfg[k] = v
vmcfg = {} kwargs = salt.utils.clean_kwargs(**kwargs) for k, v in kwargs.iteritems(): vmcfg[k] = v
from __future__ import absolute_import try: import pwd HAS_PWD = True except ImportError: HAS_PWD = False import copy import logging
import salt.utils import salt.ext.six as six from salt.exceptions import CommandExecutionError
__virtualname__ = 'user'
from __future__ import absolute_import import logging import os import tempfile
import salt.utils import salt.utils.decorators as decorators
import salt.ext.six as six
import re import logging
import salt.utils from salt.ext import six
__virtualname__ = 'firewall'
import os import stat import logging
import salt.utils import salt.utils.decorators as decorators from salt.utils.odict import OrderedDict
man = salt.utils.which('man') if not man: return False
if line.startswith('pool') and line.endswith('write'): continue if line.endswith('bandwidth'): continue
for zp in res['stdout'].splitlines(): zp = zp.split("\t") zp_data = {}
for zp in res['stdout'].splitlines(): zp = zp.split("\t") zp_data = {}
if exists(zpool): ret[zpool] = 'storage pool already exists' return ret
res = __salt__['cmd.run_all'](cmd, python_shell=False)
if res['retcode'] != 0: ret[zpool] = res['stderr'] if 'stderr' in res else res['stdout'] else: ret[zpool] = 'created'
if not exists(zpool): ret[zpool] = 'storage pool does not exist' return ret
if not exists(zpool): ret[zpool] = 'storage pool does not exist' return ret
if not exists(zpool): ret[zpool] = 'storage pool does not exist' return ret
if not exists(zpool): ret[zpool] = 'storage pool does not exist' return ret
ret[zpool] = {}
for vdev in vdevs: if os.path.isfile(vdev): ret[vdev] = 'existed' else: dlist.append(vdev)
for vdev in vdevs: if not os.path.isfile(vdev): ret[vdev] = 'failed' else: if vdev not in ret: ret[vdev] = 'created' return ret
if not exists(zpool): ret[zpool] = 'storage pool does not exist' return ret
expand = kwargs.get('expand', False)
if not exists(zpool): ret[zpool] = 'storage pool does not exist' return ret
from __future__ import absolute_import, print_function import json import logging import time
import salt.utils.http
from __future__ import absolute_import
import salt.utils from salt.exceptions import CommandExecutionError
return None
import re import time import datetime
from salt.ext.six.moves import map from salt.exceptions import CommandNotFoundError
import salt.utils
BSD = ('OpenBSD', 'FreeBSD')
if __grains__['os_family'] == 'RedHat': output = _cmd('at', '-l') else: output = _cmd('atq')
if output == '': return {'jobs': jobs}
for line in output.splitlines(): job_tag = ''
output = _cmd('at', '-d', ' '.join(opts)) if output is None: return '\'at.atrm\' is not available.'
binary = salt.utils.which('at') if not binary: return '\'at.at\' is not available.'
output = _cmd('at', '-c', str(jobid))
from __future__ import absolute_import import logging import re
import salt.utils
if len(types) == 1: return ret[types[0]] else: for key in ret.keys(): if key not in types: del ret[key]
import logging import subprocess
from salt import utils
if _TRAFFICLINE: cmd = _traffic_line('-S') else: cmd = _traffic_ctl('server', 'stop')
if _TRAFFICLINE: cmd = _traffic_line('-U') else: cmd = _traffic_ctl('server', 'start')
import glob import os import re import itertools import fnmatch
import salt.utils import salt.modules.cmdmod import salt.utils.systemd
__virtualname__ = 'service'
for utmp in '/var/run/utmp', '/run/utmp': try: result[os.stat(utmp).st_mtime] = utmp except Exception: pass return result[sorted(result).pop()]
ret = _default_runlevel()
import salt.utils from salt.exceptions import CommandExecutionError
__virtualname__ = 'desktop'
import os
import yaml import logging
import salt.utils
return {'shell': os.environ.get('SHELL', '/bin/sh')}
import salt.utils
import glob import logging import re
import salt.utils import salt.utils.decorators as decorators
import salt.modules.cmdmod
from __future__ import absolute_import import logging
from salt.exceptions import SaltSystemExit import salt.utils import salt.modules.vsphere
ret = salt.modules.vsphere.system_info(host=host, username=user, password=password)
continue
raise SaltSystemExit('Cannot complete login due to an incorrect user name or password.')
from __future__ import absolute_import
import salt.utils import salt.modules.nxos
import logging
import salt.utils
from __future__ import absolute_import import itertools import os import json import socket import sys import re import platform import logging import locale import salt.exceptions
import salt.log import salt.utils import salt.utils.network
import salt.modules.cmdmod import salt.modules.smbios
import salt.ext.six as six
try:
known_vendors = ['nvidia', 'amd', 'ati', 'intel'] gpu_classes = ('vga compatible controller', '3d controller')
sysctl = salt.utils.which('sysctl') arch = salt.utils.which('arch') cmds = {}
grains = {} grains['cpu_flags'] = []
grains = {'mem_total': 0} if osdata['kernel'] == 'Linux': meminfo = '/proc/meminfo'
tot_bytes = win32api.GlobalMemoryStatusEx()['TotalPhys'] grains['mem_total'] = int(tot_bytes / (1024 ** 2))
grains = dict() if osdata['kernel'] != 'Windows': return grains
grains['virtual'] = 'kvm'
grains = {'virtual': 'physical'}
skip_cmds = ('AIX',)
if osdata['kernel'] in skip_cmds: _cmds = ()
if salt.utils.is_windows() or 'systemd-detect-virt' in cmd or 'prtdiag' in cmd: continue failed_commands.add(command)
break
if output: grains['virtual'] = output.lower() break
failed_commands.discard('lspci') failed_commands.discard('dmidecode')
grains['virtual_subtype'] = 'Xen Dom0'
grains['virtual_subtype'] = 'Xen HVM DomU'
grains['virtual_subtype'] = 'Xen PV DomU'
grains['virtual_subtype'] = 'Xen Dom0'
grains['virtual_subtype'] = 'Xen PV DomU'
(osfullname, _) = osinfo.Name.split('|', 1) osfullname = osfullname.strip()
data[match.group(1)] = re.sub(r'\\([$"\'\\`])', r'\1', match.group(2))
(grains['kernel'], grains['nodename'], grains['kernelrelease'], version, grains['cpuarch'], _) = platform.uname()
try:
grains['os'] = grains['osfullname'] = 'Solaris' grains['osrelease'] = ''
grains['osrelease'] = grains['kernelrelease'].split('-')[0]
grains['os_family'] = _OS_FAMILY_MAP.get(grains['os'], grains['os'])
osarch = sorted(archinfo, key=archinfo.get, reverse=True)
grains.update(_hw_data(grains))
grains.update(_virtual(grains)) grains.update(_ps(grains))
if grains['os_family'] == "RedHat": grains['osmajorrelease'] = grains['osrelease'].split('.', 1)[0]
grains['locale_info']['defaultlanguage'] = 'unknown' grains['locale_info']['defaultencoding'] = 'unknown'
global __FQDN__ grains = {}
ret = {} ifaces = _get_interfaces() for face in ifaces: if 'hwaddr' in ifaces[face]: ret[face] = ifaces[face]['hwaddr'] return {'hwaddr_interfaces': ret}
return {'path': os.environ.get('PATH', '').strip()}
return {'pythonversion': list(sys.version_info)}
return {'pythonpath': sys.path}
return {'pythonexecutable': sys.executable}
salt_path = os.path.abspath(os.path.join(__file__, os.path.pardir)) return {'saltpath': os.path.dirname(salt_path)}
from salt.version import __version__ return {'saltversion': __version__}
try: import zmq
from salt.version import __version_info__ return {'saltversioninfo': list(__version_info__)}
grains['manufacturer'] = sysinfo['Manufacturer'] grains['productname'] = sysinfo['Product'] grains['uuid'] = sysinfo['UUID']
return {'master': __opts__.get('master', '')}
import contextlib import logging import hashlib import os import shutil import ftplib from tornado.httputil import parse_response_start_line, HTTPInputError
if os.path.isfile(destdir): os.remove(destdir) os.makedirs(destdir)
if not path.endswith('/'): path = path + '/'
del dirs[:]
if not os.path.isabs(url_data.path): raise CommandExecutionError( 'Path \'{0}\' is not absolute'.format(url_data.path) ) return url_data.path
write_body = [False]
return
log.error( 'Failed to render template with error: {0}'.format( data['data'] ) ) return ''
dest = self._extrn_path(url, saltenv, cachedir=cachedir) makedirs = True
path = salt.utils.url.unescape(path)
dest2check = dest if not dest2check: rel_path = self._check_proto(path)
if os.path.isdir(dest): salt.utils.rm_rf(dest) fn_ = salt.utils.fopen(dest, 'wb+')
apikey: asdff7896asdh789 sharedsecret: saltybacon driver: gogrid
import pprint import logging import time import hashlib
import salt.config as config import salt.utils.cloud from salt.exceptions import SaltCloudSystemExit, SaltCloudException
log = logging.getLogger(__name__)
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'gogrid', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
exc_info_on_loglevel=logging.DEBUG
service_url: amazonaws.com
endpoint: myendpoint.example.com:1138/services/Cloud
ssh_gateway: gateway.example.com
ssh_gateway_port: 22
ssh_gateway_username: root
ssh_gateway_private_key: /path/to/key.pem
ssh_gateway_password: ExamplePasswordHere
userdata_file: /etc/salt/my-userdata-file
from __future__ import absolute_import import os import sys import stat import time import uuid import pprint import logging import yaml
import hmac import hashlib import binascii import datetime import base64 import msgpack import json import re import decimal
import salt.utils from salt import syspaths from salt._compat import ElementTree as ET import salt.utils.http as http import salt.utils.aws as aws import salt.loader from salt.template import compile_template
import salt.utils.cloud import salt.config as config from salt.exceptions import ( SaltCloudException, SaltCloudSystemExit, SaltCloudConfigError, SaltCloudExecutionTimeout, SaltCloudExecutionFailure )
try: import Crypto
log = logging.getLogger(__name__)
access_key_id, secret_access_key, token = aws.creds(provider)
t = datetime.datetime.utcnow()
result.status_code
if not isinstance(ssh_gateway, str): return None
ssh_gateway_config = {'ssh_gateway': ssh_gateway}
ssh_gateway_config['ssh_gateway_port'] = config.get_cloud_config_value( 'ssh_gateway_port', vm_, __opts__, default=None, search_global=False )
ssh_gateway_config['ssh_gateway_user'] = config.get_cloud_config_value( 'ssh_gateway_username', vm_, __opts__, default=None, search_global=False )
ssh_gateway_config['ssh_gateway_key'] = config.get_cloud_config_value( 'ssh_gateway_private_key', vm_, __opts__, default=None, search_global=False )
ssh_gateway_config['ssh_gateway_password'] = config.get_cloud_config_value( 'ssh_gateway_password', vm_, __opts__, default=None, search_global=False )
if avz not in zones: raise SaltCloudException( 'The specified availability zone isn\'t valid in this region: ' '{0}\n'.format( avz ) )
elif zones[avz] != 'available': raise SaltCloudException( 'The specified availability zone isn\'t currently available: ' '{0}\n'.format( avz ) )
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
_associate_eip_with_interface(eni_id, associate_public_ip, vm_=vm_)
param.update({key: str(data).lower()})
raise SaltCloudSystemExit( 'The request_instance action must be called with -a or --action.' )
spot_prefix = 'LaunchSpecification.'
spot_prefix = ''
exc_info_on_loglevel=logging.DEBUG
if not rd_data: err_msg = 'There was an error querying EC2 for the root device ' \ 'of image id {0}. Empty response.'.format(image_id) raise SaltCloudSystemExit(err_msg)
dev_index = dev_list.index(rd_name)
params[ '{0}BlockDeviceMapping.{1}.DeviceName'.format( spot_prefix, dev_index ) ] = rd_name
termination_key = '{0}BlockDeviceMapping.{1}.Ebs.DeleteOnTermination'.format(spot_prefix, dev_index) params[termination_key] = str(set_del_root_vol_on_destroy).lower()
if 'Ebs.VolumeType' not in ex_blockdevicemappings[dev_index]: type_key = '{0}BlockDeviceMapping.{1}.Ebs.VolumeType'.format(spot_prefix, dev_index) params[type_key] = rd_type
exc_info_on_loglevel=logging.DEBUG
if spot_config: sir_id = data[0]['spotInstanceRequestId']
return False
return False
log.info('Spot instance status: {0}'.format( data[0]['status']['message'] )) return None
log.error('Spot instance request resulted in state \'{0}\'. ' 'Nothing else we can do here.') return False
params = {'Action': 'CancelSpotInstanceRequests', 'SpotInstanceRequestId.1': sir_id} data = aws.query(params, location=location, provider=provider, opts=__opts__, sigver='4')
raise SaltCloudSystemExit( 'The query_instance action must be called with -a or --action.' )
time.sleep(1) continue
time.sleep(1) continue
return False
return False
destroy(vm_['name'])
raise SaltCloudSystemExit( 'The wait_for_instance action must be called with -a or --action.' )
time.sleep(60)
if not salt.utils.cloud.wait_for_port(ip_address, port=445, timeout=ssh_connect_timeout): raise SaltCloudSystemExit( 'Failed to connect to remote windows host' )
if not use_winrm:
else:
winrm_port = config.get_cloud_config_value( 'winrm_port', vm_, __opts__, default=5986 )
if not salt.utils.cloud.wait_for_port(ip_address, port=winrm_port, timeout=ssh_connect_timeout): raise SaltCloudSystemExit( 'Failed to connect to remote windows host (winrm)' )
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'ec2', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
vm_['private_key'] = key_filename
vm_['gateway'] = get_ssh_gateway_config(vm_)
if keyname(vm_) is None: raise SaltCloudSystemExit( 'The required \'keyname\' configuration setting is missing from the ' '\'ec2\' driver.' )
if isinstance(data, str): log.error('Error requesting instance: {0}'.format(data)) return {}
vm_['instance_id_list'] = [] for instance in data: vm_['instance_id_list'].append(instance['instanceId'])
queue_instances(vm_['instance_id_list'])
data = query_instance(vm_)
log.info('Created node {0}'.format(vm_['name']))
ret = instance.copy()
node = _get_node(instance_id=vm_['instance_id']) ret.update(node)
delvols_on_destroy = kwargs.get('del_all_vols_on_destroy', None)
if instance_id is None: return { 'Error': 'A valid instance_id or resource_id was not specified.' }
continue
continue
time.sleep(1) continue
time.sleep(0.5)
if not locations: locations = [get_location()]
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
kwargs['size'] = '10'
if 'encrypted' in kwargs and 'snapshot' not in kwargs: params['Encrypted'] = kwargs['encrypted']
if wait_to_finish: salt.utils.cloud.run_func_until_ret_arg(fun=describe_volumes, kwargs={'volume_id': volume_id}, fun_call=call, argument_being_watched='status', required_argument_response='available')
if wait_to_finish: salt.utils.cloud.run_func_until_ret_arg(fun=describe_snapshots, kwargs={'snapshot_id': snapshot_id}, fun_call=call, argument_being_watched='status', required_argument_response='completed')
if 'snapshot_ids' in kwargs: kwargs['snapshot_id'] = kwargs['snapshot_ids']
from __future__ import absolute_import import os import os.path import time import logging import pprint import base64 import salt.cache import salt.config as config import salt.utils.cloud from salt.exceptions import SaltCloudSystemExit
cache = None storconn = None compconn = None netconn = None webconn = None resconn = None
log = logging.getLogger(__name__)
if data is None: return {}
pass
source_image = VirtualHardDisk(uri=vm_['image']) img_ref = None if win_installer: os_type = 'Windows' else: os_type = 'Linux'
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'azure', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
from __future__ import absolute_import
import salt.utils.cloud import salt.config as config
from salt.utils.openstack import pyrax as suop
id: wFGEwgregeqw3435gDger key: GDE43t43REGTrkilg43934t34qT43t4dgegerGEgg location: cn-qingdao driver: aliyun
from __future__ import absolute_import import time import json import pprint import logging import hmac import uuid import sys import base64 from hashlib import sha1
import salt.utils.cloud import salt.config as config from salt.exceptions import ( SaltCloudNotFound, SaltCloudSystemExit, SaltCloudExecutionFailure, SaltCloudExecutionTimeout )
try: import requests HAS_REQUESTS = True except ImportError: HAS_REQUESTS = False
log = logging.getLogger(__name__)
optional = [ 'InstanceName', 'InternetChargeType', 'InternetMaxBandwidthIn', 'InternetMaxBandwidthOut', 'HostName', 'Password', 'SystemDisk.Category', ]
result = query(params) return result['InstanceId']
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'aliyun', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
exc_info_on_loglevel=logging.DEBUG
return False
destroy(vm_['name'])
ret = salt.utils.cloud.bootstrap(vm_, __opts__) ret.update(data.__dict__)
stringToSign = 'GET&%2F&' + percent_encode(canonicalizedQueryString[1:])
if params: parameters.update(params)
signature = _compute_signature(parameters, access_key_secret) parameters['Signature'] = signature
time.sleep(0.5)
if 'Code' in items or len(items['Images']['Image']) == 0: raise SaltCloudNotFound('The specified image could not be found.')
from __future__ import absolute_import from random import randint from re import findall import pprint import logging import time import os.path import subprocess
import salt.utils import salt.utils.cloud import salt.utils.xmlutil import salt.utils.vmware from salt.exceptions import SaltCloudSystemExit
import salt.config as config
try: from pyVmomi import vim HAS_PYVMOMI = True except Exception: HAS_PYVMOMI = False
try: from requests.packages.urllib3 import disable_warnings disable_warnings() except Exception: pass
try: import six except ImportError: HAS_SIX = False
log = logging.getLogger(__name__)
if adapter_type: log.error("Cannot change type of '{0}' to '{1}'. Not changing type".format(network_adapter.deviceInfo.label, adapter_type)) edited_network_adapter = network_adapter
scsi_spec.device.sharedBus = vim.vm.device.VirtualSCSIController.Sharing.virtualSharing
scsi_spec.device.sharedBus = vim.vm.device.VirtualSCSIController.Sharing.physicalSharing
scsi_spec.device.sharedBus = vim.vm.device.VirtualSCSIController.Sharing.noSharing
ide_controllers[device.key] = len(device.device)
network_spec = _add_new_network_adapter_helper(network_adapter_label, network_name, adapter_type, switch_type, container_ref) adapter_mapping = _set_network_adapter_mapping(devices['network'][network_adapter_label]) device_specs.append(network_spec) nics_map.append(adapter_mapping)
scsi_controller_properties = devices['scsi'][scsi_controller_label] scsi_spec = _add_new_scsi_controller_helper(scsi_controller_label, scsi_controller_properties, bus_number) device_specs.append(scsi_spec) bus_number += 1
ide_spec = _add_new_ide_controller_helper(ide_controller_label, None, bus_number) device_specs.append(ide_spec) bus_number += 1
octets = ip_address.split('.') if len(octets) != 4: return False
for i, octet in enumerate(octets):
return False
first_octet, second_octet, third_octet, fourth_octet = octets
if first_octet < 1: return False elif first_octet > 223: return False elif first_octet == 127: return False
if first_octet == 169 and second_octet == 254: return False
for octet in (second_octet, third_octet, fourth_octet): if (octet < 0) or (octet > 255): return False return True
if snapshot.childSnapshotList: ret = _get_snapshots(snapshot.childSnapshotList, current_snapshot, snapshot_path) if current_snapshot: return ret snapshots.update(ret)
if vm.config.template: status = 'VMware tools cannot be updated on a template' return status
if vm.guest.toolsStatus == "toolsOk": status = 'VMware tools is already up to date' return status
if vm.summary.runtime.powerState != "poweredOn": status = 'VM must be powered on to upgrade tools' return status
if vm.guest.toolsStatus in ["toolsNotRunning", "toolsNotInstalled"]: status = 'VMware tools is either not running or not installed' return status
_get_si()
inv = salt.utils.vmware.get_inventory(_get_si())
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'vmware', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
container_ref = None if datacenter: datacenter_ref = salt.utils.vmware.get_mor_by_property(_get_si(), vim.Datacenter, datacenter) container_ref = datacenter_ref if datacenter_ref else None
reloc_spec = vim.vm.RelocateSpec()
config_spec = vim.vm.ConfigSpec()
clone_spec = vim.vm.CloneSpec( template=template, location=reloc_spec, config=config_spec )
pod_spec = vim.storageDrs.PodSelectionSpec(storagePod=datastore_cluster_ref)
si = _get_si()
recommended_datastores = si.content.storageResourceManager.RecommendDatastores(storageSpec=storage_spec)
task = object_ref.Clone(folder_ref, vm_name, clone_spec) salt.utils.vmware.wait_for_task(task, vm_name, 'clone', 5, 'info')
exc_info_on_loglevel=logging.DEBUG
if not clone_type and power: task = new_vm_ref.PowerOn() salt.utils.vmware.wait_for_task(task, vm_name, 'power', 5, 'info')
datacenter_ref = salt.utils.vmware.get_mor_by_property(_get_si(), vim.Datacenter, datacenter_name) if datacenter_ref: return {datacenter_name: 'datacenter already exists'}
si = _get_si()
cluster_ref = salt.utils.vmware.get_mor_by_property(_get_si(), vim.ClusterComputeResource, cluster_name) if cluster_ref: return {cluster_name: 'cluster already exists'}
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
si = _get_si()
log.warning('You can only set either memdump or quiesce to True. Setting quiesce=False') quiesce = False
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
task = host_ref.Destroy_Task()
task = host_ref.parent.Destroy_Task()
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
datastore_cluster_ref = salt.utils.vmware.get_mor_by_property(_get_si(), vim.StoragePod, datastore_cluster_name) if datastore_cluster_ref: return {datastore_cluster_name: 'datastore cluster already exists'}
exc_info_on_loglevel=logging.DEBUG
user: myuser@pam or myuser@pve password: mypassword url: hypervisor.domain.tld driver: proxmox verify_ssl: True
from __future__ import absolute_import import time import pprint import logging
import salt.ext.six as six import salt.utils
import salt.utils.cloud import salt.config as config from salt.exceptions import ( SaltCloudSystemExit, SaltCloudExecutionFailure, SaltCloudExecutionTimeout )
try: import requests HAS_REQUESTS = True except ImportError: HAS_REQUESTS = False
log = logging.getLogger(__name__)
ret[name]['config'] = get_vmconfig( ret[name]['vmid'], ret[name]['node'], ret[name]['type'] )
nodes = query('get', 'nodes')
private_ips = [] public_ips = []
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'proxmox', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
exc_info_on_loglevel=logging.DEBUG
if 'ip_address' in vm_: ip_address = str(vm_['ip_address']) elif 'public_ips' in data:
if not wait_for_created(data['upid'], timeout=300): return {'Error': 'Unable to create {0}, command timed out'.format(name)}
if not start(name, vmid, call='action'): log.error('Node {0} ({1}) failed to start!'.format(name, vmid)) raise SaltCloudExecutionFailure
log.error('Wrong VM type. Valid options are: qemu, openvz (proxmox3) or lxc (proxmox4)') raise SaltCloudExecutionFailure
vm_['host'] = config.get_cloud_config_value( 'default_host', get_configured_provider(), __opts__, search_global=False )
log.error('No host given to create this VM on') raise SaltCloudExecutionFailure
vmhost = vm_['host'] newnode['vmid'] = newid
newnode['hostname'] = vm_['name'] newnode['ostemplate'] = vm_['image']
for prop in 'cpus', 'disk', 'ip_address', 'nameserver', 'password', 'swap', 'poolid', 'storage':
newnode['hostname'] = vm_['name'] newnode['ostemplate'] = vm_['image']
if 'disk' in vm_: log.warning('The "disk" option is not supported for LXC hosts and was ignored')
if 'gw' in vm_: newnode['net0'] = newnode['net0'] + ',gw=' + vm_['gw']
for prop in 'acpi', 'cores', 'cpu', 'pool', 'storage', 'sata0', 'ostype', 'ide2', 'net0':
salt.utils.cloud.fire_event( 'event', 'requesting instance', 'salt/cloud/{0}/requesting'.format(vm_['name']), {'kwargs': newnode}, )
data = query('get', 'nodes/{0}/{1}/{2}/config'.format(node, node_type, vmid))
if get_vm_status(vmid=vmobj['vmid'])['status'] != 'stopped': stop(name, vmobj['vmid'], 'action')
if not wait_for_state(vmobj['vmid'], 'stopped'): return {'Error': 'Unable to stop {0}, command timed out'.format(name)}
time.sleep(1)
from __future__ import absolute_import import json import os import logging import copy import time from pprint import pformat
import salt.utils
import salt.utils.cloud import salt.config as config from salt.exceptions import SaltCloudSystemExit
import salt.ext.six as six
log = logging.getLogger(__name__)
return salt.runner.RunnerClient(_master_opts())
timeout = __FUN_TIMEOUT.get( fun,
profile = vm_.get( 'lxc_profile', vm_.get('container_profile', None))
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
from __future__ import absolute_import import logging
import salt.utils
import salt.utils.cloud import salt.config as config
log = logging.getLogger(__name__)
user: MYLOGIN apikey: JVkbSJDGHSDKUKSDJfhsdklfjgsjdkflhjlsdfffhgdgjkenrtuinv driver: softlayer
from __future__ import absolute_import import logging import time
import salt.utils.cloud import salt.config as config from salt.exceptions import SaltCloudSystemExit
try: import SoftLayer HAS_SLLIBS = True except ImportError: HAS_SLLIBS = False
log = logging.getLogger(__name__)
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'softlayer', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
exc_info_on_loglevel=logging.DEBUG
name = name.split('.')[0]
from __future__ import absolute_import import copy import logging import pprint import time import yaml
import salt.config as config from salt.exceptions import SaltCloudSystemExit import salt.utils.cloud
log = logging.getLogger(__name__)
if name not in nodes: return {} salt.utils.cloud.cache_node(nodes[name], __active_provider_name__, __opts__) return nodes[name]
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'azure', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
system_config.domain_join = None system_config.win_rm = None
cleanup_services = config.get_cloud_config_value( 'cleanup_services', get_configured_provider(), __opts__, search_global=False, default=False ) if cleanup_services: log.debug('Deleting service {0}'.format(service_name))
get_storage = show_storage
get_storage_keys = show_storage_keys
get_disk = show_disk
get_service_certificate = show_service_certificate
get_management_certificate = show_management_certificate
get_input_endpoint = show_input_endpoint
get_deployment = show_deployment
get_affinity_group = show_affinity_group
get_storage_container = show_storage_container
get_storage_container_metadata = show_storage_container_metadata
get_storage_container_acl = show_storage_container_acl
get_blob_service_properties = show_blob_service_properties
get_blob_properties = show_blob_properties
from __future__ import absolute_import import logging import os import pprint import time
import salt.config as config from salt.exceptions import ( SaltCloudConfigError, SaltCloudExecutionFailure, SaltCloudExecutionTimeout, SaltCloudNotFound, SaltCloudSystemExit ) from salt.utils import is_true
import salt.utils.cloud
try:
log = logging.getLogger(__name__)
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'opennebula', vm_['profile']) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
return False
destroy(vm_['name'])
time.sleep(0.5)
password: letmein apikey: 901d3f579h23c8v73q9
password: USE_KEYRING
ignore_cidr: 192.168.50.0/24
from __future__ import absolute_import import os import logging import socket import pprint
try: from libcloud.compute.base import NodeState HAS_LIBCLOUD = True except ImportError: HAS_LIBCLOUD = False
import salt.utils
import salt.utils.cloud import salt.utils.pycrypto as sup import salt.config as config from salt.utils import namespaced_function from salt.exceptions import ( SaltCloudConfigError, SaltCloudNotFound, SaltCloudSystemExit, SaltCloudExecutionFailure, SaltCloudExecutionTimeout )
try: from netaddr import all_matching_cidrs HAS_NETADDR = True except ImportError: HAS_NETADDR = False
log = logging.getLogger(__name__)
return False
raise SaltCloudSystemExit( 'The request_instance action must be called with -a or --action.' )
avz = config.get_cloud_config_value( 'availability_zone', vm_, __opts__, default=None, search_global=False ) if avz is not None: kwargs['ex_availability_zone'] = avz
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'openstack', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
data, vm_ = request_instance(vm_)
vm_['instance_id'] = data.id
exc_info_on_loglevel=logging.DEBUG
return False
return
pass
destroy(vm_['name'])
user: MYLOGIN apikey: JVkbSJDGHSDKUKSDJfhsdklfjgsjdkflhjlsdfffhgdgjkenrtuinv driver: softlayer_hw
from __future__ import absolute_import import logging import time import decimal
import salt.utils.cloud import salt.config as config from salt.exceptions import SaltCloudSystemExit
try: import SoftLayer HAS_SLLIBS = True except ImportError: HAS_SLLIBS = False
log = logging.getLogger(__name__)
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'softlayer_hw', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
port_speed = config.get_cloud_config_value( 'port_speed', vm_, __opts__, default=273 ) kwargs['prices'].append({'id': port_speed})
bandwidth = config.get_cloud_config_value( 'bandwidth', vm_, __opts__, default=1800 ) kwargs['prices'].append({'id': bandwidth})
#response = conn.verifyOrder(kwargs)
exc_info_on_loglevel=logging.DEBUG
'ssh_connect_timeout', vm_, __opts__, 900
name = name.split('.')[0]
from __future__ import absolute_import import logging import pprint import re import time import datetime
import salt.utils.cloud
log = logging.getLogger(__name__)
LASTCALL = int(time.mktime(datetime.datetime.now().timetuple()))
response = _query('linode', 'boot', args={'LinodeID': linode_id, 'ConfigID': config_id})['DATA'] boot_job_id = response['JobID']
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'linode', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
datacenter_id = 2
update_linode(node_id, update_args={'Label': name}) log.debug('Set name for {0} - was linode{1}.'.format(name, node_id))
private_ip_assignment = get_private_ip(vm_) if private_ip_assignment: create_private_ip(node_id)
ssh_interface = _get_ssh_interface(vm_)
if ssh_interface == 'private_ips' and private_ip_assignment is False: create_private_ip(node_id) private_ip_assignment = True
config_id = create_config(kwargs={'name': name, 'linode_id': node_id, 'root_disk_id': root_disk_id, 'swap_disk_id': swap_disk_id})['ConfigID']
boot(kwargs={'linode_id': node_id, 'config_id': config_id, 'check_running': False})
if ssh_interface == 'private_ips': vm_['ssh_host'] = data['private_ips'][0] else: vm_['ssh_host'] = data['public_ips'][0]
vm_['password'] = get_password(vm_)
ret = salt.utils.cloud.bootstrap(vm_, __opts__)
kernel_id = 138
access_key: 0e604a2c-aea6-4081-acb2-e1d1258ef95c token: be8fd96b-04eb-4d39-b6ba-a9edbcf17f12 driver: scaleway
from __future__ import absolute_import import json import logging import pprint import time
from salt.ext.six.moves import range import salt.utils.cloud import salt.config as config from salt.exceptions import ( SaltCloudNotFound, SaltCloudSystemExit, SaltCloudExecutionFailure, SaltCloudExecutionTimeout )
try: import requests HAS_REQUESTS = True except ImportError: HAS_REQUESTS = False
ret = {} for node in items['servers']: ret[node['name']] = {} for item in node: value = node[item] ret[node['name']][item] = value return ret
if server_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'scaleway', server_['profile'], vm_=server_) is False: return False
if 'provider' in server_: server_['driver'] = server_.pop('provider')
exc_info_on_loglevel=logging.DEBUG
destroy(server_['name'])
if request.status_code == 204: return True
time.sleep(0.5)
pip install https://pysphere.googlecode.com/files/pysphere-0.1.8.zip
import pprint import logging import time
import salt.utils.cloud import salt.utils.xmlutil from salt.exceptions import SaltCloudSystemExit from salt.utils import warn_until
import salt.config as config
HAS_LIBS = False try: from pysphere import VIServer, MORTypes, VIException HAS_LIBS = True
log = logging.getLogger(__name__)
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'vsphere', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
exc_info_on_loglevel=logging.DEBUG
ret = show_instance(name=vm_['name'], call='action')
ret['deploy_kwargs'] = deploy_kwargs
username: user@example.com password: secretpassword datacenter_id: <UUID> ssh_private_key: /path/to/private.key ssh_public_key: /path/to/public.key
from __future__ import absolute_import import logging import os import pprint import time
import salt.utils import salt.config as config from salt.exceptions import ( SaltCloudConfigError, SaltCloudNotFound, SaltCloudExecutionFailure, SaltCloudExecutionTimeout, SaltCloudSystemExit )
import salt.utils.cloud
log = logging.getLogger(__name__)
exc_info_on_loglevel=logging.DEBUG
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'profitbricks', vm_['profile']) is False: return False
vm_size = override_size(vm_)
ssh_keys = get_public_keys(vm_)
server = Server( name=vm_['name'], ram=vm_size['ram'], cores=vm_size['cores'], create_volumes=[volume] )
exc_info_on_loglevel=logging.DEBUG
return False
return
destroy(vm_['name'])
from __future__ import absolute_import import logging import socket import pprint
import salt.utils
log = logging.getLogger(__name__)
if vm_['profile'] and config.is_profile_configured( __opts__, __active_provider_name__ or 'dimensiondata', vm_['profile']) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
exc_info_on_loglevel=logging.DEBUG
return False
return
destroy(vm_['name'])
exc_info_on_loglevel=logging.DEBUG
from __future__ import absolute_import import pprint import logging
import salt.config as config
try: from libcloud.compute.drivers.cloudstack import CloudStackNetwork import libcloud.security libcloud.security.CA_CERTS_PATH.append('/etc/ssl/certs/YaST-CA.pem') HAS_LIBS = True except ImportError: HAS_LIBS = False
log = logging.getLogger(__name__)
log.warning('Cannot get projects, you may need to update libcloud to 0.15 or later') return False
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'cloudstack', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
exc_info=log.isEnabledFor(logging.DEBUG)
user: fred password: saltybacon private_key: /root/mykey.pem private_key: mykey
from __future__ import absolute_import import os import json import logging import base64 import pprint import inspect import yaml import datetime from Crypto.Hash import SHA256 from Crypto.PublicKey import RSA from Crypto.Signature import PKCS1_v1_5
import salt.ext.six as six
log = logging.getLogger(__name__)
POLL_ALL_LOCATIONS = True
raise SaltCloudSystemExit( 'The query_instance action must be called with -a or --action.' )
return False
return False
pass #destroy(vm_['name'])
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'joyent', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
if 'id' not in item: item['id'] = item['name'] ret[item['name']] = item
for key in desired_keys: if key not in item: item[key] = None
to_del = [] if not full:
if not data: data = json.dumps({})
from __future__ import absolute_import import os import re import pprint import logging import msgpack from ast import literal_eval
from salt.utils import namespaced_function import salt.ext.six as six import salt.utils.cloud import salt.config as config from salt.utils import http from salt import syspaths
log = logging.getLogger(__name__)
_UA_PRODUCT = 'salt-cloud' _UA_VERSION = '0.2.0'
all_images.extend(conn.list_images())
try: tags = literal_eval(t)
try: metadata = literal_eval(md)
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'gce', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
node_dict = show_instance(node_data.name, 'action')
user: myuser password: mypassword url: https://api.cloud.xmission.com:4465/paci/v1.0/ driver: parallels
from __future__ import absolute_import import time import pprint import logging
import salt.utils.cloud import salt.config as config from salt.exceptions import ( SaltCloudNotFound, SaltCloudSystemExit, SaltCloudExecutionFailure, SaltCloudExecutionTimeout )
log = logging.getLogger(__name__)
content = ET.Element('ve')
name = ET.SubElement(content, 'name') name.text = vm_['name']
desc = ET.SubElement(content, 'description') desc.text = config.get_cloud_config_value( 'desc', vm_, __opts__, default=vm_['name'], search_global=False )
ram = ET.SubElement(content, 'ram-size') ram.text = config.get_cloud_config_value( 'ram', vm_, __opts__, default='256', search_global=False )
bandwidth = ET.SubElement(content, 'bandwidth') bandwidth.text = config.get_cloud_config_value( 'bandwidth', vm_, __opts__, default='100', search_global=False )
ip_num = ET.SubElement(content, 'no-of-public-ip') ip_num.text = config.get_cloud_config_value( 'ip_num', vm_, __opts__, default='1', search_global=False )
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'parallels', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
exc_info_on_loglevel=logging.DEBUG
return
destroy(vm_['name'])
auth_minion: myminion config_profile: my_openstack_profile
ignore_cidr: 192.168.50.0/24
centos7-2-iad-rackspace: provider: rackspace-iad size: general1-2 block_device: - source: image id: <image_id> dest: volume size: 100 shutdown: <preserve/remove> bootindex: 0
centos7-2-iad-rackspace: provider: rackspace-iad size: general1-2 boot_volume: <volume id>
centos7-2-iad-rackspace: provider: rackspace-iad size: general1-2 snapshot: <cinder snapshot id>
centos7-2-iad-rackspace: provider: rackspace-iad size: general1-2 ephemeral: - size: 100 format: <swap/ext4>
centos7-2-iad-rackspace: provider: rackspace-iad size: general1-2 swap: <size>
from __future__ import absolute_import import os import logging import socket import pprint import yaml
import salt.ext.six as six import salt.utils import salt.client from salt.utils.openstack import nova try: import novaclient.exceptions except ImportError as exc: pass
log = logging.getLogger(__name__) request_log = logging.getLogger('requests')
script = namespaced_function(script, globals()) reboot = namespaced_function(reboot, globals())
raise SaltCloudSystemExit( 'The request_instance action must be called with -a or --action.' )
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'nova', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
data, vm_ = request_instance(vm_)
vm_['instance_id'] = data.id
exc_info_on_loglevel=logging.DEBUG
return False
return
destroy(vm_['name'])
if server_tmp is None: continue
create_volume = volume_create
attach_volume = volume_attach
create_attach_volumes = volume_create_attach
from __future__ import absolute_import import time import json import pprint import logging import hmac import base64 from hashlib import sha256
try: import requests HAS_REQUESTS = True except ImportError: HAS_REQUESTS = False
log = logging.getLogger(__name__)
signature = _compute_signature(real_parameters, access_key_secret, 'GET', '/iaas/') real_parameters['signature'] = signature
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'qingcloud', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
destroy(vm_['name'])
salt.utils.cloud.bootstrap(vm_, __opts__)
from __future__ import absolute_import import logging import socket import pprint
import salt.utils import salt.config as config from salt.utils import namespaced_function from salt.exceptions import ( SaltCloudSystemExit, SaltCloudExecutionFailure, SaltCloudExecutionTimeout )
try: from libcloud.compute.base import NodeState import libcloud.security libcloud.security.CA_CERTS_PATH.append('/etc/ssl/certs/YaST-CA.pem') HAS_LIBCLOUD = True except ImportError: HAS_LIBCLOUD = False
log = logging.getLogger(__name__)
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'rackspace', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
return False
return
destroy(vm_['name'])
api_key: <supersecretapi_key> driver: vultr
from __future__ import absolute_import import pprint import logging import time import urllib
import salt.config as config import salt.utils.cloud from salt.exceptions import ( SaltCloudConfigError, SaltCloudSystemExit )
log = logging.getLogger(__name__)
if name not in nodes: return {} salt.utils.cloud.cache_node(nodes[name], __active_provider_name__, __opts__) return nodes[name]
exc_info_on_loglevel=logging.DEBUG
ret = salt.utils.cloud.bootstrap(vm_, __opts__)
import logging
return True
if vm_info['profile'] and config.is_profile_configured( __opts__, __active_provider_name__ or 'virtualbox', vm_info['profile'] ) is False: return False
request_kwargs = { 'name': vm_info['name'], 'clone_from': vm_info['clonefrom'] }
if power: vb_start_vm(vm_name, timeout=boot_timeout) ips = vb_wait_for_network_address(wait_for_ip_timeout, machine_name=vm_name)
if deploy: vm_info['key_filename'] = key_filename vm_info['ssh_host'] = ip
return vm_result
from __future__ import absolute_import import os import time import json import pprint import logging import decimal
try: import requests HAS_REQUESTS = True except ImportError: HAS_REQUESTS = False
log = logging.getLogger(__name__)
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'digital_ocean', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
ssh_key_name = config.get_cloud_config_value( 'ssh_key_name', vm_, __opts__, search_global=False )
exc_info_on_loglevel=logging.DEBUG
return False
destroy(vm_['name'])
request.text
if request.status_code == 204: return True
time.sleep(0.5)
from __future__ import absolute_import, print_function, generators import os import copy import glob import time import signal import logging import traceback import multiprocessing import sys from itertools import groupby
from salt.exceptions import ( SaltCloudNotFound, SaltCloudException, SaltCloudSystemExit, SaltCloudConfigError )
try: import Crypto.Random except ImportError:
log = logging.getLogger(__name__)
if 'provider' in provider: driver = provider.pop('provider') else: driver = provider['driver']
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
raise SaltCloudConfigError( 'Either an instance (or list of names) or a provider must be ' 'specified, but not both.' )
exc_info_on_loglevel=logging.DEBUG
pmap[alias][driver] = []
continue
log.debug( 'The \'{0}\' cloud driver defined under \'{1}\' provider ' 'alias is unable to get the locations information'.format( driver, alias ) ) continue
exc_info_on_loglevel=logging.DEBUG
log.debug( 'The \'{0}\' cloud driver defined under \'{1}\' provider ' 'alias is unable to get the images information'.format( driver, alias ) ) continue
exc_info_on_loglevel=logging.DEBUG
log.debug( 'The \'{0}\' cloud driver defined under \'{1}\' provider ' 'alias is unable to get the sizes information'.format( driver, alias ) ) continue
exc_info_on_loglevel=logging.DEBUG
output_multip = enter_mainloop( _destroy_multiprocessing, parallel_data, pool_size=pool_size)
ret_multip = {} for obj in output_multip: ret_multip.update(obj)
for alias, driver, name in vms_to_destroy: ret = processed[alias][driver][name] if not ret: continue
if isinstance(ret, dict) and 'newname' in ret: salt.utils.cloud.remove_key( self.opts['pki_dir'], ret['newname'] ) continue
salt.utils.cloud.remove_key(self.opts['pki_dir'], os.path.basename(key_file)) continue
raise SaltCloudSystemExit( 'The following VM\'s were not found: {0}'.format( ', '.join(names) ) )
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
vm_['pub_key'] = None vm_['priv_key'] = None
salt.utils.cloud.accept_key( self.opts['pki_dir'], vm_['pub_key'], key_id )
time.sleep(3)
if valid_function is False: if invalid_functions.get(fun) is None: invalid_functions.update({fun: []}) invalid_functions[fun].append(vm_name) continue
missing_vms = names.difference(invalid_func_vms) if missing_vms: ret['Not Found'] = list(missing_vms) ret['Not Actioned/Not Running'] = list(names)
if missing_vms: return ret
ret['Not Actioned/Not Running'] = list(names) ret['Not Found'] = list(names) return ret
pass
entries = {} for name, overrides in six.iteritems(mapped): overrides.setdefault('name', name) entries[name] = overrides map_[profile] = entries continue
mapped = [mapped]
alias, driver = profile_data.get('provider').split(':') provider_details = self.opts['providers'][alias][driver].copy() del provider_details['profiles']
provider_details.update(profile_data) profile_data = provider_details
ret['create'][nodename] = nodedata alias, driver = nodedata['provider'].split(':') defined.add((alias, driver, nodename))
matching = get_matching_by_name(name) if not matching: continue
for item in matching: if name not in ret['create']: break
ret['destroy'] = exist.difference(defined)
for name, profile in create_list: make_minion = salt.config.get_cloud_config_value( 'make_minion', profile, self.opts, default=True ) if make_minion is False: continue
master_profile.setdefault('preseed_minion_keys', {}) master_profile['preseed_minion_keys'].update({name: pub})
local_master = True
out.get('deploy_kwargs', {}) or out.pop('deploy_kwargs', {})
log.info( 'Since parallel deployment is in use, ssh console output ' 'is disabled. All ssh output will be logged though' ) opts['display_ssh_output'] = False
continue
local_master = True
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
return data['alias'], data['driver'], ()
def _run_parallel_map_providers_query(*args, **kw): return communicator(run_parallel_map_providers_query)(*args[0], **kw)
import salt.defaults.exitcodes from salt.exceptions import SaltException
from __future__ import print_function from __future__ import absolute_import import os import sys import logging from salt.ext.six.moves import input
import salt.config import salt.defaults.exitcodes import salt.output import salt.utils from salt.utils import parsers from salt.utils.verify import check_user, verify_env, verify_files, verify_log
import salt.cloud import salt.utils.cloud from salt.exceptions import SaltCloudException, SaltCloudSystemExit import salt.ext.six as six import salt.syspaths as syspaths log = logging.getLogger(__name__)
self.parse_args()
verify_files([logfile], salt_master_user)
self.setup_logfile_logger() verify_log(self.config)
key, value = name.split('=', 1) kwargs[key] = value
print(msg) self.exit(1)
run_map = False
exc_info_on_loglevel=logging.DEBUG
import os import logging from salt.ext.six import string_types import salt.ext.six as six from salt.ext.six.moves import zip
import salt.utils.event import salt.client
import salt.utils import salt.utils.cloud import salt.config as config from salt.exceptions import SaltCloudNotFound, SaltCloudSystemExit
log = logging.getLogger(__name__)
import logging import re
try: import pymongo HAS_PYMONGO = True except ImportError: HAS_PYMONGO = False
log = logging.getLogger(__name__)
minion_id = kwargs['opts']['id'] if re_pattern: minion_id = re.sub(re_pattern, re_replace, minion_id)
log.debug( 'ext_tops.mongo: no document found in collection {0}'.format( collection ) ) return {}
__virtualname__ = 'reclass'
from reclass.adapters.salt import top as reclass_top from reclass.errors import ReclassException
reclass_opts = __opts__['master_tops']['reclass']
filter_out_source_path_option(reclass_opts)
set_inventory_base_uri_default(__opts__, kwargs)
minion_id = kwargs['opts']['id']
return reclass_top(minion_id, **reclass_opts)
import logging
log = logging.getLogger(__name__)
__virtualname__ = 'varstack'
import logging import subprocess
import yaml
import logging
log = logging.getLogger(__name__)
from __future__ import absolute_import import os
import salt.utils
import salt.ext.six as six
from salt import syspaths import salt.config import salt.loader from salt.client import mixins from salt.utils.error import raise_error
import logging import os
import yaml
import salt.config
import salt.utils.error
from __future__ import absolute_import import os
import salt.utils
import salt.ext.six as six
import os import hashlib
import salt.key import salt.crypt
import fnmatch import logging import os import re import time import stat import tempfile
import salt.ext.six as six
HAS_PWD = False
pillar = salt.utils.gitfs.GitPillar(opts) pillar.init_remotes( opts_dict['git'], git_pillar.PER_REMOTE_OVERRIDES ) ret.append(pillar)
keyfile = os.path.join( opts['cachedir'], '.{0}_key'.format(user.replace('\\', '_')) )
os.chmod(keyfile, stat.S_IRUSR | stat.S_IWUSR)
return False
if self.opts.get('permissive_pki_access', False) and stat.S_IWGRP & fmode.st_mode: return True elif stat.S_IWGRP & fmode.st_mode: return False
if not (stat.S_IWGRP & fmode.st_mode or stat.S_IWOTH & fmode.st_mode): return True
log.error( 'Top function {0} failed with error {1} for minion ' '{2}'.format( fun, exc, load['id'] ) )
return False
saveload_fstr = '{0}.save_load'.format(self.opts['master_job_cache']) self.mminion.returners[saveload_fstr](load['jid'], load)
if isinstance(self.opts['peer_run'][match], list): perms.update(self.opts['peer_run'][match])
log.warning( 'Minion id {0} is not who it says it is!'.format( load['id'] ) ) return {}
log.warning('Authentication failure of type "eauth" occurred.') return ''
del good
import sys from collections import namedtuple, Iterable, Sequence, Mapping import logging
from salt.utils.odict import OrderedDict import salt.ext.six as six
masters = [] for hostage in hostages: external = hostage['external'] internal = hostage['internal'] if external: external = parse_hostname(external, master_port) if not external:
import ioflo.base.deeding from ioflo.aid.odicting import odict
masterStack.keep.auto = raeting.AutoMode.always.value minionStack.keep.auto = raeting.AutoMode.always.value
import sys if sys.version_info < (2, 7): import unittest2 as unittest else: import unittest
self.join(other2, main)
self.join(other2, main)
self.join(other2, main)
from salt.daemons.flo import core from salt.daemons.test.plan import actors
self.addEnterDeed("TestOptsSetupMaster") self.addEnterDeed("SaltRaetManorLaneSetup") self.addEnterDeed("PresenterTestSetup") act = self.addRecurDeed("SaltRaetPresenter")
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 5)
self.addEnterDeed("TestOptsSetupMaster") self.addEnterDeed("SaltRaetManorLaneSetup") self.addEnterDeed("PresenterTestSetup") act = self.addRecurDeed("SaltRaetPresenter")
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 1)
self.addEnterDeed("TestOptsSetupMaster") self.addEnterDeed("SaltRaetManorLaneSetup") self.addEnterDeed("PresenterTestSetup") act = self.addRecurDeed("SaltRaetPresenter")
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 1)
self.addEnterDeed("TestOptsSetupMaster") self.addEnterDeed("SaltRaetManorLaneSetup") self.addEnterDeed("PresenterTestSetup") act = self.addRecurDeed("SaltRaetPresenter")
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 1)
self.addEnterDeed("TestOptsSetupMaster") self.addEnterDeed("SaltRaetManorLaneSetup") self.addEnterDeed("PresenterTestSetup") act = self.addRecurDeed("SaltRaetPresenter")
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 1)
self.addEnterDeed("TestOptsSetupMaster") self.addEnterDeed("SaltRaetManorLaneSetup") self.addEnterDeed("PresenterTestSetup") act = self.addRecurDeed("SaltRaetPresenter")
self.addEnterDeed("TestOptsSetupMaster") self.addEnterDeed("SaltRaetManorLaneSetup") self.addEnterDeed("PresenterTestSetup") act = self.addRecurDeed("SaltRaetPresenter")
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 0)
self.addEnterDeed("TestOptsSetupMaster") self.addEnterDeed("SaltRaetManorLaneSetup") self.addEnterDeed("PresenterTestSetup") act = self.addRecurDeed("SaltRaetPresenter")
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 1)
self.addEnterDeed("TestOptsSetupMaster") self.addEnterDeed("SaltRaetManorLaneSetup") self.addEnterDeed("PresenterTestSetup") act = self.addRecurDeed("SaltRaetPresenter")
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 1)
self.addEnterDeed("TestOptsSetupMaster") self.addEnterDeed("SaltRaetManorLaneSetup") self.addEnterDeed("PresenterTestSetup") act = self.addRecurDeed("SaltRaetPresenter")
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 1)
self.addEnterDeed("TestOptsSetupMaster") self.addEnterDeed("SaltRaetManorLaneSetup") self.addEnterDeed("PresenterTestSetup") act = self.addRecurDeed("SaltRaetPresenter")
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 1)
self.addEnterDeed("TestOptsSetupMaster") self.addEnterDeed("SaltRaetManorLaneSetup") self.addEnterDeed("PresenterTestSetup") act = self.addRecurDeed("SaltRaetPresenter")
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 1)
from __future__ import absolute_import, print_function import os
import ioflo.app.run from ioflo.base.consoling import getConsole
import sys from salt.ext.six.moves import map if sys.version_info < (2, 7): import unittest2 as unittest else: import unittest
import sys from salt.ext.six.moves import map if sys.version_info < (2, 7): import unittest2 as unittest else: import unittest
from salt.daemons.flo import core from salt.daemons.test.plan import actors
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 1)
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 1)
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 0)
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 0)
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 0)
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 1)
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 1)
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 0)
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 0)
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 0)
import os import stat
self.clients.setsockopt(zmq.IPV4ONLY, 0)
from __future__ import absolute_import import os import sys import types import logging import traceback import multiprocessing import subprocess import json
import ioflo.base.deeding
stack.addRemote(RemoteYard(stack=stack, name='manor', lanename=lanename, dirpath=sockdirpath)) console.concise("Created Jobber Stack {0}\n".format(stack.name)) return stack
try: self.proc_run(msg) except Exception as exc: log.error( 'Exception caught by jobber: {0}'.format(exc), exc_info=True)
src_estate, src_yard, src_share = msg['route']['src'] salt.transport.jobber_estate_name = src_estate salt.transport.jobber_yard_name = src_yard
import multiprocessing import os
import ioflo.base.deeding
import salt.fileserver import salt.loader import salt.utils.minions import salt.daemons.masterapi
import salt.utils.reactor import salt.utils.event import ioflo.base.deeding
from __future__ import absolute_import import os
from . import core from . import worker from . import maint from . import reactor from . import zero from . import jobber from . import dummy
import salt.daemons.masterapi
import ioflo.app.run import salt.ext.six as six
from __future__ import absolute_import import logging
import ioflo.base.deeding
import time import os import multiprocessing import logging from salt.ext.six.moves import range
import salt.daemons.flo import salt.daemons.masterapi from raet import raeting from raet.lane.stacking import LaneStack from raet.lane.yarding import RemoteYard
import ioflo.base.deeding
from __future__ import absolute_import import os import time import random import logging import itertools from collections import deque from _socket import gaierror
HAS_PSUTIL = False try: import salt.utils.psutil_compat as psutil HAS_PSUTIL = True except ImportError: pass
import salt.ext.six as six from salt.ext.six.moves import range
if modules_max_memory is True: resource.setrlimit(resource.RLIMIT_AS, old_mem_limit) self.module_refresh.value = False
Ioinits = { 'road_stack': '.salt.road.manor.stack', }
Ioinits = { 'lane_stack': '.salt.lane.manor.stack', }
if d_yard in self.lane_stack.value.nameRemotes: self.lane_stack.value.transmit(msg, self.lane_stack.value.nameRemotes[d_yard].uid) return
log.error('Received message without share: {0}'.format(msg)) return
log.error('Received local command remotely! Ignoring: {0}'.format(msg)) return
if 'load' in msg: role = self.road_stack.value.nameRemotes[sender].role
if d_estate in self.road_stack.value.nameRemotes: self.road_stack.value.message(msg, self.road_stack.value.nameRemotes[d_estate].uid) return
if d_yard in self.lane_stack.value.nameRemotes: self.lane_stack.value.transmit(msg, self.lane_stack.value.nameRemotes[d_yard].uid) return
log.error('Lane Router Received message without share: {0}'.format(msg)) return
if d_yard in self.lane_stack.value.nameRemotes: self.lane_stack.value.transmit(msg, self.lane_stack.value.nameRemotes[d_yard].uid) return return
log.error('Received message without share: {0}'.format(msg)) return
if d_estate in self.road_stack.value.nameRemotes: self.road_stack.value.message(msg, self.road_stack.value.nameRemotes[d_estate].uid) return
if d_yard in self.lane_stack.value.nameRemotes: self.lane_stack.value.transmit(msg, self.lane_stack.value.nameRemotes[d_yard].uid) return return
log.error('Lane Router Received message without share: {0}'.format(msg)) return
import os
from ioflo.aid.odicting import odict
self.saltRaetKey.status(remote.role, remote.pubber.keyhex, remote.verfer.keyhex)
import copy import logging import time
import salt.defaults.exitcodes import salt.ext.six as six
from salt.output import nested nested.__opts__ = {} ret = nested.output(obj).rstrip() return ret
exc_str = exc_str_prefix + _nested_output(self.info)
from __future__ import absolute_import, print_function import os import copy import json import stat import shutil import fnmatch import hashlib import logging
import salt.crypt import salt.utils import salt.client import salt.exceptions import salt.utils.event import salt.daemons.masterapi from salt.utils import kinds from salt.utils.event import tagify
import salt.ext.six as six from salt.ext.six.moves import input try: import msgpack except ImportError: pass
else: mpub = self.opts['pki_dir'] + '/' + 'master.pub' if os.path.isfile(mpub): self.pubkey = mpub
else: mpriv = self.opts['pki_dir'] + '/' + 'master_sign.pem' if os.path.isfile(mpriv): self.privkey = mpriv
if self.opts['transport'] in ('zeromq', 'tcp'): key_dirs = self._check_minions_directories() else: key_dirs = self._check_minions_directories()
continue
keydata = { 'minion_id': minion_id, 'pub': pub, 'verify': verify}
import salt.utils.sdb
from __future__ import absolute_import, print_function import fnmatch import logging import os
import salt.client import salt.payload import salt.utils import salt.utils.jid import salt.minion import salt.returners
import salt.ext.six as six from salt.exceptions import SaltClientError
import logging
from salt.exceptions import CommandExecutionError
try: import pycontrol.pycontrol as f5 HAS_PYCONTROL = True except ImportError: HAS_PYCONTROL = False
import salt.utils import salt.fileserver
import salt.pillar import salt.utils.minions
import logging import os
import salt.cloud
log = logging.getLogger(__name__)
import os
netapi = salt.netapi.NetapiClient(__opts__) if not netapi._is_master_running(): raise salt.exceptions.SaltDaemonNotRunning( 'Salt Master must be running.')
from __future__ import absolute_import
import salt.client.ssh.client
from __future__ import absolute_import, print_function import time import os import copy import logging
import salt.client import salt.utils import salt.utils.virt import salt.utils.cloud import salt.key from salt.utils.odict import OrderedDict as _OrderedDict
import salt.ext.six as six
__func_alias__ = { 'list_': 'list' }
ret['ping_status'] = bool(len(done))
if not done: ret['result'] = False if not quiet: __jid_event__.fire_event({'message': ret}, 'progress') return ret
import os import sys
from __future__ import absolute_import, print_function import logging
import salt.utils.reactor import salt.syspaths import salt.utils.event import salt.utils.process from salt.ext.six import string_types
from __future__ import print_function from __future__ import absolute_import
import salt.utils.thin
from __future__ import absolute_import
import salt.output import salt.minion
import salt.ext.six as six
import logging
HAS_LIBS = False HAS_SIX = False try: import requests
try: import six except ImportError: pass
import yaml import json
import salt.utils import salt.utils.pagerduty from salt.ext.six import string_types
list_maintenance_windows = salt.utils.alias_function(list_windows, 'list_maintenance_windows')
list_escalation_policies = salt.utils.alias_function(list_policies, 'list_escalation_policies')
import os import logging import json
HAS_LIBS = False try: import dns.query import dns.update import dns.tsigkeyring HAS_LIBS = True except ImportError: HAS_LIBS = False
import salt.utils
while i > 1: p = parts.pop(0) i -= 1 popped.append(p)
from __future__ import absolute_import, print_function import itertools
import salt.client import salt.runner import salt.wheel
import salt.ext.six as six from salt.exceptions import SaltClientError
from __future__ import print_function from __future__ import absolute_import import socket
import salt.utils
from __future__ import print_function from __future__ import absolute_import
import salt.loader import salt.utils.event from salt.utils.event import tagify from salt.exceptions import SaltInvocationError
import salt.utils.error
import time import salt.ext.six as six from salt.ext.six.moves import range
from __future__ import absolute_import, print_function import logging
try: import paramiko HAS_PARAMIKO = True except ImportError: HAS_PARAMIKO = False
import salt.search
import json import logging
import salt.output import salt.utils.http
from __future__ import absolute_import, print_function import os
import salt.ext.six as six try: import msgpack except ImportError:
from salt.exceptions import CommandExecutionError, SaltRenderError import salt.utils import salt.utils.gitfs import logging import salt.minion import salt.loader import salt.template
PER_REMOTE_OVERRIDES = ('ssl_verify',)
import atexit import logging
HAS_LIBS = False try: import salt.ext.six as six HAS_LIBS = True except ImportError: try: import six HAS_LIBS = True except ImportError: pass
import logging
import salt.utils import salt.utils.minions
from __future__ import absolute_import, print_function import logging
import salt.loader import salt.utils import salt.utils.event from salt.exceptions import SaltInvocationError
orch = salt.utils.alias_function(orchestrate, 'orch') sls = salt.utils.alias_function(orchestrate, 'sls')
salt 'jerry' system.reboot && \\ salt-run state.event 'salt/minion/jerry/start' count=1 quiet=True && \\ salt 'jerry' state.highstate
salt -L 'kevin,stewart,dave' system.reboot && \\ salt-run state.event 'salt/minion/*/start' count=3 quiet=True && \\ salt -L 'kevin,stewart,dave' state.highstate
salt-run state.event | while read -r tag data; do echo $tag echo $data | jq -colour-output . done
import logging
import salt.utils.http
import logging
import salt.utils.extmods
from __future__ import absolute_import, print_function import os.path import logging
import salt.client import salt.utils.virt import salt.utils.cloud import salt.key from salt.exceptions import SaltClientError
import salt.ext.six as six
print(client_error)
from __future__ import absolute_import, print_function import os import operator import re import subprocess import tempfile import time import logging import uuid
import salt.ext.six as six
version_status[2] = master_version.string
import logging
import salt.pillar.git_pillar import salt.utils.gitfs from salt.exceptions import SaltRunnerError from salt.ext import six
log = logging.getLogger(__name__)
out_file = os.path.join(conn['formula_path'], new_name)
new_name = '{0}.sls.orig'.format(package) out_file = os.path.join(conn['pillar_path'], new_name)
out_file = os.path.join(salt.syspaths.CONFIG_DIR, new_name)
out_file = os.path.join(conn['reactor_path'], member.name)
member.name = member.name.replace('{0}/'.format(package), '')
member.name = '{0}.sls.orig'.format(package) out_path = conn['pillar_path']
member.name = member.name.replace('{0}/'.format(package), '') out_path = salt.syspaths.CONFIG_DIR
out_path = __opts__['reactor_path']
comps = member.path.split('/') if len(comps) > 1 and comps[0] == comps[1]: member.path = '/'.join(comps[1:])
from __future__ import absolute_import, print_function import os import yaml import tarfile import shutil import msgpack import datetime import hashlib import logging import pwd import grp import sys
log = logging.getLogger(__name__)
self._install_indv_pkg(package, out_file)
existing_files = self._pkgfiles_fun('check_existing', pkg_name, pkg_files, formula_def)
self._pkgdb_fun('register_pkg', pkg_name, formula_def, self.db_conn)
for member in pkg_files: member.uid = uid member.gid = gid member.uname = uname member.gname = gname
if dep in inspected: continue inspected.append(dep)
pkg_info = self._pkgdb_fun('info', package, self.db_conn) if pkg_info is None: raise SPMInvocationError('Package {0} not installed'.format(package))
log = logging.getLogger(__name__)
from __future__ import absolute_import import re
if isinstance(cmd, str): funs_to_check = [cmd] else: funs_to_check = cmd for fun in funs_to_check: if re.match(blacklisted_module, fun): return True
from __future__ import absolute_import import os import sys import copy import site import fnmatch import logging import datetime import traceback import re
import salt.ext.six as six from salt.ext.six.moves import map, range, reload_module
log.info(str(ret))
comps[1] = '.'.join(comps[1:len(comps)])
ex_sls.add(exc)
decrypt = salt.loader.render( self.opts, {}).get(self._pillar_enc)
try:
return ret
self.module_refresh() return
continue
chunk[key] = name
ex_sls.add(exc)
inject_globals['__env__'] = str(cdata['kwargs']['env'])
inject_globals['__env__'] = str(low['__env__'])
inject_globals['__env__'] = 'base'
if 'warnings' in cdata: ret.setdefault('warnings', []).extend(cdata['warnings'])
if fnmatch.fnmatch(chunk['__sls__'], req_val): if requisite == 'prereq': chunk['__prereq__'] = True reqs.append(chunk) found = True continue
if '.' in high[name]: comps = high[name].split('.') high[name] = { comps[0]: [comps[1]] } continue
return errors
if not slsmod: errors.append( 'Environment {0} contains an empty sls ' 'index'.format(saltenv) )
exc_info_on_loglevel=logging.DEBUG
if env_key in matches or fnmatch.filter(self.avail[env_key], inc_sls): resolved_envs = [env_key] else: resolved_envs = []
resolved_envs = [ aenv for aenv in matches if fnmatch.filter(self.avail[aenv], inc_sls) ]
continue
continue
continue
continue
if '.' in state[name]: comps = state[name].split('.') state[name] = {'__sls__': sls, '__env__': saltenv, comps[0]: [comps[1]]} continue
if not statefiles: statefiles = [sls_match]
high, ext_errors = self.state.reconcile_extend(high) errors += ext_errors
errors += self.state.verify_high(high) high, req_in_errors = self.state.requisite_in(high) errors += req_in_errors high = self.state.apply_exclude(high)
chunks = self.state.compile_high_data(high)
stack = []
self._pydsl_all_decls = {}
self._pydsl_render_stack = []
cls.stack = []
self.channel = salt.transport.Channel.factory(self.opts['master_uri'])
from __future__ import print_function from __future__ import absolute_import import glob import logging import os import re import sqlite3 as lite from salt.exceptions import SaltInvocationError
__virtualname__ = 'sqlite'
return __virtualname__
cur.executemany(cmd, newitems)
from __future__ import absolute_import from contextlib import contextmanager import json import sys
__virtualname__ = 'pgjsonb'
cur.executemany(cmd, newitems)
EX_GENERIC = 1
EX_THIN_PYTHON_INVALID = 10 EX_THIN_DEPLOY = 11 EX_THIN_CHECKSUM = 12 EX_MOD_DEPLOY = 13 EX_SCP_NOT_FOUND = 14
EX_AGGREGATE = 20
SALT_KEEPALIVE = 99
SALT_BUILD_FAIL = 101
DEFAULT_TARGET_DELIM = ':'
from salt.modules.tomcat import _extract_war_version
ret = {'name': name, 'result': True, 'changes': {}, 'comment': ''}
if __opts__['test']: ret['result'] = None return ret
deploy_res = __salt__['tomcat.deploy_war'](war, name, 'yes', url, __env__, timeout, temp_war_location=temp_war_location)
ret = {'name': name, 'result': True, 'changes': {}, 'comment': ''}
if __opts__['test']: ret['result'] = None return ret
ret = { 'name': name, 'changes': {}, 'result': False, 'comment': '', }
existing_config = None if __salt__['marathon.has_app'](name): existing_config = __salt__['marathon.app'](name)['app']
from netaddr import IPAddress from netaddr.core import AddrFormatError
import dns.resolver
dns_reply = list() try: dns_reply = dns.resolver.query(peer) except dns.resolver.NoAnswer: continue for dns_ip in dns_reply: ip_only_peers.append(str(dns_ip))
import os
import salt.utils import salt.ext.six as six
import salt.ext.six as six
import salt.utils
settings = _normalize_server_settings(**settings)
if addresses: if addresses[0] == 'None': addresses[0] = None elif addresses is None: addresses = [None]
import logging import os
from __future__ import absolute_import import logging import os import os.path
import salt.utils
adds[k] = Tags[k]
myqueue: boto_sqs.present: - region: us-east-1 - profile: mysqsprofile
if __opts__['test']: ret['comment'] = 'SELinux mode is set to be changed to {0}'.format( tmode) ret['result'] = None return ret
from salt.exceptions import CommandExecutionError, CommandNotFoundError
import salt.ext.six as six
if pkg_ver: if installed_pkg_ver != pkg_ver: pkgs_to_install.append(pkg) else: pkgs_satisfied.append(installed_name_ver)
from __future__ import absolute_import import logging
import salt.utils
from __future__ import absolute_import
import salt.exceptions
ret = {'name': '', 'changes': {}, 'result': False, 'comment': ''}
current_state = __salt__['tuned.active']()
if profile not in valid_profiles: raise salt.exceptions.SaltInvocationError('Invalid Profile Name')
if profile in current_state: ret['result'] = True ret['comment'] = 'System already in the correct state' return ret
ret['result'] = None return ret
new_state = __salt__['tuned.profile'](profile)
ret['comment'] = 'The state of "{0}" was changed!'.format(profile)
ret['changes'] = { 'old': current_state, 'new': new_state, }
return ret
ret = {'name': 'off', 'changes': {}, 'result': False, 'comment': 'off'}
current_state = __salt__['tuned.active']()
if current_state == 'off': ret['result'] = True ret['comment'] = 'System already in the correct state' return ret
ret['result'] = None return ret
from salt.ext.six import string_types import salt.utils import salt.ext.six as six
log = logging.getLogger(__name__)
__virtualname__ = 'docker'
return ':'.join((image, tag))
- volumes: /usr/local/etc/ssl/certs/example.crt: bind: /etc/ssl/certs/com.example.internal.crt ro: True /var/run: bind: /var/run/host/ ro: False
- volumes: - /usr/local/etc/ssl/certs/example.crt: bind: /etc/ssl/certs/com.example.internal.crt ro: True
bindvolumes = volumes
kw['force'] = True build_status = built(name, **kw) result = build_status['result'] status = _ret_status(build_status, name, result=result, changes={name: result}) return status
if already_exists: return _valid(comment='Container {0!r} already exists'.format(name)) dports, denvironment = {}, {}
already_exists and cinfos['out']['Image'] == iinfos['out']['Id']
import logging
from salt.modules import postgres
mode = 'create' mtdata = __salt__['postgres.create_metadata']( name, schema=schema, ext_version=ext_version, **db_args)
if __salt__['mysql.grant_exists']( grant, database, user, host, grant_option, escape, **connection_args):
import sys
import salt.utils
if __salt__['mysql.user_exists'](name, host, unix_socket=unix_socket, **connection_args):
ret['comment'] = ( 'User {0}@{1} is not present, so it cannot be removed' ).format(name, host) return ret
from __future__ import absolute_import import pprint
import salt.ext.six as six
import salt.utils.cloud as suc
test = __opts__.get('test', False) instance = __salt__['cloud.action'](fun='show_instance', names=names) __opts__['test'] = test return instance
import re
from __future__ import absolute_import import logging
import logging
import salt.utils
ret['changes'] = {'old': '', 'new': '{0}@{1}'.format(user, host)}
join = salt.utils.alias_function(joined, 'join')
from __future__ import absolute_import import logging
import salt.utils
ret['changes'] = {'old': '', 'new': name}
ret['changes'] = {'new': '', 'old': name}
from __future__ import absolute_import import logging
from __future__ import absolute_import import logging
import salt.utils
if not isinstance(servers, list): ret['result'] = False ret['comment'] = 'servers entry is not a list !' return ret
import copy import logging import sys
import salt.utils from salt.exceptions import SaltInvocationError, CommandExecutionError from salt.modules.freebsdports import _normalize, _options_file_exists
if current_options: current_options = current_options[next(iter(current_options))] if default_options: default_options = default_options[next(iter(default_options))]
if options: ret['comment'] += ' ' + _build_option_string(options) return ret
loaded_mods = list(set(loaded_mods) & set(persist_mods))
not_loaded = list(set(mods) - set(already_loaded))
loaded_mods = list(set(loaded_mods) | set(persist_mods))
from salt.ext.six import string_types
if not isinstance(version, string_types) and version is not None: version = str(version)
name = '{0}-{1}'.format(name, version)
import salt.utils from salt.exceptions import CommandExecutionError, SaltInvocationError
return ret
state['new'] = __salt__['lxc.state'](name, path=path)
restart = False
import os.path
import salt.utils
ret['comment'] = 'Database {0} is already present, so cannot be created'\ .format(name) return ret
ret['comment'] = 'Database {0} is not present, so it cannot be removed'\ .format(name) return ret
import os.path import re
from salt.ext.six import string_types
if __grains__['os'] in ['MacOS', 'Darwin'] and opts == 'defaults': opts = 'noowners'
if isinstance(opts, string_types): opts = opts.split(',')
if not name == '/': name = name.rstrip('/')
mount_invisible_keys = [ 'actimeo', 'comment', 'direct-io-mode', 'password', 'retry', 'port', ]
mount_ignore_fs_keys = { 'ramfs': ['size'] }
mount_translate_options = { 'tcp': 'proto=tcp', 'udp': 'proto=udp', }
ret['comment'] = out ret['result'] = False return ret
ret['comment'] = 'Target was successfully mounted' ret['changes']['mount'] = True
if __grains__['os'] in ['MacOS', 'Darwin'] and config == '/etc/fstab': config = "/etc/auto_salt"
ret['comment'] = 'Cluster {0}/{1} is not present, so it cannot ' \ 'be removed'.format(version, name) return ret
host = name
import logging
schema_attr = __salt__['postgres.schema_get'](dbname, name, **db_args)
if schema_attr is None: cret = __salt__['postgres.schema_create'](dbname, name, owner=owner, **db_args) else: msg = 'Schema {0} already exists in database {1}' cret = None
from __future__ import absolute_import import logging from salt.ext.six import string_types
request_params.pop('IdentityPoolName', None) r = __salt__['boto_cognitoidentity.update_identity_pool'](**request_params)
_role_present(ret, IdentityPoolId, AuthenticatedRole, UnauthenticatedRole, conn_params)
from __future__ import absolute_import
__func_alias__ = { 'set_': 'set' }
import logging import os
import salt.version import salt.utils
__virtualname__ = 'virtualenv'
if requirements or pip_pkgs: before = set(__salt__['pip.freeze'](bin_env=name, user=user, use_vt=use_vt))
import logging import json
try: from salt._compat import ElementTree as ET HAS_ELEMENT_TREE = True except ImportError: HAS_ELEMENT_TREE = False
ensure my cloudwatch service exists: pagerduty_service.present: - name: my cloudwatch service - service: escalation_policy_id: "my escalation policy" type: aws_cloudwatch description: "my cloudwatch service controlled by salt"
resource_value = resource_object['service_key'] if '@' in resource_value: resource_value = resource_value[0:resource_value.find('@')]
import os
import salt.utils from salt.modules.cron import ( _needs_change, _cron_matched )
mode = __salt__['config.manage_mode']('0600') owner, group, crontab_dir = _get_cron_info()
source = name
source, source_hash = __salt__['file.source_list'](source, source_hash, __env__)
try: sfn, source_sum, comment = __salt__['file.get_managed']( cron_path, template, source, source_hash, owner, group, mode, __env__, context, defaults,
import salt.utils
from __future__ import absolute_import import time
from salt.exceptions import CommandExecutionError import salt.utils
try: if not _available(name, ret): return ret except CommandExecutionError as exc: ret['result'] = False ret['comment'] = exc.strerror return ret
if __opts__['test']: ret['result'] = None ret['comment'] = 'Service {0} set to be enabled'.format(name) return ret
if __opts__['test']: ret['result'] = None ret['comment'] = 'Service {0} set to be disabled'.format(name) return ret
if 'enabled' in kwargs: return _enabled_used_error(ret)
try: if not _available(name, ret): return ret except CommandExecutionError as exc: ret['result'] = False ret['comment'] = exc.strerror return ret
if __opts__['test']: ret['result'] = None ret['comment'] = 'Service {0} is set to start'.format(name) return ret
if 'enabled' in kwargs: return _enabled_used_error(ret)
date > /tmp/salt-run: cmd.run
echo "Working hard..."
from __future__ import absolute_import
import salt.utils from salt.exceptions import CommandExecutionError, SaltRenderError from salt.ext.six import string_types
data['stdout'] = '' if is_json else data.get('stdout', '')[:idx] state['changes'] = data
return state
cmd_kwargs = copy.deepcopy(cmd_kwargs) cmd_kwargs['use_vt'] = False
return True
return {'name': name, 'changes': {}, 'result': True, 'comment': ''}
watch = salt.utils.alias_function(wait, 'watch')
return {'name': name, 'changes': {}, 'result': True, 'comment': ''}
try: cmd_all = __salt__['cmd.run_all']( name, timeout=timeout, python_shell=True, **cmd_kwargs ) except CommandExecutionError as err: ret['comment'] = str(err) return ret
if source is None: source = name
if len(name.split()) > 1: cmd_kwargs.update({'args': name.split(' ', 1)[1]})
return {'name': name, 'changes': {}, 'result': True, 'comment': ''}
ret['comment'] = 'Tablespace {0} is not present, so it cannot ' \ 'be removed'.format(name) return ret
from __future__ import absolute_import
services = sorted(set(services))
for current_vname in current_communities: if current_vname not in communities: ret_communities['changes'][current_vname] = {'old': current_communities[current_vname], 'new': None}
from __future__ import absolute_import import re import logging
import salt.utils from salt.version import SaltStackVersion as _SaltStackVersion from salt.exceptions import CommandExecutionError, CommandNotFoundError
import salt.ext.six as six try: import pip HAS_PIP = True except ImportError: HAS_PIP = False
import sys del pip if 'pip' in sys.modules: del sys.modules['pip']
__virtualname__ = 'pip'
ret['result'] = True ret['prefix'] = '' ret['version_spec'] = []
ret = {'result': False, 'comment': None}
else: for prefix, state_pkg_name, version_spec in pkgs_details:
if out['result'] is None: ret['result'] = False ret['comment'] = out['comment'] return ret
if result is False: target_pkgs.append((prefix, state_pkg_name.replace(',', ';')))
elif result is True: already_installed_comments.append(out['comment'])
elif result is None: ret['result'] = None ret['comment'] = out['comment'] return ret
pkgs_str = ','.join([state_name for _, state_name in target_pkgs])
pkg_404_comms = []
if prefix: pipsearch = __salt__['pip.list'](prefix, bin_env, user=user, cwd=cwd)
from __future__ import absolute_import from distutils.version import LooseVersion import logging
import salt.utils
import logging
import salt.utils
from salt.exceptions import CommandExecutionError
from __future__ import absolute_import
import salt.utils
ret['changes'] = {name: __salt__['chocolatey.install'](name, version, source, force, install_args, override_args, force_x86, package_args)}
ret['changes'] = {name: __salt__['chocolatey.uninstall'](name, version, uninstall_args, override_args)}
import logging import sys
from salt.ext.six import string_types
__virtualname__ = 'buildout' log = logging.getLogger(__name__)
from __future__ import absolute_import import logging import os import re
import salt.ext.six as six
_repack_pkgs = _namespaced_function(_repack_pkgs, globals())
try: import msgpack except ImportError: import msgpack_pure as msgpack
if oper in ('=', ''): oper = '==' return oper, verstr
return {'name': name, 'changes': {}, 'result': False, 'comment': 'Invalidly formatted pkgs parameter. See ' 'minion log.'}
targets = [] problems = [] for pkgname, pkgver in six.iteritems(to_remove): origin = bool(re.search('/', pkgname))
origin = bool(re.search('/', name))
origin = bool(re.search('/', pkgname))
myminion: 2:7.4.160-1.el7
myminion: base: |_ httpd: 2.2.15-29.el6.centos updates: |_ httpd: 2.2.15-30.el6.centos
vim-enhanced: pkg.installed: - version: 7.4.160-1.el7 - ignore_epoch: True
if not version: version = __salt__['pkg.version'](name)
ret['changes'] = exc.info.get('changes', {}) ret['comment'] = exc.strerror_without_changes
if not changes.get('purge_desired'): changes = changes['installed']
if os.path.isfile(rtag) and refresh: os.remove(rtag)
if isinstance(cur, six.string_types): cur = {desired_pkgs[0]: cur} if isinstance(avail, six.string_types): avail = {desired_pkgs[0]: avail}
if not pkgs: up_to_date = [] else: up_to_date = [x for x in pkgs if x not in targets]
targeted_pkgs = list(targets.keys()) if pkgs else None
changes = __salt__['pkg.install'](name, refresh=False, fromrepo=fromrepo, skip_verify=skip_verify, pkgs=targeted_pkgs, **kwargs)
vim-enhanced: pkg.removed: - version: 7.4.160-1.el7 - ignore_epoch: True
ret['changes'] = exc.info.get('changes', {}) ret['comment'] = exc.strerror_without_changes
vim-enhanced: pkg.purged: - version: 7.4.160-1.el7 - ignore_epoch: True
ret['changes'] = exc.info.get('changes', {}) ret['comment'] = exc.strerror_without_changes
ret['changes'] = exc.info.get('changes', {}) ret['comment'] = exc.strerror_without_changes
ret['changes'] = exc.info.get('changes', {}) ret['comment'] = exc.strerror_without_changes
continue
import fnmatch import logging import time
import salt.syspaths import salt.utils import salt.utils.event import salt.ext.six as six from salt.ext.six import string_types
__virtualname__ = 'salt'
state_ret['result'] = None
__func_alias__ = { 'set_': 'set' }
from __future__ import absolute_import
import salt.ext.six as six
from __future__ import absolute_import import logging
import salt.ext.six as six
from __future__ import absolute_import import logging import os import re
import salt.utils from salt.exceptions import CommandExecutionError
run_check_cmd_kwargs = {'runas': user, 'python_shell': True} if 'shell' in __grains__: run_check_cmd_kwargs['shell'] = __grains__['shell']
if app: if dmg: cmd = 'ls -d *.app' out = __salt__['cmd.run'](cmd, cwd=mount_point, python_shell=True)
__salt__['macpackage.unmount'](mount_point)
return True
import salt.utils
import logging import os import shutil
import salt.utils from salt.states.git import _fail, _neutral_test
from __future__ import absolute_import import logging
import salt.utils
self.skipUI = skipUI self.skipDownloaded = skipDownloaded self.skipInstalled = skipInstalled self.skipReboot = skipReboot self.skipPresent = skipPresent self.skipHidden = skipHidden
self.download_collection = win32com.client.Dispatch('Microsoft.Update.UpdateColl')
self.install_collection = win32com.client.Dispatch('Microsoft.Update.UpdateColl')
self.win_downloader = self.update_session.CreateUpdateDownloader() self.win_downloader.Updates = self.download_collection
self.win_installer = self.update_session.CreateUpdateInstaller() self.win_installer.Updates = self.install_collection
self.download_results = None
self.install_results = None
log.debug('generated search string: {0}'.format(search_string)) return self.Search(search_string)
comment, passed, retries = _search(win_updater, retries) ret['comment'] += comment if not passed: ret['result'] = False return ret
comment, passed, retries = _download(win_updater, retries) ret['comment'] += comment if not passed: ret['result'] = False return ret
comment, passed, retries = _install(win_updater, retries) ret['comment'] += comment if not passed: ret['result'] = False return ret
comment, passed, retries = _search(win_updater, retries) ret['comment'] += comment if not passed: ret['result'] = False return ret
comment, passed, retries = _download(win_updater, retries) ret['comment'] += comment if not passed: ret['result'] = False return ret
from __future__ import absolute_import import logging
from salt.state import STATE_INTERNAL_KEYWORDS as _STATE_INTERNAL_KEYWORDS
from salt.exceptions import CommandExecutionError, SaltInvocationError from salt.modules.dockerng import ( CLIENT_TIMEOUT, STOP_TIMEOUT, VALID_CREATE_OPTS, _validate_input, _get_repo_tag ) import salt.utils import salt.ext.six as six
__virtualname__ = 'dockerng'
desired_volumes = sorted(list(data) + [ k for k in _image_get(config['image_path']) or [] if k not in data])
if bool(actual_data) != bool(data): ret.update({item: {'old': actual_data, 'new': data}})
if bool(actual_data) != bool(data): ret.update({item: {'old': actual_data, 'new': data}})
if actual_data != data: ret.update({item: {'old': actual_data, 'new': data}})
image = ':'.join(_get_repo_tag(name)) all_tags = __salt__['dockerng.list_tags']()
pass
ret['changes'] = image_update
ret['comment'] = 'Image \'{0}\' could not be {1}'.format(name, action)
targets.append(':'.join(_get_repo_tag(str(target))))
pull_result = __salt__['dockerng.pull']( image, client_timeout=client_timeout, )
create_kwargs = salt.utils.clean_kwargs(**copy.deepcopy(kwargs)) send_signal = create_kwargs.pop('send_signal', False)
new_container = True
ret['changes']['removed'] = removed_ids
pull_result = __salt__['dockerng.pull']( image, client_timeout=client_timeout, )
create_result = __salt__['dockerng.create']( image, name=name, validate_ip_addrs=False, validate_input=False, client_timeout=client_timeout, **create_kwargs )
ret['changes']['added'] = create_result
__salt__['dockerng.start']( name, )
diff[key] = changes_needed[key]
ret['changes']['diff'] = changes_needed comments.append('Container \'{0}\' was replaced'.format(name))
comments.append( 'Container \'{0}\' is already configured as specified' .format(name) )
containers = [__salt__['dockerng.inspect_container'](c)['Id'] for c in containers] networks = __salt__['dockerng.networks'](names=[name]) if networks:
kwargs['force'] = True return image_present(name, **kwargs)
ret = __salt__['k8s.label_present'](name, value, node, apiserver)
ret = __salt__['k8s.label_absent'](name, node, apiserver)
ret = __salt__['k8s.folder_absent'](name, node, apiserver)
import re import copy
base_dashboards_from_pillar = ([_DEFAULT_DASHBOARD_PILLAR] + base_dashboards_from_pillar) base_panels_from_pillar = ([_DEFAULT_PANEL_PILLAR] + base_panels_from_pillar) base_rows_from_pillar = [_DEFAULT_ROW_PILLAR] + base_rows_from_pillar
__virtualname__ = 'debconf'
if 'debconf.show' not in __salt__: return False
ret['comment'] = ('Database {0} is not present, so it cannot be removed' ).format(name) return ret
from salt.exceptions import SaltException
if __opts__['test'] is True:
from __future__ import absolute_import
import salt.ext.six as six
ret['result'] = None
if existing['code'] == 200:
elif existing['code'] == 404: response = __salt__['bigip.create_node'](hostname, username, password, name, address)
else: ret = _load_result(existing, ret)
if existing['code'] == 200:
elif existing['code'] == 404:
if new['code'] == 200:
else:
else: ret = _load_result(new, ret)
if existing['code'] == 200:
elif existing['code'] == 404: ret['comment'] = 'A node with this name was not found.' else: ret = _load_result(existing, ret)
if existing['code'] == 200:
else: ret = _load_result(existing, ret)
if existing['code'] == 200:
elif existing['code'] == 404:
else: ret = _load_result(existing, ret)
if existing['code'] == 200:
elif existing['code'] == 404:
else: ret = _load_result(new, ret)
else: ret = _load_result(existing, ret)
if existing['code'] == 200:
elif existing['code'] == 404: ret['comment'] = 'A pool with this name was not found.' else: ret = _load_result(existing, ret)
if existing['code'] == 200:
else: ret = _load_result(deleted, ret)
if existing['code'] == 200:
else: ret = _load_result(new_member, ret)
if existing['code'] == 200:
if existing['code'] == 200:
else: ret = _load_result(existing, ret)
if existing['code'] == 200:
if existing['code'] == 200:
else: ret = _load_result(existing, ret)
if existing['code'] == 200:
if existing['code'] == 200:
elif existing['code'] == 404:
else: ret = _load_result(existing, ret)
if existing['code'] == 200:
elif existing['code'] == 404:
else: ret = _load_result(existing, ret)
if existing['code'] == 200:
elif existing['code'] == 404: ret['comment'] = 'A Monitor with this name was not found.' else: ret = _load_result(existing, ret)
if existing['code'] == 200:
if existing['code'] == 200:
elif existing['code'] == 404:
else: ret = _load_result(existing, ret)
if existing['code'] == 200:
elif existing['code'] == 404:
else: ret = _load_result(existing, ret)
if existing['code'] == 200:
elif existing['code'] == 404: ret['comment'] = 'A Profile with this name was not found.' else: ret = _load_result(existing, ret)
if existing['code'] == 200:
echo "manual" > /etc/init/salt-master.override
Ensure mylc exists: boto_lc.present: - name: mylc - image_id: ami-0b9c9f62 - profile: myprofile
from __future__ import absolute_import import logging import os import os.path from copy import deepcopy import json
'ID': val
}
if not bool(Versioning) and bool(_describe.get('Versioning')): Versioning = {'Status': 'Suspended'}
replication_item = ('Replication', 'put_replication', _describe.get('Replication', {}).get('ReplicationConfiguration'), _compare_replication, Replication, 'delete_replication')
if Replication is not None: config_items.append(versioning_item) config_items.append(replication_item) else: config_items.append(replication_item) config_items.append(versioning_item)
from __future__ import absolute_import import logging import os import os.path import json
import salt.utils from salt.ext.six import string_types
_describe = __salt__['boto_iot.describe_policy'](policyName=policyName, region=region, key=key, keyid=keyid, profile=profile)['policy']
_describe = __salt__['boto_iot.describe_topic_rule'](ruleName=ruleName, region=region, key=key, keyid=keyid, profile=profile)['rule']
ret['result'] = True return ret
import re
ret['comment'] = 'User {0} is already present'.format(name) return ret
ret['comment'] = 'User {0} is not present, so it cannot be removed'\ .format(name) return ret
from __future__ import absolute_import import json import logging
import salt.ext.six as six import json
agent_version = 1
if salt_params: for key, value in six.iteritems(params): params_from_salt[key] = value params_to_use = params_from_salt else: params_to_use = params
from __future__ import absolute_import import time import logging import re import traceback
from salt.utils import dictdiffer from salt.exceptions import CommandExecutionError
import salt.ext.six as six
else: ret['comment'] = 'Datasource updated.'
from __future__ import absolute_import
__virtualname__ = 'win_iis'
import logging
import salt.utils import salt.utils.validate.net from salt.ext.six.moves import range from salt.exceptions import CommandExecutionError
log = logging.getLogger(__name__)
__virtualname__ = 'network'
if gateway is not None: if not salt.utils.validate.net.ipv4_addr(gateway): errors.append('Gateway IP {0} is invalid.'.format(gateway))
if set(dns_servers or ['None']) != set(cur_dns_servers): changes['dns_servers'] = dns_servers
from __future__ import absolute_import import logging from time import time, sleep
import salt.utils.dictupdate as dictupdate from salt.utils import exactly_one from salt.exceptions import SaltInvocationError, CommandExecutionError
ret['result'] = False ret['comment'] = "Can't determine AllocationId for address {0}.".format(ip) return ret
if error: ret['changes'] = {} ret['result'] = False ret['comment'] = str(error)
import logging
from salt.ext.six import string_types import salt.utils
from __future__ import absolute_import import logging import os import os.path import hashlib import json
import salt.utils.dictupdate as dictupdate import salt.utils from salt.exceptions import SaltInvocationError from salt.ext.six import string_types
update = True
Ensure mysecgroup exists: boto_secgroup.present: - name: mysecgroup - description: My security group - profile: myprofile
Ensure mysecgroup exists: boto_secgroup.present: - name: mysecgroup - description: My security group - profile: keyid: GKTADJGHEIQSXMKKRBJ08H key: askdjghsdfjkghWupUjasdflkdfklgjsdfjajkghs region: us-east-1
import logging
import salt.utils.dictupdate as dictupdate from salt.exceptions import SaltInvocationError from salt.ext.six import string_types
if _rule.get('from_port') is None: _rule['from_port'] = -1 if _rule.get('to_port') is None: _rule['to_port'] = -1
from __future__ import absolute_import import logging import os import os.path import hashlib import re import json import yaml
import salt.utils from salt.ext.six import string_types
swagger = _Swagger(api_name, stage_name, lambda_funcname_format, swagger_file, common_args)
stage_vars = _get_stage_variables(stage_variables)
VENDOR_EXT_PATTERN = re.compile('^x-')
JSON_SCHEMA_DRAFT_4 = 'http://json-schema.org/draft-04/schema#'
AWS_API_DESCRIPTION = _dict_to_json_pretty({"provisioned_by": "Salt boto_apigateway.present State", "context": "See deployment or stage description"})
if 'schema' not in resobj: raise ValueError('missing schema field in path {0}, ' 'op {1}, response {2}'.format(path, opname, rescode))
for field in _Swagger.SWAGGER_OBJ_V2_FIELDS_REQUIRED: if field not in self._cfg: raise ValueError('Missing Swagger Object Field: {0}'.format(field))
ret['comment'] = 'stage {0} does not exist'.format(self._stage_name)
properties = obj_schema.get('properties') if properties: for _, prop_obj_schema in properties.iteritems(): dep_models_list.extend(self._build_dependent_model_list(prop_obj_schema))
models_dict.pop(next_model) for model, dep_list in models_dict.iteritems(): if next_model in dep_list: dep_list.remove(next_model)
_schema = self._update_schema_to_aws_notation(schema) _schema.update({'$schema': _Swagger.JSON_SCHEMA_DRAFT_4, 'title': '{0} Schema'.format(model)})
model_exists_response = __salt__['boto_apigateway.api_model_exists'](restApiId=self.restApiId, modelName=model, **self._common_aws_args)
lambda_desc = __salt__['boto_lambda.describe_function'](lambda_name, **self._common_aws_args)
from salt.ext.six import string_types
ret = {'name': name, 'result': False, 'comment': '', 'changes': {},
from __future__ import absolute_import import logging import time
try: from keystoneclient.apiclient.exceptions import \ Unauthorized as kstone_Unauthorized from glanceclient.exc import \ HTTPUnauthorized as glance_Unauthorized HAS_DEPENDENCIES = True except ImportError: HAS_DEPENDENCIES = False
while len(acceptable) > 1: if acceptable[0] == wait_for: break else: acceptable.pop(0)
if name in ret['changes']: ret['changes'][name]['new']['status'] = image['status']
image = __salt__['glance.image_show'](image['id'])
from __future__ import absolute_import, print_function import errno import logging import os
import salt.utils from salt.ext import six
if env is not None and not isinstance(env, dict): ret['comment'] = ('Invalidly-formatted \'env\' parameter. See ' 'documentation.') return ret
result = __salt__['splunk.update_user']( email, profile, **kwargs )
ret['result'] = None ret['comment'] = "No changes"
import salt.utils from salt.state import STATE_INTERNAL_KEYWORDS as _STATE_INTERNAL_KEYWORDS
continue
if chunk.get('fun') != low.get('fun'): continue
from __future__ import absolute_import import logging
import salt.utils from salt.exceptions import CommandExecutionError
log = logging.getLogger(__name__)
if not enabled: ret['result'] = True ret['comment'] = enabled_msg ret['changes'].update(enabled_changes) return ret
ret['result'] = True ret['comment'] = enabled_msg return ret
if clean_current_key[0] != clean_ssh_key[0] or clean_current_key[1] != clean_ssh_key[1]: ssh_key_changed = True
ssh_key_changed = True
try: lookup_key = _lookup_syslog_config(key) except KeyError: ret['comment'] = '\'{0}\' is not a valid config variable.'.format(key) return ret
ret['comment'] = 'Database {0} is not present, so it cannot ' \ 'be removed'.format(name) return ret
import logging import os
from salt import exceptions from salt.states.git import _fail, _neutral_test
import salt.loader import salt.utils import salt.utils.jid from salt.ext.six.moves import range
watch = salt.utils.alias_function(wait, 'watch')
{% for k, v in details['servers'].iteritems() %} {{ k }}: dellchassis.blade_idrac: - idrac_password: {{ v['idrac_password'] }} {% endfor %}
from __future__ import absolute_import import logging import os
from salt.exceptions import SaltInvocationError, CommandExecutionError
do_utc = False do_zone = False
if compzone is True: ret['result'] = True messages.append('Timezone {0} already set'.format(name)) else: do_zone = True
import difflib import salt.utils import salt.utils.network import salt.loader
import logging log = logging.getLogger(__name__)
apply_ranged_setting = False
if type == 'source': return ret
if apply_routes: try: __salt__['ip.apply_network_settings'](**kwargs) except AttributeError as error: ret['result'] = False ret['comment'] = str(error) return ret
if apply_net_settings: try: __salt__['ip.apply_network_settings'](**kwargs) except AttributeError as error: ret['result'] = False ret['comment'] = str(error) return ret
import re import os
from __future__ import absolute_import import logging
from __future__ import absolute_import from salt.exceptions import CommandExecutionError, CommandNotFoundError
import salt.ext.six as six
if pkg_ver: if installed_pkgs[pkg_name].get('version') != pkg_ver: pkgs_to_install.append(pkg) else: pkgs_satisfied.append(installed_name_ver)
cache_root_path = all_cached_pkgs[0] specific_pkg = '{0}/{1}/'.format(cache_root_path, name)
from __future__ import absolute_import import logging
import salt.utils.dictupdate as dictupdate
if subnet_names: for i in subnet_names: r = __salt__['boto_vpc.get_resource_id']('subnet', name=i, region=region, key=key, keyid=keyid, profile=profile)
if x['subnet_id'] not in subnet_ids and x['subnet_id'] is not None: to_delete.append(x['id'])
if user1['member_order'] == user2['member_order'] - 1: found = True break
import logging import salt.ext.six as six
is_stopped = False for proc in all_processes: if proc.startswith(name) \ and _is_stopped_state(all_processes[proc]['state']): is_stopped = True break
is_stopped = False for proc in all_processes: if proc.startswith(name) \ and _is_stopped_state(all_processes[proc]['state']): is_stopped = True break
ret['comment'] = "Service {0} doesn't exist".format(name)
return running( name, restart=restart, update=update, user=user, conf_file=conf_file, bin_env=bin_env )
from __future__ import absolute_import import logging import json import os
import salt.utils import salt.utils.odict as odict import salt.utils.dictupdate as dictupdate import salt.ext.six as six from salt.ext.six import string_types
try: from salt._compat import ElementTree as ET HAS_ELEMENT_TREE = True except ImportError: HAS_ELEMENT_TREE = False
import os import os.path import time import logging
import salt.utils from salt.ext.six.moves import range
log = logging.getLogger(__name__)
for i in range(10):
from __future__ import absolute_import import logging
from salt.exceptions import CommandExecutionError
from __future__ import absolute_import import os
import salt.utils as utils import salt.ext.six as six
if false_unsets is not True: ret['changes'].update({key: ''})
import logging
import salt.utils
__virtualname__ = 'system'
name = str(name)
name = str(name)
import logging import random from salt.state import _gen_tag from salt.exceptions import SaltInvocationError
ret['changes'] = { 'testing': { 'old': 'Unchanged', 'new': 'Something pretended to change' } }
ret['changes'] = { 'testing': { 'old': 'Unchanged', 'new': 'Something pretended to change' } }
ret['changes'] = { 'testing': { 'old': 'Unchanged', 'new': 'Something pretended to change' } }
ret['changes'] = { 'testing': { 'old': 'Unchanged', 'new': 'Something pretended to change' } }
ret['result'] = random.choice([True, False])
present = _if_str_then_list(present) checks[None] = present boolean = _if_str_then_list(boolean) checks[bool] = boolean
integer = _if_str_then_list(integer) checks[int] = integer string = _if_str_then_list(string) checks[str] = string listing = _if_str_then_list(listing) checks[list] = listing dictionary = _if_str_then_list(dictionary) checks[dict] = dictionary
tenant = __salt__['keystone.tenant_get'](name=name, profile=profile, **connection_args)
role = __salt__['keystone.role_get'](name=name, profile=profile, **connection_args)
role = __salt__['keystone.service_get'](name=name, profile=profile, **connection_args)
from __future__ import absolute_import import logging
from salt.exceptions import CommandExecutionError import salt.utils
def __hash__(self): return (hash(self.srcport) ^ hash(self.destport) ^ hash(self.protocol) ^ hash(self.destaddr))
import salt.utils
apache: pkg: - installed - name: httpd service: - running - enable: True - name: httpd
return {'name': name, 'changes': {}, 'result': True, 'comment': ''}
- cn=foo,ou=users,dc=example,dc=com: - delete_others: True
- cn=admin,dc=example,dc=com: - delete_others: True - replace: cn: - admin description: - LDAP administrator objectClass: - simpleSecurityObject - organizationalRole userPassword: - {{pillar.ldap_admin_password}}
- 'olcDatabase={1}hdb,cn=config': - replace: olcRootDN: - cn=admin,dc=example,dc=com olcRootPW: []
pass
ldap3 = inspect.getmodule(connect)
dn_set = OrderedDict() dn_set.update(old) dn_set.update(new)
changed_old[dn] = o changed_new[dn] = n success_dn_set[dn] = True
try: return set((str(x) for x in thing)) except TypeError: return set((str(thing),))
from __future__ import absolute_import import difflib import itertools import logging import os import shutil import sys import traceback from collections import Iterable, Mapping, defaultdict
import salt.loader import salt.payload import salt.utils import salt.utils.templates import salt.utils.url from salt.utils.locales import sdecode from salt.exceptions import CommandExecutionError
import salt.ext.six as six from salt.ext.six.moves import zip_longest
return ret
pass
walk_l = list(_depth_limited_walk(name, max_depth)) walk_d = {} for i in walk_l: walk_d[i[0]] = (i[1], i[2])
fchange = _check_dir_meta(name, user, group, mode) if fchange: changes[name] = fchange if clean: keep = _gen_keep_files(name, require, walk_d)
return True, '', list(zip_longest(sources, source_hashes[:len(sources)]))
mode = __salt__['config.manage_mode'](mode)
if not __salt__['user.info'](user): user = __salt__['user.current']() if not user: user = 'SYSTEM'
mode = __salt__['config.manage_mode'](mode)
ret['comment'] = ('File {0} is not present and is not set for ' 'creation').format(name) return ret
return _error(ret, u_check)
source, source_hash = __salt__['file.source_list']( source, source_hash, __env__ )
if ret['changes']: ret = {'changes': {}, 'comment': '', 'name': name, 'result': True}
sfn = tmp_filename
if name[-1] == '/' and name != '/': name = name[:-1]
dir_mode = __salt__['config.manage_mode'](dir_mode) file_mode = __salt__['config.manage_mode'](file_mode)
return _error(ret, u_check)
if not children_only: ret, perms = __salt__['file.check_perms'](name, ret, user, group, dir_mode, follow_symlinks)
walk_l = list(_depth_limited_walk(name, max_depth)) walk_d = {} for i in walk_l: walk_d[i[0]] = (i[1], i[2])
dir_mode = __salt__['config.manage_mode'](dir_mode) file_mode = __salt__['config.manage_mode'](file_mode)
return _error(ret, u_check)
source_list = _validate_str_list(source)
if _ret['result'] is False or ret['result'] is True: ret['result'] = _ret['result']
if _ret['result'] is not True and _ret['comment']: add_comment(path, _ret['comment'])
pass_kwargs = {} faults = ['mode', 'makedirs'] for key in kwargs: if key not in faults: pass_kwargs[key] = kwargs[key]
srcpath = srcpath + '/'
if keep_symlinks: symlinks = __salt__['cp.list_master_symlinks'](__env__, srcpath) fns_ = process_symlinks(fns_, symlinks) for fn_ in fns_: if not fn_.strip(): continue
if maxdepth is not None: relpieces = relname.split('/') if not relpieces[-1]: relpieces = relpieces[:-1] if len(relpieces) > maxdepth + 1: continue
manage_directory(dirname) vdir.add(dirname)
all_files = __salt__['file.readdir'](name)
beginning_of_unix_time = datetime(1970, 1, 1)
return (None, None)
RETAIN_TO_DEPTH = { 'first_of_year': 1, 'first_of_month': 2, 'first_of_day': 3, 'first_of_hour': 4, 'most_recent': 5, }
retained_files |= get_first_n_at_depth(files_by_y_week_dow, first_of_week_depth, keep_count + 1)
- pattern: | CentOS \(2.6.32[^\n]+\n\s+root[^\n]+\n\)+
__salt__['file.comment_line'](name, regex, char, True, backup)
ret['result'] = __salt__['file.search'](name, unanchor_regex, multiline=True)
ret['changes']['diff'] = ( ''.join(difflib.unified_diff(slines, nlines)) )
__salt__['file.comment_line'](name, regex, char, False, backup)
ret['result'] = __salt__['file.search']( name, '^[ \t]*{0}'.format(regex.lstrip('^')), multiline=True )
ret['changes']['diff'] = ( ''.join(difflib.unified_diff(slines, nlines)) )
(ok_, err, sl_) = _unify_sources_and_hashes(source=source, source_hash=source_hash, sources=sources, source_hashes=source_hashes) if not ok_: return _error(ret, err)
touch(name, makedirs=makedirs) retry_res, retry_msg = _check_file(name) if not retry_res: return _error(ret, check_msg)
if sl_: tmpret = _get_template_texts(source_list=sl_, template=template, defaults=defaults, context=context) if not tmpret['result']: return tmpret text = tmpret['data']
ret['changes']['diff'] = ( ''.join(difflib.unified_diff(slines, nlines)) )
ret['changes']['diff'] = ( ''.join(difflib.unified_diff(slines, nlines)) )
(ok_, err, sl_) = _unify_sources_and_hashes(source=source, source_hash=source_hash, sources=sources, source_hashes=source_hashes) if not ok_: return _error(ret, err)
touch(name, makedirs=makedirs) retry_res, retry_msg = _check_file(name) if not retry_res: return _error(ret, check_msg)
if sl_: tmpret = _get_template_texts(source_list=sl_, template=template, defaults=defaults, context=context) if not tmpret['result']: return tmpret text = tmpret['data']
if not header: if __salt__['file.search']( name, salt.utils.build_whitespace_split_regex(chunk), multiline=True): continue
ret['changes']['diff'] = ( ''.join(difflib.unified_diff(slines, nlines)) )
ret['changes']['diff'] = ( ''.join(difflib.unified_diff(slines, nlines)) )
return _error(ret, u_check)
name = os.path.join(name, os.path.basename(source))
ret['comment'] = ('File {0} is not present and is not set for ' 'creation').format(name) return ret
return True
ret = {'name': name, 'result': False, 'comment': '', 'changes': {},
ret = {'name': name, 'result': False, 'comment': '', 'changes': {},
ret['result'] = False ret['comment'] = 'Failed to create data pipeline {0}: {1}'.format( name, result_pipeline_definition['error']) return ret
if 'enabled' not in new_item: new_item['enabled'] = True
import re
from salt.exceptions import CommandExecutionError
if 'sysctl.default_config' in __salt__: config = __salt__['sysctl.default_config']() else: config = '/etc/sysctl.conf'
ret['result'] = None ret['comment'] = ( 'Sysctl option {0} would be changed to {1}'.format(name, value) ) return ret
from __future__ import absolute_import import logging
import salt.utils import salt.ext.six as six from salt.exceptions import CommandExecutionError
if existing_vhost == '' and perms == ['', '', '']: continue perm_need_change = True
from __future__ import absolute_import import logging import os import os.path import json
from salt.ext.six import string_types
import logging
import salt.utils
__virtualname__ = 'apt'
import logging import os
import salt.utils
certs = __salt__['keychain.list_certs'](keychain)
import logging
from salt.modules import postgres
mode = 'create' group_attr = __salt__['postgres.role_get']( name, return_password=not refresh_password, **db_args) if group_attr is not None: mode = 'update'
Ensure myservice dashboard is managed: grafana.dashboard_present: - name: myservice - dashboard_from_pillar: default - rows_from_pillar: - systemhealth - requests
if isinstance(pillar_rows, list): for row in pillar_rows: rows.append(row) else: rows.append(pillar_rows)
func = 'modjk.{0}'.format(cmd) args = [worker, lbn, profile] response = __salt__['publish.publish'](target, func, args, expr_form)
errors = [] minions = [] for minion in response: minions.append(minion) if not response[minion]: errors.append(minion)
if not status: ret['result'] = False return ret
for balancer in status: if not status[balancer]: ret['errors'].append(balancer) elif status[balancer]['activation'] != activation: ret['wrong_state'].append(balancer)
from __future__ import absolute_import import re import os import logging import tarfile from contextlib import closing
import salt.ext.six as six
from salt.exceptions import CommandExecutionError import salt.utils
try: file_result = file_result[next(six.iterkeys(file_result))] except AttributeError: pass
myrole: boto_iam_role.present: - profile: myiamprofile
myrole: boto_iam_role.present: - profile: key: GKTADJGHEIQSXMKKRBJ08H keyid: askdjghsdfjkghWupUjasdflkdfklgjsdfjajkghs region: us-east-1
ret['comment'] = ('User {0} is not present, so it cannot be removed' ).format(name) return ret
import os import stat import itertools
import salt.runner import salt.utils import salt.config import salt.syspaths
winrepo_cachefile = os.path.join(winrepo_dir, winrepo_cachefile)
from __future__ import absolute_import import logging
result = __salt__['github.add_user']( name, profile=profile, **kwargs )
Ensure mykey key exists: boto_kms.key_present: - name: mykey - region: us-east-1 - profile: myprofile
Ensure myelb ELB exists: boto_elb.present: - name: myelb - region: us-east-1 - profile: myelbprofile
Ensure myelb ELB exists: boto_elb.present: - name: myelb - region: us-east-1 - profile: keyid: GKTADJGHEIQSXMKKRBJ08H key: askdjghsdfjkghWupUjasdflkdfklgjsdfjajkghs
- attributes: cross_zone_load_balancing: enabled: false - profile: myelbprofile
- alarms: UnHealthyHostCount: attributes: threshold: 2.0
from __future__ import absolute_import
import hashlib import re import salt.utils.dictupdate as dictupdate from salt.exceptions import SaltInvocationError import salt.ext.six as six
tmp = __salt__['config.option'](attributes_from_pillar, {}) if attributes: attributes = dictupdate.update(tmp, attributes) else: attributes = tmp
for p in listener_policies: if re.match(r'^ELBSecurityPolicy-\d{4}-\d{2}$', p): default_aws_policies.add(p)
from __future__ import absolute_import import logging
import salt.utils
import logging import re
value = str(value).lower()
import re import os.path import logging import difflib
import salt.utils
__virtualname__ = 'mongodb_user'
- user: admin - password: sekrit
user_exists = __salt__['mongodb.user_exists'](name, user, password, host, port, database, authdb) if user_exists is True: return ret
if not isinstance(user_exists, bool): ret['comment'] = user_exists ret['result'] = False return ret
if not isinstance(user_exists, bool): ret['comment'] = user_exists ret['result'] = False return ret
ret['comment'] = ('User {0} is not present, so it cannot be removed' ).format(name) return ret
Ensure myasg is deleted: boto_asg.absent: - name: myasg - force: True
from __future__ import absolute_import import hashlib import logging import copy
import salt.utils.dictupdate as dictupdate
import salt.ext.six as six
if scheduled_actions: tmp = dictupdate.update(tmp, scheduled_actions) return tmp
import logging
import logging import os
import salt.utils import salt.utils.files import salt.utils.atomicfile from salt.utils.odict import OrderedDict
__virtualname__ = 'smartos'
current_keys = set(current.keys()) state_keys = set(state.keys())
config = _load_config()
if isinstance(value, (bool)): value = 'true' if value else 'false' if not value: value = ""
ret['result'] = True ret['comment'] = 'property {0} already has value "{1}"'.format(name, value)
if not __opts__['test'] and len(ret['changes']) > 0: ret['result'] = _write_config(config)
config = _load_config()
ret['result'] = True ret['comment'] = 'property {0} deleted'.format(name) ret['changes'][name] = None del config[name]
ret['result'] = True ret['comment'] = 'property {0} is absent'.format(name)
if not __opts__['test'] and len(ret['changes']) > 0: ret['result'] = _write_config(config)
ret['result'] = True ret['comment'] = 'image {0} is present'.format(name)
ret['result'] = True ret['comment'] = 'image {0} is absent'.format(name)
images = []
for state in __salt__['state.show_lowstate'](): if 'state' not in state: continue
for image_uuid in __salt__['vmadm.list'](order='image_uuid'): if image_uuid in images: continue images.append(image_uuid)
vmconfig = _parse_vmconfig(vmconfig, vmconfig_type['instance']) log.debug('smartos.vm_present::{0}::vmconfig - {1}'.format(name, vmconfig))
if 'hostname' not in vmconfig: vmconfig['hostname'] = name
if vmconfig['hostname'] in __salt__['vmadm.list'](order='hostname'): ret['result'] = True
for prop in vmconfig['state']: if prop in vmconfig_type['instance'] or \ prop in vmconfig_type['collection'] or \ prop in vmconfig_type['create_only']: continue
vmconfig['changed'][prop] = vmconfig['state'][prop]
for collection in vmconfig_type['collection']: if collection in vmconfig_type['create_only']: continue
if 'set_{0}'.format(collection) not in vmconfig['changed']: vmconfig['changed']['set_{0}'.format(collection)] = {}
vmconfig['changed']['set_{0}'.format(collection)][prop] = vmconfig['state'][collection][prop]
if 'remove_{0}'.format(collection) not in vmconfig['changed']: vmconfig['changed']['remove_{0}'.format(collection)] = []
vmconfig['changed']['remove_{0}'.format(collection)].append(prop)
for instance in vmconfig_type['instance']: if instance in vmconfig_type['create_only']: continue
if instance in vmconfig['state'] and vmconfig['state'][instance] is not None: for state_cfg in vmconfig['state'][instance]: add_instance = True
for current_cfg in vmconfig['current'][instance]: if vmconfig_type['instance'][instance] not in state_cfg: continue
add_instance = False
if len(changed) > 0: for prop in changed: update_cfg[prop] = state_cfg[prop]
for prop in state_cfg: if isinstance(state_cfg[prop], (list)) and len(state_cfg[prop]) == 0: continue
if 'add_{0}'.format(instance) not in vmconfig['changed']: vmconfig['changed']['add_{0}'.format(instance)] = []
vmconfig['changed']['add_{0}'.format(instance)].append(state_cfg)
if instance in vmconfig['current'] and vmconfig['current'][instance] is not None: for current_cfg in vmconfig['current'][instance]: remove_instance = True
if instance in vmconfig['state'] and vmconfig['state'][instance] is not None: for state_cfg in vmconfig['state'][instance]: if vmconfig_type['instance'][instance] not in state_cfg: continue
remove_instance = False
if 'remove_{0}'.format(instance) not in vmconfig['changed']: vmconfig['changed']['remove_{0}'.format(instance)] = []
vmconfig['changed']['remove_{0}'.format(instance)].append( current_cfg[vmconfig_type['instance'][instance]] )
ret['result'] = True ret['comment'] = 'vm {0} is absent'.format(name)
if not __opts__['test']: if archive: __salt__['vmadm.update'](vm=name, key='hostname', archive_on_delete=True)
ret['result'] = True ret['comment'] = 'vm {0} already running'.format(name)
ret['result'] = True ret['comment'] = 'vm {0} already stopped'.format(name)
from __future__ import absolute_import import datetime import os import re import copy
import salt.exceptions import salt.utils
import salt.ext.six as six
import logging
from salt.modules import postgres import salt.ext.six as six
if encrypted is not False: encrypted = postgres._DEFAULT_PASSWORDS_ENCRYPTION password = postgres._maybe_encrypt_password(name, password, encrypted=encrypted)
mode = 'create' user_attr = __salt__['postgres.role_get']( name, return_password=not refresh_password, **db_args) if user_attr is not None: mode = 'update'
from __future__ import generators from __future__ import absolute_import import logging import socket
import salt.utils import salt.utils.cloud as suc from salt.exceptions import SaltCloudException
ret['comment'] = ret['comment'] + ' and will be started' ret['result'] = None return ret
from __future__ import absolute_import import sys
import salt.ext.six as six
if set(lgrp['members']) ^ set(members): change['members'] = members
if __opts__['test']: ret['result'] = None ret['comment'] = 'Group {0} set to be added'.format(name) return ret
if gid is not None: gid_group = None for lgrp in grps: if lgrp['gid'] == gid: gid_group = lgrp['name'] break
import logging
if __opts__['test']: ret['result'] = None ret['changes'] = {'reg': {'Will add': add_change}} return ret
ret['result'] = __salt__['reg.set_value'](hive=hive, key=key, vname=vname, vdata=vdata, vtype=vtype, use_32bit_registry=use_32bit_registry)
if __opts__['test']: ret['result'] = None ret['changes'] = {'reg': {'Will remove': remove_change}} return ret
if __opts__['test']: ret['result'] = None return ret
from __future__ import absolute_import import re import sys
import salt.ext.six as six
if source != '': source_path = __salt__['cp.get_url']( source, None, saltenv=__env__)
from __future__ import absolute_import import os import logging
import salt.utils from salt.utils.locales import sdecode, sdecode_if_string
from salt.ext.six import string_types, iteritems
ret['changes']['home'] = ''
upper_name = name.upper()
return ret
upper_name = name.upper()
return 'telemetry_alert' if 'telemetry.get_alert_config' in __salt__ else False
import logging from time import strftime, strptime, gmtime
__virtualname__ = 'zfs'
if ret['result']:
if '@' not in snapshot: ret['result'] = False ret['comment'] = 'invalid snapshot name: {0}'.format(snapshot)
if '@' not in snapshot: ret['result'] = False ret['comment'] = 'invalid snapshot name: {0}'.format(snapshot)
if not properties: properties = {}
if not properties: properties = {}
if not properties: properties = {}
prunable = [] snapshots = {} for key in schedule.keys(): snapshots[key] = []
needed_holds = [] current_timestamp = gmtime() for hold in snapshots.keys(): if schedule[hold] == 0: continue
needed_holds.append(hold)
from __future__ import absolute_import import re
ret = {'name': name, 'result': None, 'comment': '', 'changes': {},
from __future__ import absolute_import
import salt.utils
import salt.ext.six as six
if acl_name == '': _search_name = __current_perms[name].get('comment').get(_acl_type) else: _search_name = acl_name
if acl_name == '': _search_name = __current_perms[name].get('comment').get(_acl_type) else: _search_name = acl_name
from __future__ import absolute_import
from __future__ import absolute_import import datetime import math import sys import logging import copy
import salt.ext.six as six import salt.utils.dictupdate as dictupdate
from __future__ import absolute_import import time import datetime
if delta_remaining < delta_min: ret['comment'] = 'Certificate will expire in {0}, which is less than {1}'.format(delta_remaining, delta_min) return ret
from __future__ import absolute_import
from salt.exceptions import CommandExecutionError
import os
import salt.utils from salt.exceptions import CommandNotFoundError
ret['changes'] = {'feature': __salt__['win_servermanager.install'](name, recurse, restart)}
import logging log = logging.getLogger(__name__)
for key, value in kwargs.items(): if key in old: if value == 'max': value = old['{0}_max'.format(key)]
if error: ret['changes'] = {} ret['result'] = False ret['comment'] = str(error)
ret = { 'name': name, 'changes': {}, 'result': False, 'comment': '', }
existing_config = None if __salt__['chronos.has_job'](name): existing_config = __salt__['chronos.job'](name)['job']
from __future__ import absolute_import import sys import os.path
import salt.utils
import salt.ext.six as six
from __future__ import absolute_import import logging
import salt.utils
import salt.ext.six as six
log = logging.getLogger(__name__)
__virtualname__ = 'raid'
raids = __salt__['raid.list']() if raids.get(name): ret['comment'] = 'Raid {0} already present'.format(name) return ret
can_assemble = {} for dev in devices: cmd = 'mdadm -E {0}'.format(dev) can_assemble[dev] = __salt__['cmd.retcode'](cmd) == 0
if do_assemble: __salt__['raid.assemble'](name, devices, **kwargs) else: __salt__['raid.create'](name, level, devices, **kwargs)
__salt__['raid.save_config']()
import logging log = logging.getLogger(__name__)
from salt.ext import six
if not configured_probes: return { 'add': expected_probes }
if not expected_probes: return { 'remove': configured_probes }
for probe_name in new_probes_keys_set: new_probes[probe_name] = expected_probes.pop(probe_name)
for probe_name in remove_probes_keys_set: remove_probes[probe_name] = configured_probes.pop(probe_name)
configured_probes = rpm_probes_config.get('out', {}) if not isinstance(defaults, dict): defaults = {} expected_probes = _expand_probes(probes, defaults)
comment += ('\n' + config_comment)
from __future__ import absolute_import import sys
import salt.utils
kwargs['name'] = repo
if __grains__['os_family'] == 'RedHat': if not salt.utils.is_true(sanitizedkwargs[kwarg]): needs_update = True else: needs_update = True
if kwargs.get('clean_file', False): salt.utils.fopen(kwargs['file'], 'w').close()
ret['result'] = False ret['comment'] = \ 'Failed to configure repo \'{0}\': {1}'.format(name, exc) return ret
if ret['changes']: sys.modules[ __salt__['test.ping'].__module__ ].__context__.pop('pkg._avail', None)
from __future__ import absolute_import
from salt.utils import SaltInvocationError import logging log = logging.getLogger(__name__)
import os import fnmatch
import salt.utils from salt.exceptions import CommandExecutionError
import copy import logging import os import re import string from distutils.version import LooseVersion as _LooseVersion
import salt.utils import salt.utils.url from salt.exceptions import CommandExecutionError from salt.ext import six
return None
ret.append(local_branch if branch is None else branch) ret.append(desired_upstream)
ret['comment'] += '\n\nChanges made: ' + comments
git@github.com:user/repo.git: git.latest: - user: deployer - identity: /home/deployer/.ssh/id_rsa
git@github.com:user/repo.git: git.latest: - user: deployer - identity: - /home/deployer/.ssh/id_rsa - /home/deployer/.ssh/id_rsa_alternate
cret = mod_run_check( run_check_cmd_kwargs, onlyif, unless ) if isinstance(cret, dict): ret.update(cret) return ret
desired_upstream = None remote_rev_type = 'sha1'
remote_rev = None remote_rev_type = None
remote_rev = all_remote_refs['refs/tags/' + rev + '^{}'] remote_rev_type = 'tag'
remote_rev = all_remote_refs['refs/tags/' + rev] remote_rev_type = 'tag'
rev = rev.lower() remote_rev = rev remote_rev_type = 'sha1'
return _fail( ret, 'No revision matching \'{0}\' exists in the remote ' 'repository'.format(rev) )
pass
fast_forward = None
upstream = None
ret['comment'] = _format_comments(actions) return ret
if base_rev is None: fast_forward = True else: fast_forward = __salt__['git.merge_base']( target, refs=[base_rev, remote_rev], is_ancestor=True, user=user)
local_branch = local_rev = None
remote_ref_type = 'ref' if len(ref) <= 40 \ and all(x in string.hexdigits for x in ref): ref = ref.lower() remote_ref_type = 'hash'
hash_exists_locally = True
remotes = __salt__['git.remotes'](target, user=user, redact_auth=False)
current_fetch_url = None if remote in remotes: current_fetch_url = remotes[remote]['fetch']
mylocalrepo: git.config_unset: - name: foo.bar - value_regex: 'baz' - repo: /path/to/repo
mylocalrepo: git.config_unset: - name: foo.bar - all: True
mylocalrepo: git.config_unset: - name: 'foo\..+' - all: True
key = '^' + name.lstrip('^').rstrip('$') + '$'
pre_matches = __salt__['git.config_get_regexp']( cwd=repo, key=key, value_regex=value_regex, user=user, ignore_retcode=True, **{'global': global_} )
return ret
pre = __salt__['git.config_get_regexp']( cwd=repo, key=key, value_regex=None, user=user, ignore_retcode=True, **{'global': global_} )
mylocalrepo: git.config_set: - name: user.email - value: foo@bar.net - repo: /path/to/repo
mylocalrepo: git.config_set: - name: mysection.myattribute - multivar: - foo - bar - baz - repo: /path/to/repo
pre = __salt__['git.config_get']( cwd=repo, key=name, user=user, ignore_retcode=True, **{'all': True, 'global': global_} )
post = __salt__['git.config_set']( cwd=repo, key=name, value=value, multivar=multivar, user=user, **{'global': global_} )
return True
from __future__ import absolute_import
__virtualname__ = 'etcd'
__func_alias__ = { 'set_': 'set', 'rm_': 'rm' }
try:
if kwargs.get('sfun') in ['wait_rm_key', 'wait_rm']: return rm_( name, kwargs.get('profile'))
if error: ret['changes'] = {} ret['result'] = False ret['comment'] = str(error)
import logging
from __future__ import absolute_import import logging import os
from salt.exceptions import SaltInvocationError from salt.utils import exactly_one
mytopic: boto_sns.present: - region: us-east-1 - profile: mysnsprofile
import re
_subscriptions = __salt__['boto_sns.get_all_subscriptions_by_topic']( name, region=region, key=key, keyid=keyid, profile=profile )
_subscriptions = [ {'protocol': s['Protocol'], 'endpoint': s['Endpoint']} for s in _subscriptions ]
if matches is not None: subscription['endpoint'] = _endpoint.replace( matches.groupdict()['pass'], '****')
subscription['endpoint'] = _endpoint
import logging import os
import salt.utils import salt.ext.six as six
item_id_show = item_id if item in ['constraint'] or '=' in item_id: item_id_show = None
else: if is_existing['retcode'] in [0]: item_create_required = False
import re
ret['changes']['summary'] = _summary(result['stdout']) ret['result'] = True if not __opts__['test'] else None
commit = False current_rules = __salt__['firewall.get_rule'](name) if not current_rules: commit = True ret['changes'] = {'new rule': name}
from __future__ import absolute_import import pprint
if __opts__.get('requests_lib', False): from requests.exceptions import HTTPError else: from urllib2 import HTTPError
if __opts__.get('requests_lib', False): from requests.exceptions import HTTPError else: from urllib2 import HTTPError
import os import stat import logging
from salt.utils.odict import OrderedDict
__virtualname__ = 'zpool'
ret['result'] = False
properties_current = __salt__['zpool.get'](name)[name]
properties_update = [] for prop in properties: if prop not in properties_current: continue
for prop in properties_update: value = properties[prop] res = __salt__['zpool.set'](name, prop, value)
if isinstance(value, bool): value = 'on' if value else 'off' elif ' ' in value: value = "'{0}'".format(value)
params = [] params.append(name) for root_dev in layout:
from __future__ import absolute_import
import salt.ext.six as six
__virtualname__ = 'sysrc'
ret['result'] = None
ret['result'] = None
import logging import salt.utils
ret['changes'] = {'new': '', 'old': name}
import salt.utils from salt.ext.six.moves import map
BSD = ('OpenBSD', 'FreeBSD')
from __future__ import absolute_import import logging
import salt.utils
if __grains__['os'] in ['MacOS', 'Darwin']: ret['changes'] = {'new': []}
if __grains__['os'] in ['Windows']: changes_needed = False current_settings = __salt__['proxy.get_proxy_win']() current_domains = __salt__['proxy.get_proxy_bypass']()
if service not in current_settings: changes_needed = True break
changes_needed = True
if len(set(current_domains).intersection(bypass_domains)) != len(bypass_domains): changes_needed = True
import logging import copy
try: import confidant.client import confidant.formatter HAS_LIBS = True except ImportError: HAS_LIBS = False
log = logging.getLogger(__name__)
from __future__ import absolute_import import logging
from __future__ import absolute_import import logging import salt.utils.vault
import logging from uuid import uuid4 try: import couchdb HAS_COUCH = True except ImportError: HAS_COUCH = False
from salt.utils.decorators import memoize
__func_alias__ = {'set_': 'set'}
import logging
import salt.utils.memcached
import logging
from __future__ import absolute_import import logging
import logging import codecs try: import sqlite3 HAS_SQLITE3 = True except ImportError: HAS_SQLITE3 = False
import msgpack
from os import environ
return environ.setdefault(key, value)
return environ.get(key)
port: 8000 ssl_crt: /etc/pki/api/certs/server.crt ssl_key: /etc/pki/api/certs/server.key debug: False disable_ssl: False websockets: True
ws = create_connection('wss://localhost:8000/all_events/d0ce6c1a37e99dcc0374392f272fe19c0090cca7')
ws.send('websocket client ready')
while listening_to_events:
ws.close()
ws = create_connection('wss://localhost:8000/formatted_events/d0ce6c1a37e99dcc0374392f272fe19c0090cca7')
ws.send('websocket client ready')
while listening_to_events:
ws.close()
logger.debug('Websocket already connected, returning') return
pass
logger.debug('Websocket already connected, returning') return
except Exception as err: logger.debug('Error! Ending server side websocket connection. Reason = {0}'.format(str(err))) break
pass
if mod_opts.get('websockets', False): from . import saltnado_websockets
(all_events_pattern, saltnado_websockets.AllEventsHandler), (formatted_events_pattern, saltnado_websockets.FormattedEventsHandler),
from __future__ import absolute_import import json import logging import threading import salt.ext.six as six
self.jobs = {}
self.minions = {}
curr_minion = {} curr_minion.update(minion_info) curr_minion.update({'id': minion}) minions[minion] = curr_minion
from __future__ import absolute_import, print_function
port: 8000 address: 0.0.0.0 backlog: 128 ssl_crt: /etc/pki/api/certs/server.crt ssl_key: /etc/pki/api/certs/server.key debug: False disable_ssl: False webhook_disable_auth: False cors_origin: null
import time import math import fnmatch import logging from copy import copy from collections import defaultdict
import cgi import yaml import tornado.httpserver import tornado.ioloop import tornado.web import tornado.gen from tornado.concurrent import Future from zmq.eventloop import ioloop import salt.ext.six as six
ioloop.install()
import salt.netapi import salt.utils import salt.utils.event from salt.utils.event import tagify import salt.client import salt.runner import salt.auth from salt.exceptions import EauthAuthenticationError
if not self.done(): self.set_result(future)
self.tag_map = defaultdict(list)
self.request_map = defaultdict(list)
self.timeout_map = {}
self._timeout_future(tag, future) if future in self.timeout_map: tornado.ioloop.IOLoop.current().remove_timeout(self.timeout_map[future]) del self.timeout_map[future]
if request._finished: future = Future() future.set_exception(TimeoutException()) return future
self.tag_map[tag].append(future) self.request_map[request].append((tag, future))
if AUTH_TOKEN_HEADER in self.request.headers: return self.request.headers[AUTH_TOKEN_HEADER] else: return self.get_cookie(AUTH_COOKIE_NAME)
accept_header = self.request.headers.get('Accept', '*/*') parsed_accept_header = [cgi.parse_header(h)[0] for h in accept_header.split(',')]
if not content_type: self.send_error(406)
self.start = time.time() self.connected = True
self.timeout_futures()
'text/plain': json.loads
header = cgi.parse_header(self.request.headers['Content-Type']) value, parameters = header return ct_in_map[value](data)
request_headers = self.request.headers.get('Access-Control-Request-Headers') allowed_headers = request_headers.split(',')
self.set_header('Access-Control-Allow-Headers', ','.join(allowed_headers))
self.set_header('Access-Control-Expose-Headers', 'X-Auth-Token')
self.set_header('Access-Control-Allow-Methods', 'OPTIONS, GET, POST')
except KeyError: self.send_error(400) return
try: perms = self.application.opts['external_auth'][token['eauth']][token['name']]
except KeyError: self.send_error(401) return
if not self._verify_auth(): self.redirect('/login') return
for low in self.lowstate: if not self._verify_client(low): return
if self.token is not None and 'token' not in low: low['token'] = self.token
if len(inflight_futures) == 0: continue
finished_future = yield Any(inflight_futures) try: b_ret = finished_future.result() except TimeoutException: break chunk_ret.update(b_ret) inflight_futures.remove(finished_future)
minions_remaining = pub_data['minions']
if syndic_min_wait is not None: yield syndic_min_wait chunk_ret = yield self.all_returns(pub_data['jid'], finish_futures=[job_not_running], minions_remaining=minions_remaining, )
try: minions_remaining.remove(event['data']['id']) except ValueError: pass if len(minions_remaining) == 0: raise tornado.gen.Return(chunk_ret)
pub_data = self.saltclients['local_async'](*f_call.get('args', ()), **f_call.get('kwargs', {}))
raise tornado.gen.Return(event['data']['return'])
if not self._verify_auth(): self.redirect('/login') return
if not self._verify_auth(): self.redirect('/login') return
if not self._verify_auth(): self.redirect('/login') return
tag = 'salt/netapi/hook' if tag_suffix: tag += tag_suffix
'headers': dict(self.request.headers),
return allowed_origins
import inspect import os
if not self._is_master_running(): raise salt.exceptions.SaltDaemonNotRunning( 'Salt Master is not available.')
from __future__ import absolute_import import cherrypy
self.pipe = None
self.token = None
self.opts = None
import logging import os
try: import cherrypy
if not cpy_error and 'port' in mod_opts: return __virtualname__
if cpy_error:
if 'port' not in mod_opts: logger.error("Not loading '%s'. 'port' not specified in config", __name__)
from __future__ import absolute_import import json import logging
import salt.ext.six as six
import salt.netapi
self.jobs = {}
self.minions = {}
dropped_minions = set(curr_minions) - set(minions_detected)
new_minions = set(minions_detected) - set(curr_minions)
curl -sSk https://localhost:8000/login \\ -c ~/cookies.txt \\ -H 'Accept: application/x-yaml' \\ -d username=saltdev \\ -d password=saltdev \\ -d eauth=auto
curl -sSk https://localhost:8000 \\ -b ~/cookies.txt \\ -H 'Accept: application/x-yaml' \\ -d client=local \\ -d tgt='*' \\ -d fun=test.ping
from __future__ import absolute_import import collections import itertools import functools import logging import json import StringIO import tarfile import time from multiprocessing import Process, Pipe
import cherrypy from cherrypy.lib import cpstats import yaml import salt.ext.six as six
import salt import salt.auth import salt.utils.event
import salt.netapi
try: from .tools import websockets from . import event_processor
if x_auth: cherrypy.request.cookie['session_id'] = x_auth
cherrypy.response.headers['Cache-Control'] = 'private'
resp_head['Access-Control-Allow-Origin'] = req_head.get('Origin', '*') resp_head['Access-Control-Expose-Headers'] = 'GET, POST' resp_head['Access-Control-Allow-Credentials'] = 'true'
if cherrypy.request.method == 'OPTIONS': cherrypy.serving.request.handler = cors_handler
ct_out_map = ( ('application/json', json.dumps), ('application/x-yaml', functools.partial( yaml.safe_dump, default_flow_style=False)), )
best = cherrypy.lib.cptools.accept([i for (i, _) in ct_out_map])
cherrypy.response.headers['Content-Type'] = best out = cherrypy.response.processors[best] return out(ret)
cherrypy._cpreqbody.process_urlencoded(entity) cherrypy.serving.request.unserialized_data = entity.params cherrypy.serving.request.raw_body = ''
ct_in_map = { 'application/x-www-form-urlencoded': urlencoded_processor, 'application/json': json_processor, 'application/x-yaml': yaml_processor, 'text/yaml': yaml_processor, 'text/plain': text_processor, }
cherrypy.request.lowstate = [data]
if cherrypy.request.config.get('tools.sessions.on', False): cherrypy.session.release_lock()
if not isinstance(lowstate, list): raise cherrypy.HTTPError(400, 'Lowstates must be a list')
if 'arg' in chunk and not isinstance(chunk['arg'], list): chunk['arg'] = [chunk['arg']]
if isinstance(ret, collections.Iterator): for i in ret: yield i else: yield ret
curl -sSik https://localhost:8000 \\ -d client=local \\ -d tgt='*' \\ -d fun='cmd.run' \\ -d arg='du -sh .' \\ -d arg='/path/to/dir'
curl -sSik https://localhost:8000 \\ -d client=runner \\ -d fun='jobs.lookup_jid' \\ -d jid='20150129182456704682' \\ -d outputter=highstate
if isinstance(cherrypy.serving.request.lowstate, list): creds = cherrypy.serving.request.lowstate[0] else: creds = cherrypy.serving.request.lowstate
if not salt_api_acl_tool(username, cherrypy.request): raise cherrypy.HTTPError(401)
token = self.auth.mk_token(creds) if 'token' not in token: raise cherrypy.HTTPError(401, 'Could not authenticate using provided credentials')
try: eauth = self.opts.get('external_auth', {}).get(token['eauth'], {})
perms = eauth.get(token['name'], []) perms.extend(eauth.get('*', []))
'tools.salt_token.on': True, 'tools.salt_auth.on': False,
if salt_token and self.resolver.get_token(salt_token): return True
cherrypy.session.release_lock()
'tools.salt_token.on': True, 'tools.salt_auth.on': False,
while listening_to_events: print ws.recv()
if not salt_token or not self.auth.get_tok(salt_token): raise cherrypy.HTTPError(401)
cherrypy.session.release_lock()
handler = cherrypy.request.ws_handler
pipe.recv()
proc = Process(target=event_stream, args=(handler, child_pipe)) proc.start()
'tools.lowdata_fmt.on': True,
'tools.salt_token.on': True, 'tools.salt_auth.on': True,
self.url_map.update({ self.apiopts.get('webhook_url', 'hook').lstrip('/'): Webhook, })
cherrypy.config.update(conf['global'])
cherrypy.config['saltopts'] = opts cherrypy.config['apiopts'] = apiopts
import salt import salt.netapi
saltenviron(environ)
try: ret = json.dumps({'return': resp}) except TypeError as exc: code = 500 ret = str(exc)
start_response(H[code], get_headers(ret, { 'Content-Type': 'application/json', })) return (ret,)
if '__opts__' not in globals(): globals()['__opts__'] = get_opts()
httpd = make_server('localhost', mod_opts['port'], application)
from __future__ import absolute_import import logging
import salt.client from salt.ext import six from salt.ext.six.moves import zip
try: import redis HAS_REDIS = True except ImportError: HAS_REDIS = False
from __future__ import absolute_import
import salt.utils.reactor
from __future__ import absolute_import import multiprocessing import logging
import salt import salt.loader import salt.utils from salt.utils.process import SignalHandlingMultiprocessingProcess
if salt.utils.is_windows(): runners = None utils = None funcs = None
from __future__ import absolute_import import datetime import json import logging import pprint import time try: import slackclient HAS_SLACKCLIENT = True except ImportError: HAS_SLACKCLIENT = False
import salt.client import salt.loader import salt.runner import salt.utils import salt.utils.event import salt.utils.http import salt.utils.slack
channel = sc.server.channels.find(_m['channel'])
cmdline = salt.utils.shlex_split(_text[len(trigger):]) cmd = cmdline[0] args = [] kwargs = {}
if valid_commands: if cmd not in valid_commands: channel.send_message('Using {0} is not allowed.'.format(cmd)) return
else: local = salt.client.LocalClient() ret = local.cmd('{0}'.format(target), cmd, args, kwargs)
fire('{0}/{1}'.format(tag, _m['type']), _m)
fire('{0}/{1}'.format(tag, _m['type']), _m)
from __future__ import absolute_import import logging import time import json
import salt.utils.event
try: import boto.sqs HAS_BOTO = True except ImportError: HAS_BOTO = False
import salt.utils.event from salt.ext import six
try: import certifi HAS_CERTIFI = True except ImportError: HAS_CERTIFI = False
try: import ssl HAS_SSL = True
import socket import random import time import codecs import uuid import logging import json
self.INVALID_TOKEN = ("\n\nIt appears the LOGENTRIES_TOKEN " "parameter you entered is incorrect!\n\n") self.LINE_SEP = _to_unicode(r'\u2028')
time.sleep(30) raise UserWarning("Unable to connect to room {0}".format(room))
if valid_users: if partner not in valid_users: target_room.message('{0} not authorized to run Salt commands'.format(partner)) return
if 'target' not in kwargs: target = '*' else: target = kwargs['target'] del kwargs['target']
if valid_commands: if cmd not in valid_commands: target_room.message('Using {0} is not allowed.'.format(cmd)) return
else: local = salt.client.LocalClient() ret = local.cmd('{0}'.format(target), cmd, args, kwargs)
from __future__ import absolute_import import json import logging
import salt.utils.event
import salt.thorium
from __future__ import absolute_import
try: import docker import docker.utils HAS_DOCKER_PY = True except ImportError: HAS_DOCKER_PY = False
CLIENT_TIMEOUT = 60
__virtualname__ = 'docker_events'
from __future__ import absolute_import import logging import json
import salt.utils.event
try: import logstash HAS_LOGSTASH = True except ImportError: HAS_LOGSTASH = False
from __future__ import absolute_import import logging
import salt.utils.jid
import salt.ext.six as six try:
__virtualname__ = 'cassandra'
from __future__ import absolute_import import logging import time import json
import salt.utils.jid import salt.returners
__virtualname__ = 'couchdb'
retc = ret.copy()
retc["_id"] = ret["jid"]
retc["timestamp"] = time.time()
_response = _request("GET", options['url'] + "_all_dbs") if options['db'] not in _response:
_response = _request("PUT", options['url'] + options['db'])
doc = _generate_doc(ret)
if 'total_rows' not in _response: log.error('Didn\'t get valid response from requesting all docs: {0}' .format(_response)) return {}
ret = {} for row in _response['rows']: jid = row['id'] if not salt.utils.jid.is_jid(jid): continue
options = _get_options(ret=None)
_ret = {}
for minion in get_minions():
if len(_response['rows']) < 1: continue
_ret[minion] = _response['rows'][0]['value']
if not ensure_views(): return []
_response = _request("GET", options['url'] + options['db'] + "/_design/salt/_view/minions?group=true")
if 'rows' not in _response: log.error('Unable to get available minions: {0}'.format(_response)) return []
_ret = [] for row in _response['rows']: _ret.append(row['key']) return _ret
options = _get_options(ret=None)
_response = _request("GET", options['url'] + options['db'] + "/_design/salt")
if 'error' in _response: return set_salt_view()
for view in get_valid_salt_views(): if view not in _response['views']: return set_salt_view()
return True
new_doc = {} new_doc['views'] = get_valid_salt_views() new_doc['language'] = "javascript"
import cgi import logging
import salt.ext.six.moves.http_client
_options['checktype'] = '1'
_options['checktype'] = str(_options['checktype'])
import json
import salt.utils import salt.utils.jid import salt.returners
try: import redis HAS_REDIS = True except ImportError: HAS_REDIS = False
__virtualname__ = 'redis'
import json import logging
try: import salt.utils.etcd_util HAS_LIBS = True except ImportError: HAS_LIBS = False
__virtualname__ = 'etcd'
client.set( '/'.join((path, 'minions', ret['id'])), ret['jid'], ttl=ttl, )
from contextlib import contextmanager import sys import time import logging
import salt.returners import salt.utils.jid import salt.exceptions
try: import psycopg2 import psycopg2.extras HAS_PG = True except ImportError: HAS_PG = False
__virtualname__ = 'pgjsonb'
if 'port' in _options: _options['port'] = int(_options['port']) return _options
pass
import yaml import pprint import logging import urllib
import salt.ext.six.moves.http_client
import salt.returners import salt.utils.slack
result = salt.utils.slack.query(function='message', api_key=api_key, method='POST', header_dict={'Content-Type': 'application/x-www-form-urlencoded'}, data=urllib.urlencode(parameters))
cfg = __salt__.get('config.option', __opts__)
_options = dict( _options_browser( cfg, ret_config, defaults, virtualname, attrs, ) )
_options.update( _fetch_profile_opts( cfg, virtualname, __salt__, _options, profile_attr, profile_attrs ) )
if ret and 'ret_kwargs' in ret: _options.update(ret['ret_kwargs'])
if isinstance(cfg, dict): c_cfg = cfg else: c_cfg = cfg('{0}'.format(virtualname), {})
if isinstance(cfg, dict): return c_cfg.get(attr_name, cfg.get(default_cfg_key)) else: return c_cfg.get(attr_name, cfg(default_cfg_key))
ret_cfg = cfg('{0}.{1}'.format(ret_config, virtualname), {})
ret_override_cfg = ret_cfg.get( attr_name, override_cfg_default ) if ret_override_cfg: return ret_override_cfg
return c_cfg.get(attr_name, cfg(default_cfg_key))
value = _fetch_option(cfg, ret_config, virtualname, options[option])
if defaults: if option in defaults: log.info('Using default for %s %s', virtualname, option) yield option, defaults[option] continue
continue
from __future__ import absolute_import import logging
import salt.minion
MMINION = None
import json import logging import uuid import time
import salt.returners import salt.utils.jid import salt.exceptions from salt.exceptions import CommandExecutionError
import logging
import salt.utils.jid
__virtualname__ = 'sentry'
from __future__ import absolute_import from datetime import tzinfo, datetime, timedelta import uuid import logging import json
import salt.utils.jid
from contextlib import contextmanager import sys import json import logging
import salt.returners import salt.utils.jid import salt.exceptions
try: import MySQLdb HAS_MYSQL = True except ImportError: HAS_MYSQL = False
__virtualname__ = 'mysql'
if 'port' in _options: _options['port'] = int(_options['port']) return _options
pass
from __future__ import absolute_import import collections import logging import socket import struct import time from contextlib import contextmanager
import salt.utils.jid import salt.returners
import salt.ext.six as six
__virtualname__ = 'carbon'
log.debug('Destroying carbon socket')
if not metric_base.startswith('virt.'): metric_base += '.' + ret['id'].replace('.', '_')
import logging import json import datetime
import salt.utils.jid import salt.returners
try: import sqlite3 HAS_SQLITE3 = True except ImportError: HAS_SQLITE3 = False
__virtualname__ = 'sqlite3'
data.pop() for minion, ret in data: ret[minion] = json.loads(ret)
from __future__ import absolute_import import logging
import salt.returners import salt.utils.jid
__virtualname__ = 'django'
from __future__ import absolute_import, print_function
import json import logging
try: import memcache HAS_MEMCACHE = True except ImportError: HAS_MEMCACHE = False
__virtualname__ = 'memcache'
memcacheoptions = (host, port)
_append_list(serv, 'minions', minion) _append_list(serv, 'jids', jid)
ret = {} for minion, data in six.iteritems(returns): ret[minion] = json.loads(data) return ret
ret = {} for minion, data in six.iteritems(returns): ret[minion] = json.loads(data) return ret
import os import logging import smtplib import StringIO from email.utils import formatdate
import salt.utils.jid import salt.returners import salt.loader from salt.template import compile_template
import json
import salt.utils.jid import salt.returners
try: import pyodbc #import psycopg2.extras HAS_ODBC = True except ImportError: HAS_ODBC = False
__virtualname__ = 'odbc'
import logging
import salt.utils.jid import salt.returners import salt.ext.six as six
try: import pymongo version = pymongo.version version = '.'.join(version.split('.')[:2]) HAS_PYMONGO = True except ImportError: HAS_PYMONGO = False
__virtualname__ = 'mongo'
ret[minion] = data['full_ret']
if host: self.host = host else: self.host = socket.gethostname()
if not eventtime: eventtime = str(int(time.time()))
if 'host' not in payload: payload.update({"host": self.host})
data = {"time": eventtime} data.update(payload)
r = requests.post(self.server_uri, data=json.dumps(data), headers=headers, verify=http_event_collector_SSL_verify)
if http_event_collector_debug: log.debug(r.text) log.debug(data)
if 'host' not in payload: payload.update({"host": self.host})
if http_event_collector_debug: log.debug('auto flushing')
if not eventtime: eventtime = str(int(time.time()))
data = {"time": eventtime} data.update(payload)
import salt.utils import salt.utils.jid
__virtualname__ = 'couchbase'
COUCHBASE_CONN = None DESIGN_NAME = 'couchbase_returner' VERIFIED_VIEWS = False
json = salt.utils.import_json() couchbase.set_json_converters(json.dumps, json.loads)
import json import logging import requests
import salt.utils.jid import salt.returners from salt.utils.decorators import memoize
try: import influxdb import influxdb.influxdb08 HAS_INFLUXDB = True except ImportError: HAS_INFLUXDB = False
influxDBVersionHeader = "X-Influxdb-Version"
__virtualname__ = 'influxdb'
json_return = json.dumps(ret['return']) del ret['return'] json_full_ret = json.dumps(ret)
import errno import glob import logging import os import shutil import time import hashlib import bisect
import salt.payload import salt.utils import salt.utils.files import salt.utils.jid import salt.exceptions
import salt.ext.six as six
if load['jid'] == 'req': load['jid'] = prep_jid(nocache=load.get('nocache', False))
salt.utils.atomicfile.atomic_open( os.path.join(hn_dir, RETURN_P), 'w+b' )
salt.utils.atomicfile.atomic_open( os.path.join(hn_dir, OUT_P), 'w+b' )
pass
dirs_to_remove = set()
t_path_dirs = os.listdir(t_path) if not t_path_dirs and t_path not in dirs_to_remove: dirs_to_remove.add(t_path) continue
shutil.rmtree(t_path)
shutil.rmtree(t_path)
import json
import salt.utils.jid import salt.returners
try: import psycopg2 HAS_POSTGRES = True except ImportError: HAS_POSTGRES = False
import pprint import logging
from salt.ext.six.moves.urllib.parse import urlencode as _urlencode
import salt.returners import salt.utils.pushover from salt.exceptions import SaltInvocationError
import json import pprint import logging
import salt.returners
import logging
import salt.utils.jid import salt.returners import salt.ext.six as six
try: import pymongo version = pymongo.version version = '.'.join(version.split('.')[:2]) HAS_PYMONGO = True except ImportError: HAS_PYMONGO = False
__virtualname__ = 'mongo'
ret[minion] = data['full_ret']
import json try: import syslog HAS_SYSLOG = True except ImportError: HAS_SYSLOG = False
import salt.utils.jid import salt.returners
__virtualname__ = 'syslog'
level = getattr(syslog, _options['level']) facility = getattr(syslog, _options['facility'])
logoption = 0 for opt in _options['options']: logoption = logoption | getattr(syslog, opt)
if 'tag' in _options: syslog.openlog(ident=_options['tag'], logoption=logoption) else: syslog.openlog(logoption=logoption)
syslog.syslog(facility | level, '{0}'.format(json.dumps(ret)))
syslog.closelog()
from __future__ import absolute_import import json import logging import re import sys
import salt.utils import salt.utils.jid import salt.ext.six as six
try: import psycopg2 HAS_POSTGRES = True except ImportError: HAS_POSTGRES = False
LOAD_P = '.load.p' MINIONS_P = '.minions.p' RETURN_P = 'return.p' OUT_P = 'out.p'
sleekxmpp_version = distutils.version.LooseVersion(sleekxmpp.__version__) valid_version = distutils.version.LooseVersion('1.3.1') if sleekxmpp_version >= valid_version: return __virtualname__
super(SendMsgBot, self).__init__(jid, password)
from __future__ import absolute_import from ctypes import CDLL, POINTER, Structure, CFUNCTYPE, cast, pointer, sizeof from ctypes import c_void_p, c_uint, c_char_p, c_char, c_int from ctypes.util import find_library
from salt.utils import get_group_list
PAM_PROMPT_ECHO_OFF = 1 PAM_PROMPT_ECHO_ON = 2 PAM_ERROR_MSG = 3 PAM_TEXT_INFO = 4
from __future__ import absolute_import import logging
try: from Crypto.Util import asn1 import OpenSSL HAS_DEPS = True except ImportError: HAS_DEPS = False
import salt.utils
algo = cert.get_signature_algorithm()
cert_asn1 = c.dump_certificate(c.FILETYPE_ASN1, cert)
der = asn1.DerSequence() der.decode(cert_asn1)
der_cert = der[0] #der_algo = der[1] der_sig = der[2]
der_sig_in = asn1.DerObject() der_sig_in.decode(der_sig)
sig0 = der_sig_in.payload
if sig0[0] != '\x00': raise Exception('Number of unused bits is strange') sig = sig0[1:]
from __future__ import print_function import os import collections import hashlib import time import logging import random import getpass from salt.ext.six.moves import input
import salt.config import salt.loader import salt.transport.client import salt.utils import salt.utils.minions import salt.payload
rm_tok = True
return auth_data
if load.get('fun', '') != 'saltutil.find_job': return good
if 'username' in ret and not ret['username']: ret['username'] = salt.utils.get_user()
from __future__ import absolute_import import logging
from __future__ import absolute_import import logging
import salt.utils.http
from __future__ import absolute_import import logging import salt.ext.six as six
from salt.exceptions import CommandExecutionError, SaltInvocationError
from jinja2 import Environment try: import ldap import ldap.modlist import ldap.filter HAS_LDAP = True except ImportError: HAS_LDAP = False
if paramvalues['binddn']: connargs['binddn'] = paramvalues['binddn'] if paramvalues['bindpw']: params['mandatory'].append('bindpw')
return _LDAPConnection(**connargs).ldap
paramvalues['binddn'] = _render_template(paramvalues['binddn'], username) paramvalues['binddn'] = ldap.filter.escape_filter_chars(paramvalues['binddn'])
if paramvalues['binddn']: connargs['binddn'] = paramvalues['binddn'] if paramvalues['bindpw']: params['mandatory'].append('bindpw')
connargs['bindpw'] = password
pass
from __future__ import absolute_import import logging
application: 6789012345 directory: 3456789012
from __future__ import absolute_import from __future__ import print_function import logging
BaseLoader = getattr(yaml, 'CSafeLoader', yaml.SafeLoader) BaseDumper = getattr(yaml, 'CSafeDumper', yaml.SafeDumper)
from __future__ import absolute_import import logging import datetime from copy import copy
from salt.serializers import DeserializationError, SerializationError from salt.utils.aggregation import aggregate, Map, Sequence from salt.utils.odict import OrderedDict
import yaml from yaml.nodes import MappingNode from yaml.constructor import ConstructorError from yaml.scanner import ScannerError import salt.ext.six as six
BaseLoader = getattr(yaml, 'CSafeLoader', yaml.SafeLoader) BaseDumper = yaml.SafeDumper if six.PY3 else getattr(yaml, 'CSafeDumper', yaml.SafeDumper)
reset = key_node.tag == u'!reset'
obj = self.construct_scalar(node) if six.PY2: obj = obj.encode('utf-8') return SLSString(obj)
if node.value == '': node.value = '0'
tag = self.resolve(yaml.nodes.ScalarNode, node.value, [True, True]) deep = False
cp.readfp(StringIO.StringIO(stream_or_string))
from __future__ import absolute_import import logging from copy import copy
from salt.log import setup_console_logger from salt.serializers import DeserializationError, SerializationError
import salt.ext.six as six
import msgpack if msgpack.loads(msgpack.dumps([1, 2, 3]), use_list=True) is None: raise ImportError available = True
try:
from __future__ import absolute_import import logging import gc import datetime
import salt.log import salt.crypt import salt.transport.frame from salt.exceptions import SaltReqTimeoutError
import salt.ext.six as six try: import zmq except ImportError: pass
import msgpack if msgpack.loads(msgpack.dumps([1, 2, 3]), use_list=True) is None: raise ImportError HAS_MSGPACK = True
try:
ret = msgpack.loads(msg, use_list=True, encoding=encoding)
return msgpack.dumps(msg, use_bin_type=use_bin_type)
def default(obj): return msgpack.ExtType(78, obj)
raise
fn_.write(self.dumps(msg, use_bin_type=True))
self._socket = self.context.socket(zmq.REQ) if hasattr(zmq, 'RECONNECT_IVL_MAX'): self._socket.setsockopt( zmq.RECONNECT_IVL_MAX, 5000 )
from __future__ import print_function, with_statement
try: SETUP_DIRNAME = os.path.dirname(__file__) except NameError: SETUP_DIRNAME = os.path.dirname(sys.argv[0])
'BOOTSTRAP_SCRIPT_VERSION', 'v2014.06.21'
IS_PY3 = sys.version_info > (3,)
from esky import bdist_esky import bbfreeze HAS_ESKY = True
PACKAGED_FOR_SALT_SSH_FILE = os.path.join(os.path.abspath(SETUP_DIRNAME), '.salt-ssh-package') PACKAGED_FOR_SALT_SSH = os.path.isfile(PACKAGED_FOR_SALT_SSH_FILE)
exec(compile(open(SALT_VERSION).read(), SALT_VERSION, 'exec'))
continue
continue
class WriteSaltVersion(Command):
if getattr(self.distribution, 'salt_version_hardcoded_path', None) is None: print('This command is not meant to be called on it\'s own') exit(1)
open(self.distribution.salt_version_hardcoded_path, 'w').write( INSTALL_VERSION_TEMPLATE.format( date=DATE, full_version_info=__saltstack_version__.full_info ) )
if getattr(self.distribution, 'salt_syspaths_hardcoded_path', None) is None: print('This command is not meant to be called on it\'s own') exit(1)
if getattr(self.distribution, 'salt_ssh_packaging_file', None) is None: print('This command is not meant to be called on it\'s own') exit(1)
open(self.distribution.salt_ssh_packaging_file, 'w').write('Packaged for Salt-SSH\n')
self.distribution.salt_installing_m2crypto_windows = True self.run_command('install-m2crypto-windows') self.distribution.salt_installing_m2crypto_windows = None
self.distribution.salt_installing_pycrypto_windows = True self.run_command('install-pycrypto-windows') self.distribution.salt_installing_pycrypto_windows = None
self.distribution.salt_download_windows_dlls = True self.run_command('download-windows-dlls') self.distribution.salt_download_windows_dlls = None
develop.run(self)
self.distribution.running_salt_sdist = True self.distribution.salt_version_hardcoded_path = os.path.join( base_dir, 'salt', '_version.py' ) self.run_command('write_salt_version')
Sdist.run(self)
{date:%A, %d %B %Y @ %H:%m:%S UTC}.
{date:%A, %d %B %Y @ %H:%m:%S UTC}.
build.run(self) if getattr(self.distribution, 'running_salt_install', False):
self.run_command('write_salt_version')
self.distribution.salt_syspaths_hardcoded_path = os.path.join( self.build_lib, 'salt', '_syspaths.py' ) self.run_command('generate_salt_syspaths')
self.distribution.running_salt_install = True self.distribution.salt_version_hardcoded_path = os.path.join( self.build_lib, 'salt', '_version.py' ) if IS_WINDOWS_PLATFORM:
self.distribution.salt_installing_m2crypto_windows = True self.run_command('install-m2crypto-windows') self.distribution.salt_installing_m2crypto_windows = None
self.distribution.salt_download_windows_dlls = True self.run_command('download-windows-dlls') self.distribution.salt_download_windows_dlls = None
inp = self.get_inputs() out = self.get_outputs() chmod = []
freezer_includes.extend([ 'cherrypy', 'dateutils', 'pyghmi', 'croniter', 'mako', 'gnupg', ])
def parse_command_line(self): args = distutils.dist.Distribution.parse_command_line(self)
from __future__ import print_function
if getattr(sys, 'frozen', False): application_path = os.path.dirname(sys.executable) elif __file__: application_path = os.path.dirname(__file__)
from sphinx.ext.autodoc import FunctionDocumenter as FunctionDocumenter
return self.module.__func_alias__.get(self.objpath[0], self.objpath[0])
list_item = nodes.list_item() list_item['classes'] = ['lit-item']
if len(list_item.children) == 2: enum.append(list_item) list_item = nodes.list_item() list_item['classes'] = ['lit-item']
bg = nodes.container() bg['classes'] = ['lit-background'] node.append(bg)
python_domain.PythonDomain.indices = []
indices = []
return ret
'user',
'cherrypy', 'cherrypy.lib', 'cherrypy.process', 'cherrypy.wsgiserver', 'cherrypy.wsgiserver.ssl_builtin',
sys.modules['cherrypy'].config = mock_decorator_with_params
import salt.version
intersphinx_mapping = { 'python2': ('http://docs.python.org/2', None), 'python3': ('http://docs.python.org/3', None) }
locale_dirs = ['locale/'] gettext_compact = False
on_saltstack = 'SALT_ON_SALTSTACK' in os.environ
if on_saltstack: html_search_template = 'googlesearch.html' else: html_search_template = 'searchbox.html'
authors = [ 'Thomas S. Hatch <thatch45@gmail.com> and many others, please see the Authors file', ]
from __future__ import absolute_import, print_function
from salttesting.case import TestCase
import cherrypy import salt.ext.six as six from salt.ext.six.moves import StringIO
cherrypy.config.update({'environment': "test_suite"})
cherrypy.server.unsubscribe()
h = {'Host': '127.0.0.1'}
fd = None if body is not None: h['content-length'] = '{0}'.format(len(body)) fd = StringIO(body)
app = cherrypy.tree.apps.get(app_path) if not app: raise AssertionError("No application mounted at '{0}'".format(app_path))
app.release_serving()
response.collapse_body() return request, response
bytes = bytearray
assertBadSplit("10:9:8:7:6:5:4:3:42.42.42.42")
for lhs in self.objects: for rhs in self.objects: if lhs is rhs: continue self.assertNotEqual(lhs, rhs)
collapsed = ipaddress.collapse_addresses([ip1, ip2]) self.assertEqual(list(collapsed), [ipaddress.IPv4Network('1.1.0.0/23')])
ip_same1 = ip_same2 = ipaddress.IPv4Network('1.1.1.1/32') self.assertEqual(list(ipaddress.collapse_addresses( [ip_same1, ip_same2])), [ip_same1])
self.assertEqual(ip1.compare_networks(ip2), -1) self.assertEqual(ip2.compare_networks(ip1), 1)
self.assertEqual(True, ipaddress.ip_address('100::').is_reserved) self.assertEqual(True, ipaddress.ip_network('4000::1/128').is_reserved)
self.assertEqual(self.ipv6_interface.with_hostmask, '2001:658:22a:cafe:200::1/::ffff:ffff:ffff:ffff')
self.assertIn('broadcast_address', self.ipv4_network._cache) self.assertIn('hostmask', self.ipv4_network._cache)
self.assertNotIn('broadcast_address', self.ipv6_network._cache) self.assertNotIn('hostmask', self.ipv6_network._cache)
from __future__ import absolute_import
self.app = app
from __future__ import absolute_import import unittest import logging
from __future__ import absolute_import import optparse import pprint
import salt.config import salt.wheel import salt.auth
from __future__ import absolute_import
from salttesting.unit import skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import MagicMock, patch, NO_MOCK, NO_MOCK_REASON ensure_in_syspath('../..')
import integration from salt import fileclient
from __future__ import absolute_import import os import logging import pwd import shutil
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import patch, NO_MOCK, NO_MOCK_REASON
import integration from salt.fileserver import gitfs
pass
from __future__ import absolute_import import os
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import patch, NO_MOCK, NO_MOCK_REASON ensure_in_syspath('../..')
import integration from salt.fileserver import roots from salt import fileclient
self.master_opts['file_roots']['base'] = [os.path.join(integration.FILES, 'file', 'base')]
self.skipTest('This test fails when using tests/runtests.py. salt-runtests will be available soon.')
self.skipTest('This test fails when using tests/runtests.py. salt-runtests will be available soon.')
from __future__ import absolute_import
import integration
from salttesting.helpers import ensure_in_syspath
from __future__ import absolute_import
import integration
from __future__ import absolute_import
import integration
from salttesting.helpers import ensure_in_syspath
time.sleep(10)
atexit.register(self.cleanup)
_ = args _ = kwargs
os.killpg(os.getpgid(process.pid), signal.SIGINT) term_sent = True continue
os.killpg(os.getpgid(process.pid), signal.SIGKILL) process.wait()
pass
kwargs['program'] = self.script
kwargs['program'] = self.script
_base.update(copy.deepcopy(_overrides)) return _base
from __future__ import absolute_import import os from time import sleep import textwrap
from salttesting.helpers import destructiveTest, ensure_in_syspath
import integration import salt.utils
from __future__ import absolute_import
import integration from salttesting import skipIf
import salt.runner
from __future__ import absolute_import import os
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import patch, NO_MOCK, NO_MOCK_REASON
import errno import logging import os import shutil
import integration import salt.utils from salt import fileclient from salt.ext import six from salttesting.helpers import ensure_in_syspath, destructiveTest ensure_in_syspath('..')
shutil.rmtree(path) os.makedirs(path)
for saltenv in SALTENVS: saltenv_root = os.path.join(FS_ROOT, saltenv) _new_dir(saltenv_root)
_new_dir(CACHE_ROOT)
from __future__ import absolute_import import os import traceback
from salttesting.helpers import ensure_in_syspath from salttesting.mixins import RUNTIME_VARS ensure_in_syspath('../../')
import integration from salt.output import display_output import salt.config
display_output(data, opts=self.minion_opts) self.assertTrue(True)
trace = traceback.format_exc() self.assertEqual(trace, '')
from __future__ import absolute_import import socket import logging import threading from multiprocessing import Queue
import msgpack
import salt.log.setup
break
break
from __future__ import absolute_import import time
import salt.utils.decorators
from __future__ import absolute_import import os import tempfile
import salt.utils
os.environ.get('TMPDIR', tempfile.gettempdir()) if salt.utils.is_darwin() else '/tmp'
TMP = os.path.join(SYS_TMP_DIR, 'salt-tests-tmpdir')
from __future__ import absolute_import
from __future__ import absolute_import import logging
log = logging.getLogger(__name__)
MY_NAME = 'test_ext_pillar_opts'
from __future__ import absolute_import import logging
from __future__ import absolute_import import socket import logging
import salt.utils.event
from tornado import gen from tornado import ioloop from tornado import netutil
io_loop = ioloop.IOLoop() io_loop.make_current()
self.sock.bind(('localhost', port)) self.sock.listen(5) netutil.add_accept_handler( self.sock, self.handle_connection, io_loop=self.io_loop, )
from salttesting import TestCase from salttesting.case import ShellTestCase from salttesting.mixins import CheckShellBinaryNameAndVersionMixIn from salttesting.parser import PNUM, print_header, SaltTestcaseParser from salttesting.helpers import requires_sshd_server from salttesting.helpers import ensure_in_syspath, RedirectStdStreams
ensure_in_syspath(CODE_DIR)
pass
import yaml import msgpack import salt.ext.six as six if salt.utils.is_windows(): import win32api
os.environ.get('TMPDIR', tempfile.gettempdir()) if salt.utils.is_darwin() else '/tmp'
for key in list(to_cleanup.keys()): instance = to_cleanup.pop(key) del instance
port = get_unused_localhost_port() usock.close() return port
transport = None if needs_daemon: transport = self.options.transport TestDaemon.transplant_configs(transport=transport)
break
pass
import salt.utils
proc_args.insert(0, sys.executable)
if terminal.stdout is not None: terminal.recv() if terminal.stderr is not None: terminal.recv_err() time.sleep(0.125)
for key in self.colors: self.colors[key] = ''
salt_log_setup.setup_multiprocessing_logging_listener( self.master_opts )
self._enter_mockbin()
time.sleep(5)
job_finished = True
syncing.remove(name) continue
print( ' {LIGHT_RED}*{ENDC} {0} Failed to sync {2}: ' '{1}'.format( name, output['ret'], modules_kind, **self.colors) ) return False
try: syncing.remove(name) except KeyError: print( ' {LIGHT_RED}*{ENDC} {0} already synced??? ' '{1}'.format(name, output, **self.colors) )
pass
orig[minion_tgt] = self._check_state_return( orig[minion_tgt] )
return ret
keys = list(keys)
keys = [keys]
raise RuntimeError('The passed keys need to be a list')
from __future__ import absolute_import import os import textwrap
from salttesting.helpers import ensure_in_syspath
import integration import salt.utils
from __future__ import absolute_import import os
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils
self.assertTrue(self.run_function('xattr.clear', [TEST_FILE]))
self.assertEqual(self.run_function('xattr.list', [TEST_FILE]), {})
self.assertEqual(self.run_function('xattr.list', [NO_FILE]), 'ERROR: File not found: {0}'.format(NO_FILE))
self.assertTrue(self.run_function('xattr.clear', [TEST_FILE]))
self.assertTrue(self.run_function('xattr.clear', [TEST_FILE]))
self.assertTrue( self.run_function('xattr.write', [TEST_FILE, 'spongebob', 'squarepants']))
self.assertEqual( self.run_function('xattr.read', [TEST_FILE, 'spongebob']), 'squarepants')
self.assertEqual( self.run_function('xattr.read', [NO_FILE, 'spongebob']), 'ERROR: File not found: {0}'.format(NO_FILE))
self.assertEqual( self.run_function('xattr.read', [TEST_FILE, 'patrick']), 'ERROR: Attribute not found: patrick')
self.assertTrue(self.run_function('xattr.clear', [TEST_FILE]))
self.assertTrue( self.run_function('xattr.delete', [TEST_FILE, 'squidward']))
self.assertEqual( self.run_function('xattr.list', [TEST_FILE]), {'spongebob': 'squarepants', 'crabby': 'patty'})
self.assertEqual( self.run_function('xattr.delete', [NO_FILE, 'spongebob']), 'ERROR: File not found: {0}'.format(NO_FILE))
self.assertEqual( self.run_function('xattr.delete', [TEST_FILE, 'patrick']), 'ERROR: Attribute not found: patrick')
self.assertTrue(self.run_function('xattr.clear', [TEST_FILE]))
self.assertTrue(self.run_function('xattr.clear', [TEST_FILE]))
self.assertEqual(self.run_function('xattr.clear', [NO_FILE]), 'ERROR: File not found: {0}'.format(NO_FILE))
from __future__ import absolute_import import os import sys import textwrap import tempfile
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath, skip_if_binaries_missing ) from salttesting.mock import NO_MOCK, NO_MOCK_REASON, Mock, patch ensure_in_syspath('../../')
import integration import salt.utils
self.skipTest('Unable to get the SHELL environment variable')
import pwd runas = pwd.getpwuid(os.getuid())[0]
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
from __future__ import absolute_import import os
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath, requires_system_grains ) ensure_in_syspath('../../')
import integration
if os_grain['kernel'] not in 'Darwin': self.skipTest( 'Test not applicable to \'{kernel}\' kernel'.format( **os_grain ) )
from __future__ import absolute_import
from salttesting import skipIf from salttesting.helpers import ( ensure_in_syspath, requires_network ) ensure_in_syspath('../../')
import integration
from __future__ import absolute_import, print_function
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
from __future__ import absolute_import import os import random
import integration import salt.utils from salt.exceptions import CommandExecutionError
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath, requires_system_grains ) ensure_in_syspath('../../')
ASSIGN_CMD = 'net.inet.icmp.icmplim' CONFIG = '/etc/sysctl.conf'
self.has_conf = False self.val = self.run_function('sysctl.get', [ASSIGN_CMD])
if os.path.isfile(CONFIG): os.remove(CONFIG)
os.remove(self.conf)
self.__restore_sysctl()
os.remove(CONFIG)
from __future__ import absolute_import import os import time
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
time.sleep(20) ret = self.run_function('grains.item', ['setgrain'])
from __future__ import absolute_import import os import shutil
from salttesting.helpers import ensure_in_syspath, skip_if_binaries_missing ensure_in_syspath('../../')
import integration import salt.utils
from __future__ import absolute_import
from salttesting.helpers import ( ensure_in_syspath, skip_if_not_root, skip_if_binaries_missing ) from salttesting import skipIf ensure_in_syspath('../../')
import integration
import salt.ext.six as six
self.run_function('cmd.run', ['truncate -s 0 {0}'.format(f)])
from __future__ import absolute_import import os import string import random
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath, requires_system_grains ) ensure_in_syspath('../../')
import salt.utils import integration
@skipIf(not salt.utils.is_linux(), 'These tests can only be run on linux') class UseraddModuleTest(integration.ModuleCase):
uid = uinfo['uid']
from __future__ import absolute_import import os
from salt.modules import beacons from salt.exceptions import CommandExecutionError import integration
from salttesting import skipIf from salttesting.helpers import destructiveTest, ensure_in_syspath
_save = self.run_function('beacons.save') self.assertTrue(_save['result'])
_delete = self.run_function('beacons.delete', ['ps']) self.assertTrue(_delete['result'])
self.run_function('beacons.save')
self.run_function('beacons.add', ['ps', {'apache2': 'stopped'}]) self.run_function('beacons.save')
self.run_function('beacons.delete', ['ps']) self.run_function('beacons.save')
_list = self.run_function('beacons.list', return_yaml=False) self.assertIn('ps', _list)
_list = self.run_function('beacons.list', return_yaml=False) self.assertFalse(_list['enabled'])
ret = self.run_function('beacons.disable_beacon', ['ps']) self.assertTrue(ret['result'])
_list = self.run_function('beacons.list', return_yaml=False) self.assertFalse(_list['ps']['enabled'])
_list = self.run_function('beacons.list', return_yaml=False) self.assertIn('ps', _list)
ret = self.run_function('beacons.enable') self.assertTrue(ret['result'])
_list = self.run_function('beacons.list', return_yaml=False) self.assertTrue(_list['enabled'])
ret = self.run_function('beacons.enable_beacon', ['ps']) self.assertTrue(ret['result'])
_list = self.run_function('beacons.list', return_yaml=False) self.assertTrue(_list['ps']['enabled'])
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
from __future__ import absolute_import import os
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath, requires_system_grains ) ensure_in_syspath('../../')
import integration
self.run_function('assistive.install', [OSA_SCRIPT, True])
osa_script = self.run_function('assistive.installed', [OSA_SCRIPT]) if osa_script: self.run_function('assistive.remove', [OSA_SCRIPT])
from __future__ import absolute_import import os import shutil
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils
self.__clear_hosts() f = salt.utils.fopen(HFN, 'w') f.close()
from __future__ import absolute_import
from salttesting import skipIf from salttesting.helpers import ( ensure_in_syspath, requires_salt_modules, requires_system_grains, destructiveTest, ) ensure_in_syspath('../../')
import integration import salt.utils
from __future__ import absolute_import import os import random import string
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath, requires_system_grains ) ensure_in_syspath('../../')
import integration from salt.exceptions import CommandExecutionError
ADD_GROUP = __random_string() DEL_GROUP = __random_string() CHANGE_GROUP = __random_string() ADD_USER = __random_string() REP_USER_GROUP = __random_string()
ret = self.run_function('group.delete', [DEL_GROUP]) self.assertTrue(ret)
add_info = self.run_function('group.info', [ADD_GROUP]) if add_info: self.run_function('group.delete', [ADD_GROUP])
del_info = self.run_function('group.info', [DEL_GROUP]) if del_info: self.run_function('group.delete', [DEL_GROUP])
change_info = self.run_function('group.info', [CHANGE_GROUP]) if change_info: self.run_function('group.delete', [CHANGE_GROUP])
from __future__ import absolute_import import datetime import random import string
from salttesting.helpers import ensure_in_syspath, destructiveTest from salt.ext.six.moves import range ensure_in_syspath('../../')
import integration import salt.utils
ret = self.run_function('shadow.info', [TEST_USER]) self.assertEqual(ret['name'], TEST_USER)
ret = self.run_function('shadow.info', [NO_USER]) self.assertEqual(ret['name'], '')
self.assertEqual( self.run_function('shadow.get_account_created', [NO_USER]), 'ERROR: User not found: {0}'.format(NO_USER))
self.assertEqual( self.run_function('shadow.get_last_change', [NO_USER]), 'ERROR: User not found: {0}'.format(NO_USER))
self.assertEqual( self.run_function('shadow.get_login_failed_last', [NO_USER]), 'ERROR: User not found: {0}'.format(NO_USER))
self.assertEqual( self.run_function('shadow.get_login_failed_count', [TEST_USER]), '0')
self.assertEqual( self.run_function('shadow.get_login_failed_count', [NO_USER]), 'ERROR: User not found: {0}'.format(NO_USER))
self.assertTrue( self.run_function('shadow.set_maxdays', [TEST_USER, 20])) self.assertEqual( self.run_function('shadow.get_maxdays', [TEST_USER]), 20)
self.assertEqual( self.run_function('shadow.del_password', [NO_USER]), 'ERROR: User not found: {0}'.format(NO_USER))
self.assertTrue( self.run_function('shadow.set_password', [TEST_USER, 'Pa$$W0rd']))
self.assertEqual( self.run_function('shadow.set_password', [NO_USER, 'P@SSw0rd']), 'ERROR: User not found: {0}'.format(NO_USER))
from salttesting.helpers import ( destructiveTest, requires_network, requires_salt_modules, ensure_in_syspath ) ensure_in_syspath('../../')
import integration
from __future__ import absolute_import import logging
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath ) ensure_in_syspath('../../')
import integration from salt.modules import mysql as mysqlmod
import salt.ext.six as six
db_name = 'foo 1' self._db_creation_loop(db_name=db_name, returning_name=db_name, test_conn=True, connection_user=self.user, connection_pass=self.password )
db_name = "foo'3" self._db_creation_loop(db_name=db_name, returning_name=db_name, test_conn=True, character_set='utf8', connection_user=self.user, connection_pass=self.password )
ret = self.run_function( 'mysql.db_remove', name=dbname, connection_user=self.user, connection_pass=self.password ) self.assertEqual(True, ret)
password_hash='*EEF6F854748ACF841226BB1C2422BEC70AE7F1FF', new_password_hash=user2_pwd_hash, connection_user=self.user, connection_pass=self.password, connection_charset='utf8', saltenv={"LC_ALL": "en_US.utf8"}
from __future__ import absolute_import import os import string import logging
from salttesting.helpers import ensure_in_syspath, requires_salt_modules from salttesting import skipIf ensure_in_syspath('../../')
import integration
from __future__ import absolute_import import re
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
import salt.ext.six as six
funcs = self.run_function('sys.list_functions') self.assertIn('hosts.list_hosts', funcs) self.assertIn('pkg.install', funcs)
from __future__ import absolute_import import os import random import string
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath, requires_system_grains ) ensure_in_syspath('../../')
import integration from salt.exceptions import CommandExecutionError
ADD_USER = __random_string() DEL_USER = __random_string() PRIMARY_GROUP_USER = __random_string() CHANGE_USER = __random_string()
ret = self.run_function('user.delete', [DEL_USER]) self.assertTrue(ret)
add_info = self.run_function('user.info', [ADD_USER]) if add_info: self.run_function('user.delete', [ADD_USER])
del_info = self.run_function('user.info', [DEL_USER]) if del_info: self.run_function('user.delete', [DEL_USER])
change_info = self.run_function('user.info', [CHANGE_USER]) if change_info: self.run_function('user.delete', [CHANGE_USER])
from __future__ import absolute_import import os import shutil
from salttesting import skipIf from salttesting.helpers import (ensure_in_syspath, destructiveTest) ensure_in_syspath('../../')
import integration import salt.utils
import salt.ext.six as six
if os.path.isfile('/etc/mtab'): shutil.move('/etc/mtab', '/tmp/mtab')
from __future__ import absolute_import, print_function import random import string
from salttesting.helpers import ensure_in_syspath, destructiveTest from salt.ext.six.moves import range ensure_in_syspath('../../')
import integration import salt.utils
self.assertIn( 'Invalid String Value for Enabled', self.run_function('system.set_remote_login', ['spongebob']))
self.assertIn( 'Invalid String Value for Enabled', self.run_function('system.set_remote_events', ['spongebob']))
ret = self.run_function('system.list_startup_disks') self.assertIsInstance(ret, list) self.assertIn(self.run_function('system.get_startup_disk'), ret)
self.assertIn( 'Invalid value passed for path.', self.run_function('system.set_startup_disk', ['spongebob']))
self.assertTrue(self.run_function('system.set_restart_delay', [90])) self.assertEqual( self.run_function('system.get_restart_delay'), '90 seconds')
self.assertIn( 'Invalid value passed for seconds.', self.run_funcdtion('system.set_restart_delay', [70]))
self.assertTrue( self.run_function('system.set_disable_keyboard_on_lock', [True])) self.assertTrue( self.run_function('system.get_disable_keyboard_on_lock'))
self.assertIn( 'Invalid String Value for Enabled', self.run_function('system.set_disable_keyboard_on_lock', ['spongebob']))
self.assertIn( 'Invalid value passed for arch', self.run_function('system.set_boot_arch', ['spongebob']))
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
from __future__ import absolute_import, print_function
from salttesting.helpers import ensure_in_syspath, destructiveTest ensure_in_syspath('../../')
import integration import salt.utils
self.assertIn( 'Service not found', self.run_function('service.show', ['spongebob']))
self.assertIn( ' Failed to error service', self.run_function('service.launchctl', ['error']))
self.assertIn( 'Service not found', self.run_function('service.list', ['spongebob']))
self.assertEqual('', self.run_function('service.status', ['spongebob']))
from __future__ import absolute_import import os
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath, requires_salt_modules
import integration
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
from __future__ import absolute_import, print_function
from salttesting.helpers import ensure_in_syspath, destructiveTest ensure_in_syspath('../../')
import integration import salt.utils
from __future__ import absolute_import import os import time import subprocess
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils from salt.modules.virtualenv_mod import KNOWN_BINARY_NAMES
self.assertIn('sleep_service: started', ret) self.assertIn('sleep_service2: started', ret)
from __future__ import absolute_import
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
NO_BOTO_MODULE = True BOTO_NOT_CONFIGURED = True try: import boto NO_BOTO_MODULE = False try: boto.connect_iam() BOTO_NOT_CONFIGURED = False except boto.exception.NoAuthHandlerFound: pass except ImportError: pass
self.assertRegexpMatches(ret, r'^\d{12}$')
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.version from salt import config
from __future__ import absolute_import import time import threading
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration from salt.utils import event
from __future__ import absolute_import import getpass import grp import pwd import os import shutil import sys
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils from salt.modules import file as filemod
os.makedirs(self.mydir)
from __future__ import absolute_import
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch ensure_in_syspath('../../')
import integration from salt.modules import djangomod as django
from __future__ import absolute_import import os import string import random
from salttesting import skipIf from salttesting.helpers import destructiveTest, ensure_in_syspath ensure_in_syspath('../../')
import integration
uid = uinfo['uid']
from __future__ import absolute_import import sys
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
from __future__ import absolute_import import os
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath, requires_system_grains ) ensure_in_syspath('../../')
import integration from salt.exceptions import CommandExecutionError
if os_grain['kernel'] not in 'Darwin': self.skipTest( 'Test not applicable to \'{kernel}\' kernel'.format( **os_grain ) )
certs_list = self.run_function('keychain.list_certs') if CERT_ALIAS in certs_list: self.run_function('keychain.uninstall', [CERT_ALIAS])
certs_list = self.run_function('keychain.list_certs') self.assertIn(CERT_ALIAS, certs_list)
self.run_function('keychain.uninstall', [CERT_ALIAS]) certs_list = self.run_function('keychain.list_certs')
try: self.assertNotIn(CERT_ALIAS, str(certs_list)) except CommandExecutionError: self.run_function('keychain.uninstall', [CERT_ALIAS])
from __future__ import absolute_import import os import tempfile
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils from salt.modules.virtualenv_mod import KNOWN_BINARY_NAMES
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath, destructiveTest ensure_in_syspath('../../')
import integration import salt.utils
self.assertIsInstance( self.run_function('softwareupdate.list_available'), dict)
self.assertTrue(self.run_function('softwareupdate.reset_ignored')) self.assertEqual(self.run_function('softwareupdate.list_ignored'), [])
self.assertIn( 'spongebob', self.run_function('softwareupdate.list_ignored')) self.assertIn( 'squidward', self.run_function('softwareupdate.list_ignored'))
self.assertTrue( self.run_function('softwareupdate.schedule_enable', [True])) self.assertTrue(self.run_function('softwareupdate.schedule_enabled'))
self.assertTrue( self.run_function('softwareupdate.schedule_enable', [False])) self.assertFalse(self.run_function('softwareupdate.schedule_enabled'))
self.assertIsInstance( self.run_function('softwareupdate.update_all'), dict)
self.assertFalse( self.run_function('softwareupdate.update_available', ['spongebob']))
self.assertIn( 'Update not available', self.run_function('softwareupdate.update', ['spongebob']))
self.assertIn( 'Update not available', self.run_function('softwareupdate.download', ['spongebob']))
self.assertTrue(self.run_function('softwareupdate.reset_catalog')) self.assertEqual(self.run_function('softwareupdate.get_catalog'), 'Default')
self.assertTrue(self.run_function('softwareupdate.reset_catalog')) self.assertEqual(self.run_function('softwareupdate.get_catalog'), 'Default')
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
from __future__ import absolute_import import os import shutil import textwrap
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils from salt.modules.virtualenv_mod import KNOWN_BINARY_NAMES
import salt.ext.six as six
if [ -z "$debian_chroot" ] && [ -r /etc/debian_chroot ]; then debian_chroot=$(cat /etc/debian_chroot) fi
ret = self.run_function('state.sls', mods='testappend.step-2') self.assertSaltTrueReturn(ret)
if [ -z "$debian_chroot" ] && [ -r /etc/debian_chroot ]; then debian_chroot=$(cat /etc/debian_chroot) fi
if os.path.isfile(testfile): os.unlink(testfile)
ret = self.run_function('state.sls', mods='issue-1879', timeout=120) self.assertSaltTrueReturn(ret)
ret = self.run_function( 'state.sls', mods='issue-1879.step-1', timeout=120 ) self.assertSaltTrueReturn(ret)
ret = self.run_function( 'state.sls', mods='issue-1879.step-2', timeout=120 ) self.assertSaltTrueReturn(ret)
#)
#])
#])
state_run = self.run_function('state.sls', mods='requisites.onchanges_simple')
test_data = state_run['cmd_|-test_changing_state_|-echo "Success!"_|-run']['comment'] expected_result = 'Command "echo "Success!"" run' self.assertIn(expected_result, test_data)
state_run = self.run_function('state.sls', mods='requisites.onchanges_multiple')
test_data = state_run['cmd_|-test_two_changing_states_|-echo "Success!"_|-run']['comment'] expected_result = 'Command "echo "Success!"" run' self.assertIn(expected_result, test_data)
test_data = state_run['cmd_|-test_one_changing_state_|-echo "Success!"_|-run']['comment'] expected_result = 'Command "echo "Success!"" run' self.assertIn(expected_result, test_data)
state_run = self.run_function('state.sls', mods='requisites.onchanges_in_simple')
test_data = state_run['cmd_|-test_changes_expected_|-echo "Success!"_|-run']['comment'] expected_result = 'Command "echo "Success!"" run' self.assertIn(expected_result, test_data)
state_run = self.run_function('state.sls', mods='requisites.onfail_simple')
test_data = state_run['cmd_|-test_failing_state_|-echo "Success!"_|-run']['comment'] expected_result = 'Command "echo "Success!"" run' self.assertIn(expected_result, test_data)
test_data = state_run['cmd_|-test_non_failing_state_|-echo "Should not run"_|-run']['comment'] expected_result = 'State was not run because onfail req did not change' self.assertIn(expected_result, test_data)
state_run = self.run_function('state.sls', mods='requisites.onfail_in_simple')
test_data = state_run['cmd_|-test_failing_state_|-echo "Success!"_|-run']['comment'] expected_result = 'Command "echo "Success!"" run' self.assertIn(expected_result, test_data)
test_data = state_run['cmd_|-test_non_failing_state_|-echo "Should not run"_|-run']['comment'] expected_result = 'State was not run because onfail req did not change' self.assertIn(expected_result, test_data)
state_run = self.run_function('state.sls', mods='requisites.listen_simple')
listener_state = 'cmd_|-listener_test_listening_change_state_|-echo "Listening State"_|-mod_watch' self.assertIn(listener_state, state_run)
absent_state = 'cmd_|-listener_test_listening_non_changing_state_|-echo "Only run once"_|-mod_watch' self.assertNotIn(absent_state, state_run)
state_run = self.run_function('state.sls', mods='requisites.listen_in_simple')
listener_state = 'cmd_|-listener_test_listening_change_state_|-echo "Listening State"_|-mod_watch' self.assertIn(listener_state, state_run)
absent_state = 'cmd_|-listener_test_listening_non_changing_state_|-echo "Only run once"_|-mod_watch' self.assertNotIn(absent_state, state_run)
state_run = self.run_function('state.sls', mods='requisites.listen_in_simple')
listener_state = 'cmd_|-listener_test_listen_in_resolution_|-echo "Successful listen_in resolution"_|-mod_watch' self.assertIn(listener_state, state_run)
state_run = self.run_function('state.sls', mods='requisites.listen_simple')
listener_state = 'cmd_|-listener_test_listening_resolution_one_|-echo "Successful listen resolution"_|-mod_watch' self.assertIn(listener_state, state_run)
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
self.assertTrue( self.run_function( 'runtests_decorators.booldependsTrue' ) )
self.assertIn( 'is not available', self.run_function('runtests_decorators.booldependsFalse' ) )
from __future__ import absolute_import import os import pwd import shutil import re import tempfile
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils from salt.modules.virtualenv_mod import KNOWN_BINARY_NAMES
self.run_function('virtualenv.create', [self.venv_dir])
req_basepath = (self.venv_dir)
from __future__ import absolute_import import os import shutil
from salttesting.helpers import ensure_in_syspath, skip_if_binaries_missing import salt.utils ensure_in_syspath('../../')
import integration import salt.utils
from __future__ import absolute_import, print_function
from salttesting.helpers import ensure_in_syspath, destructiveTest ensure_in_syspath('../../')
import integration import salt.utils
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
from __future__ import absolute_import import os import hashlib
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath, requires_salt_modules ensure_in_syspath('../../')
import integration
from __future__ import absolute_import from contextlib import closing import errno import logging import os import re import shutil import subprocess import tarfile import tempfile
from distutils.version import LooseVersion from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath, skip_if_binaries_missing ) ensure_in_syspath('../..')
import integration
if exc.errno != errno.EEXIST: raise
os.chdir(self.repo) subprocess.check_call(['git', 'init', '--quiet', self.repo])
shutil.rmtree(clone_parent_dir)
self.assertTrue( self.run_function( 'git.clone', [clone_parent_dir, self.repo], name=clone_name ) ) shutil.rmtree(clone_parent_dir)
ret = self.run_function( 'git.merge', [self.repo], rev=self.branches[1] ) self.assertTrue('Fast-forward' in ret.splitlines())
self.assertEqual( self.run_function( 'git.rev_parse', [self.repo, 'HEAD'], opts='--abbrev-ref' ), 'master' )
self.assertTrue( 'ERROR' not in self.run_function( 'git.add', [self.repo, filename] ) )
from __future__ import absolute_import import datetime
from salttesting.helpers import ensure_in_syspath, destructiveTest ensure_in_syspath('../../')
import integration import salt.utils
self.assertTrue(self.run_function('timezone.set_time', ['3:14']))
self.assertEqual( self.run_function('timezone.set_zone', ['spongebob']), 'ERROR executing \'timezone.set_zone\': ' 'Invalid Timezone: spongebob')
from __future__ import absolute_import import os
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath, requires_system_grains ) ensure_in_syspath('../../')
import integration import salt.utils from salt.exceptions import CommandExecutionError
ADD_PKG = 'algol68g' DEL_PKG = 'acme'
self.run_function('pkg.remove', [DEL_PKG]) del_list = self.run_function('pkg.list_pkgs') try: self.assertNotIn(DEL_PKG, del_list) except AssertionError: raise
if ADD_PKG in pkg_list: self.run_function('pkg.remove', [ADD_PKG]) if DEL_PKG in pkg_list: self.run_function('pkg.remove', [DEL_PKG])
from __future__ import absolute_import import os
from salttesting.helpers import ensure_in_syspath, destructiveTest ensure_in_syspath('../../')
import integration import salt.utils
self.assertTrue( self.run_function('pkgutil.is_installed', ['com.apple.pkg.BaseSystemResources']))
self.assertFalse( self.run_function('pkgutil.is_installed', ['spongebob']))
self.assertFalse( self.run_function('pkgutil.is_installed', [TEST_PKG_NAME]))
self.run_function('cp.get_url', [TEST_PKG_URL, TEST_PKG])
self.assertTrue(self.run_function('pkgutil.forget', [TEST_PKG_NAME]))
from __future__ import absolute_import import re
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
NO_BOTO_MODULE = True BOTO_NOT_CONFIGURED = True try: import boto NO_BOTO_MODULE = False try: boto.connect_iam() BOTO_NOT_CONFIGURED = False except boto.exception.NoAuthHandlerFound: pass except ImportError: pass
from __future__ import absolute_import import os
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath, requires_system_grains )
import integration
self.run_function('desktop.set_output_volume', [current_vol])
from __future__ import absolute_import import random import string from salt.ext.six.moves import range
from __future__ import absolute_import from unittest2 import skipIf from integration.cloud.helpers import random_name from salt.utils import virtualbox import json import logging import os import unittest import integration
log.debug("running salt-cloud with %s", arg_str) output = self.run_script('salt-cloud', arg_str, catch_stderr, timeout=timeout)
if isinstance(output, tuple) and len(output) == 2: output = output[0]
if kw_function_args: args = [ "{0}='{1}'".format(key, value) for key, value in kw_function_args.iteritems() ]
from __future__ import absolute_import import os import random import string
from salttesting.helpers import ensure_in_syspath, expensiveTest from salttesting import skipIf
import integration from salt.config import cloud_providers_config from salt.ext.six.moves import range
INSTANCE_NAME = __random_name() PROVIDER_NAME = 'gogrid'
config = cloud_providers_config( os.path.join( integration.FILES, 'conf', 'cloud.providers.d', PROVIDER_NAME + '.conf' ) )
if ret_str in query: self.run_cloud('-d {0} --assume-yes'.format(INSTANCE_NAME))
from __future__ import absolute_import import os import random import string
import integration from salt.config import cloud_providers_config
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath, expensiveTest
INSTANCE_NAME = __random_name() PROVIDER_NAME = 'ec2'
profile_str = 'ec2-config' providers = self.run_cloud('--list-providers')
config = cloud_providers_config( os.path.join( integration.FILES, 'conf', 'cloud.providers.d', PROVIDER_NAME + '.conf' ) )
instance = self.run_cloud('-p ec2-test {0}'.format(INSTANCE_NAME)) ret_str = '{0}:'.format(INSTANCE_NAME)
try: self.assertIn(ret_str, instance) except AssertionError: self.run_cloud('-d {0} --assume-yes'.format(INSTANCE_NAME)) raise
delete = self.run_cloud('-d {0} --assume-yes'.format(INSTANCE_NAME)) ret_str = ' shutting-down'
try: self.assertIn(ret_str, delete) except AssertionError: raise
if ret_str in query: self.run_cloud('-d {0} --assume-yes'.format(INSTANCE_NAME))
from __future__ import absolute_import import os import random import string import time
from salttesting.helpers import ensure_in_syspath, expensiveTest
import integration from salt.config import cloud_providers_config
INSTANCE_NAME = __random_name() PROVIDER_NAME = 'vultr'
config = cloud_providers_config( os.path.join( integration.FILES, 'conf', 'cloud.providers.d', PROVIDER_NAME + '.conf' ) )
from __future__ import absolute_import import os import random import string from distutils.version import LooseVersion
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath, expensiveTest
import integration from salt.config import cloud_providers_config
from salt.ext.six.moves import range
INSTANCE_NAME = __random_name() PROVIDER_NAME = 'azure' PROFILE_NAME = 'azure-test' REQUIRED_AZURE = '0.11.1'
if ret_str in query: self.run_cloud('-d {0} --assume-yes'.format(INSTANCE_NAME))
from __future__ import absolute_import import os import random import string
from salttesting.helpers import ensure_in_syspath, expensiveTest
import integration from salt.config import cloud_providers_config from salt.ext.six.moves import range
INSTANCE_NAME = __random_name() PROVIDER_NAME = 'linode'
config = cloud_providers_config( os.path.join( integration.FILES, 'conf', 'cloud.providers.d', PROVIDER_NAME + '.conf' ) )
if ret_str in query: self.run_cloud('-d {0} --assume-yes'.format(INSTANCE_NAME))
from __future__ import absolute_import import os import random import string
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath, expensiveTest
import integration from salt.config import cloud_providers_config from salt.ext.six.moves import range
try:
INSTANCE_NAME = __random_name() PROVIDER_NAME = 'profitbricks' DRIVER_NAME = 'profitbricks'
config = cloud_providers_config( os.path.join( integration.FILES, 'conf', 'cloud.providers.d', PROVIDER_NAME + '.conf' ) )
if ret in query: self.run_cloud('-d {0} --assume-yes'.format(INSTANCE_NAME))
from __future__ import absolute_import import os import random import string
from salttesting.helpers import ensure_in_syspath, expensiveTest
import integration from salt.config import cloud_providers_config
INSTANCE_NAME = __random_name() PROVIDER_NAME = 'joyent'
config = cloud_providers_config( os.path.join( integration.FILES, 'conf', 'cloud.providers.d', PROVIDER_NAME + '.conf' ) )
if ret_str in query: self.run_cloud('-d {0} --assume-yes'.format(INSTANCE_NAME))
from __future__ import absolute_import import os import random import string
import integration from salt.config import cloud_providers_config
from salttesting.helpers import ensure_in_syspath, expensiveTest
profile_str = 'gce-config:' provider = 'gce' providers = self.run_cloud('--list-providers') self.INSTANCE_NAME = _random_name()
path = os.path.join(integration.FILES, 'conf', 'cloud.providers.d', provider + '.conf') config = cloud_providers_config(path)
instance = self.run_cloud('-p gce-test {0}'.format(self.INSTANCE_NAME)) ret_str = '{0}:'.format(self.INSTANCE_NAME)
try: self.assertIn(ret_str, instance) except AssertionError: self.run_cloud('-d {0} --assume-yes'.format(self.INSTANCE_NAME)) raise
delete = self.run_cloud('-d {0} --assume-yes'.format(self.INSTANCE_NAME)) delete_str = ''.join(delete)
try: self.assertIn(self.INSTANCE_NAME, delete_str) self.assertIn('True', delete_str) except AssertionError: raise
instance = self.run_cloud('-p gce-test-extra {0}'.format(self.INSTANCE_NAME)) ret_str = '{0}:'.format(self.INSTANCE_NAME)
try: self.assertIn(ret_str, instance) except AssertionError: self.run_cloud('-d {0} --assume-yes'.format(self.INSTANCE_NAME)) raise
delete = self.run_cloud('-d {0} --assume-yes'.format(self.INSTANCE_NAME)) delete_str = ''.join(delete)
try: self.assertIn(self.INSTANCE_NAME, delete_str) self.assertIn('True', delete_str) except AssertionError: raise
query = self.run_cloud('--query') ret_str = ' {0}:'.format(self.INSTANCE_NAME)
if ret_str in query: self.run_cloud('-d {0} --assume-yes'.format(self.INSTANCE_NAME))
from __future__ import absolute_import import os import random import string
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath, expensiveTest
import integration from salt.config import cloud_providers_config from salt.ext.six.moves import range
try:
INSTANCE_NAME = __random_name() PROVIDER_NAME = 'rackspace' DRIVER_NAME = 'openstack'
config = cloud_providers_config( os.path.join( integration.FILES, 'conf', 'cloud.providers.d', PROVIDER_NAME + '.conf' ) )
if ret in query: self.run_cloud('-d {0} --assume-yes'.format(INSTANCE_NAME))
from __future__ import absolute_import from salt.ext.six.moves import range
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath
import integration from salt.config import cloud_providers_config, vm_profiles_config from utils.virtualbox import vb_xpcom_to_attribute_dict, vb_clone_vm, vb_destroy_machine, vb_create_machine, \ vb_get_box, vb_machine_exists, XPCOM_ATTRIBUTES, vb_start_vm, vb_stop_vm, \ vb_get_network_addresses, vb_wait_for_network_address, machine_get_machinestate_str
log = logging.getLogger() log = logging.getLogger(__name__) info = log.info
MINIMAL_MACHINE_ATTRIBUTES = [ "id", "image", "size", "state", "private_ips", "public_ips", ]
profile_str = 'virtualbox-config' providers = self.run_cloud('--list-providers') log.debug("providers: %s", providers)
self.test_cloud_create() ret = self.run_cloud_destroy(INSTANCE_NAME)
self.assertIn(INSTANCE_NAME, ret.keys())
provider_str = CONFIG_NAME providers = self.run_cloud('--list-providers') log.debug("providers: %s", providers)
ip_addresses = vb_get_network_addresses(machine_name=BOOTABLE_BASE_BOX_NAME)
vb_start_vm(BOOTABLE_BASE_BOX_NAME) ip_addresses = vb_wait_for_network_address(20, machine_name=BOOTABLE_BASE_BOX_NAME) network_count = len(ip_addresses) self.assertGreater(network_count, 0)
from __future__ import absolute_import import os import random import string
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath, expensiveTest
import integration from salt.config import cloud_providers_config
INSTANCE_NAME = __random_name() PROVIDER_NAME = 'digital_ocean'
config = cloud_providers_config( os.path.join( integration.FILES, 'conf', 'cloud.providers.d', PROVIDER_NAME + '.conf' ) )
self.assertIn( finger_print, [i.strip() for i in _key] )
list_keypairs = self.run_cloud('-f list_keypairs {0}'.format(PROVIDER_NAME))
show_keypair = self.run_cloud('-f show_keypair {0} keyname={1}'.format(PROVIDER_NAME, 'MyPubKey'))
self.run_cloud('-f remove_key {0} id={1}'.format(PROVIDER_NAME, finger_print)) raise
self.assertTrue(self.run_cloud('-f remove_key {0} id={1}'.format(PROVIDER_NAME, finger_print)))
try: self.assertIn( 'True', [i.strip() for i in self.run_cloud('-d {0} --assume-yes'.format(INSTANCE_NAME))] ) except AssertionError: raise
if INSTANCE_NAME in [i.strip() for i in self.run_cloud('--query')]: self.run_cloud('-d {0} --assume-yes'.format(INSTANCE_NAME))
from __future__ import absolute_import import integration
import salt.wheel
from __future__ import absolute_import import integration
import salt.auth import salt.wheel
from __future__ import absolute_import
from salttesting.unit import skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath
import salt.utils.event
e = salt.utils.event.get_event('minion', sock_dir=self.minion_opts['sock_dir'], opts=self.minion_opts)
from __future__ import absolute_import import os import sys import re import shutil import yaml from datetime import datetime import logging
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils from salttesting.helpers import ( destructiveTest )
if os.path.isfile(logfile): os.unlink(logfile)
ellapsed = datetime.now() - start timeout = ellapsed.seconds + 3
if os.path.isfile(this_minion_key): os.unlink(this_minion_key)
self.assertIn( 'Failed to setup the Syslog logging handler', '\n'.join(ret[1]) ) self.assertEqual(ret[2], 2)
ret = self.run_script( 'salt-call', '-c {0} --output-file={1} test.versions'.format( self.get_config_dir(), output_file_append ), catch_stderr=True, with_retcode=True )
self.run_script( 'salt-call', '-c {0} --output-file={1} -g'.format( self.get_config_dir(), output_file ), catch_stderr=True, with_retcode=True ) stat1 = os.stat(output_file)
os.umask(0o777)
self.assertTrue(stat1.st_size < stat2.st_size)
os.unlink(output_file)
os.umask(current_umask)
from __future__ import absolute_import import os import sys import getpass import platform import yaml import signal import shutil import tempfile import logging
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration from integration.utils import testprogram import salt.utils import salt.defaults.exitcodes
self._test_dir = tempfile.mkdtemp(prefix='salt-testdaemon-')
if self._test_dir and os.path.sep == self._test_dir[0]: shutil.rmtree(self._test_dir) self._test_dir = None
minion.setup() _minions.append(minion)
defaults.write( 'TIMEOUT=60\n' 'TICK=1\n' )
ret = self._run_initscript(init_script, minions, False, 'bogusaction', 2)
for minion in minions: minion.shutdown()
from __future__ import absolute_import import os import yaml import shutil import time
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils
subnet = yaml_data['minion'][0]
self.assertIn( 'Failed to setup the Syslog logging handler', '\n'.join(ret[1]) ) self.assertEqual(ret[2], 2)
from __future__ import absolute_import import os import yaml import shutil
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils
raise
from __future__ import absolute_import, print_function
from salttesting.unit import skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../')
test_options.pop(0) if len(test_options) <= 1: break
test_options.pop(0) if len(test_options) <= 1: break
from __future__ import absolute_import import os import pwd import grp import random
from salttesting import skipIf from salttesting.helpers import ( ensure_in_syspath, destructiveTest) ensure_in_syspath('../../')
import salt.utils from salt.utils.pycrypto import gen_hash import integration
for _ in range(20): next_index = random.randrange(len(alphabet)) password += alphabet[next_index]
hashed_pwd = gen_hash('salt', password, 'sha512')
set_pw_cmd = "shadow.set_password {0} '{1}'".format( self.userA, password if salt.utils.is_darwin() else hashed_pwd ) self.run_call(set_pw_cmd)
set_pw_cmd = "shadow.set_password {0} '{1}'".format( self.userB, password if salt.utils.is_darwin() else hashed_pwd ) self.run_call(set_pw_cmd)
from __future__ import absolute_import import os import textwrap
from salttesting.helpers import ensure_in_syspath
import integration import salt.utils
disabled_ret = ('first second third | wc -l ; export SALTY_VARIABLE=saltines ' '&& echo $SALTY_VARIABLE ; echo duh &> /dev/null') ret_key = 'test_|-shell_enabled_|-{0}_|-configurable_test_state'.format(disabled_ret)
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
from __future__ import absolute_import import os import yaml import signal import shutil
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils
from __future__ import absolute_import import os import yaml import shutil import tempfile
from salttesting.helpers import ensure_in_syspath from salttesting import skipIf ensure_in_syspath('../../')
import integration import salt.utils
from __future__ import absolute_import import os import yaml import pipes import shutil
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils
import salt.ext.six as six
old_cwd = None
raise
from __future__ import absolute_import import os import yaml import signal import shutil
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath, requires_salt_modules
import integration
from __future__ import absolute_import import inspect import tempfile import shutil import os import collections
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath
import salt.ext.six as six from salt.ext.six.moves import range from salt.config import minion_config
self.loader = LazyLoader([self.module_dir], self.opts, tag='module')
self.assertTrue( inspect.isfunction( self.loader[self.module_name + '.loaded'] ) ) self.assertTrue(self.module_name + '.not_loaded' not in self.loader)
self.assertEqual(self.loader._dict, {}) self.assertTrue(inspect.isfunction(self.loader['test.ping']))
for key, val in six.iteritems(self.loader._dict): self.assertEqual(key.split('.', 1)[0], 'test')
self.assertFalse('test.missing_func' in self.loader._dict)
for key, func in six.iteritems(self.loader): break self.assertNotEqual(self.loader._dict, {})
for key, val in six.iteritems(func_globals['__opts__']): self.assertEqual(self.opts[key], val)
self.assertNotIn(self.module_key, self.loader)
self.assertTrue(inspect.isfunction(self.loader[self.module_key]))
for k, v in six.iteritems(self.loader._dict): self.assertTrue(k.startswith(self.module_name))
self.assertNotIn(self.module_key, self.loader)
self.assertNotIn(self.module_key + '2', self.loader)
self.assertNotIn(self.module_key + '3', self.loader) self.assertNotIn(self.module_key + '4', self.loader)
self.assertNotIn(self.module_key, self.loader)
for x in range(1, 3): self.update_module() self.loader.clear() self.assertEqual(self.loader[self.module_key](), self.count)
self.assertEqual(self.loader[self.module_key](), self.count) self.loader.clear() self.assertNotIn(self.module_key, self.loader)
self.assertNotIn(self.module_key, self.loader)
self.assertNotIn(self.module_key, self.loader)
self.assertNotIn(self.module_key, self.loader)
self.update_module() self.update_lib() self.loader.clear() self.assertEqual(self.loader[self.module_key](), (self.count, self.lib_count))
self.rm_lib() self.loader.clear() self.assertNotIn(self.module_key, self.loader)
with open(os.path.join(self.module_dir, '__init__.py'), 'w') as fh: fh.write(deep_init_base) fh.flush()
for lib in self.libs: for x in xrange(5): self.update_lib(lib) self.loader.clear() self._verify_libs()
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath from salttesting.unit import skipIf ensure_in_syspath('../')
import integration from salt.config import minion_config
self.assertEqual({'k2': 'v2'}, grains['a_custom'])
self.assertIn('a_custom', __grain__) self.assertEqual({'k1': 'v1', 'k2': 'v2'}, __grain__['a_custom'])
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../')
import integration
self.assertIn('test.ping', funcs)
self.assertNotIn('brain.left_hemisphere', funcs)
self.assertIn('test.recho', funcs)
from __future__ import absolute_import
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath
from salt.config import minion_config
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath
import integration import salt.loader import inspect import yaml
import salt.ext.six as six
self.assertNotEqual(global_vars, [], msg='No modules were loaded.')
func_name = inspect.stack()[1][3] names = next(six.itervalues(yaml.load(getattr(self, func_name).__doc__)))
for item in global_vars: for name in names: self.assertIn(name, list(item.keys()))
from __future__ import absolute_import
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
ret = self.run_run_plus(fun='fileserver.dir_list', args=['backend="roots"']) self.assertIsInstance(ret['fun'], list)
ret = self.run_run_plus(fun='fileserver.dir_list', args=['backend="[roots]"']) self.assertIsInstance(ret['fun'], list)
ret = self.run_run_plus(fun='fileserver.empty_dir_list', args=['backend="roots"']) self.assertIsInstance(ret['fun'], list)
ret = self.run_run_plus(fun='fileserver.empty_dir_list', args=['backend="[roots]"']) self.assertIsInstance(ret['fun'], list)
ret = self.run_run_plus(fun='fileserver.envs', args=['backend="roots"']) self.assertIsInstance(ret['fun'], list)
ret = self.run_run_plus(fun='fileserver.envs', args=['backend="[roots]"']) self.assertIsInstance(ret['fun'], list)
ret = self.run_run_plus(fun='fileserver.file_list', args=['backend="roots"']) self.assertIsInstance(ret['fun'], list)
ret = self.run_run_plus(fun='fileserver.file_list', args=['backend="[roots]"']) self.assertIsInstance(ret['fun'], list)
ret = self.run_run_plus(fun='fileserver.symlink_list', args=['backend="roots"']) self.assertIsInstance(ret['fun'], dict)
ret = self.run_run_plus(fun='fileserver.symlink_list', args=['backend="[roots]"']) self.assertIsInstance(ret['fun'], dict)
ret = self.run_run_plus(fun='fileserver.update', args=['backend="roots"']) self.assertTrue(ret['fun'])
ret = self.run_run_plus(fun='fileserver.update', args=['backend="[roots]"']) self.assertTrue(ret['fun'])
from __future__ import absolute_import import os import shutil import tempfile
from salt.runners import winrepo from salttesting import skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import patch, NO_MOCK, NO_MOCK_REASON
import integration
from __future__ import absolute_import
from salttesting.helpers import ( ensure_in_syspath, ) ensure_in_syspath('../../')
import integration
self.assertIsNot(bad_out, ret_output)
for item in good_out: self.assertIn(item, ret_output)
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
from __future__ import absolute_import import json
from salttesting import skipIf from salttesting.helpers import destructiveTest, ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils
from __future__ import absolute_import import os
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils
from __future__ import absolute_import import os
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
from __future__ import absolute_import import os import shutil
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath, with_system_user, skip_if_binaries_missing ) ensure_in_syspath('../../')
import integration import salt.utils
ret = self.run_state('ssh_known_hosts.present', test=True, **kwargs) self.assertSaltNoneReturn(ret)
self.run_state('ssh_known_hosts.present', **kwargs)
ret = self.run_state('ssh_known_hosts.present', test=True, **kwargs) self.assertSaltTrueReturn(ret)
ret = self.run_state('ssh_known_hosts.present', **dict(kwargs, name=GITHUB_IP)) self.assertSaltStateChangesEqual( ret, GITHUB_FINGERPRINT, keys=('new', 'fingerprint') )
ret = self.run_state('ssh_known_hosts.absent', test=True, **kwargs) self.assertSaltNoneReturn(ret)
ret = self.run_state('ssh_known_hosts.absent', **kwargs) self.assertSaltStateChangesEqual( ret, GITHUB_FINGERPRINT, keys=('old', 'fingerprint') )
ret = self.run_state('ssh_known_hosts.absent', **kwargs) self.assertSaltStateChangesEqual(ret, {})
ret = self.run_state('ssh_known_hosts.absent', test=True, **kwargs) self.assertSaltTrueReturn(ret)
contents='ssh-rsa AAAAB3NzaC1kc3MAAACBAL0sQ9fJ5bYTEyY== root'
from __future__ import absolute_import import os import shutil
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils
from __future__ import absolute_import import os import textwrap import tempfile
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils
from __future__ import absolute_import import os import time
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath, requires_system_grains, requires_salt_modules ) ensure_in_syspath('../../')
import integration import salt.utils
_PKG_TARGETS_DOT = { 'RedHat': {'5': 'python-migrate0.5', '6': 'tomcat6-el-2.1-api', '7': 'tomcat-el-2.2-api'} }
_PKG_TARGETS_EPOCH = { 'RedHat': {'7': 'comps-extras'}, }
self.assertTrue(pkg_targets)
self.assertFalse(version)
if os_family == 'FreeBSD': return
self.assertTrue(pkg_targets)
self.assertTrue(version)
self.assertTrue(pkg_targets) version = self.run_function('pkg.version', pkg_targets)
if os_family == 'FreeBSD': return
self.assertTrue(bool(pkg_targets))
self.assertTrue(bool(version))
self.assertFalse(bool(version))
if os_name == 'CentOS' \ and grains['osrelease'].startswith('5.'): target = target.replace('.i686', '.i386')
self.assertTrue(bool(version))
from __future__ import absolute_import import os
from salttesting import skipIf from salttesting.helpers import destructiveTest, ensure_in_syspath ensure_in_syspath('../../')
import integration
from __future__ import absolute_import import logging
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath ) ensure_in_syspath('../../')
import integration import salt.ext.six as six from salt.modules import mysql as mysqlmod
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
from __future__ import absolute_import import os import shutil import socket
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
socket.setdefaulttimeout(10)
from __future__ import absolute_import
from salttesting import skipIf from salttesting.helpers import destructiveTest, ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils
from __future__ import absolute_import import os import time import subprocess
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils from salt.modules.virtualenv_mod import KNOWN_BINARY_NAMES
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
from __future__ import absolute_import from distutils.version import LooseVersion import glob import grp import os import pwd import sys import shutil import stat import tempfile import textwrap import filecmp import textwrap
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath, with_system_user_and_group )
import integration import salt.utils
import salt.ext.six as six
os.makedirs(name)
- file: {good_file}
with salt.utils.fopen(path_test, 'r') as fp_test_: self.assertTrue((sum(1 for _ in fp_test_) == 1))
with salt.utils.fopen(path_test, 'r') as fp_test_: self.assertTrue(fp_test_.read().startswith('en_US.UTF-8'))
for item in ret: self.assertSaltTrueReturn(item)
shutil.copyfile(path_in, path_test)
self.assertTrue(filecmp.cmp(path_test, path_out))
self.assertTrue(filecmp.cmp(path_test + '.bak', path_in))
for item in ret: self.assertSaltTrueReturn(item)
shutil.copyfile(path_in, path_test)
self.assertTrue(filecmp.cmp(path_test, path_out))
self.assertTrue(filecmp.cmp(path_test + '.bak', path_in))
for item in ret: self.assertSaltTrueReturn(item)
shutil.copyfile(path_in, path_test)
self.assertTrue(filecmp.cmp(path_test, path_out))
self.assertTrue(filecmp.cmp(path_test + '.bak', path_in))
for item in ret: self.assertSaltTrueReturn(item)
shutil.copyfile(path_in, path_test)
self.assertTrue(filecmp.cmp(path_test, path_out))
self.assertTrue(filecmp.cmp(path_test + '.bak', path_in))
for item in ret: self.assertSaltTrueReturn(item)
shutil.copyfile(path_in, path_test)
fstats_orig = os.stat(path_test)
age = 5*24*60*60
os.utime(path_test, (fstats_orig.st_mtime-age, fstats_orig.st_atime-age))
fstats_post = os.stat(path_test)
self.assertTrue(filecmp.cmp(path_in, path_test))
self.assertFalse(os.path.exists(path_test + '.bak'))
self.assertTrue(fstats_post.st_mtime, fstats_orig.st_mtime-age)
self.assertSaltTrueReturn(ret)
shutil.copyfile(path_in, path_test)
fstats_orig = os.stat(path_test)
age = 5*24*60*60
os.utime(path_test, (fstats_orig.st_mtime-age, fstats_orig.st_atime-age))
fstats_post = os.stat(path_test)
self.assertTrue(filecmp.cmp(path_in, path_test))
self.assertFalse(os.path.exists(path_test + '.bak'))
self.assertTrue(fstats_post.st_mtime, fstats_orig.st_mtime-age)
self.assertSaltTrueReturn(ret)
ret = self.run_state( 'file.append', name=name, text='cheese' ) self.assertSaltTrueReturn(ret) self.assertTrue(os.path.isfile(name))
ret = self.run_state( 'file.prepend', name=name, text='cheese' ) self.assertSaltTrueReturn(ret) self.assertTrue(os.path.isfile(name))
os.makedirs(name)
ret = self.run_function( 'state.sls', mods='testappend.issue-2227' ) self.assertSaltTrueReturn(ret)
ret = self.run_function( 'state.template_str', [template], timeout=120 )
tmp_file = os.path.join(integration.TMP, 'issue-2379-file-append.txt') salt.utils.fopen(tmp_file, 'w').write(
if os.path.isdir(tmp_dir): shutil.rmtree(tmp_dir) elif os.path.isfile(tmp_dir): os.remove(tmp_dir)
ret = self.run_state( 'file.directory', name=tmp_dir, follow_symlinks=True, user=user, group=group, recurse=['user', 'group'] ) self.assertSaltTrueReturn(ret)
if os.path.isdir(tmp_dir): shutil.rmtree(tmp_dir) elif os.path.isfile(tmp_dir): os.remove(tmp_dir)
ret = self.run_state( 'file.directory', name=tmp_dir, follow_symlinks=False, user=user, group=group, recurse=['user', 'group'] ) self.assertSaltTrueReturn(ret)
from __future__ import absolute_import import os
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
from __future__ import absolute_import import os import shutil
from salttesting import skipIf from salttesting.helpers import destructiveTest, ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils from salt.modules.virtualenv_mod import KNOWN_BINARY_NAMES
with salt.utils.fopen(requirements_file_path, 'a') as fhw: fhw.write('pep8==1.3.3\n')
try: ret = self.run_function( 'state.template_str', ['\n'.join(template)] )
if os.path.exists(venv_path): shutil.rmtree(venv_path) if os.path.exists(requirements_file_path): os.unlink(requirements_file_path) raise
with salt.utils.fopen(requirements_file_path, 'w') as fhw: fhw.write('zope.interface==4.0.1\n')
try: ret = self.run_function( 'state.template_str', ['\n'.join(template)] )
if os.path.exists(venv_path): shutil.rmtree(venv_path) if os.path.exists(requirements_file_path): os.unlink(requirements_file_path) raise
if os.path.exists(venv_path): shutil.rmtree(venv_path) if os.path.exists(requirements_file_path): os.unlink(requirements_file_path)
from __future__ import absolute_import import os import sys from random import randint import grp
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath, requires_system_grains ) ensure_in_syspath('../../')
import salt.utils import integration
gid_from_name = False if grains['os_family'] == 'MacOS' else True
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
self.assertIsInstance(ret, list)
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
from __future__ import absolute_import import os import pwd import glob import shutil
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath, requires_system_grains, with_system_user ) ensure_in_syspath('../../')
import integration import salt.utils from salt.modules.virtualenv_mod import KNOWN_BINARY_NAMES from salt.exceptions import CommandExecutionError
import salt.ext.six as six
ret = self.run_function('virtualenv.create', [venv_dir]) self.assertEqual(ret['retcode'], 0)
ret = self.run_function('state.sls', mods='pip-installed-errors') self.assertSaltTrueReturn(ret)
self.skipTest( 'You don\'t have the required permissions to run this test' )
ret = self.run_function( 'state.sls', mods='pip-installed-weird-install' ) self.assertSaltTrueReturn(ret)
ret = self.run_function('virtualenv.create', [venv_dir]) self.assertEqual(ret['retcode'], 0)
self.assertEqual( self.run_function('pip.list', ['pip'], bin_env=venv_dir), {'pip': '6.0'} )
venv_dir = os.path.join( integration.TMP, 'pip-installed-specific-env' )
from __future__ import absolute_import
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath, requires_system_grains ) ensure_in_syspath('../../')
import integration import salt.utils
import salt.ext.six as six
self.assertReturnNonEmptySaltType(ret) for state_id, state_result in six.iteritems(ret): self.assertSaltTrueReturn(dict([(state_id, state_result)]))
self.assertReturnNonEmptySaltType(ret) for state_id, state_result in six.iteritems(ret): self.assertSaltTrueReturn(dict([(state_id, state_result)]))
from __future__ import absolute_import import os import shutil import socket import subprocess import tempfile
from salttesting.helpers import ensure_in_syspath, skip_if_binaries_missing ensure_in_syspath('../../')
import integration import salt.utils
socket.setdefaulttimeout(10)
from __future__ import absolute_import import re
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
NO_BOTO_MODULE = True BOTO_NOT_CONFIGURED = True try: import boto NO_BOTO_MODULE = False try: boto.connect_iam() BOTO_NOT_CONFIGURED = False except boto.exception.NoAuthHandlerFound: pass except ImportError: pass
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
from __future__ import absolute_import from __future__ import print_function import json import time
from salt.netapi.rest_tornado import saltnado from unit.netapi.rest_tornado.test_handlers import SaltnadoTestCase
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath
self.assertEqual(response_obj['return'][0]['minion']['id'], 'minion')
self.application = application self.events_to_fire = 0 return application
else: ZMQIOLoop.current().add_timeout(time.time() + 0.5, self._stop)
from __future__ import absolute_import import json
from salttesting.unit import skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../../')
('arg', [1234]), ('kwarg', {'ext_source': 'redis'}),
from __future__ import absolute_import import os
from salttesting.unit import skipIf from salttesting.helpers import ( ensure_in_syspath, destructiveTest) ensure_in_syspath('../../../')
import salt.utils from tests import integration
if USERA in user_list: self.run_function('user.delete', [USERA], remove=True) #need to exit cherypy engine cherrypy.engine.exit()
from __future__ import absolute_import import os
from integration import TMP_CONF_DIR from salttesting import TestCase
import salt.config import salt.netapi
self.assertIn('tag', ret) ret.pop('tag')
from __future__ import absolute_import, print_function import subprocess import hashlib import pprint import optparse
from salt.utils import get_colors
import yaml import salt.ext.six as six
from __future__ import absolute_import
from salt.cli.batch import Batch
from salttesting import skipIf, TestCase from salttesting.mock import MagicMock, patch, NO_MOCK, NO_MOCK_REASON from salttesting.helpers import ensure_in_syspath
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath
from salt.utils import schema
try: import jsonschema import jsonschema.exceptions HAS_JSONSCHEMA = True except ImportError: HAS_JSONSCHEMA = False
try: import rfc3987 HAS_RFC3987 = True except ImportError: HAS_RFC3987 = False
from __future__ import absolute_import import os import time import signal import multiprocessing
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import salt.utils import salt.utils.process
import salt.ext.six as six
if process_manager._process_map.keys(): process_manager.send_signal_to_processes(signal.SIGILL) process_manager.stop_restarting() process_manager.kill_children()
if process_manager._process_map.keys(): process_manager.send_signal_to_processes(signal.SIGILL) process_manager.stop_restarting() process_manager.kill_children()
self.assertEqual(counter.value, 0) self.assertEqual(pool._job_queue.qsize(), 1)
from __future__ import absolute_import import copy
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath
from salt.utils import configcomparer
from __future__ import absolute_import import os import copy
from salttesting import skipIf, TestCase from salttesting.mock import MagicMock, patch, NO_MOCK, NO_MOCK_REASON from salttesting.helpers import ensure_in_syspath
import salt.config from salt.utils.schedule import Schedule
from __future__ import absolute_import
from salt.utils import args
from salttesting import TestCase, skipIf from salttesting.mock import NO_MOCK, NO_MOCK_REASON from salttesting.helpers import ensure_in_syspath
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import MagicMock, patch, NO_MOCK, NO_MOCK_REASON from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import salt.utils.gitfs from salt.exceptions import FileserverConfigError
OPTS = {'cachedir': '/tmp/gitfs-test-cache'}
role_class(*args) role_class(*args)
role_class(*args)
self.assertRaises( FileserverConfigError, role_class, *args )
self.assertRaises( FileserverConfigError, role_class, *args )
from __future__ import absolute_import import os from os.path import join from shutil import rmtree from tempfile import mkdtemp
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils import salt.utils.find
from __future__ import absolute_import import copy
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath
from salt.utils import dictupdate
from __future__ import absolute_import import getpass import os import sys import stat import shutil import resource import tempfile import socket
from salttesting import skipIf, TestCase from salttesting.helpers import ( ensure_in_syspath, requires_network, TestsLoggingHandler ) ensure_in_syspath('../../')
import salt.utils import integration from salt.utils.verify import ( check_user, verify_env, verify_socket, zmq_version, check_max_open_files, valid_id )
class FakeWriter(object): def __init__(self): self.output = ""
self.assertEqual( [logmsg_dbg.format(newmax)], handler.messages )
self.skipTest('We\'ve hit the max open files setting')
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import MagicMock, patch, NO_MOCK, NO_MOCK_REASON from salt.ext.six.moves import range
from salt.utils import mac_utils from salt.exceptions import SaltInvocationError, CommandExecutionError
from __future__ import absolute_import import os import sys import shutil import tempfile import stat
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils import salt.utils.find
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch, MagicMock from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
from salt.exceptions import SaltInvocationError import salt.utils.boto import salt.utils.boto3
try: import boto import boto.exception from boto.exception import BotoServerError
self.assertNotEqual(id(boto_ec2_conn), id(boto3_ec2_conn))
from __future__ import absolute_import
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath
from salt.utils.args import KWARG_REGEX
from __future__ import absolute_import import time
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
from salt.utils import cache
self.assertRaises(KeyError, cd.__getitem__, 'foo')
from __future__ import absolute_import import os
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch ensure_in_syspath('../../')
import integration import salt.utils
@patch('salt.utils.which', lambda exe: None) def test_missing_binary_in_linux(self): self.assertTrue( salt.utils.which('this-binary-does-not-exist') is None )
@patch('salt.utils.which', lambda exe: exe) def test_existing_binary_in_linux(self): self.assertTrue(salt.utils.which('this-binary-exists-under-linux'))
from __future__ import absolute_import
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
from salt.utils import immutabletypes
from __future__ import absolute_import
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
from salt.utils.aggregation import aggregate, Map, Scalar
from __future__ import absolute_import import os import shutil
from salttesting import TestCase, skipIf from salttesting.mock import NO_MOCK, NO_MOCK_REASON from salt.utils.cache import context_cache
import salt.payload import salt.utils
with salt.utils.fopen(target_cache_file, 'rb') as fp_: target_cache_data = salt.payload.Serial(__opts__).load(fp_) self.assertDictEqual(__context__, target_cache_data)
cc = salt.utils.cache.ContextCache(__opts__, __name__) retrieved_cache = cc.get_cache_context() self.assertDictEqual(retrieved_cache, __context__)
@context_cache def _test_set_cache(): pass _test_set_cache()
@context_cache def _test_refill_cache(comparison_context): self.assertEqual(__context__, comparison_context)
from __future__ import absolute_import import os import sys import random import subprocess import time
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
from salt.utils import fopen, is_darwin, vt
self.assertEqual( terminal.getwinsize(), (24, cols) ) terminal.wait() terminal.close()
try: if os.path.exists('/proc/sys/kernel/pty/nr'): with fopen('/proc/sys/kernel/pty/nr') as fh_: return int(fh_.read().strip())
self.skipTest( 'Unable to find out how many PTY\'s are open on Darwin - ' 'Skipping for now' )
if stdout is None and stderr is None: self.assertFalse(term.isalive())
self.assertEqual(buffer_o, expected_data) self.assertFalse(term.isalive())
if stdout is None and stderr is None: self.assertFalse(term.isalive())
self.assertEqual(buffer_e, expected_data) self.assertFalse(term.isalive())
if stdout is None and stderr is None: self.assertFalse(term.isalive())
time.sleep(0.1)
self.assertEqual(buffer_o, expected_data) self.assertFalse(term.isalive())
from __future__ import absolute_import
from salttesting import skipIf from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch ensure_in_syspath('../../')
from salt.utils import network
from __future__ import absolute_import
from yaml.constructor import ConstructorError from salt.utils.yamlloader import SaltYamlSafeLoader import salt.utils
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import patch, NO_MOCK, NO_MOCK_REASON, mock_open
from __future__ import absolute_import
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
from salt.utils.filebuffer import BufferedReader, InvalidFileMode
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
from salt.utils import etcd_util try: from urllib3.exceptions import ReadTimeoutError, MaxRetryError HAS_URLLIB3 = True except ImportError: HAS_URLLIB3 = False
mock.side_effect = ValueError self.assertEqual(client.get('not-found'), None)
from __future__ import absolute_import
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath
from salt.utils.rsax931 import RSAX931Signer, RSAX931Verifier
from __future__ import absolute_import
import tornado.testing import tornado.gen from tornado.testing import AsyncTestCase
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import NO_MOCK, NO_MOCK_REASON from salttesting.helpers import ensure_in_syspath
from salt.utils import http
from __future__ import absolute_import
from salt.utils.validate import net
from salttesting import TestCase, skipIf from salttesting.mock import NO_MOCK, NO_MOCK_REASON from salttesting.helpers import ensure_in_syspath
from __future__ import absolute_import import sys import warnings
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
from salt.utils import warn_until, kwargs_warn_until from salt.version import SaltStackVersion
warnings.filterwarnings('always', '', DeprecationWarning, __name__)
with warnings.catch_warnings(record=True) as recorded_warnings: raise_warning() self.assertEqual( 'Deprecation Message!', str(recorded_warnings[0].message) )
with warnings.catch_warnings(record=True) as recorded_warnings: raise_named_version_warning() self.assertEqual( 'Deprecation Message!', str(recorded_warnings[0].message) )
warnings.filterwarnings('always', '', DeprecationWarning, __name__)
with warnings.catch_warnings(record=True) as recorded_warnings:
with warnings.catch_warnings(record=True) as recorded_warnings: kwargs_warn_until(
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import patch, NO_MOCK, NO_MOCK_REASON
ensure_in_syspath('../../') import salt.ext.six as six from salt.ext.six.moves import reload_module from salt.utils import locales
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( patch, DEFAULT, create_autospec, NO_MOCK, NO_MOCK_REASON ) ensure_in_syspath('../../')
from salt.utils import args from salt.utils.odict import OrderedDict from salt.exceptions import (SaltInvocationError, SaltSystemExit, CommandNotFoundError) from salt import utils
import os import datetime import yaml import zmq from collections import namedtuple
import salt.ext.six as six from salt.ext.six.moves import range try:
incorrect_jid_length = 2012 self.assertEqual(utils.jid.jid_to_time(incorrect_jid_length), '')
self.assertRaises(SaltInvocationError, utils.format_call, dummy_func, {'1': 2})
self.skipTest('\'timelib\' is not installed')
ret = utils.find_json(test_sample_json) self.assertDictEqual(ret, expected_ret)
garbage_prepend_json = '{0}{1}'.format(LORUM_IPSUM, test_sample_json) ret = utils.find_json(garbage_prepend_json) self.assertDictEqual(ret, expected_ret)
self.assertRaises(ValueError, utils.find_json, LORUM_IPSUM)
self.assertFalse(utils.is_bin_str(''))
yaml_key_val_pair = '- key1: val1' ret = utils.repack_dictlist(yaml_key_val_pair) self.assertDictEqual(ret, {'key1': 'val1'})
ret = utils.repack_dictlist(LORUM_IPSUM) self.assertDictEqual(ret, {})
self.assertEqual(str(ret['LIGHT_YELLOW']), str(ret['LIGHT_GRAY']))
with patch('sys.argv', ['salt-call']): ret = utils.daemonize_if({}) self.assertEqual(None, ret)
self.assertRaises(RuntimeError, utils.kwargs_warn_until, {}, [])
from __future__ import absolute_import import os import sys import posixpath import ntpath import platform import tempfile
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
from salt.utils import path_join
import salt.ext.six as six
from __future__ import absolute_import
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
from salt.utils import format_call from salt.exceptions import SaltInvocationError
from __future__ import absolute_import import re
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
from salt.utils import build_whitespace_split_regex
if [ -z '$debian_chroot' ] && [ -r /etc/debian_chroot ]; then debian_chroot=$(cat /etc/debian_chroot) fi
if [ -z '$debian_chroot' ] && [ -r /etc/debian_chroot ]; then debian_chroot=$(cat /etc/debian_chroot) fi
if [ -z "$debian_chroot" ] && [ -r /etc/debian_chroot ]; then debian_chroot=$(cat /etc/debian_chroot) fi
if [ -z "$debian_chroot" ] && [ -r /etc/debian_chroot ]; then debian_chroot=$(cat /etc/debian_chroot) fi
if [ -z '$debian_chroot' ] && [ -r /etc/debian_chroot ]; then debian_chroot=$(cat /etc/debian_chroot) fi
from salttesting import (expectedFailure, skipIf) from salttesting import TestCase from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration from salt.utils.process import clean_proc from salt.utils import event
time.sleep(10)
if os.environ.get('TRAVIS_PYTHON_VERSION', None) is not None: time.sleep(10) else: time.sleep(2)
evt1 = me.get_event(wait=0, tag='evt1', no_block=False) self.assertGotEvent(evt1, {'data': 'foo1'})
from __future__ import absolute_import import os
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
from salt.utils import cloud from integration import TMP, CODE_DIR
if not os.path.isdir(GPG_KEYDIR): os.makedirs(GPG_KEYDIR)
try: import keyring import keyring.backend
keyring.set_keyring(TestKeyring()) HAS_KEYRING = True
from __future__ import absolute_import
import salt.utils.url
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from __future__ import absolute_import
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salt.utils import decorators from salt.version import SaltStackVersion from salt.exceptions import CommandExecutionError
from __future__ import absolute_import import os import shutil import tempfile import uuid
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath
from salt://map.sls import Samba
from __future__ import absolute_import
from salt.beacons import adb
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch, Mock
from __future__ import absolute_import
from salt.beacons import glxinfo
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch, Mock
from __future__ import absolute_import import os import shutil import tempfile
from salt.beacons import inotify
from salttesting import skipIf, TestCase from salttesting.helpers import destructiveTest, ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON
try:
from __future__ import absolute_import import os import os.path import tempfile
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath
import salt.loader import salt.config import integration from salt.exceptions import SaltRenderError from salt.ext.six.moves import StringIO
import salt.ext.six as six
from __future__ import absolute_import import os import copy import tempfile import json import datetime import pprint
from salttesting.unit import skipIf, TestCase from salttesting.case import ModuleCase from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import yaml from jinja2 import Environment, DictLoader, exceptions try:
with self.assertRaises(exceptions.TemplateRuntimeError): env.from_string("{{ document|load_json }}").render(document="{'foo': 'it works'}")
with self.assertRaises(exceptions.TemplateRuntimeError): env.from_string('{{ document|load_json }}').render(document={"foo": "it works"})
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import patch, NO_MOCK, NO_MOCK_REASON ensure_in_syspath('../')
import integration from salt import client from salt.exceptions import EauthAuthenticationError, SaltInvocationError, SaltClientError
with patch('os.path.exists', return_value=False): self.assertRaises(SaltClientError, lambda: self.client.pub('*', 'test.ping'))
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import patch, call, mock_open, NO_MOCK, NO_MOCK_REASON, MagicMock
import salt.utils from salt import crypt
try:
from __future__ import absolute_import import re
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath
from salt.version import SaltStackVersion
from __future__ import absolute_import import os import os.path import tempfile
from salttesting import TestCase from salttesting.mock import patch, MagicMock from salttesting.helpers import ensure_in_syspath
import integration import salt.config from salt.state import HighState from salt.utils.odict import OrderedDict, DefaultOrderedDict
from __future__ import absolute_import import os import copy import shutil import tempfile
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import salt.utils from salt.utils import files as util_files
import salt.ext.six as six
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch from salttesting.helpers import ensure_in_syspath
from salt.pillar import consul_pillar
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import NO_MOCK, NO_MOCK_REASON
from salt.pillar import Pillar, git_pillar
raise RuntimeError("Infinite loop detected")
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, call, patch from salttesting.helpers import ensure_in_syspath
from salt.pillar import nodegroups
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import NO_MOCK, NO_MOCK_REASON from salttesting.helpers import ensure_in_syspath
from salt.pillar import sqlite3
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import NO_MOCK, NO_MOCK_REASON from salttesting.helpers import ensure_in_syspath
from salt.pillar import sqlcipher
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import NO_MOCK, NO_MOCK_REASON from salttesting.helpers import ensure_in_syspath
from salt.pillar import mysql
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import NO_MOCK, NO_MOCK_REASON
from salt.pillar import hg_pillar HGLIB = hg_pillar.hglib
from __future__ import absolute_import import tempfile
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch ensure_in_syspath('../')
import salt.pillar
pillar.client.get_state = MagicMock( return_value={ 'dest': '/path/to/pillar/files/foo.sls', 'source': 'salt://foo.sls' } )
matcher = Matcher.return_value matcher.confirm_top.return_value = True
client = get_file_client.return_value client.cache_file.return_value = self.top_file.name
matcher = Matcher.return_value matcher.confirm_top.return_value = True
client = get_file_client.return_value client.cache_file.return_value = self.top_file.name
loaderCls = MockLoader
from __future__ import absolute_import
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath
from salt import template
from __future__ import absolute_import
import salt.transport.client
del self.pub_channel
from __future__ import absolute_import import os import threading
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../') import integration
from unit.transport.req_test import ReqChannelMixin from unit.transport.pub_test import PubChannelMixin
cls.req_server_channel = salt.transport.server.ReqServerChannel.factory(cls.master_opts) cls.req_server_channel.pre_fork(cls.process_manager)
from __future__ import absolute_import import os import logging
import integration
from __future__ import absolute_import import os import threading import platform import time
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../')
from unit.transport.req_test import ReqChannelMixin from unit.transport.pub_test import PubChannelMixin
cls.req_server_channel = salt.transport.server.ReqServerChannel.factory(cls.master_opts) cls.req_server_channel.pre_fork(cls.process_manager)
from __future__ import absolute_import import logging from salt.ext.six.moves import StringIO
from salttesting.case import TestCase from salttesting.helpers import ensure_in_syspath, TestsLoggingHandler
from salt.log import setup as saltlog from salt.log.handlers import StreamHandler
log = saltlog.SaltLoggingClass(__name__)
log_format = '[%(name)-15s] %(message)s' handler = TestsLoggingHandler(format=log_format) log.addHandler(handler)
log.removeHandler(handler)
log_format = '[%(name)s] %(message)s' handler = TestsLoggingHandler(format=log_format) log.addHandler(handler)
log.removeHandler(handler)
stream1 = StringIO() stream2 = StringIO() handler1 = StreamHandler(stream1) handler2 = StreamHandler(stream2)
stream1 = StringIO() stream2 = StringIO() handler1 = StreamHandler(stream1) handler2 = StreamHandler(stream2)
stream1 = StringIO() stream2 = StringIO() handler1 = StreamHandler(stream1) handler2 = StreamHandler(stream2)
from __future__ import absolute_import, print_function
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath
from salt.config.schemas import ssh as ssh_schemas from salt.config.schemas.minion import MinionConfiguration
try: import jsonschema import jsonschema.exceptions HAS_JSONSCHEMA = True except ImportError: HAS_JSONSCHEMA = False
'minion_opts': ssh_schemas.DictItem(title='Minion Options', description='Dictionary of minion options', properties=MinionConfiguration()).serialize(),
from __future__ import absolute_import import logging import os import shutil import tempfile from contextlib import contextmanager
from salttesting import TestCase from salttesting.mock import MagicMock, patch from salttesting.helpers import ensure_in_syspath, TestsLoggingHandler from salt.exceptions import CommandExecutionError
import salt.minion import salt.utils import salt.utils.network import integration from salt import config as sconfig from salt.exceptions import SaltCloudConfigError
import yaml
MOCK_HOSTNAME = 'very.long.complex.fqdn.that.is.crazy.extra.long.example.com'
self.assertEqual(config['key_logfile'], os.path.join('/', 'key')) self.assertNotEqual(config['key_logfile'], '//key')
config = sconfig.master_config('/etc/salt/master') self.assertEqual(config['log_file'], env_fpath) os.environ.clear() os.environ.update(original_environ)
config = sconfig.minion_config('/etc/salt/minion') self.assertEqual(config['log_file'], env_fpath) os.environ.clear() os.environ.update(original_environ)
salt.utils.fopen(minion_config, 'w').write( 'blah: false\n' 'root_dir: {0}\n' 'log_file: {1}\n'.format(tempdir, minion_config) )
config = sconfig.minion_config(minion_config)
self.assertTrue(config['blah'])
salt.utils.fopen(master_config, 'w').write( 'blah: false\n' 'root_dir: {0}\n' 'log_file: {1}\n'.format(tempdir, master_config) )
config = sconfig.master_config(master_config)
self.assertTrue(config['blah'])
self.assertTrue(search_paths[0].endswith(etc_deploy_path))
self.assertTrue(search_paths[1].endswith(deploy_path))
config = sconfig.cloud_config('/etc/salt/cloud') self.assertEqual(config['log_file'], env_fpath) os.environ.clear() os.environ.update(original_environ)
os.environ['SALT_CLOUD_CONFIG'] = env_fpath config = sconfig.cloud_config(fpath) self.assertEqual(config['log_file'], fpath)
os.environ.clear() os.environ.update(original_environ)
self.assertIn( deploy_dir_path, default_config['deploy_scripts_search_path'] )
self.assertEqual( deploy_dir_path, default_config['deploy_scripts_search_path'][0] )
from __future__ import absolute_import import os import sys import shutil import tempfile import textwrap import copy
from salttesting.unit import TestCase from salttesting.helpers import ensure_in_syspath
import integration import salt.loader import salt.config import salt.utils from salt.state import HighState from salt.utils.pydsl import PyDslError
import salt.ext.six as six from salt.ext.six.moves import StringIO
finally: HIGHSTATE.pop_active()
state('A').cmd.run('echo this is state A', cwd='/')
extend(state('.start').stateconf.require(stateconf='xxx::goal'))
extend(state('.goal').stateconf.require_in(stateconf='yyy::start'))
from __future__ import absolute_import import time import errno import threading
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath, MockWraps from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch ensure_in_syspath('../')
import salt.payload from salt.utils.odict import OrderedDict import salt.exceptions
import msgpack import zmq import salt.ext.six as six
SREQTestCase.thread_running.clear() SREQTestCase.echo_server.join()
assert sreq.send_auto({}) == {'enc': 'clear', 'load': {}}
assert sreq.send_auto({'load': 'foo'}) == {'load': 'foo', 'enc': 'clear'}
sreq.destroy()
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.renderers import gpg from salt.exceptions import SaltRenderError
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath
import salt.state from salt.config import minion_config from salt.template import compile_template_str from salt.serializers import yamlex
from __future__ import absolute_import import os import shutil import tempfile
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath, destructiveTest
)
from __future__ import absolute_import import tornado.stack_context import tornado.gen from tornado.testing import AsyncTestCase, gen_test import threading import time
from salttesting import TestCase from salt.ext.six.moves import range from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
from salt.utils.context import ContextDict, NamespacedDictWrapper
num_concurrent_tasks = 5
self.cd['foo'] = 'global'
self.assertEqual( dict(self.cd), {'foo': 'global'}, )
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.modules import smf
smf.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( mock_open, MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import grub_legacy from salt.exceptions import CommandExecutionError
grub_legacy.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import cassandra
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import useradd from salt.exceptions import CommandExecutionError import pwd
useradd.__grains__ = {} useradd.__salt__ = {} useradd.__context__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch )
from salt.modules import rdp
rdp.__salt__ = {}
IS_RDP = rdp.__virtual__()
from __future__ import absolute_import import os
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch ensure_in_syspath('../../')
from salt.modules import pip from salt.exceptions import CommandExecutionError
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import haproxyconn
haproxyconn.__opts__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath, TestsLoggingHandler from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch
from salt.modules import alternatives
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
from salt.modules import keyboard
keyboard.__salt__ = {} keyboard.__grains__ = {'os_family': ''}
from __future__ import absolute_import
from salt.modules import cmdmod from salt.exceptions import CommandExecutionError from salt.log import LOG_LEVELS
from salttesting import TestCase, skipIf from salttesting.mock import ( mock_open, MagicMock, NO_MOCK, NO_MOCK_REASON, patch ) from salttesting.helpers import ensure_in_syspath
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.modules import solr import os
solr.__salt__ = {} solr.__opts__ = {}
from __future__ import absolute_import import salt.utils import sys import types
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, Mock, NO_MOCK, NO_MOCK_REASON )
wmi = types.ModuleType('wmi') sys.modules['wmi'] = wmi
from salt.modules import win_network
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import launchctl
launchctl.__salt__ = {}
from __future__ import absolute_import import os
from salt.exceptions import CommandExecutionError from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import systemd
systemd.__salt__ = {} systemd.__context__ = {}
from __future__ import absolute_import import sys
from salttesting import TestCase, skipIf from salttesting.mock import ( patch, mock_open, NO_MOCK, NO_MOCK_REASON )
from salt.modules import pam
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import pw_group
pw_group.__grains__ = {} pw_group.__salt__ = {} pw_group.__context__ = {} pw_group.grinfo = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.modules import service import os
service.__grains__ = {} service.__salt__ = {}
from __future__ import absolute_import import grp
from salttesting import TestCase from salttesting.mock import MagicMock, patch
from salt.modules import mac_group from salt.exceptions import SaltInvocationError, CommandExecutionError
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch from salttesting.helpers import ensure_in_syspath
import salt.config import salt.loader from salt.modules import boto_apigateway
try: import boto3 from botocore.exceptions import ClientError HAS_BOTO = True except ImportError: HAS_BOTO = False
required_boto3_version = '1.2.1'
diff = self._diff_list_dicts(api_keys, items_dt, 'id')
from __future__ import absolute_import
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch )
from salt.exceptions import CommandExecutionError from salt.modules import mac_assistive as assistive
from __future__ import absolute_import
from salt.modules import vsphere from salt.exceptions import CommandExecutionError
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
HOST = '1.2.3.4' USER = 'root' PASSWORD = 'SuperSecret!' ERROR = 'Some Testing Error Message'
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import saltcloudmod
saltcloudmod.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import defaults
defaults.__grains__ = {} defaults.__salt__ = {} defaults.__opts__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch )
from salt.modules import win_autoruns
win_autoruns.__salt__ = {} win_autoruns.__grains__ = {}
IS_WIN = win_autoruns.__virtual__()
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import ilo
ilo.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import schedule from salt.utils.event import SaltEvent
schedule.__salt__ = {} schedule.__opts__ = {} schedule.__pillar__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import logadm
logadm.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import pw_user from salt.exceptions import CommandExecutionError try: import pwd HAS_PWD = True except ImportError: HAS_PWD = False
pw_user.__grains__ = {} pw_user.__salt__ = {} pw_user.__context__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import firewalld
firewalld.__grains__ = {} firewalld.__salt__ = {} firewalld.__context__ = {} firewalld.__opts__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import znc
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import bower from salt.exceptions import CommandExecutionError
bower.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import htpasswd
htpasswd.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
from salt.modules import mac_desktop from salt.exceptions import CommandExecutionError
mac_desktop.__salt__ = {}
from __future__ import absolute_import from datetime import datetime
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import win_system
try:
import re self.assertTrue(re.search(r'^\d{2}:\d{2} \w{2}$', win_tm))
from __future__ import absolute_import
from mock import call
from salt.modules import gentoo_service
gentoo_service.__grains__ = {} gentoo_service.__salt__ = {} gentoo_service.__context__ = {} gentoo_service.__opts__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
import salt.utils import os.path from salt.modules import key
key.__opts__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import gnomedesktop
gnomedesktop.__grains__ = {} gnomedesktop.__salt__ = {} gnomedesktop.__context__ = {} gnomedesktop.__opts__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch ensure_in_syspath('../../')
import salt.modules.gem as gem
from __future__ import absolute_import, print_function from mock import call import re
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, Mock, patch ensure_in_syspath('../../')
from salt.modules import postgres from salt.exceptions import SaltInvocationError
from __future__ import absolute_import
from salttesting.case import ModuleCase
import salt.loader
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import win_service
try: WINAPI = True import win32serviceutil except ImportError: WINAPI = False
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import debian_service
debian_service.__grains__ = {} debian_service.__salt__ = {} debian_service.__context__ = {} debian_service.__opts__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, Mock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import localemod from salt.exceptions import CommandExecutionError
localemod.__context__ = {} localemod.__grains__ = {} localemod.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import ipset
ipset.__salt__ = {}
from __future__ import absolute_import import os
from salttesting import TestCase, skipIf from salt.exceptions import SaltInvocationError from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, mock_open, NO_MOCK, NO_MOCK_REASON )
import salt.utils from salt.modules import state
state.__salt__ = {} state.__context__ = {} state.__opts__ = {} state.__pillar__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( mock_open, MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import xapi
from __future__ import absolute_import
from salt.modules import win_certutil as certutil
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch )
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath
from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON, )
from salt.modules import zpool
from salt.utils.odict import OrderedDict
zpool.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import drbd
drbd.__grains__ = {} drbd.__salt__ = {} drbd.__context__ = {}
from __future__ import absolute_import import textwrap
from salt.modules import parallels from salt.exceptions import SaltInvocationError
from salttesting import TestCase, skipIf from salttesting.mock import MagicMock, patch, NO_MOCK, NO_MOCK_REASON from salttesting.helpers import ensure_in_syspath
import salt.ext.six as six
with patch('salt.utils.which', mock_true): ret = parallels.__virtual__() self.assertTrue(ret) self.assertEqual(ret, 'parallels')
str_args = 'electrolytes --aqueous --anion hydroxide --cation=ammonium free radicals -- hydrogen' _validate_ret(parallels._normalize_args(str_args))
list_args = ' '.join(str_args) _validate_ret(parallels._normalize_args(list_args))
tuple_args = tuple(list_args) _validate_ret(parallels._normalize_args(tuple_args))
other_args = {'anion': 'hydroxide', 'cation': 'ammonium'} _validate_ret(parallels._normalize_args(other_args))
mock_plain = MagicMock() with patch.object(parallels, 'prlctl', mock_plain): parallels.list_vms(runas=runas) mock_plain.assert_called_once_with('list', [], runas=runas)
mock_stop = MagicMock() with patch.object(parallels, 'prlctl', mock_stop): parallels.stop(name, runas=runas) mock_stop.assert_called_once_with('stop', [name], runas=runas)
self.assertRaises(SaltInvocationError, parallels.snapshot_id_to_name, name, '{8-4-4-4-12}')
mock_no_data = MagicMock(return_value='') with patch.object(parallels, 'prlctl', mock_no_data): self.assertRaises(SaltInvocationError, parallels.snapshot_id_to_name, name, snap_id)
self.assertEqual(parallels._validate_snap_name(name, snap_id), snap_id)
mock_prlctl = MagicMock(return_value=guid_str) with patch.object(parallels, 'prlctl', mock_prlctl): parallels.list_snapshots(name) mock_prlctl.assert_called_once_with('snapshot-list', [name], runas=None)
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, mock_open, NO_MOCK, NO_MOCK_REASON )
from salt.modules import apache
apache.__grains__ = {} apache.__salt__ = {} apache.__context__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch ensure_in_syspath('../../')
from salt.exceptions import CommandExecutionError from salt.modules import parted
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import twilio_notify
from __future__ import absolute_import import os
from salttesting.case import ModuleCase from salttesting.mixins import RUNTIME_VARS
import salt.config import salt.loader
from __future__ import absolute_import
from salt.modules import mac_keychain as keychain
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch )
from __future__ import absolute_import import logging from copy import deepcopy
try: import boto import boto.ec2.elb HAS_BOTO = True except ImportError: HAS_BOTO = False
import salt.config import salt.loader from salt.modules import boto_elb
from salttesting import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON from salttesting.helpers import ensure_in_syspath
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salt.exceptions import CommandExecutionError
from salt.modules import zypper
zypper.__salt__ = dict() zypper.__context__ = dict() zypper.rpm = None
self.assertEqual(len(installed), 2)
for pkg_name, pkg_info in installed.items(): self.assertEqual(installed[pkg_name].get('source'), run_out[pkg_name]['source_rpm'])
for pn_key, pn_val in run_out['virgo-dummy'].items(): if pn_key == 'source_rpm': continue self.assertEqual(installed['virgo-dummy'][pn_key], pn_val)
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.utils.odict import OrderedDict from salt.modules import pillar as pillarmod
if __name__ == '__main__': from integration import run_tests run_tests(PillarModuleTestCase, needs_daemon=False)
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON )
from salt.modules import random_org
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
from salt.modules import locate
locate.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import qemu_img import os
qemu_img.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, mock_open, NO_MOCK, NO_MOCK_REASON )
from salt.modules import data
data.__grains__ = {} data.__salt__ = {} data.__opts__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import debconfmod import os
debconfmod.__grains__ = {} debconfmod.__salt__ = {} debconfmod.__context__ = {} debconfmod.__opts__ = {}
from __future__ import absolute_import import json import hashlib import base64 import time from subprocess import Popen, PIPE
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath, skip_if_binaries_missing
import salt.modules.k8s as k8s
self.assertTrue(isinstance(kubectl_out, dict))
b = kubectl_out.get("data", {}) self.assertTrue(isinstance(kubectl_out, dict)) self.assertEqual(expected_data, b)
from __future__ import absolute_import
from salt.modules import aliases from salt.exceptions import SaltInvocationError
from salttesting import TestCase, skipIf from salttesting.mock import MagicMock, patch, NO_MOCK, NO_MOCK_REASON from salttesting.helpers import ensure_in_syspath
from __future__ import absolute_import import os import shutil
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
import salt.utils.odict from salt.modules import seed from salttesting.helpers import ensure_in_syspath
seed.__salt__ = {} seed.__opts__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import incron
incron.__grains__ = {} incron.__salt__ = {} incron.__context__ = {} incron.__opts__ = {}
from __future__ import absolute_import import os
from salttesting import TestCase, skipIf from salttesting.mock import MagicMock, patch
from salt.modules import kmod
from __future__ import absolute_import import sys import re
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch ensure_in_syspath('../../')
from salt.modules import virt from salt.modules import config from salt._compat import ElementTree as ET import salt.utils
import yaml import salt.ext.six as six
self.assertTrue(len(controllers) == 0)
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
from salt.modules import monit
monit.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import MagicMock, patch, NO_MOCK, NO_MOCK_REASON from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
from salt.modules import dig
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.modules import svn
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( mock_open, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import nfs3
nfs3.__grains__ = {} nfs3.__salt__ = {} nfs3.__context__ = {} nfs3.__opts__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( patch, MagicMock, NO_MOCK, NO_MOCK_REASON )
import salt.utils from salt.modules import pagerduty import json
pagerduty.__opts__ = {} pagerduty.__salt__ = { 'config.option': MagicMock(return_value=None) }
from __future__ import absolute_import import copy import logging import os import subprocess from distutils.version import LooseVersion
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
git_mod.__salt__ = {} git_mod.__context__ = {} log = logging.getLogger(__name__)
return {'stdout': _cmd_run_values[' '.join(key)], 'stderr': '', 'retcode': 0, 'pid': 12345}
return not WORKTREE_INFO[key].get('stale', False)
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch )
from salt.modules import win_ntp
win_ntp.__salt__ = {}
IS_WIN = win_ntp.__virtual__()
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.modules import sysbench
sysbench.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
from salt.modules import munin
munin.__salt__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.modules import varnish
varnish.__salt__ = {}
from __future__ import absolute_import
from salt.modules import archive from salt.exceptions import CommandNotFoundError from salt.utils import which_bin
archive.__salt__ = {} archive.__pillar__ = {} archive.__grains__ = {"id": "0"} archive.__opts__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import influx
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import djangomod
djangomod.__grains__ = {} djangomod.__salt__ = {} djangomod.__context__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import hadoop
hadoop.__salt__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.modules import sensors
sensors.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import drac
drac.__grains__ = {} drac.__salt__ = {} drac.__context__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import openstack_config from salt.exceptions import CommandExecutionError
openstack_config.__salt__ = {}
from __future__ import absolute_import
from salt.modules import win_powercfg as powercfg
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch, call )
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( MagicMock, NO_MOCK, NO_MOCK_REASON, patch )
import salt.utils.s3 from salt.modules import s3
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.modules import postfix
postfix.__salt__ = {}
from __future__ import absolute_import
import shutil import tempfile import os from distutils.version import LooseVersion try:
from salttesting import TestCase, skipIf from salttesting.mock import ( mock_open, MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import destructiveTest
from salt.modules import tls import integration
tls.__grains__ = {} tls.__salt__ = {} tls.__context__ = {} tls.__opts__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import win_path
win_path.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import supervisord
from __future__ import absolute_import
from salt.modules import mac_pkgutil
from salttesting import TestCase, skipIf from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch
source = "/foo/bar/fubar.pkg" package_id = "com.foo.fubar.pkg"
_install_from_path.assert_called_with(source)
source = "/foo/bar/fubar.pkg" package_id = "com.foo.fubar.pkg"
self.assertEqual(_install_from_path.called, 0)
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch from salttesting.helpers import ensure_in_syspath
import salt.config import salt.loader from salt.modules import boto_elasticsearch_domain
import logging
from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch
try: import boto3 from botocore.exceptions import ClientError HAS_BOTO = True except ImportError: HAS_BOTO = False
required_boto3_version = '1.2.1'
from __future__ import absolute_import
from salt.utils import is_linux from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
try: import salt.modules.shadow as shadow HAS_SHADOW = True except ImportError: HAS_SHADOW = False
import salt.ext.six as six
from __future__ import absolute_import
from salt.modules import cp from salt.utils import templates from salt.exceptions import CommandExecutionError
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, mock_open, patch, NO_MOCK, NO_MOCK_REASON )
cp.__salt__ = {} cp.__opts__ = {} cp.__pillar__ = {} cp.__grains__ = {} cp.__context__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import pyenv
pyenv.__grains__ = {} pyenv.__salt__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON ensure_in_syspath('../../')
from salt.modules import portage_config
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import win_groupadd
try: import win32com import pythoncom import pywintypes HAS_WIN_LIBS = True except ImportError: HAS_WIN_LIBS = False
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON )
from salt.modules import win_disk
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON )
from salt.modules import sdb
sdb.__opts__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
from salt.modules import match
match.__grains__ = {} match.__salt__ = {} match.__opts__ = {} match.__pillar__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import neutron
from __future__ import absolute_import
from salt.modules import mac_brew
from salttesting import skipIf, TestCase from salttesting.mock import MagicMock, patch, NO_MOCK, NO_MOCK_REASON from salttesting.helpers import ensure_in_syspath
mac_brew.__context__ = {} mac_brew.__salt__ = {} mac_brew.__opts__ = {'user': MagicMock(return_value='bar')}
from __future__ import absolute_import import re
from salttesting.unit import TestCase from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
__salt__
from salt.ext.six.moves import builtins as __builtin__ __builtin__.__salt__ = {}
assert False
self.fail('An exception should be thrown')
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch from salttesting.helpers import ensure_in_syspath
import salt.config import salt.loader from salt.modules import boto_cloudtrail
import logging
from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch
try: import boto import boto3 from botocore.exceptions import ClientError HAS_BOTO = True except ImportError: HAS_BOTO = False
required_boto3_version = '1.2.1'
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import modjk
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import bridge
bridge.__grains__ = {}
from __future__ import absolute_import from textwrap import dedent
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch
import salt from salt.modules import syslog_ng
from __future__ import absolute_import
from salt.modules import linux_sysctl from salt.modules import systemd from salt.exceptions import CommandExecutionError
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, mock_open, patch, NO_MOCK, NO_MOCK_REASON )
linux_sysctl.__salt__ = {} linux_sysctl.__context__ = {} systemd.__context__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
from salt.modules import introspect
introspect.__salt__ = {}
from __future__ import absolute_import import sys
from salttesting import skipIf, TestCase from salttesting.helpers import ( ensure_in_syspath, TestsLoggingHandler, ForceImportErrorOn ) from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch ensure_in_syspath('../../')
from salt.modules import virtualenv_mod from salt.exceptions import CommandExecutionError
virtualenv_mod.__salt__ = {'cmd.which_bin': lambda _: 'pyvenv'}
from __future__ import absolute_import
from salt.modules import proxy as proxy
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch, call )
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) import os
from salt.modules import daemontools from salt.exceptions import CommandExecutionError
daemontools.__grains__ = {} daemontools.__salt__ = {} daemontools.__context__ = {} daemontools.__opts__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, patch)
from salt.modules import riak
riak.__salt__ = {}
from salttesting import TestCase, skipIf from salttesting.mock import ( mock_open, MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
import salt import salt.utils.fsutils from salt.modules import btrfs from salt.exceptions import CommandExecutionError
btrfs.__grains__ = {} btrfs.__salt__ = {} btrfs.__context__ = {}
from __future__ import absolute_import import sys import types
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, Mock, NO_MOCK, NO_MOCK_REASON )
wmi = types.ModuleType('wmi') sys.modules['wmi'] = wmi
from salt.modules import win_dns_client
win_dns_client.__salt__ = {} win_dns_client.__context__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath
from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON, )
from salt.modules import zfs from salt.utils.odict import OrderedDict
zfs.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import pkgutil from salt.exceptions import CommandExecutionError, MinionError
pkgutil.__salt__ = {} pkgutil.__context__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch ensure_in_syspath('../../')
from salt.modules import mdadm
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch from salttesting.helpers import ensure_in_syspath
import salt.config import salt.loader from salt.modules import boto_s3_bucket
import logging
from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch
try: import boto import boto3 from botocore.exceptions import ClientError HAS_BOTO = True except ImportError: HAS_BOTO = False
required_boto3_version = '1.2.1'
from __future__ import absolute_import import os
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import s6
s6.__salt__ = {} s6.SERVICE_DIR = '/etc/service'
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
from salt.modules import mine
mine.__salt__ = {} mine.__opts__ = {}
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch from salttesting.helpers import ensure_in_syspath
import salt.config import salt.loader from salt.modules import boto_vpc from salt.exceptions import SaltInvocationError, CommandExecutionError from salt.modules.boto_vpc import _maybe_set_name_tag, _maybe_set_tags from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch
import salt.ext.six as six try: import boto import boto3 from boto.exception import BotoServerError HAS_BOTO = True except ImportError: HAS_BOTO = False
required_boto_version = '2.8.0' required_moto_version = '0.3.7'
from __future__ import absolute_import
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import MagicMock, patch ensure_in_syspath('../../')
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch, call ensure_in_syspath('../../')
import salt.modules.rvm as rvm
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON )
from salt.modules import sqlite3 import salt
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import MagicMock, patch, NO_MOCK, NO_MOCK_REASON
from salt.modules import extfs
from __future__ import absolute_import
from salttesting import TestCase from salttesting.mock import ( MagicMock, mock_open, patch, ) from salt.modules import hosts from salt.ext.six.moves import StringIO
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import dpkg
dpkg.__grains__ = {} dpkg.__salt__ = {} dpkg.__context__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import MagicMock, patch, call, Mock
from salt.modules import ps import salt.ext.six as six
from __future__ import absolute_import
from salt.modules import mac_sysctl from salt.exceptions import CommandExecutionError
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, mock_open, patch, call, NO_MOCK, NO_MOCK_REASON )
mac_sysctl.__salt__ = {}
from __future__ import absolute_import from __future__ import unicode_literals import sys import time from salttesting import TestCase, skipIf from salttesting.helpers import destructiveTest from salt.modules import reg as win_mod_reg try:
TIMEINT = int(time.time())
UNICODETEST_WITH_SIGNS = 'Testing Unicode \N{COPYRIGHT SIGN},\N{TRADE MARK SIGN},\N{REGISTERED SIGN} '+TIMESTR UNICODETEST_WITHOUT_SIGNS = 'Testing Unicode'+TIMESTR UNICODE_TEST_KEY = 'UnicodeKey \N{TRADE MARK SIGN} '+TIME_INT_UNICODE UNICODE_TEST_KEY_DEL = 'Delete Me \N{TRADE MARK SIGN} '+TIME_INT_UNICODE
test = isinstance(test_list, tuple) and (not test_list[0]) self.assertTrue(test)
test = isinstance(test_list, tuple) and (not test_list[0]) self.assertTrue(test)
test_success = win_mod_reg.delete_value( 'HKEY_LOCAL_MACHINE', subkey, vname ) self.assertTrue(test_success)
test_success = win_mod_reg.delete_key_recursive('HKEY_CURRENT_USER', subkey) self.assertTrue(test_success)
test_success = win_mod_reg.delete_key_recursive('HKEY_LOCAL_MACHINE', subkey) self.assertTrue(test_success)
from __future__ import absolute_import import copy
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.exceptions import SaltException from salt.modules import grains as grainsmod from salt.utils import dictupdate
from salt.utils.odict import OrderedDict
res = grainsmod.filter_by(dict1, grain='xxx') self.assertIs(res, None)
res = grainsmod.filter_by(dict1) self.assertIs(res, None)
res = grainsmod.filter_by(dict1, grain='xxx', merge=mdict1, default='Z') self.assertEqual(res, mdict1)
res = grainsmod.filter_by(dict1, grain='xxx', default='Z') self.assertIs(res, None)
res = grainsmod.filter_by(dict2, grain='xxx', default='xxx', base='default') self.assertEqual(res, dict2['default'])
res = grainsmod.filter_by(dict2, grain='xxx', base='default') self.assertEqual(res, dict2['default'])
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import devmap import os.path
devmap.__grains__ = {} devmap.__salt__ = {} devmap.__context__ = {} devmap.__opts__ = {}
from __future__ import absolute_import
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import MagicMock, patch
from salt.exceptions import CommandExecutionError from salt.modules import mac_xattr as xattr import salt.utils.mac_utils
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import glusterfs from salt.exceptions import SaltInvocationError
glusterfs.__salt__ = {}
self.assertTrue(glusterfs.create_volume('newvolume', 'host1:/brick')) self.assertFalse(mock_start_volume.called)
self.assertTrue(glusterfs.create_volume('newvolume', 'host1:/brick', start=True)) self.assertTrue(mock_start_volume.called)
self.assertFalse(glusterfs.create_volume('newvolume', 'host1:/brick', start=True))
self.assertFalse(glusterfs.delete_volume('Newvolume3'))
self.assertFalse(glusterfs.delete_volume('Newvolume1', False)) self.assertFalse(mock_run.called) self.assertFalse(mock_stop_volume.called)
self.assertTrue(glusterfs.delete_volume('Newvolume1')) self.assertTrue(mock_run.called) self.assertTrue(mock_stop_volume.called)
mock_run.return_value = xml_command_fail self.assertFalse(glusterfs.add_volume_bricks('Newvolume1', ['new:/path']))
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
from salt.modules import nagios import os
nagios.__salt__ = {}
from __future__ import absolute_import import os import tempfile
from salttesting.unit import TestCase from salttesting.helpers import ensure_in_syspath
import salt.utils from salt.modules import ini_manage as ini
option1=main1
option2=main2
test1=value 1
test3 = value 3B
option1 = main1
option2 = main2
test1 = value 1
test3 = new value 3B
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import hipchat
hipchat.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import swift
from __future__ import absolute_import import os.path
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import linux_lvm from salt.exceptions import CommandExecutionError
linux_lvm.__salt__ = {}
from __future__ import absolute_import, print_function
from salt.exceptions import CommandExecutionError from salt.modules import uptime
from __future__ import absolute_import import logging
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, mock_open, call, NO_MOCK, NO_MOCK_REASON )
from salt.modules import dnsutil
from __future__ import absolute_import import yaml
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
import salt.utils from salt.modules import pkg_resource import salt.ext.six as six
pkg_resource.__grains__ = {} pkg_resource.__salt__ = {}
from __future__ import absolute_import
from salt.modules import status from salt.exceptions import CommandExecutionError
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, )
status.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import deb_apache deb_apache.__grains__ = {} deb_apache.__salt__ = {} deb_apache.__context__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import debian_ip
import jinja2.exceptions
debian_ip.__grains__ = {} debian_ip.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( create_autospec, MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
from salt.modules import etcd_mod from salt.utils import etcd_util
etcd_mod.__opts__ = {} etcd_mod.__utils__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import keystone
keystone.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import chef
chef.__grains__ = {} chef.__salt__ = {} chef.__context__ = {}
from __future__ import absolute_import
from salttesting import skipIf from tests.unit import ModuleTestCase, hasDependency from salttesting.mock import ( patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath from salt.modules import servicenow
from __future__ import absolute_import import time
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
from salt.modules import ldapmod
ldapmod.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import http import salt.utils.http
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
from salt.modules import logrotate
logrotate.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import at
import salt.utils
at.__grains__ = {} at.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
from salt.modules import lvs
lvs.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import smtp
from __future__ import absolute_import import uuid
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
from salt.modules import iptables
iptables.__grains__ = {} iptables.__salt__ = {} iptables.__context__ = {} iptables.__opts__ = {}
self.assertEqual(iptables.build_rule(**{'if': '!eth0'}), '! -i eth0')
self.assertEqual(iptables.build_rule(**{'if': 'not eth0'}), '! -i eth0')
self.assertEqual(iptables.build_rule(dports=['!80', 443], proto='tcp'), '-p tcp -m multiport ! --dports 80,443')
self.assertEqual(iptables.build_rule(jump='REDIRECT', **{'to-port': 8080}), '--jump REDIRECT --to-port 8080')
self.assertEqual(iptables.build_rule(jump='LOG', **{'log-prefix': 'long prefix'}), '--jump LOG --log-prefix "long prefix"')
self.assertEqual(iptables.build_rule(jump='LOG', **{'log-prefix': 'spam: '}), '--jump LOG --log-prefix "spam: "')
self.assertEqual(iptables.build_rule(jump='CLUSTERIP', **{'new': ''}), '--jump CLUSTERIP --new ')
self.assertEqual(iptables.build_rule(**{'match-set': 'src flag1,flag2'}), '-m set --match-set src flag1,flag2')
self.assertEqual(iptables.build_rule(**{'match-set': '!src flag'}), '-m set ! --match-set src flag')
#self.assertEqual(iptables.build_rule(jump='CONNSECMARK',
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( mock_open, MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import dnsmasq
import os
dnsmasq.__salt__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch
from salt.modules import config
from __future__ import absolute_import import os
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( mock_open, MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
import salt.utils from salt.modules import puppet from salt.exceptions import CommandExecutionError
puppet.__salt__ = {}
from __future__ import absolute_import import grp import pwd
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import MagicMock, patch, NO_MOCK, NO_MOCK_REASON
from salt.modules import mac_user from salt.exceptions import SaltInvocationError, CommandExecutionError
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, mock_open, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import nftables import salt.utils from salt.exceptions import CommandExecutionError
nftables.__grains__ = {} nftables.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import genesis
genesis.__grains__ = {} genesis.__salt__ = {} genesis.__context__ = {} genesis.__opts__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON
from salt.modules import mac_power from salt.exceptions import SaltInvocationError
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import rpm
rpm.__salt__ = {}
from __future__ import absolute_import
from salt.modules import win_license as license
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch )
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import nova
nova.__grains__ = {} nova.__salt__ = {} nova.__context__ = {} nova.__opts__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch )
from salt.modules import win_shadow import salt.utils
win_shadow.__salt__ = {}
from __future__ import absolute_import import socket import os.path
from salttesting import TestCase, skipIf from salttesting.mock import ( mock_open, MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
import salt.ext.six as six import salt.utils from salt.modules import network from salt.exceptions import CommandExecutionError if six.PY2: import salt.ext.ipaddress
network.__grains__ = {} network.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import guestfs
guestfs.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import oracle import os
oracle.__salt__ = {} oracle.cx_Oracle = object()
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch from salttesting.helpers import ensure_in_syspath
import salt.config import salt.loader from salt.modules import boto_iot
import logging
from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch
try: import boto import boto3 from botocore.exceptions import ClientError HAS_BOTO = True except ImportError: HAS_BOTO = False
required_boto3_version = '1.2.1'
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch from salttesting.helpers import ensure_in_syspath
import salt.config import salt.loader from salt.modules import boto_lambda from salt.exceptions import SaltInvocationError
from tempfile import NamedTemporaryFile import logging import os
from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch
try: import boto3 from botocore.exceptions import ClientError HAS_BOTO = True except ImportError: HAS_BOTO = False
required_boto3_version = '1.2.1'
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import ret import salt.loader
ret.__opts__ = {} ret.__salt__ = {}
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch from salttesting.helpers import ensure_in_syspath
import salt.config import salt.loader from salt.modules import boto_cognitoidentity
import logging
from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch
try: import boto3 from botocore.exceptions import ClientError HAS_BOTO = True except ImportError: HAS_BOTO = False
required_boto3_version = '1.2.1'
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock ensure_in_syspath('../../')
from salt.modules import linux_acl from salt.exceptions import CommandExecutionError
def test_version(self): pass
from __future__ import absolute_import
from salt.modules import mac_package as macpackage
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, call )
from __future__ import absolute_import
from __future__ import absolute_import import os.path import glob
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import qemu_nbd
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
from salt.modules import memcached from salt.exceptions import CommandExecutionError, SaltInvocationError from salt.ext.six import integer_types
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import sysmod
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salt.modules import cpan cpan.__grains__ = {} cpan.__salt__ = {} cpan.__context__ = {}
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
from salt.modules import artifactory
import salt.ext.six as six
from __future__ import absolute_import import sys import types
import salt.ext.six as six
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
wmi = types.ModuleType('wmi') sys.modules['wmi'] = wmi
import salt.modules.win_status as status
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import bluez from salt.exceptions import CommandExecutionError import salt.utils.validate.net
bluez.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, call )
from salt.modules import win_firewall
win_firewall.__salt__ = {}
IS_WIN = win_firewall.__virtual__()
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, Mock, NO_MOCK, NO_MOCK_REASON, patch )
from salt.modules import dockerng as dockerng_mod from salt.exceptions import CommandExecutionError, SaltInvocationError
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import rabbitmq from salt.exceptions import CommandExecutionError
rabbitmq.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
from salt.modules import mod_random import salt.utils.pycrypto from salt.exceptions import SaltInvocationError
mod_random.__grains__ = {} mod_random.__salt__ = {} mod_random.__context__ = {} mod_random.__opts__ = {}
from __future__ import absolute_import
from salt.modules import win_dism as dism
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch )
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch, call
from salt.modules import cron from salt.ext.six.moves import builtins, StringIO
set_crontab(
set_crontab(
set_crontab(
def test__get_cron_cmdstr(self): self.assertEqual('crontab /tmp', cron._get_cron_cmdstr(STUB_PATH))
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import MagicMock, patch, NO_MOCK, NO_MOCK_REASON
from salt.modules import groupadd
import grp
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.modules import rh_ip import jinja2.exceptions import os
rh_ip.__grains__ = {} rh_ip.__salt__ = {}
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch ensure_in_syspath('../../')
import salt.modules.blockdev as blockdev import salt.utils
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, Mock, patch
from __future__ import absolute_import
import contextlib import textwrap import json try: import dns.query import dns.tsigkeyring HAS_DNS = True except ImportError: HAS_DNS = False
from salttesting import TestCase, skipIf from salttesting.mock import ( mock_open, MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import ddns
ddns.__grains__ = {} ddns.__salt__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.modules import redismod from datetime import datetime
redismod.__grains__ = {} redismod.__salt__ = {} redismod.__context__ = {} redismod.__opts__ = {}
from __future__ import absolute_import, print_function import os
from salt.modules import deb_postgres
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.modules import rsync from salt.exceptions import CommandExecutionError, SaltInvocationError
rsync.__salt__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch, call
from salt.modules import mysql
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.modules import system
system.__salt__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.modules import powerpath
powerpath.__salt__ = {}
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
from salt.utils.odict import OrderedDict from salt.modules import jboss7
__salt__
from salt.ext.six.moves import builtins as __builtin__ __builtin__.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import publish import salt.crypt import salt.transport from salt.exceptions import SaltReqTimeoutError
publish.__opts__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import win_ip from salt.exceptions import CommandExecutionError, SaltInvocationError
win_ip.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, mock_open, NO_MOCK, NO_MOCK_REASON )
from salt.modules import poudriere
poudriere.__salt__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.modules import scsi import os import salt.utils import copy
scsi.__salt__ = {} scsi.__context__ = {}
cmd_mock = MagicMock(return_value=lsscsi) with patch.dict(scsi.__salt__, {'cmd.run_all': cmd_mock}): self.assertDictEqual(scsi.ls_(get_size=False), result)
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.modules import rbenv import os
rbenv.__grains__ = {} rbenv.__salt__ = {}
from __future__ import absolute_import import json
from salt.modules import kapacitor
from salttesting import TestCase from salttesting.mock import Mock, patch
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
from salt.modules import moosefs
moosefs.__salt__ = {}
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
event.__grains__ = {} event.__salt__ = {} event.__context__ = {} event.__opts__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import pecl
from __future__ import absolute_import import random import string from copy import deepcopy
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON from salttesting.helpers import ensure_in_syspath
import salt.config import salt.loader
from salt.utils.odict import OrderedDict from salt.modules import boto_secgroup
group_vpc = conn.create_security_group(name=group_name, description=group_description, vpc_id=vpc_id) retrieved_group_id = boto_secgroup.get_group_id(group_name, **conn_parameters) self.assertEqual(group_classic.id, retrieved_group_id)
group_vpc = conn.create_security_group(name=group_name, description=group_description, vpc_id=vpc_id) retrieved_group_id = boto_secgroup.get_group_id(group_name, group_vpc, **conn_parameters) self.assertEqual(group_vpc.id, retrieved_group_id)
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import hg
hg.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch )
from salt.modules import win_timezone
win_timezone.__salt__ = {}
IS_WIN = win_timezone.__virtual__()
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import netscaler
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import npm from salt.exceptions import CommandExecutionError import json
npm.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import serverdensity_device from salt.exceptions import CommandExecutionError
from __future__ import absolute_import import textwrap
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import rh_service
rh_service.__salt__ = {}
from __future__ import absolute_import import os import tempfile import logging import shutil
import salt.ext.six as six from salt.ext.six.moves.urllib.error import URLError from salt.ext.six.moves.urllib.request import urlopen
from salttesting import TestCase, skipIf from salttesting.helpers import ( ensure_in_syspath, requires_network, skip_if_binaries_missing ) ensure_in_syspath('../..')
self.assertTrue( ('Got ' in comment and 'Generated script' in comment) or ('setuptools>=0.7' in comment) )
from __future__ import absolute_import import os
from salttesting import TestCase, skipIf from salttesting.mock import ( mock_open, MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
import salt.utils from salt.exceptions import CommandExecutionError
mount.__grains__ = {} mount.__salt__ = {} mount.__context__ = {}
from __future__ import absolute_import
from salt.modules import mac_defaults as macdefaults
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch )
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON )
ensure_in_syspath('../../') from salt.modules import ssh from salt.exceptions import CommandExecutionError
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, Mock, patch ensure_in_syspath('../../')
from salt.modules import nginx
from __future__ import absolute_import import os import tempfile import textwrap
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import MagicMock, patch
import salt.utils from salt.modules import file as filemod from salt.modules import config as configmod from salt.modules import cmdmod from salt.exceptions import CommandExecutionError
filemod.replace(self.tfile.name, r'Etiam', 'Salticus', flags=['MULTILINE', 'ignorecase'])
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import environ import os
environ.__grains__ = {} environ.__salt__ = {} environ.__context__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.modules import raet_publish import salt.transport from salt.exceptions import SaltReqTimeoutError
raet_publish.__opts__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import composer from salt.exceptions import CommandExecutionError, CommandNotFoundError, SaltInvocationError
composer.__grains__ = {} composer.__salt__ = {} composer.__context__ = {} composer.__opts__ = {}
mock = MagicMock(return_value=False) with patch.object(composer, '_valid_composer', mock): self.assertRaises(CommandNotFoundError, composer.install, 'd')
mock = MagicMock(return_value=True) with patch.object(composer, '_valid_composer', mock): self.assertRaises(SaltInvocationError, composer.install, None)
mock = MagicMock(return_value=False) with patch.object(composer, '_valid_composer', mock): self.assertRaises(CommandNotFoundError, composer.update, 'd')
from __future__ import absolute_import import os import platform
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
import salt.utils from salt.grains import core
core.__salt__ = {}
from __future__ import absolute_import from copy import deepcopy
from salttesting import TestCase, skipIf from salttesting.mock import MagicMock, NO_MOCK, NO_MOCK_REASON, patch from salttesting.helpers import ensure_in_syspath
from salt.cloud.clouds import vmware from salt.exceptions import SaltCloudSystemExit
HAS_LIBS = True try:
from __future__ import absolute_import
from salttesting import TestCase
from salt.cloud.clouds import saltify
saltify.__opts__ = {} saltify.__opts__['providers'] = {}
from __future__ import absolute_import import libcloud.security import platform import os
from salt.cloud.clouds import dimensiondata from salt.exceptions import SaltCloudSystemExit
from salttesting import TestCase, skipIf from salttesting.mock import MagicMock, NO_MOCK, NO_MOCK_REASON, patch from salttesting.helpers import ensure_in_syspath
from __future__ import absolute_import import libcloud.security import platform import os
from salt.cloud.clouds import gce from salt.exceptions import SaltCloudSystemExit
from salttesting import TestCase, skipIf from salttesting.mock import MagicMock, NO_MOCK, NO_MOCK_REASON, patch from salttesting.helpers import ensure_in_syspath
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import NO_MOCK, NO_MOCK_REASON from salttesting.helpers import ensure_in_syspath
from salt.cloud.clouds import linode
self.assertFalse(linode._validate_name('-foo'))
self.assertFalse(linode._validate_name('_foo'))
self.assertFalse(linode._validate_name('foo-'))
self.assertFalse(linode._validate_name('foo_'))
self.assertFalse(linode._validate_name(''))
self.assertFalse(linode._validate_name('ab'))
self.assertTrue(linode._validate_name('abc'))
self.assertEqual(len(long_name), 48) self.assertTrue(linode._validate_name(long_name))
long_name += '1' self.assertEqual(len(long_name), 49) self.assertFalse(linode._validate_name(long_name))
self.assertFalse(linode._validate_name('foo;bar'))
self.assertFalse(linode._validate_name('fooàààààbar'))
self.assertFalse(linode._validate_name('foo bar'))
self.assertTrue(linode._validate_name('foo123bar'))
self.assertTrue(linode._validate_name('foo-bar'))
self.assertTrue(linode._validate_name('foo_bar'))
self.assertTrue(linode._validate_name('1foo')) self.assertTrue(linode._validate_name('foo0'))
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import MagicMock, NO_MOCK, NO_MOCK_REASON, patch from salttesting.helpers import ensure_in_syspath
from salt.cloud.clouds import opennebula from salt.exceptions import SaltCloudSystemExit, SaltCloudNotFound
opennebula.__active_provider_name__ = '' opennebula.__opts__ = {} VM_NAME = 'my-vm'
from __future__ import absolute_import
from salttesting import TestCase
import salt.cloud.libcloudfuncs as libcloud
from salttesting.helpers import ensure_in_syspath
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import patch, MagicMock, NO_MOCK, NO_MOCK_REASON
import integration from salt.cli import daemons
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import patch, call, NO_MOCK, NO_MOCK_REASON, MagicMock
import salt.master import integration from salt import auth
self.clear._send_pub = lambda payload: True
self.clear.mminion.returners = {'.prep_jid': lambda x: 1}
self.clear.publish(self.valid_clear_load) self.assertEqual(fire_event_mock.call_args[0][0]['fun'], 'test.ping')
sys_doc_load = self.valid_clear_load sys_doc_load['fun'] = 'sys.doc' self.clear.publish(sys_doc_load)
self.assertEqual(fire_event_mock.call_args[0][0]['fun'], 'test.echo')
self.valid_clear_load['fun'] = 'sys.doc' self.assertNotEqual(fire_event_mock.call_args[0][0]['fun'], 'sys.doc')
pass
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, patch )
from salt.runners import cache import salt.utils
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import rabbitmq_cluster
rabbitmq_cluster.__salt__ = {} rabbitmq_cluster.__opts__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import rdp
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import sysctl
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch ensure_in_syspath('../../')
import integration from salt.states import pip_state
try: import pip HAS_PIP = True except ImportError: HAS_PIP = False
try: original_pip_version = pip.__version__ pip.__version__ = MagicMock( side_effect=AttributeError( 'Faked missing __version__ attribute' ) ) except AttributeError: pass
if hasattr(pip, '__version__'): pip.__version__ = original_pip_version
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) import os
from salt.states import virtualenv_mod
virtualenv_mod.__salt__ = {} virtualenv_mod.__opts__ = {} virtualenv_mod.__env__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import alternatives
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import keyboard
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import win_update
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import process
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import win_network
win_network.__salt__ = {} win_network.__opts__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import rabbitmq_plugin
from __future__ import absolute_import import contextlib
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import service
service.__salt__ = {} service.__opts__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import boto_elasticache
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import boto_sqs
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import win_servermanager
win_servermanager.__salt__ = {} win_servermanager.__opts__ = {}
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch from salttesting.helpers import ensure_in_syspath
import salt.config import salt.loader
import yaml
from unit.modules.boto_apigateway_test import BotoApiGatewayTestCaseMixin
try: import boto3 from botocore.exceptions import ClientError HAS_BOTO = True except ImportError: HAS_BOTO = False
required_boto3_version = '1.2.1'
self.conn.create_stage.side_effect = ClientError(error_content, 'create_stage') self.conn.create_deployment.side_effect = ClientError(error_content, 'create_deployment')
self.conn.put_method.return_value = method_ret self.conn.put_integration.return_value = method_integration_ret self.conn.put_method_response.return_value = method_response_200_ret self.conn.put_intgration_response.return_value = method_integration_response_200_ret
self.conn.get_rest_apis.return_value = no_apis_ret self.conn.create_rest_api.side_effect = ClientError(error_content, 'create_rest_api')
self.conn.put_method.side_effect = ClientError(error_content, 'put_method')
self.conn.put_method.return_value = method_ret self.conn.put_integration.side_effect = ClientError(error_content, 'put_integration')
self.conn.put_method.return_value = method_ret self.conn.put_integration.return_value = method_integration_ret self.conn.put_method_response.side_effect = ClientError(error_content, 'put_method_response')
from __future__ import absolute_import
from salt.states import mac_assistive as assistive
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch )
from __future__ import absolute_import from contextlib import contextmanager
from salttesting import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import schedule
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import boto_sns
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import bower from salt.exceptions import CommandExecutionError
bower.__salt__ = {} bower.__opts__ = {'test': False}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import zk_concurrency
zk_concurrency.__salt__ = {} zk_concurrency.__opts__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( mock_open, NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import augeas
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import htpasswd
htpasswd.__salt__ = {} htpasswd.__opts__ = {'test': False}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import xmpp
xmpp.__salt__ = {} xmpp.__opts__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import win_system
win_system.__salt__ = {} win_system.__opts__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON)
from salt.states import gnomedesktop
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch ensure_in_syspath('../../')
import salt.states.gem as gem gem.__salt__ = {} gem.__opts__ = {'test': False}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, Mock, MagicMock, patch
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import group
group.__salt__ = {} group.__opts__ = {}
from __future__ import absolute_import from inspect import ArgSpec
from salt.states import module
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, call, patch)
from salt.states import ipset
expected_calls = expected_calls[:1]
from __future__ import absolute_import
from salt.states import win_certutil as certutil
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch )
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import apache_module
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch, mock_open)
from salt.states import apache import salt.utils
from __future__ import absolute_import
from salt.states import host
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import splunk_search
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import apache_site
from __future__ import absolute_import
from salt.states import mac_keychain as keychain
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, call )
from __future__ import absolute_import import copy
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import boto_elb
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import ssh_known_hosts
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( patch, MagicMock, NO_MOCK, NO_MOCK_REASON )
from salt.states import test
test.__salt__ = {} test.__opts__ = {} test.__low__ = {'__reqs__': {'watch': ''}}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import debconfmod
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import incron
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import kmod
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import vbox_guest
vbox_guest.__salt__ = {} vbox_guest.__opts__ = {}
from __future__ import absolute_import
from salt.exceptions import CommandExecutionError from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import timezone
timezone.__salt__ = {} timezone.__opts__ = {}
from __future__ import absolute_import import os
from salt.states import svn
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import pagerduty
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import postgres_user
from __future__ import absolute_import import os import tempfile
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import archive as archive
archive.__salt__ = {} archive.__opts__ = {"cachedir": "/tmp", "test": False} archive.__env__ = 'test'
db = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import pkgng
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import mysql_user import salt
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import drac
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import modjk_worker
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import influxdb_user
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import openstack_config
from __future__ import absolute_import
from salt.states import win_powercfg as powercfg
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import postgres_database
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import rabbitmq_vhost
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import win_path
win_path.__salt__ = {} win_path.__opts__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import supervisord
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch from salttesting.helpers import ensure_in_syspath
import salt.config import salt.loader
import logging
from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch
from unit.modules.boto_elasticsearch_domain_test import BotoElasticsearchDomainTestCaseMixin
try: import boto import boto3 from botocore.exceptions import ClientError HAS_BOTO = True except ImportError: HAS_BOTO = False
required_boto3_version = '1.2.1'
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import ipmi
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import pyenv
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import sysrc
sysrc.__salt__ = {} sysrc.__opts__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import selinux
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch from salttesting.helpers import ensure_in_syspath
import salt.config import salt.loader
import logging
from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch
from unit.modules.boto_cloudtrail_test import BotoCloudTrailTestCaseMixin
try: import boto import boto3 from botocore.exceptions import ClientError HAS_BOTO = True except ImportError: HAS_BOTO = False
required_boto3_version = '1.2.1'
from __future__ import absolute_import
import salt.config from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) import os
from salt.states import winrepo
winrepo.__salt__ = {} winrepo.__opts__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import lxc import salt.utils
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON)
from salt.states import modjk import salt.ext.six as six
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import boto_asg
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import rabbitmq_policy
from __future__ import absolute_import import yaml import re import tempfile import os
class SyslogNGTestCase(TestCase): def test_generate_source_config(self): self._config_generator_template(SOURCE_1_CONFIG, SOURCE_1_EXPECTED)
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import ports import os
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import slack
from __future__ import absolute_import
from salt.states import proxy as proxy
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch, call )
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import win_dns_client
win_dns_client.__salt__ = {} win_dns_client.__opts__ = {}
from __future__ import absolute_import
from salttesting.case import TestCase from salttesting.helpers import ensure_in_syspath
from salt.states import boto_secgroup from salt.utils.odict import OrderedDict
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import grafana
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import mdadm
mdadm.__salt__ = {} mdadm.__opts__ = {}
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch from salttesting.helpers import ensure_in_syspath
import salt.config import salt.loader
import logging
from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch
from unit.modules.boto_s3_bucket_test import BotoS3BucketTestCaseMixin
try: import boto import boto3 from botocore.exceptions import ClientError HAS_BOTO = True except ImportError: HAS_BOTO = False
required_boto3_version = '1.2.1'
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import mongodb_database
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch from salttesting.helpers import ensure_in_syspath
import salt.config import salt.loader import salt.utils.boto
from unit.modules.boto_vpc_test import BotoVpcTestCaseMixin
try: import boto import boto3 from boto.exception import BotoServerError
conn_parameters['key'] = ''.join(random.choice(string.ascii_lowercase + string.digits) for _ in range(50))
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import disk
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch ensure_in_syspath('../../')
import salt.modules.rvm import salt.states.rvm as rvm
import salt.ext.six as six
self.assertEqual(mock.call_count, 0)
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import pyrax_queues
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import boto_iam_role
import os import yaml
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch
from __future__ import absolute_import
from salt.states import mac_xattr as xattr
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch )
from __future__ import absolute_import import socket
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import glusterfs import salt.utils.cloud import salt.modules.glusterfs as mod_glusterfs
glusterfs.__salt__ = {'glusterfs.peer': mod_glusterfs.peer} glusterfs.__opts__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import boto_lc
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import boto_cloudwatch_alarm
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( MagicMock, NO_MOCK, NO_MOCK_REASON, patch)
from salt.states import ini_manage
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import hipchat
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import apache_conf
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import status
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import user
user.__salt__ = {} user.__opts__ = {} user.__grains__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import keystone
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import postgres_extension
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import boto_route53
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import chef
from __future__ import absolute_import
from salt.states import http
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import at
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import postgres_group
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import smtp
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import ssh_auth
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import iptables
iptables.__salt__ = {} iptables.__opts__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import lvs_server
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import mongodb_user
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import nftables
nftables.__salt__ = {} nftables.__opts__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, Mock, MagicMock, patch )
from salt.states import grafana_datasource
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import lvm
from __future__ import absolute_import
from salt.states import win_license as license
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch )
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import makeconf
from __future__ import absolute_import import sys
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import network
network.__salt__ = {} network.__grains__ = {} network.__opts__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import locale
locale.__salt__ = {} locale.__opts__ = {}
from __future__ import absolute_import import os.path
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import cmd
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch from salttesting.helpers import ensure_in_syspath
import salt.config import salt.loader
import logging
from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch
from unit.modules.boto_iot_test import BotoIoTTestCaseMixin
try: import boto import boto3 from botocore.exceptions import ClientError HAS_BOTO = True except ImportError: HAS_BOTO = False
required_boto3_version = '1.2.1'
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch from salttesting.helpers import ensure_in_syspath
import salt.config import salt.loader
import logging
from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch
from unit.modules.boto_lambda_test import BotoLambdaTestCaseMixin, TempZipFile
try: import boto3 from botocore.exceptions import ClientError HAS_BOTO = True except ImportError: HAS_BOTO = False
required_boto3_version = '1.2.1'
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch from salttesting.helpers import ensure_in_syspath
import salt.config import salt.loader
import logging
from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch
from unit.modules.boto_cognitoidentity_test import BotoCognitoIdentityTestCaseMixin
try: import boto3 from botocore.exceptions import ClientError HAS_BOTO = True except ImportError: HAS_BOTO = False
required_boto3_version = '1.2.1'
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import linux_acl
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import influxdb_database
from __future__ import absolute_import
from salt.states import mac_package as macpackage
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch )
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import memcached
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import artifactory
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import saltmod
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import win_firewall import salt.utils
win_firewall.__salt__ = {} win_firewall.__opts__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import ntp
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salt.exceptions import SaltInvocationError from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, Mock, NO_MOCK, NO_MOCK_REASON, patch )
from salt.exceptions import CommandExecutionError from salt.modules import dockerng as dockerng_mod from salt.states import dockerng as dockerng_state
self.assertNotEqual(v['Name'], name)
self.assertEqual(1, len(removed)) volumes.remove(removed[0]) return removed[0]
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import boto_dynamodb
from __future__ import absolute_import
from salt.states import win_dism as dism
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch )
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch
set_crontab(
set_crontab(
from __future__ import absolute_import import os
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import blockdev import salt.utils
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import ddns
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import boto_ec2
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import redismod
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import alias
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import aws_sqs
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import layman
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import powerpath
from __future__ import absolute_import import os
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, mock_open, patch)
from salt.states import virt import salt.utils
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
from salt.states import jboss7 from salt.exceptions import CommandExecutionError
import salt.ext.six as six
__salt__
__builtin__.__salt__ = {}
datasource_properties = {'connection-url': 'jdbc:/old-connection-url'} ds_status = {'created': False}
result = jboss7.datasource_exists(name='appDS', jboss_config={}, datasource_properties=datasource_properties, profile=None)
binding_status = {'created': False}
result = jboss7.bindings_exist(name='bindings', jboss_config={}, bindings={'env': 'DEV'}, profile=None)
binding_status = {'updated': False}
result = jboss7.bindings_exist(name='bindings', jboss_config={}, bindings={'env': 'DEV2'}, profile=None)
__salt__['jboss7.read_simple_binding'].return_value = {'success': True, 'result': {'value': 'DEV2'}}
result = jboss7.bindings_exist(name='bindings', jboss_config={}, bindings={'env': 'DEV2'})
try:
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import postgres_schema
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import tomcat
tomcat.__salt__ = {} tomcat.__opts__ = {} tomcat.__env__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import rbenv
from __future__ import absolute_import
from salttesting import TestCase from salttesting.mock import Mock, patch, mock_open
from salt.states import kapacitor
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import mysql_grants
from __future__ import absolute_import
from salt.states import event
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import quota
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import pecl
from __future__ import absolute_import import os
from salt.states import hg
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import npm
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import serverdensity_device
from __future__ import absolute_import import os
from salttesting import skipIf from salttesting.helpers import ( ensure_in_syspath, requires_network, )
import salt.utils from unit.modules.zcbuildout_test import Base, KNOWN_VIRTUALENV_BINARY_NAMES from salt.modules import zcbuildout as modbuildout from salt.states import zcbuildout as buildout from salt.modules import cmdmod as cmd
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import mount import os
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import postgres_cluster
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import cloud import salt.utils.cloud
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import eselect
from __future__ import absolute_import
from salt.states import mac_defaults as macdefaults
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch )
from __future__ import absolute_import from datetime import datetime from dateutil.relativedelta import relativedelta import json import pprint import tempfile
from salttesting import skipIf, TestCase from salttesting.helpers import destructiveTest, ensure_in_syspath from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, call, mock_open, patch)
import yaml
pillar_value = 'i am the pillar value\n'
pillar_mock = MagicMock(return_value=pillar_value) filestate.__salt__['pillar.get'] = pillar_mock
self.assertEqual(None, ret)
ts = datetime(starting.year, starting.month, starting.day - starting.weekday())
fake_no_match_file_list = generate_fake_files(format='no_match_%Y%m%dT%H%M%S.tar.bz2', every=relativedelta(days=1))
if len(new_retains) < fake_retain[retainable]: new_retains.add(fake_file_list[0]) retained_files |= new_retains
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import reg
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import mysql_query import os
from __future__ import absolute_import import os
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch ) ensure_in_syspath('../../')
import salt.states.environ as envstate import salt.modules.environ as envmodule
ret = envstate.setenv('test', 'other') self.assertEqual(ret['changes'], {})
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import lvs_service
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import aptpkg
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import portage_config
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import composer
from __future__ import absolute_import import os
from salttesting.unit import skipIf from salttesting.case import TestCase from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../..')
try: import tornado.testing import tornado.concurrent from tornado.testing import AsyncTestCase HAS_TORNADO = True except ImportError: HAS_TORNADO = False
class AsyncTestCase(object): pass
futures = [] for x in range(0, 3): future = tornado.concurrent.Future() future.add_done_callback(self.stop) futures.append(future)
any_ = saltnado.Any(futures) self.assertIs(any_.done(), False)
futures[0].set_result('foo') self.wait()
self.assertEqual(any_.result(), futures[0])
from __future__ import absolute_import import json import yaml import os
from salttesting.unit import skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../..')
try: from salt.netapi.rest_tornado import saltnado from salt.netapi.rest_tornado import saltnado_websockets HAS_TORNADO = True except ImportError: HAS_TORNADO = False import salt.auth
class AsyncHTTPTestCase(object): pass
response = self.fetch('/', headers={'Accept': self.content_type_map['xml']}) self.assertEqual(response.code, 406)
response = self.fetch('/', method='POST', body=json.dumps(valid_lowstate), headers={'Content-Type': self.content_type_map['json']})
request_lowstate = { "client": "local", "tgt": "*", "fun": "test.fib", "arg": ["10"] }
request_lowstate = [{ "client": "local", "tgt": "*", "fun": "test.fib", "arg": "10" }]
request_lowstate = { "client": "local", "tgt": "*", "fun": "test.fib", "arg": "10" }
response = self.fetch('/', method='POST', body=json.dumps(request_lowstate), headers={'Content-Type': self.content_type_map['json']})
response = self.fetch('/login', method='POST', body=urlencode(self.auth_creds), headers={'Content-Type': self.content_type_map['form']})
response = self.fetch('/login', method='POST', body=json.dumps(self.auth_creds_dict), headers={'Content-Type': self.content_type_map['json']})
response = self.fetch('/login', method='POST', body=yaml.dump(self.auth_creds_dict), headers={'Content-Type': self.content_type_map['yaml']})
from __future__ import absolute_import import json
import yaml
from __future__ import absolute_import import os
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch
from salt import minion from salt.utils import event from salt.exceptions import SaltSystemExit import salt.syspaths
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch
from salt.returners import smtp_return as smtp
from __future__ import absolute_import import os import shutil import tempfile
from salttesting import TestCase, skipIf from salttesting.helpers import destructiveTest, ensure_in_syspath from salttesting.mock import ( MagicMock, NO_MOCK, NO_MOCK_REASON, patch )
import salt.utils from salt.returners import local_cache
jid_dir, jid_file = self._make_tmp_jid_dirs(create_files=False)
self.assertEqual(jid_file, None)
with patch.dict(local_cache.__opts__, {'keep_jobs': 0.00000001}): local_cache.clean_old_jobs()
self.assertEqual([], os.listdir(TMP_JID_DIR))
jid_dir, jid_file = self._make_tmp_jid_dirs(create_files=False)
self.assertEqual(jid_file, None)
local_cache.clean_old_jobs()
jid_dir_name = jid_dir.rpartition('/')[2]
self.assertEqual([jid_dir_name], os.listdir(TMP_JID_DIR))
jid_dir, jid_file = self._make_tmp_jid_dirs()
jid_dir_name = jid_file.rpartition('/')[2] self.assertEqual(jid_dir_name, 'jid')
with patch('os.path.isfile', MagicMock(return_value=False)) as mock: local_cache.clean_old_jobs()
self.assertEqual([], os.listdir(TMP_JID_DIR))
jid_dir, jid_file = self._make_tmp_jid_dirs()
jid_dir_name = jid_file.rpartition('/')[2] self.assertEqual(jid_dir_name, 'jid')
with patch.dict(local_cache.__opts__, {'keep_jobs': 0.00000001}): local_cache.clean_old_jobs()
self.assertEqual([], os.listdir(TMP_JID_DIR))
if not os.path.exists(TMP_JID_DIR): os.makedirs(TMP_JID_DIR)
temp_dir = tempfile.mkdtemp(dir=TMP_JID_DIR)
from __future__ import absolute_import
from salttesting import TestCase, expectedFailure from salttesting.helpers import ensure_in_syspath
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
from textwrap import dedent
import jinja2
from salt.serializers import json, yamlex, yaml, msgpack, python, configparser from salt.serializers import SerializationError from salt.utils.odict import OrderedDict
assert isinstance(sls_data, dict) assert isinstance(yml_data, dict) assert sls_data == yml_data
assert isinstance(sls_data, OrderedDict) assert not isinstance(yml_data, OrderedDict)
assert isinstance(sls_data, dict) assert isinstance(yml_data, dict) assert sls_data == yml_data
assert isinstance(sls_data, OrderedDict) assert not isinstance(yml_data, OrderedDict)
obj = OrderedDict([ ('foo', 1), ('bar', 2), ('baz', {'qux': True}) ])
yml_obj = obj.copy()
final_obj = OrderedDict(yaml.deserialize(yml_src)) assert obj != final_obj
assert sls_obj.__str__() == '{foo: bar, baz: qux}' assert sls_obj.__repr__() == '{foo: bar, baz: qux}'
assert sls_obj['foo'].__str__() == '"bar"' assert sls_obj['foo'].__repr__() == '"bar"'
serialized = configparser.serialize(data).strip() assert serialized == "[foo]\nbar = baz", serialized
from __future__ import absolute_import, print_function import os import tempfile import time
from salttesting.parser import PNUM, print_header from salttesting.parser.cover import SaltCoverageTestingParser
os.environ['EXPENSIVE_TESTS'] = 'True'
if not self.options.name and not \ self._check_enabled_suites(include_unit=True, include_cloud_provider=True): self._enable_suites(include_unit=True)
TestDaemon.transplant_configs(transport=self.options.transport)
prev_soft, prev_hard = resource.getrlimit(resource.RLIMIT_NOFILE)
min_soft = MAX_OPEN_FILES[limits]['soft_limit'] min_hard = MAX_OPEN_FILES[limits]['hard_limit']
set_limits = False if prev_soft < min_soft: soft = min_soft set_limits = True else: soft = prev_soft
return [True]
if not self._check_enabled_suites(include_cloud_provider=True) and not self.options.name: return status
return [True]
self.set_filehandle_limits('unit')
return status
continue
from __future__ import absolute_import, print_function import os import pwd import time import signal import optparse import subprocess import random import tempfile import shutil import sys
import salt
import yaml import salt.ext.six as six
if opts['root_dir']: tmpdir = os.path.join(opts['root_dir'], 'tmp') else: tmpdir = opts['root_dir']
sys.stdout.write('Generating master config...') self.mkconf() print('done')
if __name__ == '__main__': swarm = Swarm(parse()) try: swarm.start() finally: swarm.shutdown()
from __future__ import absolute_import, print_function import glob import os import re import sys import json import time import shutil import optparse import subprocess import random
import yaml try: import requests HAS_REQUESTS = True except ImportError: HAS_REQUESTS = False
if HAS_REQUESTS is False: parser.error( 'The python \'requests\' library needs to be installed' )
cloud_downtime = random.randint(0, opts.splay) print('Sleeping random period before calling salt-cloud: {0}'.format(cloud_downtime)) time.sleep(cloud_downtime)
#sys.exit(retcode)
print('Cloud configuration files provisioned via pillar.')
retcode = 1
retcode = 1
retcode = 1 if outstr: raise
download_packages(opts)
download_unittest_reports(opts) if opts.test_without_coverage is False: download_coverage_report(opts)
from __future__ import absolute_import from __future__ import print_function import sys import getopt import re import email.utils import datetime
from __future__ import absolute_import, print_function import os import sys import pprint
import msgpack
from __future__ import absolute_import, print_function import socket from struct import unpack import pcapy import sys
cap = pcapy.open_live(self.iface, 65536, 1, 0)
if protocol == 6:
ip_header = packet[eth_length:20+eth_length]
iph = unpack('!BBHHHBBH4s4s', ip_header)
if 'SYN' in flags and len(flags) == 1: return 10 elif 'FIN' in flags: return 12
if 'SYN' in flags and len(flags) == 1: return 100 elif 'FIN' in flags: return 120
args = vars(ArgParser().parse_args())
r_time = 0
ports = [4505, 4506]
stat['4505/est'], stat['4506/est'] = next(SaltNetstat().run())
for item in stat: stat[item] = 0 r_time = s_time
from __future__ import absolute_import, print_function import optparse import pprint import time import os
import salt.utils.event
import salt.ext.six as six
opts.log_file = os.path.join(opts.artifact_dir, 'salt-buildpackage.log')
_run_command(['yum', '-y', 'install'] + build_reqs)
try: sdist = _make_sdist(opts, python_bin=python_bin) except NameError: sdist = _make_sdist(opts)
cmd = ['rpmbuild', '-ba'] cmd.extend(define_opts) cmd.append(spec_path) stdout, stderr, rcode = _run_command(cmd)
from __future__ import absolute_import import atexit import os import readline import sys from code import InteractiveConsole
import salt.client import salt.config import salt.loader import salt.output import salt.pillar import salt.runner
import jinja2
__opts__ = salt.config.client_config( os.environ.get('SALT_MINION_CONFIG', '/etc/salt/minion'))
if 'grains' not in __opts__ or not __opts__['grains']: __opts__['grains'] = salt.loader.grains(__opts__)
if 'file_client' not in __opts__ or not __opts__['file_client']: __opts__['file_client'] = 'local'
if 'id' not in __opts__ or not __opts__['id']: __opts__['id'] = 'saltsh_mid'
__salt__ = salt.loader.minion_mods(__opts__) __grains__ = __opts__['grains']
readline.set_history_length(300)
import sys import time import datetime
import salt.config import salt.client.raet
runtime_reqs_sec = self.total_complete / elapsed_time.total_seconds() print('Recalibrating. Current reqs/sec: {0}'.format(runtime_reqs_sec)) return
_, region, _, _ = _get_profile(service, region, None, None, profile) return region
return self
self.close()
pass
vb_get_manager() vbox = _virtualboxManager.vbox return vbox
state_value = getattr(_virtualboxManager.constants, "SessionState_" + expected_state) return xp_session.state == state_value
manager = vb_get_manager() machines = manager.getArray(vb_get_box(), "machines") return [ vb_xpcom_to_attribute_dict(machine, "IMachine", **kwargs) for machine in machines ]
try: return machine.launchVMProcess(session, "", "") except Exception as e: log.debug(e.message, exc_info=True) return None
return vb_machinestate_to_tuple(machinestate)[0]
return vb_machinestate_to_tuple(machinestate)[1]
if isinstance(machinestate, int): return MACHINE_STATES_ENUM.get(machinestate, UNKNOWN_MACHINE_STATE) elif isinstance(machinestate, str): return MACHINE_STATES.get(machinestate, UNKNOWN_MACHINE_STATE) else: return UNKNOWN_MACHINE_STATE
func.__doc__ = doc
__import__(name) return sys.modules[name]
return hasattr(self.__get_module(fullname), "__path__")
setattr(_MovedItems, move.name, move)
try: delattr(_MovedItems, name) except AttributeError: try: del moves.__dict__[name] except KeyError: raise AttributeError("no such move, %r" % (name,))
try: return _int_to_bytes(address, 16, 'big') except: raise ValueError("Address negative or too large for IPv6")
addr = str(address).split('/') if len(addr) > 2: raise AddressValueError("Only one '/' permitted in %r" % address) return addr
first = last = addresses[0] for ip in addresses[1:]: if ip._ip == last._ip + 1: last = ip else: break return (first, last)
return self._explode_shorthand_ip_string()
return str(self)
network = int(self.network_address) broadcast = int(self.broadcast_address) for x in long_range(network + 1, broadcast): yield self._address_class(x)
return self.network_address in other or ( self.broadcast_address in other or ( other.network_address in self or ( other.broadcast_address in self)))
return int(self.broadcast_address) - int(self.network_address) + 1
return (self._version, self.network_address, self.netmask)
return (self.network_address.is_multicast and self.broadcast_address.is_multicast)
return (self.network_address.is_reserved and self.broadcast_address.is_reserved)
return (self.network_address.is_link_local and self.broadcast_address.is_link_local)
return (self.network_address.is_private and self.broadcast_address.is_private)
return not self.is_private
return (self.network_address.is_unspecified and self.broadcast_address.is_unspecified)
return (self.network_address.is_loopback and self.broadcast_address.is_loopback)
reverse_octets = str(self).split('.')[::-1] return '.'.join(reverse_octets) + '.in-addr.arpa'
return v4_int_to_packed(self._ip)
reverse_chars = self.exploded[::-1].replace(':', '') return '.'.join(reverse_chars) + '.ip6.arpa'
return v6_int_to_packed(self._ip)
multicast_network = IPv6Network('ff00::/8') return self in multicast_network
linklocal_network = IPv6Network('fe80::/10') return self in linklocal_network
return not self.is_private
return self._ip == 0
return self._ip == 1
if (self._ip >> 32) != 0xFFFF: return None return IPv4Address(self._ip & 0xFFFFFFFF)
if (self._ip >> 96) != 0x20010000: return None return (IPv4Address((self._ip >> 64) & 0xFFFFFFFF), IPv4Address(~self._ip & 0xFFFFFFFF))
if (self._ip >> 112) != 0x2002: return None return IPv4Address((self._ip >> 80) & 0xFFFFFFFF)
fi exit 0
pass
sys.stdout.write("{0}\next_mods\n".format(OPTIONS.delimiter)) sys.exit(EX_MOD_DEPLOY)
if salt.utils.is_windows() and HAS_DEPENDENCIES: return True return (False, "Module win_wua: module has failed dependencies or is not on Windows client")
return install_updates([guid])
re.MULTILINE)
if port_name.match(name): return True else: return False
if dns_label.match(name): return True else: return False
if salt.utils.is_windows(): return __virtualname__ return (False, 'Module cyg: module only works on Windows systems.')
if cyg_arch == 'x86_64': return 'cygwin64' elif cyg_arch == 'x86': return 'cygwin' raise SaltInvocationError( 'Invalid architecture {arch}'.format(arch=cyg_arch))
return cloud.list_nodes_select( list_nodes_full('function'), __opts__['query.selection'], call, )
return not isinstance(obj, six.string_types) and isinstance(obj, Iterable)
return not isinstance(obj, six.string_types) and isinstance(obj, Sequence)
super(PresenterTestCase, self).setUp()
super(PresenterTestCase, self).tearDown()
super(StatsEventerTestCase, self).setUp()
super(StatsEventerTestCase, self).tearDown()
log.debug('SQL Query: {0}'.format(cmd)) cur.execute(cmd) return True
log.debug('SQL Query: {0}'.format(cmd)) cur.execute(cmd) return True
return HAS_SQLITE3
if not ret: return None if 'ret_config' not in ret: return '' return str(ret['ret_config'])
self.base_url = base_url self.anchor = anchor self.section = section
return '{0}#{1}{2}'.format(self.base_url, self.anchor, self.section)
return self.assertCleanError(ipaddress.AddressValueError, details, *args)
return self.assertCleanError(ipaddress.NetmaskValueError, details, *args)
self.assertEqual(self.factory(lhs), self.factory(rhs))
addr = "camelot" msg = '%r does not appear to be an IPv4 or IPv6 %s' with self.assertCleanError(ValueError, msg, addr, kind): factory(addr)
output = self.run_cloud('-d {0} --assume-yes --log-level=debug'.format(machine_name)) return output.get(CONFIG_NAME, {}).get(PROVIDER_NAME, {})
machines = self.run_cloud('-p {0} {1} --log-level=debug'.format(PROFILE_NAME, INSTANCE_NAME)) self.assertIn(INSTANCE_NAME, machines.keys())
if vb_machine_exists(INSTANCE_NAME): vb_destroy_machine(INSTANCE_NAME)
self.backend = backend return 'salt'
self.backend = backend self.server = server return 'server enabled'
self.backend = backend self.server = server return 'server disabled'
self.backend = backend self.server = server self.weight = weight return 'server weight'
return 'server frontend'
return 'server backend'
def __init__(self): self.cmds = Mockcmds()
self.ha_cmd = ha_cmd self.objectify = objectify return True
return True
return True
return tenant_id
return (tenant_id, subnet, router, network, floatingip, port, security_group, security_group_rule)
return tenant_id
return True
return True
return port
return (name, network, device_id, admin_state_up)
return (port, name, admin_state_up)
return port
return True
return network
return (name, admin_state_up, router_ext, network_type, physical_network, segmentation_id, shared)
return (network, name)
return network
return True
return subnet
return (network, cidr, name, ip_version)
return (subnet, name)
return subnet
return True
return router
return (name, ext_network, admin_state_up)
return (router, name, admin_state_up, kwargs)
return router
return (router, subnet)
return (router, subnet)
return (router, ext_network)
return router
return True
return floatingip_id
return (floating_network, port)
return (floating_network, port)
return floatingip_id
return True
return security_group
return (name, description)
return (security_group, name, description)
return security_group
return True
return security_group_rule_id
return (security_group, remote_group_id, direction, protocol, port_range_min, port_range_max, ethertype)
return security_group_rule_id
return (retrieve_all, kwargs)
return (vpnservice, kwargs)
return (subnet, router, name, admin_state_up)
return (vpnservice, desc)
return vpnservice
return True
return ipsec_site_connection
return (name, ipsecpolicy, ikepolicy, vpnservice, peer_cidrs, peer_address, peer_id, psk, admin_state_up, kwargs)
return ipsec_site_connection
return True
return ikepolicy
return (name, kwargs)
return ikepolicy
return True
return ipsecpolicy
return (name, kwargs)
return ipsecpolicy
cr_ec2 = MockEC2() cr_ec2.tenant_id = tenantid cr_ec2.user_id = userid return cr_ec2
self.access = accesskey self.user_id = userid return True
cr_ec2 = MockEC2() cr_ec2.profile = profile cr_ec2.access = access cr_ec2.user_id = user_id cr_ec2.connection_args = connection_args return cr_ec2
cr_ec2 = MockEC2() cr_ec2.user_id = user_id return [cr_ec2]
return [MockEndpoints()]
return (region, service_id, publicurl, adminurl, internalurl)
return id
service = MockServices() service.id = '005' service.name = name service.description = description service.type = service_type return service
service = MockServices() if self.flag == 1: service.id = 'asd' return [service] elif self.flag == 2: service.id = service_id return service return [service]
service = MockServices() if self.flag == 1: service.id = 'asd' return [service] return [service]
return service_id
return name
role = MockRoles() if self.flag == 1: role.id = None return role role.id = role_id return role
return [MockRoles()]
return role
return (user_id, role_id, tenant_id)
return (user_id, role_id, tenant_id)
role = MockRoles() role.user_id = user role.tenant_id = tenant return [role]
tenant = MockTenants() tenant.name = name tenant.description = description tenant.enabled = enabled return tenant
tenant = MockTenants() if self.flag == 1: tenant.id = None return tenant tenant.id = tenant_id return tenant
return [MockTenants()]
return tenant_id
return {'id': self.id, 'expires': self.expires, 'user_id': self.user_id, 'tenant_id': self.tenant_id}
user = MockUsers() user.name = name user.password = password user.email = email user.enabled = enabled self.tenant_id = tenant_id return user
user = MockUsers() if self.flag == 1: user.id = None return user user.id = user_id return user
return [MockUsers()]
return user_id
return (user, name, email, enabled)
return (user, password)
def __init__(self, message='Test'): super(Unauthorized, self).__init__(message) self.msg = message
def __init__(self): self.Unauthorized = Unauthorized self.AuthorizationFailure = AuthorizationFailure
def __init__(self): self.exceptions = MockExceptions()
if self.flag == 1: raise Unauthorized return True
return [('127.0.0.1:11211 (1)', {})]
return []
return [('127.0.0.1:11211 (1)', {})]
return key
return [('127.0.0.1:11211 (1)', {})]
self.key = key self.value = value self.time = time self.min_compress_len = min_compress_len return True
return [('127.0.0.1:11211 (1)', {})]
self.key = key self.time = time return True
return [('127.0.0.1:11211 (1)', {})]
self.key = key self.value = value self.time = time self.min_compress_len = min_compress_len return True
return [('127.0.0.1:11211 (1)', {})]
self.key = key self.value = value self.time = time self.min_compress_len = min_compress_len return True
return [('127.0.0.1:11211 (1)', {})]
self.key = key return 1
self.key = key if not isinstance(delta, integer_types): raise SaltInvocationError('Delta value must be an integer') return key
return [('127.0.0.1:11211 (1)', {})]
self.key = key return key
return [('127.0.0.1:11211 (1)', {})]
self.key = key return None
return [('127.0.0.1:11211 (1)', {})]
self.key = key return 1
self.key = key if not isinstance(delta, integer_types): raise SaltInvocationError('Delta value must be an integer') return key
return [('127.0.0.1:11211 (1)', {})]
self.key = key return key
return [('127.0.0.1:11211 (1)', {})]
states = [] def __init__(self, opts): pass
functions = [] def __init__(self, opts): pass
self.opts = opts self.lst = lst if self.flag: return {} return []
self.opts = opts self.lst = lst return {}
def read(self): return MOCK_STATUS_OUTPUT def close(self): pass
__module__ = 'A'
__context__ = {'ports.install_error': 'salt'}
def __init__(self): self.modules = {'A': MockContext()}
indices = np.array(indices, dtype=int) indices.shape = (-1, 2) return indices
step = None if 0 in cost_matrix.shape else _step1
results = np.array(np.where(state.marked == 1)).T
if state.transposed: results = results[:, ::-1]
state.marked[state.marked == 2] = 0 return _step3
warnings.simplefilter('ignore', _NonBLASDotWarning)
raise TypeError('Expected sequence or array-like, got ' 'estimator %s' % x)
joined += ','
spmatrix = spmatrix.asformat(accept_sparse[0]) changed_format = True
spmatrix = spmatrix.astype(dtype)
spmatrix = spmatrix.copy()
dtype_numeric = dtype == "numeric"
dtype_orig = None
dtype = np.float64
dtype = None
dtype = dtype[0]
array = np.array(array, dtype=dtype, order=order, copy=copy)
raise _NotFittedError(msg % {'name': type(estimator).__name__})
update_wrapper(self, fn)
from ..preprocessing import LabelEncoder
weight = np.ones(classes.shape[0], dtype=np.float64, order='C')
y_subsample = y[indices, k] classes_subsample = np.unique(y_subsample)
weight_k[in1d(y_full, list(classes_missing))] = 0.
version.append(x)
out *= .5 np.tanh(out, out) out += 1 out *= .5
if 'order' in signature(np.copy).parameters: def safe_copy(X): return np.copy(X, order='K') else: safe_copy = np.copy
def astype(array, dtype, copy=True): if not copy and array.dtype == dtype: return array return array.astype(dtype)
warnings.simplefilter('always') sp.csr_matrix([1.0, 2.0, 3.0]).max(axis=0)
value = np.zeros_like(X.data)
def argpartition(a, kth, axis=-1, kind='introselect', order=None): return np.argsort(a, axis=axis, order=order)
def frombuffer_empty(buf, dtype): if len(buf) == 0: return np.empty(0, dtype=dtype) else: return np.frombuffer(buf, dtype=dtype)
ar1 = np.asarray(ar1).ravel() ar2 = np.asarray(ar2).ravel()
if not assume_unique: ar1, rev_idx = np.unique(ar1, return_inverse=True) ar2 = np.unique(ar2)
from ._scipy_sparse_lsqr_backport import lsqr as sparse_lsqr
weight_cdf = sample_weight[sorted_idx].cumsum() percentile_idx = np.searchsorted( weight_cdf, (percentile / 100.) * weight_cdf[-1]) return array[sorted_idx[percentile_idx]]
import os import inspect import pkgutil import warnings import sys import re import platform import struct
from urllib2 import urlopen from urllib2 import HTTPError
from urllib.request import urlopen from urllib.error import HTTPError
try: WindowsError except NameError: WindowsError = None
from nose.tools import assert_equal from nose.tools import assert_not_equal from nose.tools import assert_true from nose.tools import assert_false from nose.tools import assert_raises from nose.tools import raises from nose import SkipTest from nose import with_setup
assert_raises_regexp = assert_raises_regex
if not len(w) > 0: raise AssertionError("No warning raised when calling %s" % func.__name__)
for index in [i for i, x in enumerate(found) if x]:
def assert_no_warnings(func, *args, **kw):
clean_warning_registry() with warnings.catch_warnings(record=True) as w: warnings.simplefilter('always')
w = [e for e in w if e.category is not np.VisibleDeprecationWarning]
clean_warning_registry() with warnings.catch_warnings(): warnings.simplefilter("ignore", self.category) return fn(*args, **kwargs)
if isinstance(exceptions, tuple): names = " or ".join(e.__name__ for e in exceptions) else: names = exceptions.__name__
for name in datasets: datasets[name] = datasets[name].T
from sklearn import datasets datasets.mldata.urlopen = mock_mldata_urlopen(mock_datasets)
from sklearn import datasets datasets.mldata.urlopen = urlopen
estimators = [c for c in estimators if not is_abstract(c[1])]
if not include_meta_estimators: estimators = [c for c in estimators if not c[0] in META_ESTIMATORS] if type_filter is not None: if not isinstance(type_filter, list): type_filter = [type_filter] else:
return sorted(set(estimators), key=itemgetter(0))
import matplotlib.pyplot as plt plt.figure()
shutil.rmtree(folder_path)
try: return X.iloc[indices] except ValueError: warnings.warn("Copying input dataframe for slicing.", DataConversionWarning) return X.copy().iloc[indices]
return X.take(indices, axis=0)
arnorm = alfa * beta if arnorm == 0: print(msg[0]) return x, istop, itn, r1norm, r2norm, anorm, acond, arnorm, xnorm, var
rhobar1 = sqrt(rhobar**2 + damp**2) cs1 = rhobar / rhobar1 sn1 = damp / rhobar1 psi = sn1 * phibar phibar = cs1 * phibar
cs, sn, rho = _sym_ortho(rhobar1, beta)
t1 = phi / rho t2 = -theta / rho dk = (1 / rho) * w
acond = anorm * sqrt(ddnorm) res1 = phibar**2 res2 = res2 + psi**2 rnorm = sqrt(res1 + res2) arnorm = alfa * abs(tau)
r1sq = rnorm**2 - dampsq * xxnorm r1norm = sqrt(abs(r1sq)) if r1sq < 0: r1norm = -r1norm r2norm = rnorm
if test3 <= ctol: istop = 3 if test2 <= atol: istop = 2 if test1 <= rtol: istop = 1
yield check_estimators_empty_data_messages
yield check_pipeline_consistency
yield check_estimators_nan_inf
yield check_estimators_overwrite_params
yield check_estimators_pickle
yield check_estimators_unfitted if 'class_weight' in Classifier().get_params().keys(): yield check_class_weight_classifiers
yield check_clustering yield check_estimators_partial_fit_n_features
estimator.set_params(alpha=.5)
estimator.set_params(n_components=1)
estimator.set_params(k=1)
X -= X.min() - .1 this_X = NotAnArray(X) this_y = NotAnArray(np.asarray(y)) _check_transformer(name, Transformer, this_X, this_y)
with warnings.catch_warnings(record=True): transformer = Transformer() set_random_state(transformer) set_testing_parameters(transformer)
transformer_clone = clone(transformer) X_pred = transformer_clone.fit_transform(X, y=y_)
assert_equal(X_pred.shape[0], n_samples)
if hasattr(X, 'T'): assert_raises(ValueError, transformer.transform, X.T)
msg = name + ' is non deterministic on 32bit Python' raise SkipTest(msg)
assert_raises(ValueError, e.fit, X_zero_samples, [])
X -= X.min()
y = multioutput_estimator_convert_y_2d(name, y)
with warnings.catch_warnings(record=True): estimator = Estimator()
pickled_estimator = pickle.dumps(estimator) unpickled_estimator = pickle.loads(pickled_estimator)
alg.fit(X) alg.fit(X.tolist())
if name is 'SpectralClustering': return set_random_state(alg) with warnings.catch_warnings(record=True): pred2 = alg.fit_predict(X) assert_array_equal(pred, pred2)
if hasattr(clusterer, "random_state"): clusterer.set_params(random_state=0)
assert_raises(ValueError, classifier.decision_function, X.T) assert_raises(ValueError, classifier.decision_function, X.T)
X -= X.min()
X, y = _boston_subset()
return
with warnings.catch_warnings(record=True): estimator = Estimator() set_testing_parameters(estimator) set_random_state(estimator) estimator.fit(X, y) y_pred = estimator.predict(X)
X -= X.min() - .1 y_names = np.array(["one", "two", "three"])[y]
with warnings.catch_warnings(record=True): regressor_1 = Regressor() regressor_2 = Regressor() set_testing_parameters(regressor_1) set_testing_parameters(regressor_2) set_random_state(regressor_1) set_random_state(regressor_2)
rng = np.random.RandomState(0) X = rng.normal(size=(10, 4)) y = multioutput_estimator_convert_y_2d(name, X[:, 0]) regressor = Regressor()
regressor.n_components = 1
continue
raise SkipTest
raise SkipTest
classifier.set_params(n_iter=1000)
classifier.set_params(class_weight='balanced') coef_balanced = classifier.fit(X, y).coef_.copy()
n_samples = len(y) n_classes = float(len(np.unique(y)))
X -= X.min() with warnings.catch_warnings(record=True): estimator = Estimator()
params = estimator.get_params() original_params = deepcopy(params)
estimator.fit(X, y)
new_params = estimator.get_params() for param_name, original_value in original_params.items(): new_value = new_params[param_name]
est.sparsify() assert_true(sparse.issparse(est.coef_)) pred = est.predict(X) assert_array_equal(pred, pred_orig)
est = pickle.loads(pickle.dumps(est)) assert_true(sparse.issparse(est.coef_)) pred = est.predict(X) assert_array_equal(pred, pred_orig)
with warnings.catch_warnings(record=True): estimator_1 = Estimator() estimator_2 = Estimator() set_testing_parameters(estimator_1) set_testing_parameters(estimator_2) set_random_state(estimator_1) set_random_state(estimator_2)
init = getattr(estimator.__init__, 'deprecated_original', estimator.__init__)
return
init_params = init_params[1:]
assert_true(init_param.default is None) continue
if "MultiTask" in name: return np.reshape(y, (-1, 1)) return y
if not (name == 'HuberRegressor' and estimator.n_iter_ is None): assert_greater_equal(estimator.n_iter_, 1)
if name in CROSS_DECOMPOSITION: for iter_ in estimator.n_iter_: assert_greater_equal(iter_, 1) else: assert_greater_equal(estimator.n_iter_, 1)
_unique_labels = _FN_UNIQUE_LABELS.get(label_type, None) if not _unique_labels: raise ValueError("Unknown label type: %s" % repr(ys))
if (len(set(isinstance(label, string_types) for label in ys_labels)) > 1): raise ValueError("Mix of label input types (string and number)")
return 'unknown'
if y.ndim > 2 or (y.dtype == object and len(y) and not isinstance(y.flat[0], string_types)):
if y.dtype.kind == 'f' and np.any(y != y.astype(int)): return 'continuous' + suffix
clf.classes_ = unique_labels(classes) return True
if 0 in classes_k: class_prior_k[classes_k == 0] += zeros_samp_weight_sum
if 0 not in classes_k and y_nnz[k] < y.shape[0]: classes_k = np.insert(classes_k, 0, 0) class_prior_k = np.insert(class_prior_k, 0, zeros_samp_weight_sum)
def __init__(self, array): self.array = array self.shape = array.shape self.ndim = array.ndim self.iloc = ArraySlicingWrapper(array)
return self.array
if np_version < (1, 7, 1): _ravel = np.ravel else: _ravel = partial(np.ravel, order='K')
if X.flags.c_contiguous: return check_array(X.T, copy=False, order='F'), True else: return check_array(X, copy=False, order='F'), False
return np.dot(A, B)
Q = random_state.normal(size=(A.shape[1], size))
if power_iteration_normalizer == 'auto': if n_iter <= 2: power_iteration_normalizer = 'none' else: power_iteration_normalizer = 'LU'
Q, _ = linalg.qr(safe_sparse_dot(A, Q), mode='economic') return Q
n_iter = 4 n_iter_specified = False
M = M.T
B = safe_sparse_dot(Q.T, M)
Uhat, s, V = linalg.svd(B, full_matrices=False) del B U = np.dot(Q, Uhat)
U, V = svd_flip(U, V, u_based_decision=False)
return V[:n_components, :].T, s[:n_components], U[:, :n_components].T
vmax = arr.max(axis=0) out = np.log(np.sum(np.exp(arr - vmax), axis=0)) out += vmax return out
above_cutoff = (abs(s) > cond * np.max(abs(s))) psigma_diag = np.zeros_like(s) psigma_diag[above_cutoff] = 1.0 / s[above_cutoff]
last_sum = last_mean * last_sample_count new_sum = X.sum(axis=0)
ret = line_search_wolfe2(f, fprime, xk, pk, gfk, old_fval, old_old_fval, **kwargs)
while k < maxiter: fgrad, fhess_p = grad_hess(xk, *args)
xsupi = _cg(fhess_p, fgrad, maxiter=maxinner, tol=termcond)
if (not directed) and isspmatrix_csc(csgraph): csgraph = csgraph.T
csgraph_from_dense = None
from __future__ import division import numpy as np import scipy.sparse as sp import operator import array
idx = idx.item(0)
if a.ndim == 0: return idx
if class_probability is None: class_prob_j = np.empty(shape=classes[j].shape[0]) class_prob_j.fill(1 / classes[j].shape[0]) else: class_prob_j = np.asarray(class_probability[j])
if 0 not in classes[j]: classes[j] = np.insert(classes[j], 0, 0) class_prob_j = np.insert(class_prob_j, 0, 0.0)
from sklearn.utils.linear_assignment_ import _hungarian
([[400, 150, 400], [400, 450, 600], [300, 225, 300]],
([[400, 150, 400, 1], [400, 450, 600, 2], [300, 225, 300, 3]],
([[10, 10, 8], [9, 8, 1], [9, 7, 4]], 18 ),
([[10, 10, 8, 11], [9, 8, 1, 1], [9, 7, 4, 10]], 15 ),
([[], []], 0 ),
b_float32 = astype(a_int32, dtype=np.float32, copy=False) assert_equal(b_float32.dtype, np.float32)
assert_false(np.may_share_memory(b_float32, a_int32))
c_int32 = astype(a_int32, dtype=np.int32, copy=False) assert_true(c_int32 is a_int32)
d_int32 = astype(a_int32, dtype=np.int32, copy=True) assert_false(np.may_share_memory(d_int32, a_int32))
def test_invalid_sample_without_replacement_algorithm(): assert_raises(ValueError, sample_without_replacement, 5, 4, "unknown")
assert_raises(ValueError, sample_without_replacement, 0, 1) assert_raises(ValueError, sample_without_replacement, 1, 2)
assert_equal(sample_without_replacement(0, 0).shape, (0, ))
assert_equal(sample_without_replacement(5, 0).shape, (0, )) assert_equal(sample_without_replacement(5, 1).shape, (1, ))
assert_raises(ValueError, sample_without_replacement, -1, 5) assert_raises(ValueError, sample_without_replacement, 5, -1)
n_population = 100
assert_equal(np.size(sample_without_replacement(0, 0)), 0)
n_population = 10
n_trials = 10000
n_expected = combinations(n_population, n_samples, exact=True)
check_estimator(AdaBoostClassifier) check_estimator(MultiTaskElasticNet)
msg = "AttributeError or ValueError not raised by predict" assert_raises_regex(AssertionError, msg, check_estimators_unfitted, "estimator", NoSparseClassifier)
check_estimators_unfitted("estimator", CorrectNotFittedErrorClassifier)
X = np.arange(12).reshape(3, 4)
X_inf = np.arange(4).reshape(2, 2).astype(np.float) X_inf[0, 0] = np.inf assert_raises(ValueError, check_array, X_inf)
X_nan = np.arange(4).reshape(2, 2).astype(np.float) X_nan[0, 0] = np.nan assert_raises(ValueError, check_array, X_nan)
assert_equal(X.format, X_checked.format)
assert_equal(X_checked.format, accept_sparse[0])
if (X.dtype == X_checked.dtype and X.format == X_checked.format): assert_true(X is X_checked)
X_dense = check_array([[1, 2], [3, 4]]) assert_true(isinstance(X_dense, np.ndarray)) assert_raises(ValueError, check_array, X_ndim.tolist())
X_no_array = NotAnArray(X_dense) result = check_array(X_no_array) assert_true(isinstance(result, np.ndarray))
msg = "0 feature(s) (shape=(1, 0)) while a minimum of 1 is required." assert_raise_message(ValueError, msg, check_array, [[]])
msg = "0 sample(s) (shape=(0,)) while a minimum of 1 is required." assert_raise_message(ValueError, msg, check_array, [], ensure_2d=False)
msg = "Singleton array array(42) cannot be considered a valid collection." assert_raise_message(TypeError, msg, check_array, 42, ensure_2d=False)
X_checked = assert_warns(DeprecationWarning, check_array, [42], ensure_2d=True) assert_array_equal(np.array([[42]]), X_checked)
assert_raise_message(ValueError, msg, check_X_y, X, y, ensure_min_samples=2, ensure_2d=False)
assert_raise_message(ValueError, msg, check_X_y, X, y, ensure_min_features=3, allow_nd=True)
assert_raises(ValueError, check_symmetric, arr_bad)
for arr_format, arr in test_arrays.items(): assert_warns(UserWarning, check_symmetric, arr) assert_raises(ValueError, check_symmetric, arr, raise_exception=True)
assert_raises(ValueError, check_is_fitted, ARDRegression, "coef_") assert_raises(TypeError, check_is_fitted, "SVR", "support_")
assert_raises_regexp(TypeError, 'estimator', check_consistent_length, [1, 2], RandomForestRegressor())
dist_matrix[dist_matrix != 0] = 1
dist_dict = defaultdict(int) dist_dict.update(single_source_shortest_path_length(dist_matrix, i))
assert_true(check_random_state(None) is np.random.mtrand._rand) assert_true(check_random_state(np.random) is np.random.mtrand._rand)
with warnings.catch_warnings(record=True) as w: warnings.simplefilter("always")
with warnings.catch_warnings(record=True) as w: warnings.simplefilter("always")
assert_true(resample() is None)
random_state = check_random_state(42)
v0 = random_state.uniform(-1,1, A.shape[0]) w, _ = eigsh(A, k=k, sigma=0.0, v0=v0)
assert_greater_equal(w[0], 0)
X.setflags(write=False) X_df_readonly = pd.DataFrame(X) with warnings.catch_warnings(record=True): X_df_ro_indexed = safe_indexing(X_df_readonly, inds)
some_range = range(10) joined_range = list(chain(*[some_range[slice] for slice in gen_even_slices(10, 3)])) assert_array_equal(some_range, joined_range)
slices = gen_even_slices(10, -1) assert_raises_regex(ValueError, "gen_even_slices got n_packs=-1, must be" " >=1", next, slices)
X[0, 0] = 0 X[2, 1] = 0 X[4, 3] = 0 X_lil = sp.lil_matrix(X) X_lil[1, 0] = 0 X[1, 0] = 0
X[0, 0] = 0 X[2, 1] = 0 X[4, 3] = 0 X_lil = sp.lil_matrix(X) X_lil[1, 0] = 0 X[1, 0] = 0
last_mean = np.zeros(n_features) last_var = np.zeros_like(last_mean) last_n = 0
X = np.vstack(data_chunks) X_lil = sp.lil_matrix(X) X_csr = sp.csr_matrix(X_lil) X_csc = sp.csc_matrix(X_lil)
assert_raises(TypeError, csc_median_axis_0, sp.csr_matrix(X))
class_counts = np.bincount(y)[2:] assert_almost_equal(np.dot(cw, class_counts), y.shape[0]) assert_true(cw[0] < cw[1] < cw[2])
assert_array_almost_equal(np.asarray([1.0, 2.0, 3.0]), cw)
sample_weight = compute_sample_weight({1: 2, 2: 1}, y) assert_array_almost_equal(sample_weight, [2., 2., 2., 1., 1., 1.])
sample_weight = compute_sample_weight(None, y) assert_array_almost_equal(sample_weight, [1., 1., 1., 1., 1., 1., 1.])
assert_raises(ValueError, compute_sample_weight, {1: 2, 2: 1}, y, range(4))
assert_raises(ValueError, compute_sample_weight, {1: 2, 2: 1}, y_)
assert_raises(ValueError, compute_sample_weight, [{1: 2, 2: 1}], y_)
xi_, yi, swi, idx = dataset._next_py() xi = sp.csr_matrix((xi_), shape=(1, X.shape[1]))
xi_, yi, swi, idx = dataset._random_py() xi = sp.csr_matrix((xi_), shape=(1, X.shape[1]))
[{0: 'a', 1: 'b'}, {0: 'a'}],
np.array([[], []]),
np.array([[[0, 1], [2, 3]], [[4, 5], [6, 7]]]),
assert_raises(ValueError, unique_labels)
assert_array_equal(unique_labels(np.array([[0, 0, 1], [1, 0, 1], [0, 0, 0]])), np.arange(3))
for format in ["binary", "multiclass", "multilabel-indicator"]: for y in EXAMPLES[format]: unique_labels(y)
for example in NON_ARRAY_LIKE_EXAMPLES: assert_raises(ValueError, unique_labels, example)
mix_clf_format = product(EXAMPLES["multilabel-indicator"], EXAMPLES["multiclass"] + EXAMPLES["binary"])
if group == 'multilabel-indicator' and issparse(example): sparse_assert_, sparse_exp = assert_true, 'True' else: sparse_assert_, sparse_exp = assert_false, 'False'
if issparse(example): example = example.toarray()
rng = np.random.RandomState(0) x = rng.randint(10, size=(10, 5)) weights = np.ones(x.shape)
mode_result = 6
x = np.array([1e-40] * 1000000) logx = np.log(x) assert_almost_equal(np.exp(logsumexp(logx)), x.sum())
n_samples = 100 n_features = 500 rank = 5 k = 10
X = make_low_rank_matrix(n_samples=n_samples, n_features=n_features, effective_rank=rank, tail_strength=0.0, random_state=0) assert_equal(X.shape, (n_samples, n_features))
U, s, V = linalg.svd(X, full_matrices=False)
assert_almost_equal(s[:k], sa)
assert_almost_equal(np.dot(U[:, :k], V[:k, :]), np.dot(Ua, Va))
X = sparse.csr_matrix(X)
Ua, sa, Va = \ randomized_svd(X, k, power_iteration_normalizer=normalizer, random_state=0) assert_almost_equal(s[:rank], sa[:rank])
n_samples = 100 n_features = 500 rank = 5 k = 10
X = make_low_rank_matrix(n_samples=n_samples, n_features=n_features, effective_rank=rank, tail_strength=0.1, random_state=0) assert_equal(X.shape, (n_samples, n_features))
_, s, _ = linalg.svd(X, full_matrices=False)
_, sa, _ = randomized_svd(X, k, n_iter=0, power_iteration_normalizer=normalizer, random_state=0)
assert_greater(np.abs(s[:k] - sa).max(), 0.01)
_, sap, _ = randomized_svd(X, k, power_iteration_normalizer=normalizer, random_state=0)
assert_almost_equal(s[:k], sap, decimal=3)
n_samples = 100 n_features = 500 rank = 5 k = 10
X = make_low_rank_matrix(n_samples=n_samples, n_features=n_features, effective_rank=rank, tail_strength=1.0, random_state=0) assert_equal(X.shape, (n_samples, n_features))
assert_greater(np.abs(s[:k] - sa).max(), 0.1)
_, sap, _ = randomized_svd(X, k, n_iter=5, power_iteration_normalizer=normalizer)
assert_almost_equal(s[:k], sap, decimal=3)
n_samples = 100 n_features = 500 rank = 4 k = 10
assert_almost_equal(s2, s3)
rs = np.random.RandomState(1999) n_samples = 20 n_features = 10 X = rs.randn(n_samples, n_features)
u_flipped, _, v_flipped = randomized_svd(mat, 3, flip_sign=True) u_based, v_based = max_loading_is_positive(u_flipped, v_flipped) assert_true(u_based) assert_false(v_based)
u_flipped_with_transpose, _, v_flipped_with_transpose = randomized_svd( mat, 3, flip_sign=True, transpose=True) u_based, v_based = max_loading_is_positive( u_flipped_with_transpose, v_flipped_with_transpose) assert_true(u_based) assert_false(v_based)
x = np.arange(3) assert_array_equal(x[:, np.newaxis], cartesian((x,)))
def naive_log_logistic(x): return np.log(1 / (1 + np.exp(-x)))
if fast_dot is np.dot: return
E = np.empty(0) assert_raises(ValueError, _fast_dot, E, E)
assert_raises(ValueError, _fast_dot, A, A[0])
assert_raises(ValueError, _fast_dot, A.T, np.array([A, A]))
assert_raises(ValueError, _fast_dot, A, A[0, :][None, :])
assert_raises(ValueError, _fast_dot, A, A)
for dtype in ['f8', 'f4']: A = A.astype(dtype) B = B.astype(dtype)
C = np.dot(A.T, A) C_ = fast_dot(A.T, A) assert_almost_equal(C, C_, decimal=5)
A = rng.random_sample([2, 2]) for dtype in ['f8', 'f4']: A = A.astype(dtype) B = B.astype(dtype)
def two_pass_var(X): mean = X.mean(axis=0) Y = X.copy() return np.mean((Y - mean)**2, axis=0)
if np.abs(np_var(A) - two_pass_var(A)).max() < 1e-6: stable_var = np_var else: stable_var = two_pass_var
assert_greater(np.abs(stable_var(A) - one_pass_var(A)).max(), tol)
incremental_count = batch.shape[0] sample_count = batch.shape[0]
assert_less(0, 1) _assert_less(0, 1) assert_raises(AssertionError, assert_less, 1, 0) assert_raises(AssertionError, _assert_less, 1, 0)
assert_greater(1, 0) _assert_greater(1, 0) assert_raises(AssertionError, assert_greater, 0, 1) assert_raises(AssertionError, _assert_greater, 0, 1)
set_random_state(lda, 3) set_random_state(tree, 3) assert_equal(tree.random_state, 3)
assert_raises(AssertionError, assert_raise_message, (ValueError, AttributeError), "test", _no_raise)
def _warning_function(): warnings.warn("deprecation warning", DeprecationWarning)
@ignore_warnings def decorator_no_warning(): _warning_function() _multiple_warning_function()
def context_manager_no_warning(): with ignore_warnings(): _warning_function()
#`clean_warning_registry()` is called internally by assert_warns class TestWarns(unittest.TestCase): def test_warn(self): def f(): warnings.warn("yo") return 3
warnings.simplefilter("ignore", UserWarning) assert_equal(assert_warns(UserWarning, f), 3)
assert_equal(sys.modules['warnings'].filters, [])
assert_warns(UserWarning, f) failed = True
init = cls.__init__
_SEUPD_WHICH = ['LM', 'SM', 'LA', 'SA', 'BE']
_NEUPD_WHICH = ['LM', 'SM', 'LR', 'SR', 'LI', 'SI']
buf = buf[offset:offset+size+1][:-1] data = np.ndarray(shape, dtype, buf, order=order) data.fill(0) return data
self.resid = np.array(v0, copy=True) info = 1
self.resid = np.zeros(n, tp) info = 0
self.sigma = 0
ishfts = 1 self.mode = mode self.iparam[0] = ishfts self.iparam[2] = maxiter self.iparam[3] = 1 self.iparam[6] = mode
self.workd = _aligned_zeros(3 * n, self.tp) self.workl = _aligned_zeros(self.ncv * (self.ncv + 8), self.tp)
self.workd[yslice] = self.OP(self.workd[xslice])
self.workd = _aligned_zeros(3 * n, self.tp) self.workl = _aligned_zeros(3 * self.ncv * (self.ncv + 2), self.tp)
self.rwork = _aligned_zeros(self.ncv, self.tp.lower())
self.workd[yslice] = self.OP(self.workd[xslice])
d = dr + 1.0j * di
z = zr.astype(self.tp.upper())
d = d[:nreturned] z = z[:, :nreturned]
tol = 2 * np.finfo(M.dtype).eps
tol = 2 * np.finfo(A.dtype).eps
mode = 1 M_matvec = None Minv_matvec = None if Minv is not None: raise ValueError("Minv should not be " "specified with M = None.")
mode = 2 if Minv is None: Minv_matvec = get_inv_matvec(M, symmetric=True, tol=tol) else: Minv = _aslinearoperator_with_dtype(Minv) Minv_matvec = Minv.matvec M_matvec = _aslinearoperator_with_dtype(M).matvec
mode = 1 M_matvec = None Minv_matvec = None if Minv is not None: raise ValueError("Minv should not be " "specified with M = None.")
mode = 2 if Minv is None: Minv_matvec = get_inv_matvec(M, symmetric=True, tol=tol) else: Minv = _aslinearoperator_with_dtype(Minv) Minv_matvec = Minv.matvec M_matvec = _aslinearoperator_with_dtype(M).matvec
if Minv is not None: raise ValueError("Minv should not be specified when sigma is")
else: raise ValueError("unrecognized mode '%s'" % mode)
eigvals, eigvec = eigsh(XH_X, k=k, tol=tol ** 2, maxiter=maxiter, ncv=ncv, which=which, v0=v0)
if which == 'LM':
eigvals = np.maximum(eigvals.real, 0)
import scipy.sparse as sp import numpy as np
if m > n: m, n = n, m
X.indptr[m + 2:n] += nz_n - nz_m X.indptr[m + 1] = m_start + nz_n X.indptr[n] = n_stop - nz_m
data = np.copy(X.data[start: end]) nz = n_samples - data.size median[f_ind] = _get_median(data, nz)
log_prob_x = logsumexp(jll, axis=1) return jll - np.atleast_2d(log_prob_x).T
total_mu = (n_new * new_mu + n_past * mu) / n_total
epsilon = 1e-9 * np.var(X, axis=0).max()
self.sigma_[:, :] -= epsilon
if self.priors is None: self.class_prior_ = self.class_count_ / self.class_count_.sum()
self.class_log_prior_ = (np.log(self.class_count_) - np.log(self.class_count_.sum()))
Y = Y.astype(np.float64) if sample_weight is not None: sample_weight = np.atleast_2d(sample_weight) Y *= check_array(sample_weight).T
self._count(X, Y)
self._update_feature_log_prob() self._update_class_log_prior(class_prior=class_prior) return self
Y = Y.astype(np.float64) if sample_weight is not None: sample_weight = np.atleast_2d(sample_weight) Y *= check_array(sample_weight).T
def _get_coef(self): return (self.feature_log_prob_[1:] if len(self.classes_) == 2 else self.feature_log_prob_)
jll = safe_sparse_dot(X, (self.feature_log_prob_ - neg_prob).T) jll += self.class_log_prior_ + neg_prob.sum(axis=1)
components = rng.binomial(1, 0.5, (n_components, n_features)) * 2 - 1 return 1 / np.sqrt(n_components) * components
data = rng.binomial(1, 0.5, size=np.size(indices)) * 2 - 1
components = sp.csr_matrix((data, indices, indptr), shape=(n_components, n_features))
self.components_ = self._make_random_matrix(self.n_components_, n_features)
assert_equal( self.components_.shape, (self.n_components_, n_features), err_msg=('An error has occurred the self.components_ matrix has ' ' not the proper shape.'))
X, y = check_X_y(X, y, accept_sparse=("csr", "csc"), multi_output=True, y_numeric=True)
config.add_subpackage('__check_build') config.add_subpackage('_build_utils')
config.add_extension( '_isotonic', sources=['_isotonic.c'], include_dirs=[numpy.get_include()], libraries=libraries, )
config.add_subpackage('linear_model') config.add_subpackage('utils')
config.add_subpackage('tests')
warnings.filterwarnings('always', category=DeprecationWarning, module='^{0}\.'.format(re.escape(__name__)))
__SKLEARN_SETUP__
else: from . import __check_build from .base import clone
'clone']
from _dummy_thread import get_ident as _get_ident
if key not in self: root = self.__root last = root[0] last[1] = root[0] = self.__map[key] = [last, root, key] dict_setitem(self, key, value)
dict_delitem(self, key) link_prev, link_next, key = self.__map.pop(key) link_prev[1] = link_next link_next[0] = link_prev
PY2 = sys.version_info[0] == 2 PY3 = sys.version_info[0] == 3
MAXSIZE = int((1 << 31) - 1)
delattr(tp, self.name) return result
int2byte = operator.methodcaller("to_bytes", 1, "big")
logging.debug("[%s]: %s" % (self, msg))
time_lapse = time.time() - self.start_time full_msg = "%s: %.2fs, %.1f min" % (msg, time_lapse, time_lapse / 60)
self.last_time = time.time()
protocol = (pickle.DEFAULT_PROTOCOL if PY3_OR_LATER else pickle.HIGHEST_PROTOCOL) Pickler.__init__(self, self.stream, protocol=protocol) self._hash = hashlib.new(hash_name)
dispatch[type(len)] = save_global dispatch[type(object)] = save_global dispatch[type(Pickler)] = save_global dispatch[type(pickle.dump)] = save_global
Pickler.save(self, _ConsistentSet(set_items))
import numpy as np self.np = np if hasattr(np, 'getbuffer'): self._getbuffer = np.getbuffer else: self._getbuffer = memoryview
obj = (klass, ('HASHED', obj.dtype, obj.shape, obj.strides))
from cPickle import loads from cPickle import dumps
from pickle import Pickler
from multiprocessing.pool import Pool
SYSTEM_SHARED_MEM_FS = '/dev/shm'
FOLDER_PERMISSIONS = stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR FILE_PERMISSIONS = stat.S_IRUSR | stat.S_IWUSR
return a
return _get_backing_memmap(b)
mode = 'r+'
return np.memmap(filename, dtype=dtype, shape=shape, mode=mode, offset=offset, order=order)
base = np.memmap(filename, dtype=dtype, shape=total_buffer_len, mode=mode, offset=offset, order=order) return as_strided(base, shape=shape, strides=strides)
a_start, a_end = np.byte_bounds(a) m_start = np.byte_bounds(m)[0] offset = a_start - m_start
offset += m.offset
order = 'C'
strides = None total_buffer_len = None
strides = a.strides total_buffer_len = (a_end - a_start) // a.itemsize
return _reduce_memmap_backed(a, m)
return (loads, (dumps(np.asarray(a), protocol=HIGHEST_PROTOCOL),))
return _reduce_memmap_backed(a, m)
try: os.makedirs(self._temp_folder) os.chmod(self._temp_folder, FOLDER_PERMISSIONS) except OSError as e: if e.errno != errno.EEXIST: raise e
load(filename, mmap_mode=self._mmap_mode).max()
return (load, (filename, self._mmap_mode))
self.dispatch = Pickler.dispatch.copy()
self.dispatch_table = copyreg.dispatch_table.copy()
def dispatcher(self, obj): reduced = reduce_func(obj) self.save_reduce(obj=obj, *reduced) self.dispatch[type] = dispatcher
self.put = send
if prewarm == "auto": prewarm = not use_shared_mem forward_reduce_ndarray = ArrayMemmapReducer( max_nbytes, pool_folder, mmap_mode, verbose, prewarm=prewarm) forward_reducers[np.ndarray] = forward_reduce_ndarray forward_reducers[np.memmap] = reduce_memmap
backward_reduce_ndarray = ArrayMemmapReducer( None, pool_folder, mmap_mode, verbose) backward_reducers[np.ndarray] = backward_reduce_ndarray backward_reducers[np.memmap] = reduce_memmap
_FUNCTION_HASHES = weakref.WeakKeyDictionary()
def __getstate__(self): return {"valid": self.valid, "value": self.value}
def __init__(self, func): self.func = func
pass
#
doc = func.__doc__
#
func_code, source_file, first_line = get_func_code(self.func) func_dir = self._get_func_dir() func_code_file = os.path.join(func_dir, 'func_code.py')
return _load_output(output_dir, _get_func_fullname(self.func), timestamp=self.timestamp, mmap_mode=self.mmap_mode, verbose=self._verbose)
#
#
return functools.partial(self.cache, ignore=ignore, verbose=verbose, mmap_mode=mmap_mode)
#
cachedir = self.cachedir[:-7] if self.cachedir is not None else None return (self.__class__, (cachedir, self.mmap_mode, self.compress, self._verbose))
_ZFILE_PREFIX = asbytes('ZF') _MAX_LEN = len(hex_str(2 ** 64))
file_handle.seek(0) return magic
file_handle.write(asbytes(length.ljust(_MAX_LEN))) file_handle.write(zlib.compress(asbytes(data), compress))
np_ver = [int(x) for x in unpickler.np.__version__.split('.', 2)[:2]]
self._npy_counter = 1 if protocol is None: protocol = (pickle.DEFAULT_PROTOCOL if PY3_OR_LATER else pickle.HIGHEST_PROTOCOL)
try: import numpy as np except ImportError: np = None self.np = np
if type(obj) is self.np.memmap: obj = self.np.asarray(obj) return Pickler.save(self, obj)
obj, filename = self._write_array(obj, filename) self._filenames.append(filename) self._npy_counter += 1
print('Failed to save %s to .npy file:\n%s' % ( type(obj), traceback.format_exc()))
if PY3_OR_LATER: dispatch[pickle.BUILD[0]] = load_build else: dispatch[pickle.BUILD] = load_build
compress = 3
raise ValueError( 'Second argument should be a filename, %s (type %s) was given' % (filename, type(filename)) )
JOBLIB_SPAWNED_PROCESS = "__JOBLIB_SPAWNED_PARALLEL__"
MIN_IDEAL_BATCH_DURATION = .2
MAX_IDEAL_BATCH_DURATION = 2
raise WorkerInterrupt()
if check_pickle: pickle.dumps(function)
self.results = batch()
old_duration = self.parallel._smoothed_batch_duration if old_duration == 0: new_duration = this_batch_duration else: new_duration = 0.8 * old_duration + 0.2 * this_batch_duration self.parallel._smoothed_batch_duration = new_duration
backend = "multiprocessing"
self._mp_context = backend backend = "multiprocessing"
self._pool = None self._output = None self._jobs = list() self._managed_pool = False
self._lock = threading.Lock()
return 1
self.exceptions = [TransportableException]
self._pool = None
self._pool = None warnings.warn( 'Multiprocessing-backed parallel loops cannot be nested,' ' setting n_jobs=1', stacklevel=3) return 1
self._pool = None warnings.warn( 'Multiprocessing backed parallel loops cannot be nested' ' below threads, setting n_jobs=1', stacklevel=3) return 1
os.environ[JOBLIB_SPAWNED_PROCESS] = '1'
self.exceptions.extend([KeyboardInterrupt, WorkerInterrupt])
if self._aborting: return
batch_size = 1
batch_size = old_batch_size
self._smoothed_batch_duration = 0
batch_size = self.batch_size
return False
time.sleep(0.01) continue
self._aborting = True
exception_type = _mk_exception(exception.etype)[0] exception = exception_type(report)
self._aborting = False if not self._managed_pool: n_jobs = self._initialize_pool() else: n_jobs = self._effective_n_jobs()
self._original_iterator = None self._pre_dispatch_amount = 0
iterator = itertools.islice(iterator, pre_dispatch)
while self.dispatch_one_batch(iterator): self._iterating = True else: self._iterating = False
self._iterating = False
size += (stat.st_size // 512 + 1) * 512
RM_SUBDIRS_RETRY_TIME = 0.1
err_count = 0 while True: try: shutil.rmtree(fullname, False, None) break except os.error: if err_count > 0: raise err_count += 1 time.sleep(RM_SUBDIRS_RETRY_TIME)
from tokenize import open as open_py_source
from codecs import lookup, BOM_UTF8 import re from io import TextIOWrapper, open cookie_re = re.compile("coding[:=]\s*([-\w.]+)")
raise SyntaxError("unknown encoding: " + encoding)
raise SyntaxError('encoding problem: utf-8')
rec_check = records[tb_offset:] try: rname = rec_check[0][1] if rname == '<ipython console>' or rname.endswith('<string>'): return rec_check except IndexError: pass
names = []
unique_names = uniq_stable(names)
try: etype = etype.__name__ except AttributeError: pass
try: records = _fixed_getframes(etb, context, tb_offset) except: raise print('\nUnfortunately, your original traceback can not be ' 'constructed.\n') return ''
mp = int(os.environ.get('JOBLIB_MULTIPROCESSING', 1)) or None if mp: try: import multiprocessing as mp import multiprocessing.pool except ImportError: mp = None
if mp is not None: try: _sem = mp.Semaphore()
if mp is not None: try: from multiprocessing.context import assert_spawning except ImportError: from multiprocessing.forking import assert_spawning else: assert_spawning = None
JoblibException.__init__(self, message, etype) self.message = message self.etype = etype
locals().update(_mk_common_exceptions())
source_lines = list(islice(source_file_obj, first_line - 1, None))
import urllib.parse quote = urllib.parse.quote
module = ''
arg_spec_for_format = arg_spec[:7 if PY3_OR_LATER else 4]
raise ValueError( 'ignore_lst must be a list of parameters to ignore ' '%s (type %s) was given' % (ignore_lst, type(ignore_lst)))
args = [func.__self__, ] + args
raise ValueError( 'Wrong number of arguments for %s:\n' ' %s was called.' % (_signature_str(name, arg_spec), _function_called_str(name, args, kwargs)) )
#self.debug(msg)
return meth
return sig.replace(parameters=tuple(sig.parameters.values())[1:])
wrapped = obj.__wrapped__
call = _get_user_defined_method(type(obj), '__call__', 'im_func') if call is not None: sig = signature(call)
return sig.replace(parameters=tuple(sig.parameters.values())[1:])
msg = 'no signature found for builtin function {0!r}'.format(obj) raise ValueError(msg)
if self._annotation is not _empty: formatted = '{0}:{1}'.format(formatted, formatannotation(self._annotation))
break
break
args.extend(arg)
args.append(arg)
kwargs.update(arg)
kwargs[param_name] = arg
non_default_count = pos_count - pos_default_count for name in positional[:non_default_count]: annotation = annotations.get(name, _empty) parameters.append(Parameter(name, annotation=annotation, kind=_POSITIONAL_OR_KEYWORD))
for offset, name in enumerate(positional[non_default_count:]): annotation = annotations.get(name, _empty) parameters.append(Parameter(name, annotation=annotation, kind=_POSITIONAL_OR_KEYWORD, default=defaults[offset]))
if func_code.co_flags & 0x04: name = arg_names[pos_count + keyword_only_count] annotation = annotations.get(name, _empty) parameters.append(Parameter(name, annotation=annotation, kind=_VAR_POSITIONAL))
for name in keyword_only: default = _empty if kwdefaults is not None: default = kwdefaults.get(name, _empty)
if func_code.co_flags & 0x08: index = pos_count + keyword_only_count if func_code.co_flags & 0x04: index += 1
for param_name, param in self.parameters.items(): if (param._partial_kwarg and param_name not in kwargs): kwargs[param_name] = param.default
values = [arg_val] values.extend(arg_vals) arguments[param.name] = tuple(values) break
kwargs_param = param continue
if (not partial and param.kind != _VAR_POSITIONAL and param.default is _empty): raise TypeError('{arg!r} parameter lacking default value'. \ format(arg=param_name))
arguments[kwargs_param.name] = kwargs
render_kw_only_separator = False
is_x_old_in_X = int(mask.sum() < X.shape[0])
y_subpopulation = np.zeros((max(n_subsamples, n_features))) lstsq, = get_lapack_funcs(('gelss',), (X_subpopulation, y_subpopulation))
alpha_ = 1. / np.var(y) lambda_ = 1.
for iter_ in range(self.n_iter):
keep_lambda = np.ones(n_features, dtype=bool)
alpha_ = 1. / np.var(y) lambda_ = np.ones(n_features)
keep_lambda = lambda_ < self.threshold_lambda coef_[~keep_lambda] = 0
config.add_subpackage('tests')
return 4.0 / (max_squared_sum + int(fit_intercept) + 4.0 * alpha_scaled)
return 1.0 / (max_squared_sum + int(fit_intercept) + alpha_scaled)
if max_iter is None: max_iter = 1000
alpha_scaled = float(alpha) / n_samples
n_classes = int(y.max()) + 1 if loss == 'multinomial' else 1
if sample_weight is None: sample_weight = np.ones(n_samples, dtype=np.float64, order='C')
coef_init = np.zeros((n_features, n_classes), dtype=np.float64, order='C')
solve_triangular_args = {'check_finite': False}
L = np.empty((max_features, max_features), dtype=X.dtype)
L = np.zeros((max_features, max_features), dtype=X.dtype)
warnings.warn(premature, RuntimeWarning, stacklevel=2) break
L = np.empty((max_features, max_features), dtype=Gram.dtype)
L = np.zeros((max_features, max_features), dtype=Gram.dtype)
warnings.warn(premature, RuntimeWarning, stacklevel=3) break
n_nonzero_coefs = max(int(0.1 * X.shape[1]), 1)
copy_Gram = True
self.n_nonzero_coefs_ = max(int(0.1 * n_features), 1)
import itertools from abc import ABCMeta, abstractmethod import warnings
n_samples, n_features = X.shape
X -= X.mean(axis=0) y -= y.mean()
alphas /= alphas[0] alphas = alphas[::-1] coefs = coefs[:, ::-1] mask = alphas >= eps mask[0] = True alphas = alphas[mask] coefs = coefs[:, mask] return alphas, coefs
linear_loss = y - safe_sparse_dot(X, w) if fit_intercept: linear_loss -= intercept abs_linear_loss = np.abs(linear_loss) outliers_mask = abs_linear_loss > epsilon * sigma
outliers = abs_linear_loss[outliers_mask] num_outliers = np.count_nonzero(outliers_mask) n_non_outliers = X.shape[0] - num_outliers
outliers_sw = sample_weight[outliers_mask] n_sw_outliers = np.sum(outliers_sw) outlier_loss = (2. * epsilon * np.sum(outliers_sw * outliers) - sigma * n_sw_outliers * epsilon ** 2)
non_outliers = linear_loss[~outliers_mask] weighted_non_outliers = sample_weight[~outliers_mask] * non_outliers weighted_loss = np.dot(weighted_non_outliers.T, non_outliers) squared_loss = weighted_loss / sigma
X_non_outliers = -axis0_safe_slice(X, ~outliers_mask, n_non_outliers) grad[:n_features] = ( 2. / sigma * safe_sparse_dot(weighted_non_outliers, X_non_outliers))
grad[:n_features] += alpha * 2. * w
grad[-1] = n_samples grad[-1] -= n_sw_outliers * epsilon ** 2 grad[-1] -= squared_loss / sigma
if fit_intercept: grad[-2] = -2. * np.sum(weighted_non_outliers) / sigma grad[-2] -= 2. * epsilon * np.sum(sw_outliers)
bounds = np.tile([-np.inf, np.inf], (parameters.shape[0], 1)) bounds[-1][0] = 1e-12
out = -np.sum(sample_weight * log_logistic(yz)) + .5 * alpha * np.dot(w, w)
if grad.shape[0] > n_features: grad[-1] = z0.sum() return out, grad
out = -np.sum(sample_weight * log_logistic(yz)) + .5 * alpha * np.dot(w, w) return out
if fit_intercept: grad[-1] = z0.sum()
dd_intercept = np.squeeze(np.array(dX.sum(axis=0)))
if fit_intercept: ret[:n_features] += s[-1] * dd_intercept ret[-1] = dd_intercept.dot(s[:n_features]) ret[-1] += d.sum() * s[-1] return ret
loss, grad, p = _multinomial_loss_grad(w, X, Y, alpha, sample_weight) sample_weight = sample_weight[:, np.newaxis]
pos_class = classes[1]
le = LabelEncoder() if isinstance(class_weight, dict) or multi_class == 'multinomial': class_weight_ = compute_class_weight(class_weight, classes, y) sample_weight *= class_weight_[le.fit_transform(y)]
if class_weight in ("auto", "balanced"): class_weight_ = compute_class_weight(class_weight, mask_classes, y_bin) sample_weight *= class_weight_[le.fit_transform(y_bin)]
le = LabelEncoder() Y_multi = le.fit_transform(y)
y_test = check_array(y_test, dtype=np.float64, ensure_2d=False)
if self.multi_class == 'multinomial': classes_ = [None] warm_start_coef = [warm_start_coef]
cv = check_cv(self.cv, y, classifier=True) folds = list(cv.split(X, y))
n_classes = 1 labels = labels[1:]
iter_labels = labels if self.multi_class == 'multinomial': iter_labels = [None]
raise ValueError("class_weight provided should be a " "dict or 'balanced'")
if self.multi_class == 'multinomial': scores = multi_scores coefs_paths = multi_coefs_paths
X, y, _, _, _ = _preprocess_data(X, y, fit_intercept, normalize, copy=False)
_, _, X_offset, _, X_scale = _preprocess_data(X, y, fit_intercept, normalize, return_mean=True) mean_dot = X_offset * np.sum(y)
return self
precompute = False
X_train = check_array(X_train, 'csc', dtype=dtype, order=X_order) alphas, coefs, _ = path(X_train, y_train, **path_params) del X_train, y_train
coefs = coefs[np.newaxis, :, :] y_offset = np.atleast_1d(y_offset) y_test = y_test[:, np.newaxis]
copy_X = self.copy_X and self.fit_intercept
alphas = np.tile(np.sort(alphas)[::-1], (n_l1_ratio, 1))
if not (self.n_jobs == 1 or self.n_jobs is None): path_params['copy_X'] = False
cv = check_cv(self.cv)
folds = list(cv.split(X)) best_mse = np.inf
else: self.alphas_ = np.asarray(alphas[0])
X = check_array(X, dtype=np.float64, order='F', copy=self.copy_X and self.fit_intercept) y = check_array(y, dtype=np.float64, ensure_2d=False)
return self
self.t_ = None
self._get_penalty_type(self.penalty) self._get_learning_rate_type(self.learning_rate)
sample_weight = np.ones(n_samples, dtype=np.float64, order='C')
sample_weight = np.asarray(sample_weight, dtype=np.float64, order="C")
random_state = check_random_state(est.random_state) seed = random_state.randint(0, np.iinfo(np.int32).max)
self._expanded_class_weight = compute_class_weight(self.class_weight, self.classes_, y) sample_weight = self._validate_sample_weight(sample_weight, n_samples)
classes = np.unique(y)
self.t_ = None
prob_sum = prob.sum(axis=1) all_zero = (prob_sum == 0) if np.any(all_zero): prob[all_zero, :] = 1 prob_sum[all_zero] = len(self.classes_)
prob /= prob_sum.reshape((prob.shape[0], -1))
sample_weight = self._validate_sample_weight(sample_weight, n_samples)
self.t_ = None
seed = random_state.randint(0, np.iinfo(np.int32).max)
sqrt_alpha = np.sqrt(alpha)
n_samples, n_features = X.shape n_targets = y.shape[1]
n_samples = K.shape[0] n_targets = y.shape[1]
sw = np.sqrt(np.atleast_1d(sample_weight)) y = y * sw[:, np.newaxis] K *= np.outer(sw, sw)
K.flat[::n_samples + 1] += alpha[0]
dual_coef = linalg.solve(K, y, sym_pos=True, overwrite_a=False)
K.flat[::n_samples + 1] -= alpha[0]
dual_coefs = np.empty([n_targets, n_samples])
if not sparse.issparse(X) or has_sw: solver = 'cholesky' else: solver = 'sparse_cg'
X, y = _rescale_data(X, y, sample_weight)
solver = 'svd'
solver = 'svd'
max_squared_sum = row_norms(X, squared=True).max()
coef = coef.ravel()
raise ValueError( "%s doesn't support multi-label classification" % ( self.__class__.__name__))
sample_weight = (sample_weight * compute_sample_weight(self.class_weight, y))
return (v_prime * Q ** 2).sum(axis=-1)
if len(y.shape) != 1: G_diag = G_diag[:, np.newaxis] return G_diag, c
G_diag = G_diag[:, np.newaxis]
warnings.warn("non-uniform sample weights unsupported for svd, " "forcing usage of eigen") gcv_mode = 'eigen'
_pre_compute = self._pre_compute_svd _errors = self._errors_svd _values = self._values_svd
def identity_estimator(): pass identity_estimator.decision_function = lambda y_predict: y_predict identity_estimator.predict = lambda y_predict: y_predict
sample_weight = (sample_weight * compute_sample_weight(self.class_weight, y))
min_samples = X.shape[1] + 1
residual_threshold = np.median(np.abs(y - np.median(y)))
n_samples = X.shape[0] sample_idxs = np.arange(n_samples)
subset_idxs = sample_without_replacement(n_samples, min_samples, random_state=random_state) X_subset = X[subset_idxs] y_subset = y[subset_idxs]
if (self.is_data_valid is not None and not self.is_data_valid(X_subset, y_subset)): continue
if sample_weight is None: base_estimator.fit(X_subset, y_subset) else: base_estimator.fit(X_subset, y_subset, sample_weight=sample_weight[subset_idxs])
if (self.is_model_valid is not None and not self.is_model_valid(base_estimator, X_subset, y_subset)): continue
y_pred = base_estimator.predict(X)
inlier_mask_subset = residuals_subset < residual_threshold n_inliers_subset = np.sum(inlier_mask_subset)
inlier_idxs_subset = sample_idxs[inlier_mask_subset] X_inlier_subset = X[inlier_idxs_subset] y_inlier_subset = y[inlier_idxs_subset]
score_subset = base_estimator.score(X_inlier_subset, y_inlier_subset)
if (n_inliers_subset == n_inliers_best and score_subset < score_best): continue
n_inliers_best = n_inliers_subset score_best = score_subset inlier_mask_best = inlier_mask_subset X_inlier_best = X_inlier_subset y_inlier_best = y_inlier_subset
if (n_inliers_best >= self.stop_n_inliers or score_best >= self.stop_score or self.n_trials_ >= _dynamic_max_trials(n_inliers_best, n_samples, min_samples, self.stop_probability)): break
base_estimator.fit(X_inlier_best, y_inlier_best)
seed = rng.randint(1, np.iinfo(np.int32).max)
X_var *= X.shape[0] X_std = np.sqrt(X_var, X_var) del X_var X_std[X_std == 0] = 1 inplace_column_scale(X, 1. / X_std)
X_var *= X.shape[0] X_scale = np.sqrt(X_var, X_var) del X_var X_scale[X_scale == 0] = 1 inplace_column_scale(X, 1. / X_scale)
prob /= prob.sum(axis=1).reshape((prob.shape[0], -1)) return prob
X, y = _rescale_data(X, y, sample_weight)
X, y, X_offset, y_offset, X_scale = _preprocess_data( X, y, fit_intercept=fit_intercept, normalize=normalize, copy=copy)
precompute = 'auto' Xy = None
if isinstance(precompute, six.string_types) and precompute == 'auto': precompute = (n_samples > n_features)
precompute = np.empty(shape=(n_features, n_features), dtype=X.dtype, order='C') np.dot(X.T, X, out=precompute)
Xy = np.empty(shape=n_features, dtype=common_dtype, order='C') np.dot(X.T, y, out=Xy)
n_targets = y.shape[1] Xy = np.empty(shape=(n_features, n_targets), dtype=common_dtype, order='F') np.dot(y.T, X, out=Xy.T)
from sklearn.externals.six.moves import cStringIO as StringIO import sys old_stdout = sys.stdout try: sys.stdout = StringIO()
assert_true(ocur == X.shape[1])
assert_true(ocur == X.shape[1])
X, y = 3 * diabetes.data, diabetes.target G = np.dot(X.T, X) Xy = np.dot(X.T, y)
y = [5, 0, 5] for X in ([[5, 0], [0, 5], [10, 10]],
X = 3 * diabetes.data
assert_equal(len(lars.alphas_), 7)
X = diabetes.data Y = np.vstack([diabetes.target, diabetes.target ** 2]).T n_targets = Y.shape[1]
lars_broken = linear_model.LassoLarsIC('<unknown>') assert_raises(ValueError, lars_broken.fit, X, y)
splitted_data = train_test_split(X, y, random_state=42) with TempMemmap(splitted_data) as (X_train, X_test, y_train, y_test): _lars_path_residues(X_train, y_train, X_test, y_test, copy=False)
X = 3 * diabetes.data
X = [[1], [2]] Y = [1, 2]
X = [[1]] Y = [0]
for n_samples, n_features in ((6, 5), ):
reg = LinearRegression(fit_intercept=intercept) reg.fit(X, y, sample_weight=sample_weight) coefs1 = reg.coef_ inter1 = reg.intercept_
reg.fit(X, y, sample_weights_OK) reg.fit(X, y, sample_weights_OK_1) reg.fit(X, y, sample_weights_OK_2)
X, y = make_regression(random_state=random_state)
expected_X_norm = (np.sqrt(X.shape[0]) * np.mean((X - expected_X_mean) ** 2, axis=0) ** .5)
X = sparse.csr_matrix(X)
assert_greater(clf.score(X_test, y_test), 0.99)
clf_unconstrained = LassoCV(n_alphas=3, eps=1e-1, max_iter=max_iter, cv=2, n_jobs=1) clf_unconstrained.fit(X, y) assert_true(min(clf_unconstrained.coef_) < 0)
clf_constrained = LassoCV(n_alphas=3, eps=1e-1, max_iter=max_iter, positive=True, cv=2, n_jobs=1) clf_constrained.fit(X, y) assert_true(min(clf_constrained.coef_) >= 0)
X, y, X_test, y_test = build_dataset(n_samples=200, n_features=100, n_informative_features=100) max_iter = 150
assert_almost_equal(clf.alpha_, min(clf.alphas_)) assert_equal(clf.l1_ratio_, min(clf.l1_ratio))
assert_greater(clf.score(X_test, y_test), 0.99)
enetcv_unconstrained = ElasticNetCV(n_alphas=3, eps=1e-1, max_iter=max_iter, cv=2, n_jobs=1) enetcv_unconstrained.fit(X, y) assert_true(min(enetcv_unconstrained.coef_) < 0)
enetcv_constrained = ElasticNetCV(n_alphas=3, eps=1e-1, max_iter=max_iter, cv=2, positive=True, n_jobs=1) enetcv_constrained.fit(X, y) assert_true(min(enetcv_constrained.coef_) >= 0)
clf = MultiTaskLasso(alpha=1, tol=1e-8).fit(X, Y) assert_true(0 < clf.dual_gap_ < 1e-5) assert_array_almost_equal(clf.coef_[0], clf.coef_[1])
assert_greater(n_iter_reference, 2)
model.fit(X, y) n_iter_cold_start = model.n_iter_ assert_equal(n_iter_cold_start, n_iter_reference)
model.set_params(warm_start=True) model.fit(X, y) n_iter_warm_start = model.n_iter_ assert_equal(n_iter_warm_start, 1)
final_alpha = 1e-5 low_reg_model = ElasticNet(alpha=final_alpha).fit(X, y)
high_reg_model = ElasticNet(alpha=final_alpha * 10).fit(X, y) assert_greater(low_reg_model.n_iter_, high_reg_model.n_iter_)
warm_low_reg_model = deepcopy(high_reg_model) warm_low_reg_model.set_params(warm_start=True, alpha=final_alpha) warm_low_reg_model.fit(X, y) assert_greater(low_reg_model.n_iter_, warm_low_reg_model.n_iter_)
clf_random = ElasticNet(selection='invalid') assert_raises(ValueError, clf_random.fit, X, y)
X = check_array(X, order='C', dtype='float64') assert_raises(ValueError, clf.fit, X, y, check_input=False)
F, _ = f_regression(X, y)
scaling = 0.3 coef_grid, scores_path = lasso_stability_path(X, y, scaling=scaling, random_state=42, n_resampling=30)
scaling = 0.3 selection_threshold = 0.5
iris = load_iris() X = iris.data[:, [0, 2]] y = iris.target X = X[y != 2] y = y[y != 2]
iris = load_iris() X = iris.data[:, [0, 2]] y = iris.target X = X[y != 2] y = y[y != 2]
X, _, _, _, _ = _preprocess_data(X, y, True, True)
if (isinstance(self, SparseSGDClassifierTestCase) or isinstance(self, SparseSGDRegressorTestCase)): decay = .01
clf = self.factory(alpha=0.01, eta0=0.01, n_iter=5, shuffle=False, learning_rate=lr) clf.fit(X, Y)
clf3 = self.factory(alpha=0.01, eta0=0.01, n_iter=5, shuffle=False, warm_start=True, learning_rate=lr) clf3.fit(X, Y)
clf = self.factory(alpha=0.01, n_iter=5, shuffle=False) clf.fit(X, Y) Y_ = np.array(Y)[:, np.newaxis]
self.factory(alpha=0, learning_rate="optimal")
assert_array_equal(clf.predict(T), true_result)
self.factory(l1_ratio=1.1)
self.factory(learning_rate="<unknown>")
self.factory(eta0=0, learning_rate="constant")
self.factory(alpha=-.1)
self.factory(penalty='foobar', l1_ratio=0.85)
self.factory(loss="foobar")
self.factory(n_iter=-10000)
self.factory(shuffle="false")
self.factory(coef_init=np.zeros((3,))).fit(X, Y)
self.factory().fit(X, Y, coef_init=np.zeros((3,)))
self.factory().fit(X, Y, intercept_init=np.zeros((3,)))
self.factory().fit(X5, Y5, intercept_init=0)
y = np.dot(X, w) y = np.sign(y)
self.factory(alpha=0.01, n_iter=20).fit(X2, np.ones(9))
clf = self.factory(loss='squared_loss', learning_rate='constant', eta0=eta, alpha=alpha, fit_intercept=True, n_iter=1, average=True, shuffle=False)
clf = self.factory() assert_raises(ValueError, clf.fit, X2, Y2, coef_init=np.zeros((2, 2)))
clf = self.factory().fit(X2, Y2, coef_init=np.zeros((3, 2)))
clf = self.factory() assert_raises(ValueError, clf.fit, X2, Y2, intercept_init=np.zeros((1,)))
clf = self.factory().fit(X2, Y2, intercept_init=np.zeros((3,)))
clf = self.factory(loss="log", alpha=0.01, n_iter=10).fit(X2, Y2)
x = X.mean(axis=0) d = clf.decision_function([x])
n = len(X4) rng = np.random.RandomState(13) idx = np.arange(n) rng.shuffle(idx)
clf.sparsify() assert_true(sp.issparse(clf.coef_)) pred = clf.predict(X) assert_array_equal(pred, Y)
clf = pickle.loads(pickle.dumps(clf)) assert_true(sp.issparse(clf.coef_)) pred = clf.predict(X) assert_array_equal(pred, Y)
clf = self.factory(alpha=0.1, n_iter=1000, fit_intercept=False, class_weight={1: 0.001}) clf.fit(X, y)
assert_array_equal(clf.predict([[0.2, -1.0]]), np.array([-1]))
assert_almost_equal(clf.coef_, clf_weighted.coef_, decimal=2)
clf = self.factory(alpha=0.1, n_iter=1000, class_weight={0: 0.5}) clf.fit(X, Y)
clf = self.factory(alpha=0.1, n_iter=1000, class_weight=[0.5]) clf.fit(X, Y)
assert_array_almost_equal(clf.coef_, clf_balanced.coef_, 6)
X_0 = X[y == 0, :] y_0 = y[y == 0]
clf.fit(X, y, sample_weight=[0.001] * 3 + [1] * 2)
assert_array_equal(clf.predict([[0.2, -1.0]]), np.array([-1]))
clf = self.factory(alpha=0.1, n_iter=1000, fit_intercept=False) clf.fit(X, Y, sample_weight=np.arange(7))
clf.partial_fit(X3, Y3)
assert_true(id1, id2)
assert_true(id1, id2)
clf = self.factory() clf.fit(X2, Y2)
clf = self.factory(alpha=0.01, n_iter=5, shuffle=False) clf.fit(X, Y) assert_true(hasattr(clf, "coef_"))
y = [["ham", "spam"][i] for i in LabelEncoder().fit_transform(Y)] clf.fit(X[:, :-1], y)
self.factory(penalty='foobar', l1_ratio=0.85)
self.factory(loss="foobar")
y = np.dot(X, w)
y = np.dot(X, w)
y = 0.5 * X.ravel()
y = 0.5 * X.ravel() + rng.randn(n_samples, 1).ravel()
y = 0.5 * X.ravel()
y = 0.5 * X.ravel() \ + np.random.randn(n_samples, 1).ravel()
y = 0.5 * X.ravel()
y = 0.5 * X.ravel() + rng.randn(n_samples, 1).ravel()
ground_truth_coef = rng.randn(n_features) y = np.dot(X, ground_truth_coef)
assert_true(id1, id2)
X, y = datasets.make_classification(n_samples=1000, n_features=100, n_informative=20, random_state=1234)
rng = np.random.RandomState(0) n_samples = 100 n_features = 10
X_scaled = MinMaxScaler().fit_transform(X) assert_true(np.isfinite(X_scaled).all())
model.fit(X_scaled, y) assert_true(np.isfinite(model.coef_).all())
for p, y, expected in cases: assert_almost_equal(loss_function.dloss(p, y), expected)
raise SkipTest("XFailed Test") diabetes = datasets.load_diabetes() X, y = diabetes.data, diabetes.target
clf.fit(X, y) assert_array_equal(np.diff(clf.scores_) > 0, True)
X = X[:5, :] y = y[:5] clf.fit(X, y) assert_array_equal(np.diff(clf.scores_) > 0, True)
test = [[1], [3], [4]] assert_array_almost_equal(clf.predict(test), [1, 3, 4], 2)
X = np.array([[1], [2], [3]]) Y = np.array([1, 2, 3]) clf = ARDRegression(compute_score=True) clf.fit(X, Y)
test = [[1], [3], [4]] assert_array_almost_equal(clf.predict(test), [1, 3, 4], 2)
X, y = make_regression( n_samples=n_samples, n_features=n_features, random_state=0, noise=0.05)
assert_array_almost_equal(huber_warm.coef_, huber_warm_coef, 1)
if huber_warm.n_iter_ is not None: assert_equal(1, huber_warm.n_iter_)
assert_greater(ridge_outlier_score, huber_outlier_score)
n_samples, n_features = 6, 5 y = rng.randn(n_samples) X = rng.randn(n_samples, n_features)
ridge.fit(X, y, sample_weight=np.ones(n_samples)) assert_greater(ridge.score(X, y), 0.47)
ridge.fit(X, y, sample_weight=np.ones(n_samples)) assert_greater(ridge.score(X, y), 0.9)
coefs2 = ridge_regression( X * np.sqrt(sample_weight)[:, np.newaxis], y * np.sqrt(sample_weight), alpha=alpha, solver=solver) assert_array_almost_equal(coefs, coefs2)
est = Ridge(alpha=alpha, fit_intercept=intercept, solver=solver) est.fit(X, y, sample_weight=sample_weight) coefs = est.coef_ inter = est.intercept_
n_samples, n_features = 5, 4 y = rng.randn(n_samples) X = rng.randn(n_samples, n_features)
ridge = Ridge(alpha=penalties[:-1]) assert_raises(ValueError, ridge.fit, X, y)
n_samples = X_diabetes.shape[0]
assert_almost_equal(errors, errors2) assert_almost_equal(values, values2)
assert_almost_equal(errors, errors3) assert_almost_equal(values, values3)
ridge_gcv.fit(filter_(X_diabetes), y_diabetes) alpha_ = ridge_gcv.alpha_ ret.append(alpha_)
scorer = get_scorer('mean_squared_error') ridge_gcv4 = RidgeCV(fit_intercept=False, scoring=scorer) ridge_gcv4.fit(filter_(X_diabetes), y_diabetes) assert_equal(ridge_gcv4.alpha_, alpha_)
ridge_gcv.fit(filter_(X_diabetes), y_diabetes, sample_weight=np.ones(n_samples)) assert_equal(ridge_gcv.alpha_, alpha_)
Y = np.vstack((y_diabetes, y_diabetes)).T
Y = np.vstack((y_diabetes, y_diabetes)).T n_features = X_diabetes.shape[1]
ret_dense = test_func(DENSE_FILTER) ret_sparse = test_func(SPARSE_FILTER) if ret_dense is not None and ret_sparse is not None: assert_array_almost_equal(ret_dense, ret_sparse, decimal=3)
reg = RidgeClassifier(class_weight={1: 0.001}) reg.fit(X, y)
assert_array_equal(reg.predict([[0.2, -1.0]]), np.array([-1]))
reg = RidgeClassifier(class_weight='balanced') reg.fit(X, y) assert_array_equal(reg.predict([[0.2, -1.0]]), np.array([1]))
reg = RidgeClassifierCV(class_weight={1: 0.001}, alphas=[.01, .1, 1, 10]) reg.fit(X, y)
rng = rng = np.random.RandomState(42)
y = rng.randn(n_samples) r.fit(x, y) assert_equal(r.cv_values_.shape, (n_samples, n_alphas))
n_responses = 3 y = rng.randn(n_samples, n_responses) r.fit(x, y) assert_equal(r.cv_values_.shape, (n_samples, n_responses, n_alphas))
parameters = {'alpha': alphas} fit_params = {'sample_weight': sample_weight} gs = GridSearchCV(Ridge(), parameters, fit_params=fit_params, cv=cv) gs.fit(X, y)
ridge.fit(X, y, sample_weights_OK) ridge.fit(X, y, sample_weights_OK_1) ridge.fit(X, y, sample_weights_OK_2)
n_targets = 2 X, y = X_diabetes, y_diabetes y_n = np.tile(y, (n_targets, 1)).T
check_predictions(LogisticRegression(random_state=0), X, Y1) check_predictions(LogisticRegression(random_state=0), X_sp, Y1)
n_samples, n_features = iris.data.shape
target = (iris.target > 0).astype(np.intp) target = np.array(["setosa", "not-setosa"])[target]
n_samples, n_features = iris.data.shape target = iris.target_names[iris.target] clf = LogisticRegression(random_state=0).fit(iris.data, target)
rng = np.random.RandomState(0) X_ = rng.random_sample((5, 10)) y_ = np.ones(X_.shape[0]) y_[0] = 0
y_wrong = y_[:-1] assert_raises(ValueError, clf.fit, X, y_wrong)
assert_raises(ValueError, clf.fit(X_, y_).predict, rng.random_sample((3, 12)))
clf = LogisticRegression(random_state=0) clf.fit(X, Y1) clf.coef_[:] = 0 clf.intercept_[:] = 0 assert_array_almost_equal(clf.decision_function(X), 0)
Xnan = np.array(X, dtype=np.float64) Xnan[0, 1] = np.nan LogisticRegression(random_state=0).fit(Xnan, Y1)
assert_array_almost_equal(lr1.coef_, lr2.coef_) msg = "Arrays are not almost equal to 6 decimals" assert_raise_message(AssertionError, msg, assert_array_almost_equal, lr1.coef_, lr3.coef_)
w = np.zeros(n_features + 1) loss_interp, grad_interp = _logistic_loss_and_grad( w, X, y, alpha=1. ) assert_array_almost_equal(loss, loss_interp)
loss, grad = _logistic_loss_and_grad(w, X, y, alpha=1.) grad_2, hess = _logistic_grad_hess(w, X, y, alpha=1.) assert_array_almost_equal(grad, grad_2)
vector = np.zeros_like(grad) vector[1] = 1 hess_col = hess(vector)
assert_almost_equal(loss_interp + 0.5 * (w[-1] ** 2), loss)
assert_array_almost_equal(grad_interp[:n_features], grad[:n_features]) assert_almost_equal(grad_interp[-1] + alpha * w[-1], grad[-1])
train, target = iris.data, iris.target n_samples, n_features = train.shape
n_cv = 2 cv = StratifiedKFold(n_cv) precomputed_folds = list(cv.split(train, target))
clf = LogisticRegressionCV(cv=precomputed_folds) clf.fit(train, target)
clf1 = LogisticRegressionCV(cv=precomputed_folds) target_copy = target.copy() target_copy[target_copy == 0] = 1 clf1.fit(train, target_copy)
assert_array_almost_equal(clf.scores_[2], clf1.scores_[2]) assert_array_almost_equal(clf.intercept_[2:], clf1.intercept_) assert_array_almost_equal(clf.coef_[2][np.newaxis, :], clf1.coef_)
classes = np.unique(y) class_weight = compute_class_weight("balanced", classes, y) class_weight_dict = dict(zip(classes, class_weight)) return class_weight_dict
X = iris.data[45:, :] y = iris.target[45:] solvers = ("lbfgs", "newton-cg") class_weight_dict = _compute_class_weight_dictionary(y)
X = iris.data[45:100, :] y = iris.target[45:100] solvers = ("lbfgs", "newton-cg", "liblinear") class_weight_dict = _compute_class_weight_dictionary(y)
assert_warns_message(DeprecationWarning, "class_weight='auto' heuristic is deprecated", model.fit, X, y)
n_samples, n_features, n_classes = 50, 20, 3 X, y = make_classification(n_samples=n_samples, n_features=n_features, n_informative=10, n_classes=n_classes, random_state=0)
assert_almost_equal(ref_i.coef_, clf_i.coef_, decimal=3) assert_almost_equal(ref_w.coef_, clf_w.coef_, decimal=3) assert_almost_equal(ref_i.intercept_, clf_i.intercept_, decimal=3)
vec = np.zeros(n_features * n_classes) vec[0] = 1 hess_col = hessp(vec)
X = np.zeros((5, 5)) assert_array_equal(clf.predict(X), np.zeros(5))
clf_multi_loss = log_loss(y, clf_multi.predict_proba(X)) clf_wrong_loss = log_loss(y, clf_multi._predict_proba_lr(X)) assert_greater(clf_wrong_loss, clf_multi_loss)
X, y_bin = iris.data, iris.target.copy() y_bin[y_bin == 2] = 0
if sp_version >= (0, 12): solvers.append('lbfgs')
X, y = iris.data, iris.target y_bin = y.copy() y_bin[y_bin == 2] = 0
n_classes = 1 if solver in ('liblinear', 'sag'): break
X, y = iris.data, iris.target
if sp_version >= (0, 12): solvers.append('lbfgs')
fermat_weber = fmin_bfgs(cost_func, median, disp=False) assert_array_almost_equal(median, fermat_weber) assert_warns(ConvergenceWarning, _spatial_median, X, max_iter=30, tol=0.)
assert_array_almost_equal(theil_sen.coef_, lstq.coef_, 9)
with no_stdout_stderr(): TheilSenRegressor(verbose=True, random_state=0).fit(X, y) TheilSenRegressor(verbose=True, max_subpopulation=10, random_state=0).fit(X, y)
clf = PassiveAggressiveClassifier().fit(X, y) assert_array_equal(clf.classes_, np.unique(y))
clf = PassiveAggressiveClassifier(C=0.1, n_iter=100, class_weight={1: 0.001}, random_state=100) clf.fit(X2, y2)
assert_array_equal(clf.predict([[0.2, -1.0]]), np.array([-1]))
clf = PassiveAggressiveClassifier(class_weight="balanced") assert_raises(ValueError, clf.partial_fit, X, y, classes=np.unique(y))
clf_balanced = PassiveAggressiveClassifier(C=0.1, n_iter=1000, class_weight="balanced") clf_balanced.fit(X2, y2)
assert_almost_equal(clf.coef_, clf_weighted.coef_, decimal=2) assert_almost_equal(clf.coef_, clf_balanced.coef_, decimal=2)
clf = ElasticNet() clf.coef_ = [1, 2, 3]
f = ignore_warnings X = sp.lil_matrix((3, 1)) X[0, 0] = -1 X[2, 0] = 1
T = sp.lil_matrix((3, 1)) T[0, 0] = 2 T[1, 0] = 3 T[2, 0] = 4
w = random_state.randn(n_features, n_targets)
y = np.dot(X, w) X = sp.csc_matrix(X) if n_targets == 1: y = np.ravel(y) return X, y
d_clf = ElasticNet(alpha=alpha, l1_ratio=0.8, fit_intercept=fit_intercept, max_iter=max_iter, tol=1e-7, positive=positive, warm_start=True) d_clf.fit(X_train.toarray(), y_train)
assert_less(np.sum(s_clf.coef_ != 0.0), 2 * n_informative)
assert_equal(np.sum(s_clf.coef_ != 0.0), n_informative)
estimator.fit(X, y) coef, intercept, dual_gap = (estimator.coef_, estimator.intercept_, estimator.dual_gap_)
def squared_dloss(p, y): return p - y
if sparse: decay = .01
if sparse: decay = .01
idx = int(rng.rand(1) * n_samples) entry = X[idx] seen.add(idx)
max_squared_sum = 4 + 9 + 16 max_squared_sum_ = row_norms(X, squared=True).max() assert_almost_equal(max_squared_sum, max_squared_sum_, decimal=4)
y = 0.5 * X.ravel()
y = 0.5 * X.ravel() + rng.randn(n_samples, 1).ravel()
X, y = iris.data, iris.target.astype(np.float64) n_samples, n_features = X.shape n_classes = len(np.unique(y))
assert_array_almost_equal(grad_1, grad_2) assert_almost_equal(loss_1, loss_2)
X = np.arange(-200, 200) y = 0.2 * X + 20 data = np.column_stack([X, y])
ransac_estimator.fit(X, y)
ref_inlier_mask = np.ones_like(ransac_estimator.inlier_mask_ ).astype(np.bool_) ref_inlier_mask[outliers] = False
base_estimator = LinearRegression() ransac_estimator = RANSACRegressor(base_estimator, min_samples=2, residual_threshold=0.0, random_state=0)
yyy = np.column_stack([y, y, y])
ransac_estimator.fit(X, yyy)
ref_inlier_mask = np.ones_like(ransac_estimator.inlier_mask_ ).astype(np.bool_) ref_inlier_mask[outliers] = False
def test_ransac_residual_metric(): residual_metric1 = lambda dy: np.sum(np.abs(dy), axis=1) residual_metric2 = lambda dy: np.sum(dy ** 2, axis=1)
ransac_estimator0.fit(X, y) assert_warns(DeprecationWarning, ransac_estimator2.fit, X, y) assert_array_almost_equal(ransac_estimator0.predict(X), ransac_estimator2.predict(X))
ransac_estimator.fit(X, y)
ref_inlier_mask = np.ones_like(ransac_estimator.inlier_mask_ ).astype(np.bool_) ref_inlier_mask[outliers] = False
assert_equal(_dynamic_max_trials(100, 100, 2, 0.99), 1)
assert_equal(_dynamic_max_trials(1, 100, 10, 0), 0) assert_equal(_dynamic_max_trials(1, 100, 10, 1), float('inf'))
assert_equal(ransac_estimator.inlier_mask_.shape[0], n_samples)
assert_array_equal(ransac_estimator.inlier_mask_, ref_inlier_mask)
base_estimator = Lasso() ransac_estimator = RANSACRegressor(base_estimator) assert_raises(ValueError, ransac_estimator.fit, X, y, weights)
sign_active = np.empty(max_features, dtype=np.int8) drop = False
X = X.copy('F')
if n_iter > 0: ss = ((prev_alpha[0] - alpha_min) / (prev_alpha[0] - alpha[0])) coef[:] = prev_coef + ss * (coef - prev_coef) alpha[0] = alpha_min
if n_active: linalg.solve_triangular(L[:n_active, :n_active], L[n_active, :n_active], trans=0, lower=1, overwrite_b=True, **solve_triangular_args)
least_squares, info = solve_cholesky(L[:n_active, :n_active], sign_active[:n_active], lower=True)
least_squares[...] = 1 AA = 1.
AA = 1. / np.sqrt(np.sum(least_squares * sign_active[:n_active]))
eq_dir = np.dot(X.T[:n_active].T, least_squares) corr_eq_dir = np.dot(X.T[n_active:], eq_dir)
corr_eq_dir = np.dot(Gram[:n_active, n_active:].T, least_squares)
sign_active[idx] = -sign_active[idx]
prev_coef = coef prev_alpha[0] = alpha[0] coef = np.zeros_like(coef)
Cov -= gamma_ * corr_eq_dir
if drop and method == 'lasso':
[arrayfuncs.cholesky_delete(L[:n_active, :n_active], ii) for ii in idx]
drop_idx = [active.pop(ii) for ii in idx]
alphas = alphas[:n_iter + 1] coefs = coefs[:n_iter + 1]
precompute = self.precompute if hasattr(precompute, '__array__'): Gram = precompute elif precompute == 'auto': Gram = 'auto' else: Gram = None return Gram
cv = check_cv(self.cv, classifier=False)
all_alphas = np.unique(all_alphas) stride = int(max(1, int(len(all_alphas) / float(self.max_n_alphas)))) all_alphas = all_alphas[::stride]
i_best_alpha = np.argmin(mse_path.mean(axis=-1)) best_alpha = all_alphas[i_best_alpha]
self.alpha_ = best_alpha self.cv_alphas_ = all_alphas self.cv_mse_path_ = mse_path
Lars.fit(self, X, y) return self
return self.alpha_
df[k] = np.sum(mask)
score = estimator.predict_proba(X)[:, 1]
Y = np.array([e.predict_proba(X)[:, 1] for e in self.estimators_]).T
Y = np.concatenate(((1 - Y), Y), axis=1)
Y /= np.sum(Y, axis=1)[:, np.newaxis]
self.code_book_ = random_state.random_sample((n_classes, code_size_)) self.code_book_[self.code_book_ > 0.5] = 1
if n_local_trials is None: n_local_trials = 2 + int(np.log(n_clusters))
center_id = random_state.randint(n_samples) if sp.issparse(X): centers[0] = X[center_id].toarray() else: centers[0] = X[center_id]
closest_dist_sq = euclidean_distances( centers[0, np.newaxis], X, Y_norm_squared=x_squared_norms, squared=True) current_pot = closest_dist_sq.sum()
for c in range(1, n_clusters): rand_vals = random_state.random_sample(n_local_trials) * current_pot candidate_ids = np.searchsorted(closest_dist_sq.cumsum(), rand_vals)
distance_to_candidates = euclidean_distances( X[candidate_ids], X, Y_norm_squared=x_squared_norms, squared=True)
best_candidate = None best_pot = None best_dist_sq = None for trial in range(n_local_trials): new_dist_sq = np.minimum(closest_dist_sq, distance_to_candidates[trial]) new_pot = new_dist_sq.sum()
if (best_candidate is None) or (new_pot < best_pot): best_candidate = candidate_ids[trial] best_pot = new_pot best_dist_sq = new_dist_sq
if sp.issparse(X): centers[c] = X[best_candidate].toarray() else: centers[c] = X[best_candidate] current_pot = best_pot closest_dist_sq = best_dist_sq
if not sp.issparse(X) or hasattr(init, '__array__'): X_mean = X.mean(axis=0) if not sp.issparse(X): X -= X_mean
x_squared_norms = row_norms(X, squared=True)
algorithm = "full"
centers = _init_centroids(X, n_clusters, init, random_state=random_state, x_squared_norms=x_squared_norms) if verbose: print("Initialization complete")
distances = np.zeros(shape=(X.shape[0],), dtype=np.float64)
for i in range(max_iter): centers_old = centers.copy() labels, inertia = \ _labels_inertia(X, x_squared_norms, centers, precompute_distances=precompute_distances, distances=distances)
if sp.issparse(X): centers = _k_means._centers_sparse(X, labels, n_clusters, distances) else: centers = _k_means._centers_dense(X, labels, n_clusters, distances)
best_labels, best_inertia = \ _labels_inertia(X, x_squared_norms, best_centers, precompute_distances=precompute_distances, distances=distances)
distances[:] = mindist
X = self._check_fit_data(X) return self.fit(X)._transform(X)
nearest_center, inertia = _labels_inertia(X, x_squared_norms, centers, distances=distances)
counts[to_reassign] = np.min(counts[~to_reassign])
if sp.issparse(X): return inertia, _k_means._mini_batch_update_csr( X, x_squared_norms, centers, counts, nearest_center, old_center_buffer, compute_squared_diff)
k = centers.shape[0] squared_diff = 0.0 for center_idx in range(k): center_mask = nearest_center == center_idx count = center_mask.sum()
centers[center_idx] *= counts[center_idx]
centers[center_idx] += np.sum(X[center_mask], axis=0)
counts[center_idx] += count
centers[center_idx] /= counts[center_idx]
if compute_squared_diff: diff = centers[center_idx].ravel() - old_center_buffer.ravel() squared_diff += np.dot(diff, diff)
batch_inertia /= model.batch_size centers_squared_diff /= model.batch_size
if tol > 0.0 and ewa_diff <= tol: if verbose: print('Converged (small centers change) at iteration %d/%d' % (iteration_idx + 1, n_iter)) return True
context['ewa_diff'] = ewa_diff context['ewa_inertia'] = ewa_inertia context['ewa_inertia_min'] = ewa_inertia_min context['no_improvement'] = no_improvement return False
old_center_buffer = np.zeros(n_features, np.double)
old_center_buffer = np.zeros(0, np.double)
cluster_centers = _init_centroids( X, self.n_clusters, self.init, random_state=random_state, x_squared_norms=x_squared_norms, init_size=init_size)
batch_inertia, centers_squared_diff = _mini_batch_step( X_valid, x_squared_norms[validation_indices], cluster_centers, counts, old_center_buffer, False, distances=None, verbose=self.verbose)
convergence_context = {}
for iteration_idx in range(n_iter): minibatch_indices = random_state.randint( 0, n_samples, self.batch_size)
if _mini_batch_convergence( self, iteration_idx, n_iter, tol, n_samples, centers_squared_diff, batch_inertia, convergence_context, verbose=self.verbose): break
self.cluster_centers_ = _init_centroids( X, self.n_clusters, self.init, random_state=self.random_state_, x_squared_norms=x_squared_norms, init_size=self.init_size)
def _mean_shift_single_seed(my_mean, X, nbrs, max_iter): bandwidth = nbrs.get_params()['radius']
i_nbrs = nbrs.radius_neighbors([my_mean], bandwidth, return_distance=False)[0] points_within = X[i_nbrs] if len(points_within) == 0:
if (extmath.norm(my_mean - my_old_mean) < stop_thresh or completed_iterations == max_iter): return tuple(my_mean), len(points_within) completed_iterations += 1
raise ValueError("No point was within bandwidth=%f of any seed." " Try a different seeding strategy \ or increase the bandwidth." % bandwidth)
bin_sizes = defaultdict(int) for point in X: binned_point = np.round(point / bin_size) bin_sizes[tuple(binned_point)] += 1
import os from os.path import join
S.flat[::(n_samples + 1)] = preference
tmp = np.zeros((n_samples, n_samples))
e = np.zeros((n_samples, convergence_iter))
np.add(A, S, tmp) I = np.argmax(tmp, axis=1)
np.subtract(S, Y[:, None], tmp) tmp[ind, I] = S[ind, I] - Y2
tmp *= 1 - damping R *= damping R += tmp
np.maximum(R, 0, tmp) tmp.flat[::n_samples + 1] = R.flat[::n_samples + 1]
tmp *= 1 - damping A *= damping A -= tmp
E = (np.diag(A) + np.diag(R)) > 0 e[:, it % convergence_iter] = E K = np.sum(E, axis=0)
cluster_centers_indices = np.unique(labels) labels = np.searchsorted(cluster_centers_indices, labels)
import warnings
vectors = vectors / np.sqrt((vectors ** 2).sum(axis=1))[:, np.newaxis]
while (svd_restarts < max_svd_restarts) and not has_converged:
rotation = np.zeros((n_components, n_components)) rotation[:, 0] = vectors[random_state.randint(n_samples), :].T
last_objective_value = ncut_value rotation = np.dot(Vh.T, U.T)
from __future__ import division
self.centroids_ = self.init_centroids_[:n_samples + 1, :] self.squared_norm_ = self.init_sq_norm_[:n_samples + 1]
dist_matrix = np.dot(self.centroids_, subcluster.centroid_) dist_matrix *= -2. dist_matrix += self.squared_norm_ closest_index = np.argmin(dist_matrix) closest_subcluster = self.subclusters_[closest_index]
if closest_subcluster.child_ is not None: split_child = closest_subcluster.child_.insert_cf_subcluster( subcluster)
closest_subcluster.update(subcluster) self.init_centroids_[closest_index] = \ self.subclusters_[closest_index].centroid_ self.init_sq_norm_[closest_index] = \ self.subclusters_[closest_index].sq_norm_ return False
else: new_subcluster1, new_subcluster2 = _split_node( closest_subcluster.child_, threshold, branching_factor) self.update_split_subclusters( closest_subcluster, new_subcluster1, new_subcluster2)
else: merged = closest_subcluster.merge_subcluster( subcluster, self.threshold) if merged: self.init_centroids_[closest_index] = \ closest_subcluster.centroid_ self.init_sq_norm_[closest_index] = \ closest_subcluster.sq_norm_ return False
elif len(self.subclusters_) < self.branching_factor: self.append_subcluster(subcluster) return False
else: self.append_subcluster(subcluster) return True
self.dummy_leaf_ = _CFNode(threshold, branching_factor, is_leaf=True, n_features=n_features) self.dummy_leaf_.next_leaf_ = self.root_ self.root_.prev_leaf_ = self.dummy_leaf_
if not sparse.issparse(X): iter_func = iter else: iter_func = _iterate_sparse_X
self._global_clustering() return self
has_partial_fit = hasattr(self, 'partial_fit_')
if not (is_fitted or has_partial_fit): raise NotFittedError("Fit training data before predicting")
self._subcluster_norms = row_norms( self.subcluster_centers_, squared=True)
self.subcluster_labels_ = clusterer.fit_predict( self.subcluster_centers_)
S = -euclidean_distances(X, squared=True) preference = np.median(S) * 10 cluster_centers_indices, labels = affinity_propagation( S, preference=preference)
_, labels_no_copy = affinity_propagation(S, preference=preference, copy=False) assert_array_equal(labels, labels_no_copy)
af = AffinityPropagation(affinity="euclidean") labels = af.fit_predict(X) labels2 = af.predict(X) assert_array_equal(labels, labels2)
af = AffinityPropagation(affinity="euclidean") assert_raises(ValueError, af.predict, X)
S = np.dot(X, X.T) af = AffinityPropagation(affinity="precomputed") af.fit(S) assert_raises(ValueError, af.predict, X)
def __init__(self): pass
return (np.where([True, True, False, False, True])[0], np.where([False, False, True, True])[0])
S, rows, cols = make_checkerboard((30, 30), 3, noise=0.5, random_state=0)
assert_raises(ValueError, model.fit, mat) continue
generator = np.random.RandomState(0) mat = generator.rand(100, 100) scaled = _log_normalize(mat) + 1 _do_bistochastic_test(scaled)
x_squared_norms = (X ** 2).sum(axis=1) labels_array, inertia_array = _labels_inertia( X, x_squared_norms, noisy_centers) assert_array_almost_equal(inertia_array, inertia_gold) assert_array_equal(labels_array, labels_gold)
x_squared_norms_from_csr = row_norms(X_csr, squared=True) labels_csr, inertia_csr = _labels_inertia( X_csr, x_squared_norms_from_csr, noisy_centers) assert_array_almost_equal(inertia_csr, inertia_gold) assert_array_equal(labels_csr, labels_gold)
rng = np.random.RandomState(42) old_centers = centers + rng.normal(size=centers.shape)
X_mb = X[:10] X_mb_csr = X_csr[:10] x_mb_squared_norms = x_squared_norms[:10] x_mb_squared_norms_csr = x_squared_norms_csr[:10]
old_inertia, incremental_diff = _mini_batch_step( X_mb, x_mb_squared_norms, new_centers, counts, buffer, 1, None, random_reassign=False) assert_greater(old_inertia, 0.0)
labels, new_inertia = _labels_inertia( X_mb, x_mb_squared_norms, new_centers) assert_greater(new_inertia, 0.0) assert_less(new_inertia, old_inertia)
effective_diff = np.sum((new_centers - old_centers) ** 2) assert_almost_equal(incremental_diff, effective_diff)
old_inertia_csr, incremental_diff_csr = _mini_batch_step( X_mb_csr, x_mb_squared_norms_csr, new_centers_csr, counts_csr, buffer_csr, 1, None, random_reassign=False) assert_greater(old_inertia_csr, 0.0)
labels_csr, new_inertia_csr = _labels_inertia( X_mb_csr, x_mb_squared_norms_csr, new_centers_csr) assert_greater(new_inertia_csr, 0.0) assert_less(new_inertia_csr, old_inertia_csr)
effective_diff = np.sum((new_centers_csr - old_centers) ** 2) assert_almost_equal(incremental_diff_csr, effective_diff)
assert_array_equal(labels, labels_csr) assert_array_almost_equal(new_centers, new_centers_csr) assert_almost_equal(incremental_diff, incremental_diff_csr) assert_almost_equal(old_inertia, old_inertia_csr) assert_almost_equal(new_inertia, new_inertia_csr)
centers = km.cluster_centers_ assert_equal(centers.shape, (n_clusters, n_features))
assert_equal(v_measure_score(true_labels, labels), 1.0) assert_greater(km.inertia_, 0.0)
assert_raises(ValueError, km.fit, [[0., 1.]])
this_labels = np.unique(this_labels, return_index=True)[1][this_labels] np.testing.assert_array_equal(this_labels, labels)
km = KMeans(precompute_distances="wrong") assert_raises(ValueError, km.fit, X)
assert_raises_regex(ValueError, "n_init", KMeans(n_init=0).fit, X) assert_raises_regex(ValueError, "n_init", KMeans(n_init=-1).fit, X)
assert_warns(RuntimeWarning, mb_k_means.fit, X)
mb_k_means = MiniBatchKMeans(init="random", n_clusters=n_clusters, random_state=42, n_init=10).fit(X) _check_fitted_model(mb_k_means)
mb_k_means = MiniBatchKMeans(init="random", n_clusters=n_clusters, random_state=42, n_init=10).fit(X_csr) _check_fitted_model(mb_k_means)
mb_k_means = MiniBatchKMeans(n_clusters=20, batch_size=201, random_state=42, init="random") mb_k_means.fit(zeroed_X) assert_greater(mb_k_means.cluster_centers_.any(axis=1).sum(), 10)
assert_greater(mb_k_means.cluster_centers_.any(axis=1).sum(), 10)
for this_X in (X, X_csr): mb_k_means = MiniBatchKMeans(n_clusters=n_clusters, batch_size=100, random_state=42) mb_k_means.fit(this_X)
msg = "does not match the number of clusters" assert_raises_regex(ValueError, msg, MiniBatchKMeans(init=test_init, random_state=42).fit, X_csr)
mb_k_means = MiniBatchKMeans(n_clusters=3, init=test_init, random_state=42).fit(X_csr) _check_fitted_model(mb_k_means)
for X_minibatch in np.array_split(X, 10): km.partial_fit(X_minibatch)
labels = km.predict(X) assert_equal(v_measure_score(true_labels, labels), 1.0)
my_X = X.copy() km = KMeans(copy_x=False, n_clusters=n_clusters, random_state=42) km.fit(my_X) _check_fitted_model(km)
assert_array_almost_equal(my_X, X)
assert_equal(len(np.unique(km.labels_)), 3)
pred = km.predict(km.cluster_centers_) assert_array_equal(pred, np.arange(n_clusters))
pred = km.predict(X) assert_array_equal(pred, km.labels_)
pred = km.fit_predict(X) assert_array_equal(pred, km.labels_)
pred = mb_k_means.predict(mb_k_means.cluster_centers_) assert_array_equal(pred, np.arange(n_clusters))
pred = mb_k_means.predict(X) assert_array_equal(mb_k_means.predict(X), mb_k_means.labels_)
assert_array_equal(mb_k_means.predict(X_csr), mb_k_means.labels_)
pred = mb_k_means.predict(mb_k_means.cluster_centers_) assert_array_equal(pred, np.arange(n_clusters))
assert_array_equal(mb_k_means.predict(X), mb_k_means.labels_)
assert_array_equal(mb_k_means.predict(X_csr), mb_k_means.labels_)
pred = mb_k_means.predict(mb_k_means.cluster_centers_) assert_array_equal(pred, np.arange(n_clusters))
assert_array_equal(mb_k_means.predict(X), mb_k_means.labels_)
assert_equal(v_measure_score(true_labels, labels), 1.0) assert_greater(inertia, 0.0)
assert_warns(RuntimeWarning, k_means, X, n_clusters=n_clusters, init=centers)
assert_raises(ValueError, k_means, X, n_clusters=X.shape[0] + 1)
brc_partial.set_params(n_clusters=3) brc_partial.partial_fit(None) assert_array_equal(brc_partial.subcluster_labels_, brc.subcluster_labels_)
rng = np.random.RandomState(0) X = generate_clustered_data(n_clusters=3, n_features=3, n_samples_per_cluster=10)
gc = AgglomerativeClustering(n_clusters=10) brc2 = Birch(n_clusters=gc) brc2.fit(X) assert_array_equal(brc1.subcluster_labels_, brc2.subcluster_labels_) assert_array_equal(brc1.labels_, brc2.labels_)
clf = ElasticNet() brc3 = Birch(n_clusters=clf) assert_raises(ValueError, brc3.fit, X)
brc4 = Birch(threshold=10000.) assert_warns(UserWarning, brc4.fit, X)
X, y = make_blobs(n_samples=100, centers=10) brc = Birch(n_clusters=10) brc.fit(X)
X, y = make_blobs() branching_factor = 9
brc = Birch(n_clusters=None, branching_factor=1, threshold=0.01) assert_raises(ValueError, brc.fit, X)
X, y = make_blobs(n_samples=80, centers=4) brc = Birch(threshold=0.5, n_clusters=None) brc.fit(X) check_threshold(brc, 0.5)
bandwidth = estimate_bandwidth(X, n_samples=200) assert_true(0.9 <= bandwidth <= 1.5)
bandwidth = 1.2
ms = MeanShift(bandwidth=1.2) labels = ms.fit_predict(X) labels2 = ms.predict(X) assert_array_equal(labels, labels2)
ms = MeanShift() assert_false(hasattr(ms, "cluster_centers_")) assert_false(hasattr(ms, "labels_"))
with warnings.catch_warnings(record=True): test_bins = get_bin_seeds(X, 0.01, 1) assert_array_equal(test_bins, X)
from tempfile import mkdtemp import shutil from functools import partial
FeatureAgglomeration().fit(X)
dis = cosine_distances(X)
res = linkage_tree(X, affinity=manhattan_distances) assert_array_equal(res[0], linkage_tree(X, affinity="manhattan")[0])
clustering = AgglomerativeClustering( n_clusters=10, connectivity=connectivity.toarray(), affinity="manhattan", linkage="ward") assert_raises(ValueError, clustering.fit, X)
assert_raises(ValueError, agglo.fit, X[:0])
n, p, k = 10, 5, 3 rng = np.random.RandomState(0)
assert_raises(ValueError, _hc_cut, n_leaves + 1, children, n_leaves)
n, p = 10, 5 rng = np.random.RandomState(0)
n, p = 10, 5 rng = np.random.RandomState(0)
children_unstructured = out_unstructured[0] children_structured = out_structured[0]
assert_array_equal(children_unstructured, children_structured)
dist_unstructured = out_unstructured[-1] dist_structured = out_structured[-1]
assert_array_equal(linkage_X_ward[:, :2], out_X_unstructured[0]) assert_array_equal(linkage_X_ward[:, :2], out_X_structured[0])
assert_array_almost_equal(linkage_X_ward[:, 2], out_X_unstructured[4]) assert_array_almost_equal(linkage_X_ward[:, 2], out_X_structured[4])
assert_array_equal(X_truth[:, :2], out_X_unstructured[0]) assert_array_equal(X_truth[:, :2], out_X_structured[0])
assert_array_almost_equal(X_truth[:, 2], out_X_unstructured[4]) assert_array_almost_equal(X_truth[:, 2], out_X_structured[4])
rng = np.random.RandomState(0) X = rng.randn(10, 2) connectivity = kneighbors_graph(X, 5, include_self=False)
agc = AgglomerativeClustering(n_clusters=2, connectivity=connectivity) agc.fit(X) n_samples = X.shape[0] n_nodes = agc.children_.shape[0] assert_equal(n_nodes, n_samples - 1)
rng = np.random.RandomState(0) X = rng.rand(5, 5)
connectivity = np.eye(5)
assert_greater(np.mean(labels == true_labels), .3)
sp = SpectralClustering(n_clusters=2, affinity='<unknown>') assert_raises(ValueError, sp.fit, X)
eps = 0.8 min_samples = 10 metric = 'euclidean' core_samples, labels = dbscan(X, metric=metric, eps=eps, min_samples=min_samples)
n_clusters_1 = len(set(labels)) - int(-1 in labels) assert_equal(n_clusters_1, n_clusters)
eps = 0.8 min_samples = 10 metric = distance.euclidean core_samples, labels = dbscan(X, metric=metric, eps=eps, min_samples=min_samples, algorithm='ball_tree')
n_clusters_1 = len(set(labels)) - int(-1 in labels) assert_equal(n_clusters_1, n_clusters)
eps = 0.8 min_samples = 10
n_clusters_1 = len(set(labels)) - int(-1 in labels) assert_equal(n_clusters_1, n_clusters)
X = [[1., 2.], [3., 4.]]
assert_raises(ValueError, dbscan, [[0], [1]], sample_weight=[2]) assert_raises(ValueError, dbscan, [[0], [1]], sample_weight=[2, 3, 4])
D = pairwise_distances(X) core3, label3 = dbscan(D, sample_weight=sample_weight, metric='precomputed') assert_array_equal(core1, core3) assert_array_equal(label1, label3)
est = DBSCAN().fit(X, sample_weight=sample_weight) core4 = est.core_sample_indices_ label4 = est.labels_ assert_array_equal(core1, core4) assert_array_equal(label1, label4)
core_samples, labels = dbscan(X, algorithm=algorithm, eps=1, min_samples=3) assert_array_equal(core_samples, [2]) assert_array_equal(labels, [-1, 0, 0, 0, -1, -1, -1])
core_samples, labels = dbscan(X, algorithm=algorithm, eps=1, min_samples=4) assert_array_equal(core_samples, []) assert_array_equal(labels, -np.ones(n_samples))
if metric == 'precomputed' and sparse.issparse(X): neighborhoods = np.empty(X.shape[0], dtype=object)
neighborhoods = neighbors_model.radius_neighbors(X, eps, return_distance=False)
labels = -np.ones(X.shape[0], dtype=np.intp)
core_samples = np.asarray(n_neighbors >= min_samples, dtype=np.uint8) dbscan_inner(core_samples, neighborhoods, labels) return np.where(core_samples)[0], labels
self.components_ = X[self.core_sample_indices_].copy()
self.components_ = np.empty((0, X.shape[1]))
v0 = random_state.uniform(-1, 1, A.shape[0]) _, u = eigsh(A, ncv=self.n_svd_vecs, v0=v0)
connectivity = connectivity + connectivity.T
if not sparse.isspmatrix_lil(connectivity): if not sparse.isspmatrix(connectivity): connectivity = sparse.lil_matrix(connectivity) else: connectivity = connectivity.tolil()
n_components, labels = connected_components(connectivity)
parent = np.arange(n_nodes, dtype=np.intp) used_node = np.ones(n_nodes, dtype=bool) children = [] if return_distance: distances = np.empty(n_nodes - n_samples)
moments_1[k] = moments_1[i] + moments_1[j] moments_2[k] = moments_2[i] + moments_2[j]
[heappush(inertia, (ini[idx], k, coord_col[idx])) for idx in range(n_additions)]
n_leaves = n_samples children = [c[::-1] for c in children]
distances = np.sqrt(2. * distances) return children, n_components, n_leaves, parent, distances
i, j = np.triu_indices(X.shape[0], k=1) X = X[i, j]
affinity = 'euclidean'
diag_mask = (connectivity.row != connectivity.col) connectivity.row = connectivity.row[diag_mask] connectivity.col = connectivity.col[diag_mask] connectivity.data = connectivity.data[diag_mask] del diag_mask
distances = paired_distances(X[connectivity.row], X[connectivity.col], metric=affinity)
A = np.empty(n_nodes, dtype=object) inertia = list()
parent = np.arange(n_nodes, dtype=np.intp) used_node = np.ones(n_nodes, dtype=np.intp) children = []
for k in xrange(n_samples, n_nodes): while True: edge = heappop(inertia) if used_node[edge.a] and used_node[edge.b]: break i = edge.a j = edge.b
distances[k - n_samples] = edge.weight
n_i = used_node[i] n_j = used_node[j] used_node[k] = n_i + n_j used_node[i] = used_node[j] = False
n_leaves = n_samples
children = np.array(children)[:, ::-1]
def _complete_linkage(*args, **kwargs): kwargs['linkage'] = 'complete' return linkage_tree(*args, **kwargs)
compute_full_tree = self.n_clusters < max(100, .02 * n_samples)
import warnings import operator import sys import time
self.store_precision = True
X = check_array(X, ensure_min_features=2, ensure_min_samples=2, estimator=self)
self.store_precision = True
path = list() n_alphas = self.alphas inner_verbose = max(0, self.verbose - 1)
warnings.simplefilter('ignore', ConvergenceWarning)
import warnings import numbers import numpy as np from scipy import linalg from scipy.stats import chi2
if remaining_iterations == 0: if verbose: print('Maximum number of iterations reached') results = location, covariance, det, support, dist
if support_fraction is None: n_support = int(np.ceil(0.5 * (n_samples + n_features + 1))) else: n_support = int(support_fraction * n_samples)
from __future__ import division import warnings import numpy as np from scipy import linalg
self.covariance_ = covariance if self.store_precision: self.precision_ = pinvh(covariance) else: self.precision_ = None
test_cov = empirical_covariance( X_test - self.location_, assume_centered=True) res = log_likelihood(test_cov, self.get_precision())
centered_obs = observations - self.location_ mahalanobis_dist = np.sum( np.dot(centered_obs, precision) * centered_obs, 1)
if not alpha == 0: assert_array_less(np.diff(costs), 0)
indices = np.arange(10, 13)
GraphLassoCV(alphas=[0.8, 0.5], tol=1e-1, n_jobs=1).fit(X)
launch_mcd_on_dataset(1000, 5, 450, 0.1, 0.1, 540)
launch_mcd_on_dataset(1700, 5, 800, 0.1, 0.1, 870)
launch_mcd_on_dataset(500, 1, 100, 0.001, 0.001, 350)
rnd = np.random.RandomState(0) X = rnd.normal(size=(3, 1)) mcd = MinCovDet() mcd.fit(X)
cov = EmpiricalCovariance(assume_centered=True) cov.fit(X) assert_array_equal(cov.location_, np.zeros(X.shape[1]))
cov = ShrunkCovariance(shrinkage=0.5) cov.fit(X) assert_array_almost_equal( shrunk_covariance(empirical_covariance(X), shrinkage=0.5), cov.covariance_, 4)
cov = ShrunkCovariance() cov.fit(X) assert_array_almost_equal( shrunk_covariance(empirical_covariance(X)), cov.covariance_, 4)
cov = ShrunkCovariance(shrinkage=0.) cov.fit(X) assert_array_almost_equal(empirical_covariance(X), cov.covariance_, 4)
cov = ShrunkCovariance(shrinkage=0.5, store_precision=False) cov.fit(X) assert(cov.precision_ is None)
X_centered = X - X.mean(axis=0) lw = LedoitWolf(assume_centered=True) lw.fit(X_centered) shrinkage_ = lw.shrinkage_
lw = LedoitWolf(store_precision=False, assume_centered=True) lw.fit(X_centered) assert_almost_equal(lw.score(X_centered), score_, 4) assert(lw.precision_ is None)
lw = LedoitWolf(store_precision=False) lw.fit(X) assert_almost_equal(lw.score(X), score_, 4) assert(lw.precision_ is None)
X_small = X[:, :4] lw = LedoitWolf() lw.fit(X_small) shrinkage_ = lw.shrinkage_
lw = LedoitWolf(block_size=25).fit(X) assert_almost_equal(lw.covariance_, cov)
oa = OAS(store_precision=False, assume_centered=True) oa.fit(X_centered) assert_almost_equal(oa.score(X_centered), score_, 4) assert(oa.precision_ is None)
oa = OAS(store_precision=False) oa.fit(X) assert_almost_equal(oa.score(X), score_, 4) assert(oa.precision_ is None)
from __future__ import division import warnings import numpy as np
if not assume_centered: X = X - X.mean(0)
n_splits = int(n_features / block_size) X2 = X ** 2 emp_cov_trace = np.sum(X2, axis=0) / n_samples mu = np.sum(emp_cov_trace) / n_features
alpha = np.mean(emp_cov ** 2) num = alpha + mu ** 2 den = (n_samples + 1.) * (alpha - (mu ** 2) / n_features)
if self.assume_centered: self.location_ = np.zeros(X.shape[1]) else: self.location_ = X.mean(0)
rho, _ = spearmanr(x, y) increasing_bool = rho >= 0
rho_0 = math.tanh(F - 1.96 * F_se) rho_1 = math.tanh(F + 1.96 * F_se)
C = np.dot(sample_weight, y * y) * 10 if y_min is not None: y[0] = y_min sample_weight[0] = C if y_max is not None: y[-1] = y_max sample_weight[-1] = C
self.f_ = lambda x: y.repeat(x.shape)
if self.increasing == 'auto': self.increasing_ = check_increasing(X, y) else: self.increasing_ = self.increasing
self._X_ = X = unique_X self._y_ = y = isotonic_regression(unique_y, unique_sample_weight, self.y_min, self.y_max, increasing=self.increasing_)
self.X_min_, self.X_max_ = np.min(X), np.max(X)
return X, y
X, y = self._build_y(X, y, sample_weight)
self._necessary_X_, self._necessary_y_ = X, y
self._build_f(X, y) return self
state = dict(self.__dict__) state.pop('f_', None) return state
y = check_array(y, accept_sparse='csr', ensure_2d=False, dtype=None)
pos_switch = pos_label == 0 if pos_switch: pos_label = -neg_label
y_in_classes = in1d(y, classes) y_seen = y[y_in_classes] indices = np.searchsorted(sorted_class, y_seen) indptr = np.hstack((0, np.cumsum(y_in_classes)))
if np.any(classes != sorted_class): indices = np.searchsorted(sorted_class, classes) Y = Y[:, indices]
y_i_all_argmax = np.flatnonzero(y_data_repeated_max == y.data)
if row_max[-1] == 0: y_i_all_argmax = np.append(y_i_all_argmax, [len(y.data)])
class_mapping = defaultdict(int) class_mapping.default_factory = class_mapping.__len__ yt = self._transform(y, class_mapping)
tmp = sorted(class_mapping, key=class_mapping.get)
if array.size > 0: mode = stats.mode(array) most_frequent_value = mode[0][0] most_frequent_count = mode[1][0] else: most_frequent_value = 0 most_frequent_count = 0
if self.axis == 0: X = check_array(X, accept_sparse='csc', dtype=np.float64, force_all_finite=False)
if missing_values == 0: n_zeros_axis = np.zeros(X.shape[not axis], dtype=int) else: n_zeros_axis = X.shape[axis] - np.diff(X.indptr)
if strategy == "mean": if missing_values != 0: n_non_missing = n_zeros_axis
mask_missing_values = _get_mask(X.data, missing_values) mask_valids = np.logical_not(mask_missing_values)
with np.errstate(all="ignore"): return np.ravel(sums) / np.ravel(n_non_missing)
columns = [col[astype(mask, bool, copy=False)] for col, mask in zip(columns_all, mask_valids)]
if strategy == "median": median = np.empty(len(columns)) for i, column in enumerate(columns): median[i] = _get_median(column, n_zeros_axis[i])
elif strategy == "most_frequent": most_frequent = np.empty(len(columns))
elif strategy == "most_frequent":
if axis == 0: X = X.transpose() mask = mask.transpose()
if self.axis == 1: X = check_array(X, accept_sparse='csr', dtype=FLOAT_DTYPES, force_all_finite=False, copy=self.copy)
if hasattr(self, 'scale_'): del self.scale_ del self.min_ del self.n_samples_seen_ del self.data_min_ del self.data_max_ del self.data_range_
self._reset() return self.partial_fit(X, y)
X = check_array(X, copy=False, ensure_2d=False, warn_on_dtype=True, dtype=FLOAT_DTYPES) original_ndim = X.ndim
if hasattr(self, 'scale_'): del self.scale_ del self.n_samples_seen_ del self.mean_ del self.var_
self._reset() return self.partial_fit(X, y)
if hasattr(self, 'scale_'): del self.scale_ del self.n_samples_seen_ del self.max_abs_
self._reset() return self.partial_fit(X, y)
if not hasattr(self, 'n_samples_seen_'): self.n_samples_seen_ = X.shape[0] else: max_abs = np.maximum(self.max_abs_, max_abs) self.n_samples_seen_ += X.shape[0]
X = check_array(X, accept_sparse=('csr', 'csc'), copy=False, ensure_2d=False, dtype=FLOAT_DTYPES) original_ndim = X.ndim
XP = np.empty((n_samples, self.n_output_features_), dtype=X.dtype)
return X
return transform(X)
imputer = Imputer(missing_values, strategy=strategy, axis=0) imputer.fit(sparse.csc_matrix(X)) X_trans = imputer.transform(sparse.csc_matrix(X.copy()))
X = np.random.randn(10, 2) X[::2] = np.nan
length = arr.size if hasattr(arr, 'size') else len(arr) return np.nan if length == 0 else np.median(arr, *args, **kwargs)
length = arr.size if hasattr(arr, 'size') else len(arr) return np.nan if length == 0 else np.mean(arr, *args, **kwargs)
rng = np.random.RandomState(0)
X[:, j] = np.hstack((v, z, p))
np.random.RandomState(j).shuffle(X[:, j]) np.random.RandomState(j).shuffle(X_true[:, j])
if strategy == "median": cols_to_keep = ~np.isnan(X_true).any(axis=0) else: cols_to_keep = ~np.isnan(X_true).all(axis=0)
X = np.array([
pipeline = Pipeline([('imputer', Imputer(missing_values=0)), ('tree', tree.DecisionTreeRegressor(random_state=0))])
import pickle
X_orig = sparse_random_matrix(5, 5, density=0.75, random_state=0)
for X in [X_1row, X_1col, X_list_1row, X_list_1row]:
X_scaled_back = scaler.inverse_transform(X_scaled) assert_array_almost_equal(X_scaled_back, X)
X_list = [1., 3., 5., 0.] X_arr = np.array(X_list)
x_scaled = assert_no_warnings(scale, x) assert_array_almost_equal(scale(x), np.zeros(8))
rng = np.random.RandomState(0) n_features = 5 n_samples = 4 X = rng.randn(n_samples, n_features)
assert_true(X_scaled is not X)
X_scaled_back = scaler.inverse_transform(X_scaled) assert_true(X_scaled_back is not X) assert_true(X_scaled_back is not X_scaled) assert_array_almost_equal(X_scaled_back, X)
assert_true(X_scaled is not X)
assert_true(X_scaled is X)
assert_true(X_scaled is not X)
X = X_2d n = X.shape[0]
scaler_batch = MinMaxScaler().fit(X)
batch0 = slice(0, chunk_size) scaler_batch = MinMaxScaler().fit(X[batch0]) scaler_incr = MinMaxScaler().partial_fit(X[batch0])
scaler_batch = MinMaxScaler().fit(X)
X = X_2d n = X.shape[0]
scaler_batch = StandardScaler(with_std=False).fit(X)
scaler_batch = StandardScaler().fit(X)
scaler = StandardScaler(with_mean=False).fit(X) scaler_incr = StandardScaler(with_mean=False)
scaler_incr = scaler_incr.partial_fit(chunk)
X = np.array([[1.], [0.], [0.], [5.]]) X_csr = sparse.csr_matrix(X) X_csc = sparse.csc_matrix(X)
X = X_2d[:100, :]
assert_equal((i + 1), scaler_incr.n_samples_seen_)
scaler = MinMaxScaler(feature_range=(2, 1)) assert_raises(ValueError, scaler.fit, X)
X = [[0., 1., +0.5], [0., 1., -0.1], [0., 1., +1.1]]
X_trans = minmax_scale(X) assert_array_almost_equal(X_trans, X_expected_0_1) X_trans = minmax_scale(X, feature_range=(1, 2)) assert_array_almost_equal(X_trans, X_expected_1_2)
for X in [X_1row, X_1col, X_list_1row, X_list_1row]:
X_scaled_back = scaler.inverse_transform(X_scaled) assert_array_almost_equal(X_scaled_back, X)
assert_true(X_scaled is not X) assert_true(X_csr_scaled is not X_csr)
rng = np.random.RandomState(42) X = rng.randint(20, size=(4, 5))
assert_true(X_scaled is not X) assert_true(X_csr_scaled is not X_csr)
rng = np.random.RandomState(42) X = rng.randn(4, 5)
assert_raises(ValueError, scale, X_csr, with_mean=True) assert_raises(ValueError, StandardScaler(with_mean=True).fit, X_csr)
scaler = StandardScaler(with_mean=True).fit(X) assert_raises(ValueError, scaler.transform, X_csr) assert_raises(ValueError, scaler.transform, X_csc)
X = [np.nan, 5, 6, 7, 8] assert_raises_regex(ValueError, "Input contains NaN, infinity or a value too large", scale, X)
X_csc_scaled = scale(X_csr.tocsc(), with_mean=False) assert_array_almost_equal(X_scaled, X_csc_scaled.toarray())
assert_raises(ValueError, scale, X_csr, with_mean=False, axis=1)
assert_true(X_scaled is not X)
X_csr_scaled = scale(X_csr, with_mean=False, with_std=False, copy=True) assert_array_almost_equal(X_csr.toarray(), X_csr_scaled.toarray())
X_trans = maxabs_scale(X) assert_array_almost_equal(X_trans, X_expected)
X = np.array([[1, 2, 0], [0, 0, 0]], dtype=np.uint8)
for X in [X_1row, X_1col, X_list_1row, X_list_1row]:
X_scaled_back = scaler.inverse_transform(X_scaled) assert_array_almost_equal(X_scaled_back, X)
X_1d = X_1row.ravel() max_abs = np.abs(X_1d).max() assert_array_almost_equal(X_1d / max_abs, maxabs_scale(X_1d, copy=True))
X = X_2d[:100, :] n = X.shape[0]
scaler_batch = MaxAbsScaler().fit(X)
batch0 = slice(0, chunk_size) scaler_batch = MaxAbsScaler().fit(X[batch0]) scaler_incr = MaxAbsScaler().partial_fit(X[batch0])
scaler_batch = MaxAbsScaler().fit(X)
X_dense[3, :] = 0.0
indptr_3 = X_sparse_unpruned.indptr[3] indptr_4 = X_sparse_unpruned.indptr[4] X_sparse_unpruned.data[indptr_3:indptr_4] = 0.0
X_sparse_pruned = sparse.csr_matrix(X_dense)
for X in (X_dense, X_sparse_pruned, X_sparse_unpruned):
X_dense[3, :] = 0.0
indptr_3 = X_sparse_unpruned.indptr[3] indptr_4 = X_sparse_unpruned.indptr[4] X_sparse_unpruned.data[indptr_3:indptr_4] = 0.0
X_sparse_pruned = sparse.csr_matrix(X_dense)
for X in (X_dense, X_sparse_pruned, X_sparse_unpruned):
X_dense[3, :] = 0.0
indptr_3 = X_sparse_unpruned.indptr[3] indptr_4 = X_sparse_unpruned.indptr[4] X_sparse_unpruned.data[indptr_3:indptr_4] = 0.0
X_sparse_pruned = sparse.csr_matrix(X_dense)
for X in (X_dense, X_sparse_pruned, X_sparse_unpruned):
assert_raises(ValueError, binarizer.transform, sparse.csc_matrix(X))
centerer = KernelCenterer() K_fit_centered = np.dot(X_fit_centered, X_fit_centered.T) K_fit_centered2 = centerer.fit_transform(K_fit) assert_array_almost_equal(K_fit_centered, K_fit_centered2)
assert_array_equal(X_trans, [[0., 1., 0., 1., 1.], [1., 0., 1., 0., 1.]])
assert_raises(ValueError, enc.fit, [[0], [-1]])
enc.fit([[0], [1]]) assert_raises(ValueError, enc.transform, [[0], [-1]])
cat = [False, False, False] _check_one_hot(X, X2, cat, 3)
cat = [True, True, True] _check_one_hot(X, X2, cat, 5)
oh = OneHotEncoder(handle_unknown='error') oh.fit(X) assert_raises(ValueError, oh.transform, y)
oh = OneHotEncoder(handle_unknown='42') oh.fit(X) assert_raises(ValueError, oh.transform, y)
scalers = [StandardScaler(with_mean=False, with_std=False), MinMaxScaler(), MaxAbsScaler()]
scaler.fit_transform(X_2d)
testing.assert_array_equal( FunctionTransformer(np.log1p).transform(X), np.log1p(X), )
testing.assert_array_equal(F.transform(X), np.around(X, decimals=3))
testing.assert_array_equal(F.transform(X), np.around(X, decimals=1))
testing.assert_array_equal(F.transform(X), np.around(X, decimals=1))
one_class = np.array([0, 0, 0, 0]) lb = LabelBinarizer().fit(one_class)
assert_raises(ValueError, _inverse_binarize_thresholding, y=csr_matrix([[1, 2], [2, 1]]), output_type="foo", classes=[1, 2], threshold=0)
y_seq_of_seqs = [[], [1, 2], [3], [0, 1, 3], [2]] assert_raises(ValueError, LabelBinarizer().fit_transform, y_seq_of_seqs)
assert_raises(ValueError, _inverse_binarize_thresholding, y=csr_matrix([[1, 2], [2, 1]]), output_type="foo", classes=[1, 2, 3], threshold=0)
assert_raises(ValueError, _inverse_binarize_thresholding, y=np.array([[1, 2, 3], [2, 1, 3]]), output_type="binary", classes=[1, 2, 3], threshold=0)
le = LabelEncoder() ret = le.fit_transform([1, 1, 4, 5, -1, 0]) assert_array_equal(ret, [2, 2, 3, 4, 0, 1])
le = LabelEncoder() assert_raises(ValueError, le.transform, []) assert_raises(ValueError, le.inverse_transform, [])
le = LabelEncoder() le.fit([1, 2, 3, 1, -1]) assert_raises(ValueError, le.inverse_transform, [-1])
mlb = MultiLabelBinarizer(classes=[1, 3, 2]) assert_array_equal(mlb.fit_transform(inp), indicator_mat) assert_array_equal(mlb.classes_, [1, 3, 2])
mlb = MultiLabelBinarizer(classes=[1, 3, 2]) assert_array_equal(mlb.fit(inp).transform(inp), indicator_mat) assert_array_equal(mlb.classes_, [1, 3, 2])
inp = iter(inp) mlb = MultiLabelBinarizer(classes=[1, 3, 2]) assert_array_equal(mlb.fit(inp).transform(inp), indicator_mat)
mlb = MultiLabelBinarizer() assert_array_equal(mlb.fit(inp).transform(inp), indicator_mat) assert_array_equal(mlb.inverse_transform(indicator_mat), inp)
mlb = MultiLabelBinarizer() assert_array_equal(mlb.fit_transform(inp), indicator_mat) assert_array_equal(mlb.classes_, classes) assert_array_equal(mlb.inverse_transform(indicator_mat), inp)
assert_raises(ValueError, mlb.inverse_transform, np.array([[1]])) assert_raises(ValueError, mlb.inverse_transform, np.array([[1, 1, 1]]))
binarized = label_binarize(y, classes, neg_label=neg_label, pos_label=pos_label, sparse_output=sparse_output) assert_array_equal(toarray(binarized), expected) assert_equal(issparse(binarized), sparse_output)
y_type = type_of_target(y) if y_type == "multiclass": inversed = _inverse_binarize_multiclass(binarized, classes=classes)
base_estimator = LinearSVC(random_state=0)
mean_proba = np.zeros((X.shape[0], len(self.classes_))) for calibrated_classifier in self.calibrated_classifiers_: proba = calibrated_classifier.predict_proba(X) mean_proba += proba
if n_classes == 2: proba[:, 0] = 1. - proba[:, 1] else: proba /= np.sum(proba, axis=1)[:, np.newaxis]
proba[np.isnan(proba)] = 1. / n_classes
proba[(1.0 < proba) & (proba <= 1.0 + 1e-5)] = 1.0
** blas_info)
loss_l = self.loss.lower()
loss_l = self.loss.lower()
n_class = dual_coef.shape[0] + 1
alpha1 = dual_coef[class2 - 1, sv_locs[class1]:sv_locs[class1 + 1]] alpha2 = dual_coef[class1, sv_locs[class2]:sv_locs[class2 + 1]]
_sparse_kernels = ["linear", "poly", "rbf", "sigmoid", "precomputed"]
kernel = self.kernel return kernel == "precomputed" or callable(kernel)
self.class_weight_ = np.empty(0) return column_or_1d(y, warn=True).astype(np.float64)
kernel = self.kernel if callable(kernel): kernel = 'precomputed'
X = self._validate_for_predict(X) X = self._compute_kernel(X)
if self._impl in ['c_svc', 'nu_svc'] and len(self.classes_) == 2: return -dec_func.ravel()
if sp.issparse(coef): coef.data.flags.writeable = False else: coef.flags.writeable = False return coef
coef = safe_sparse_dot(self.dual_coef_, self.support_vectors_)
n_iter_ = max(n_iter_) if n_iter_ >= max_iter and verbose > 0: warnings.warn("Liblinear failed to converge, increase " "the number of iterations.", ConvergenceWarning)
X_blobs, y_blobs = make_blobs(n_samples=100, centers=10, random_state=0) X_blobs = sparse.csr_matrix(X_blobs)
digits = load_digits() X, y = digits.data[:50], digits.target[:50] X_test = sparse.csr_matrix(digits.data[50:100])
assert_array_almost_equal(coef_dense, coef_sorted.toarray())
assert_false(X_sparse_unsorted.has_sorted_indices) assert_false(X_test_unsorted.has_sorted_indices)
assert_array_almost_equal(coef_unsorted.toarray(), coef_sorted.toarray()) assert_array_almost_equal(sparse_svc.predict_proba(X_test_unsorted), sparse_svc.predict_proba(X_test))
svc = svm.SVC(kernel='linear', C=0.1, decision_function_shape='ovo') clf = svc.fit(iris.data, iris.target)
assert_raises(ValueError, svm.SVC(C=-1).fit, X, Y)
clf = svm.NuSVC(nu=0.0) assert_raises(ValueError, clf.fit, X_sp, Y)
clf = svm.LinearSVC(random_state=0).fit(X, Y) sp_clf = svm.LinearSVC(random_state=0).fit(X_sp, Y)
pred = np.argmax(sp_clf.decision_function(iris.data), 1) assert_array_almost_equal(pred, clf.predict(iris.data.toarray()))
clf.sparsify() assert_array_equal(pred, clf.predict(iris.data)) sp_clf.sparsify() assert_array_equal(pred, sp_clf.predict(iris.data))
X_, y_ = make_classification(n_samples=200, n_features=100, weights=[0.833, 0.167], random_state=0)
clf = svm.SVC() clf.fit(X_sp, Y) assert_array_equal(clf.predict([X[2]]), [1.])
test_svm.test_dense_liblinear_intercept_handling(svm.LinearSVC)
X_blobs, _ = make_blobs(n_samples=100, centers=10, random_state=0) X_blobs = sparse.csr_matrix(X_blobs)
a = svm.SVC(C=1, kernel=lambda x, y: x * y.T, probability=True, random_state=0) b = base.clone(a)
iris = datasets.load_iris() rng = check_random_state(42) perm = rng.permutation(iris.target.size) iris.data = iris.data[perm] iris.target = iris.target[perm]
pred2 = svm.libsvm.cross_validation(iris.data, iris.target.astype(np.float64), 5, kernel='linear', random_seed=0) assert_array_equal(pred, pred2)
KT = np.zeros_like(KT) for i in range(len(T)): for j in clf.support_: KT[i, j] = np.dot(T[i], X[j])
clf = svm.SVR(kernel=lambda x, y: np.array([[1.0]])) clf.fit(X, y) assert_raises(ValueError, clf.predict, X)
clf = svm.OneClassSVM() clf.fit(X) pred = clf.predict(T)
clf = svm.OneClassSVM() rnd = check_random_state(2)
X = 0.3 * rnd.randn(100, 2) X_train = np.r_[X + 2, X - 2]
clf = svm.OneClassSVM(nu=0.1, kernel="rbf", gamma=0.1) clf.fit(X_train)
clf = svm.SVC(kernel='linear', C=0.1, decision_function_shape='ovo').fit(iris.data, iris.target)
clf = svm.SVC(kernel='rbf', gamma=1, decision_function_shape='ovo') clf.fit(X, Y)
X, y = make_blobs(n_samples=80, centers=5, random_state=0) X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
reg = svm.SVR(kernel='linear', C=0.1).fit(X, y)
reg = svm.SVR(kernel='rbf', gamma=1).fit(X, y)
clf = svm.SVC(class_weight={1: 0.1}) clf.fit(X, Y) assert_array_almost_equal(clf.predict(X), [2] * 6)
assert_raises(ValueError, svm.SVC(C=-1).fit, X, Y)
clf = svm.NuSVC(nu=0.0) assert_raises(ValueError, clf.fit, X, Y)
clf = svm.SVC(kernel='precomputed') assert_raises(ValueError, clf.fit, X, Y)
clf = svm.SVC() assert_raises(ValueError, clf.fit, X, Y, sample_weight=range(len(X) - 1))
clf = svm.SVC().fit(X, Y) assert_raises(ValueError, clf.predict, sparse.lil_matrix(X))
assert_raises_regexp(ValueError, ".*loss='l3' is not supported.*", svm.LinearSVC(loss="l3").fit, X, y)
def test_linearsvx_loss_penalty_deprecations(): X, y = [[0.0], [1.0]], [0, 1]
clf = svm.LinearSVC(random_state=0).fit(X, Y)
assert_true(clf.fit_intercept)
clf = svm.LinearSVC(penalty='l1', loss='squared_hinge', dual=False, random_state=0).fit(X, Y) assert_array_equal(clf.predict(T), true_result)
clf = svm.LinearSVC(penalty='l2', dual=True, random_state=0).fit(X, Y) assert_array_equal(clf.predict(T), true_result)
clf = svm.LinearSVC(penalty='l2', loss='hinge', dual=True, random_state=0) clf.fit(X, Y) assert_array_equal(clf.predict(T), true_result)
dec = clf.decision_function(T) res = (dec > 0).astype(np.int) + 1 assert_array_equal(res, true_result)
assert_true((ovr_clf.predict(iris.data) == cs_clf.predict(iris.data)).mean() > .9)
assert_true((ovr_clf.coef_ != cs_clf.coef_).all())
X, y = make_classification(n_classes=2, random_state=0)
clf.intercept_scaling = 1 clf.fit(X, y) assert_almost_equal(clf.intercept_, 0, decimal=5)
clf.intercept_scaling = 100 clf.fit(X, y) intercept1 = clf.intercept_ assert_less(intercept1, -1)
clf.intercept_scaling = 1000 clf.fit(X, y) intercept2 = clf.intercept_ assert_array_almost_equal(intercept1, intercept2, decimal=2)
X = [[2, 1], [3, 1], [1, 3], [2, 3]] y = [0, 0, 1, 1]
import os
clf = svm.LinearSVC(verbose=1) clf.fit(X, Y)
X = np.random.RandomState(21).randn(10, 3) y = np.random.RandomState(12).randn(10)
param_grid = [param_grid]
product = partial(reduce, operator.mul) return sum(product(len(v) for v in p.values()) if p else 1 for p in self.param_grid)
for sub_grid in self.param_grid: if not sub_grid: if ind == 0: return {} else: ind -= 1 continue
keys, values_lists = zip(*sorted(sub_grid.items())[::-1]) sizes = [len(v_list) for v_list in values_lists] total = np.product(sizes)
ind -= total
all_lists = np.all([not hasattr(v, "rvs") for v in self.param_distributions.values()]) rnd = check_random_state(self.random_state)
param_grid = ParameterGrid(self.param_distributions) grid_size = len(param_grid)
n_fits = len(out) n_folds = len(cv)
best = sorted(grid_scores, key=lambda x: x.mean_validation_score, reverse=True)[0] self.best_params_ = best.parameters self.best_score_ = best.mean_validation_score
from __future__ import division
raise ValueError("The constant target value must be " "present in training data")
n_samples = int(X.shape[0]) rs = check_random_state(self.random_state)
n_classes_ = [n_classes_] classes_ = [classes_] class_prior_ = [class_prior_] constant = [constant]
n_samples = int(X.shape[0]) rs = check_random_state(self.random_state)
n_classes_ = [n_classes_] classes_ = [classes_] class_prior_ = [class_prior_] constant = [constant]
self.converged_ = False
current_log_likelihood = None self.converged_ = False
log_likelihoods, responsibilities = self.score_samples(X) current_log_likelihood = log_likelihoods.mean()
responsibilities = np.zeros((X.shape[0], self.n_components))
ESTIMATE_PRECISION_ERROR_MESSAGE = ("The algorithm has diverged because of " "too few samples per components. Try to " "decrease the number of components, " "or increase reg_covar.")
_, n_features = self.means_.shape
self._check_parameters(X)
do_init = not(self.warm_start and hasattr(self, 'converged_')) n_init = self.n_init if do_init else 1
log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
X, y = make_blobs(random_state=1) for Model in [DPGMM, VBGMM]: dpgmm = Model(n_components=10, random_state=1, alpha=20, n_iter=50, verbose=1)
X, y = make_blobs(random_state=1) for Model in [DPGMM, VBGMM]: dpgmm = Model(n_components=10, random_state=1, alpha=20, n_iter=50, verbose=2)
cv = (rng.rand() + 1.0) ** 2 samples = mixture.sample_gaussian( mu, cv, covariance_type='spherical', n_samples=n_samples)
@ignore_warnings(category=DeprecationWarning) def test_eval(self): if not self.do_test_eval:
X = g.sample(n_samples=100) g = self.model(n_components=self.n_components, covariance_type=self.covariance_type, random_state=rng, min_covar=1e-1, n_iter=1, init_params=params) g.fit(X)
@ignore_warnings(category=DeprecationWarning) def score(self, g, X): return g.score(X).sum()
@ignore_warnings(category=DeprecationWarning) def test_aic(): n_samples, n_dim, n_components = 50, 3, 2 X = rng.randn(n_samples, n_dim)
X = rng.randn(100, 2)
gmm.fit(X)
for covariance_type in ["full", "tied", "diag", "spherical"]: yield check_positive_definite_covars, covariance_type
@ignore_warnings(category=DeprecationWarning) def test_verbose_first_level(): X = rng.randn(30, 5) X[:10] += 2 g = mixture.GMM(n_components=2, n_init=2, verbose=1)
@ignore_warnings(category=DeprecationWarning) def test_verbose_second_level(): X = rng.randn(30, 5) X[:10] += 2 g = mixture.GMM(n_components=2, n_init=2, verbose=2)
rng = np.random.RandomState(0) X = rng.rand(10, 2)
weights = rand_data.weights g = GaussianMixture(weights_init=weights, n_components=n_components) g.fit(X) assert_array_equal(weights, g.weights_init)
means = rand_data.means g.means_init = means g.fit(X) assert_array_equal(means, g.means_init)
precisions_not_pos = np.ones((n_components, n_features, n_features)) precisions_not_pos[0] = np.eye(n_features) precisions_not_pos[0, 0, 0] = -1.
g.precisions_init = precisions_bad_shape[covar_type] assert_raise_message(ValueError, "The parameter '%s precision' should have " "the shape of" % covar_type, g.fit, X)
g.precisions_init = precisions_not_positive[covar_type] assert_raise_message(ValueError, "'%s precision' should be %s" % (covar_type, not_positive_errors[covar_type]), g.fit, X)
g.precisions_init = rand_data.precisions[covar_type] g.fit(X) assert_array_equal(rand_data.precisions[covar_type], g.precisions_init)
rng = np.random.RandomState(0) n_samples, n_features = 500, 2
rng = np.random.RandomState(0) n_samples, n_features, n_components = 500, 2, 2
rng = np.random.RandomState(0) n_samples, n_features, n_components = 500, 2, 2
rng = np.random.RandomState(0) n_samples, n_features = 500, 2
rng = np.random.RandomState(0) rand_data = RandomData(rng) n_samples = 500 n_features = rand_data.n_features n_components = rand_data.n_components
precs_full = np.array([np.diag(1. / np.sqrt(x)) for x in covars_diag])
precs_chol_diag = 1. / np.sqrt(covars_diag) log_prob = _estimate_log_gaussian_prob_diag(X, means, precs_chol_diag) assert_array_almost_equal(log_prob, log_prob_naive)
covars_tied = np.array([x for x in covars_diag]).mean(axis=0) precs_tied = np.diag(np.sqrt(1. / covars_tied))
rng = np.random.RandomState(0) rand_data = RandomData(rng, scale=5) n_samples = rand_data.n_samples n_features = rand_data.n_features n_components = rand_data.n_components
assert_raise_message(NotFittedError, "This GaussianMixture instance is not fitted " "yet. Call 'fit' with appropriate arguments " "before using this method.", g.predict, X)
rng = np.random.RandomState(0) rand_data = RandomData(rng) n_features = rand_data.n_features n_components = rand_data.n_components
assert_allclose(np.sort(g.weights_), np.sort(rand_data.weights), rtol=0.1, atol=1e-2)
assert_allclose(ecov.error_norm(prec_pred[k]), 0, atol=0.1)
g = GaussianMixture(n_components=n_components, n_init=1, max_iter=2, reg_covar=0, random_state=random_state, warm_start=False) h = GaussianMixture(n_components=n_components, n_init=1, max_iter=1, reg_covar=0, random_state=random_state, warm_start=True)
g = GaussianMixture(n_components=n_components, n_init=1, max_iter=5, reg_covar=0, random_state=random_state, warm_start=False, tol=1e-6) h = GaussianMixture(n_components=n_components, n_init=1, max_iter=5, reg_covar=0, random_state=random_state, warm_start=True, tol=1e-6)
gmm2 = GaussianMixture(n_components=n_components, n_init=1, reg_covar=0, random_state=rng, covariance_type=covar_type).fit(X) assert_greater(gmm2.score(X), gmm1.score(X))
rng = np.random.RandomState(0) rand_data = RandomData(rng, scale=7) n_components = rand_data.n_components
for _ in range(300): prev_log_likelihood = current_log_likelihood try: current_log_likelihood = gmm.fit(X).score(X) except ConvergenceWarning: pass assert_greater_equal(current_log_likelihood, prev_log_likelihood)
rng = np.random.RandomState(0) n_samples, n_features = 10, 5
del dgamma1, dgamma2, sd
X = check_array(X) if X.ndim == 1: X = X[:, np.newaxis]
current_log_likelihood = None self.converged_ = False
curr_logprob, z = self.score_samples(X)
if prev_log_likelihood is not None: change = abs(current_log_likelihood - prev_log_likelihood) if change < self.tol: self.converged_ = True break
self._do_mstep(X, z, self.params)
z = np.zeros((X.shape[0], self.n_components))
n_samples_per_label = np.bincount(labels)
indices = np.argsort(n_samples_per_label)[::-1] n_samples_per_label = n_samples_per_label[indices]
n_samples_per_fold = np.zeros(n_folds)
label_to_fold = np.zeros(len(unique_labels))
for label_index, weight in enumerate(n_samples_per_label): lightest_fold = np.argmin(n_samples_per_fold) n_samples_per_fold[lightest_fold] += weight label_to_fold[indices[label_index]] = lightest_fold
if self.shuffle: rng = check_random_state(self.random_state) else: rng = self.random_state
self.labels = np.array(labels, copy=True) self.unique_labels = np.unique(labels) self.n_unique_labels = len(self.unique_labels)
permutation = rng.permutation(self.n) ind_test = permutation[:self.n_test] ind_train = permutation[self.n_test:self.n_test + self.n_train] yield ind_train, ind_test
return v
if sp.issparse(preds[0]): preds = sp.vstack(preds, format=preds[0].format) else: preds = np.concatenate(preds) return preds[inv_locs]
fit_params = fit_params if fit_params is not None else {} fit_params = dict([(k, _index_param_value(X, v, train)) for k, v in fit_params.items()])
fit_params = fit_params if fit_params is not None else {} fit_params = dict([(k, _index_param_value(X, v, train)) for k, v in fit_params.items()])
raise ValueError("Cannot use a custom kernel function. " "Precompute the kernel matrix instead.")
n_features = X.shape[1] if self.n_features_to_select is None: n_features_to_select = n_features // 2 else: n_features_to_select = self.n_features_to_select
while np.sum(support_) > n_features_to_select: features = np.arange(n_features)[support_]
estimator = clone(self.estimator) if self.verbose > 0: print("Fitting estimator with %d features." % np.sum(support_))
if coefs.ndim > 1: ranks = np.argsort(safe_sqr(coefs).sum(axis=0)) else: ranks = np.argsort(safe_sqr(coefs))
ranks = np.ravel(ranks)
threshold = min(step, np.sum(support_) - n_features_to_select)
if step_score: self.scores_.append(step_score(estimator, features)) support_[features[ranks][:threshold]] = False ranking_[np.logical_not(support_)] += 1
features = np.arange(n_features)[support_] self.estimator_ = clone(self.estimator) self.estimator_.fit(X[:, features], y)
if step_score: self.scores_.append(step_score(self.estimator_, features)) self.n_features_ = support_.sum() self.support_ = support_ self.ranking_ = ranking_
rfe = RFE(estimator=self.estimator, n_features_to_select=n_features_to_select, step=self.step)
self.grid_scores_ = scores[::-1] / cv.get_n_splits(X, y) return self
from __future__ import division
nn = NearestNeighbors(metric='chebyshev', n_neighbors=n_neighbors)
nn.set_params(algorithm='kd_tree')
mask = label_counts > 1 n_samples = np.sum(mask) label_counts = label_counts[mask] k_all = k_all[mask] c = c[mask] radius = radius[mask]
try: mask = importances >= threshold except TypeError: raise ValueError("Invalid threshold: all features are discarded.")
names_t_actual = sel.transform([feature_names]) assert_array_equal(feature_names_t, names_t_actual.ravel())
assert_raises(ValueError, sel.transform, np.array([[1], [2]]))
assert_raises(ValueError, sel.transform, np.array([[1], [2]]))
names_inv_actual = sel.inverse_transform([feature_names_t]) assert_array_equal(feature_names_inv, names_inv_actual.ravel())
assert_raises(ValueError, sel.inverse_transform, np.array([[1], [2]]))
assert_raises(ValueError, sel.inverse_transform, np.array([[1], [2]]))
X = [[2, 1, 2], [9, 1, 1], [6, 1, 2], [0, 1, 2]] y = [0, 1, 2, 2]
Xtrans = Xtrans.toarray() Xtrans2 = mkchi2(k=2).fit_transform(Xsp, y).toarray() assert_equal(Xtrans, Xtrans2)
Xcoo = coo_matrix(X) mkchi2(k=2).fit_transform(Xcoo, y)
assert_array_equal(rfe.get_support(), rfe_svc.get_support())
clf_sparse = SVC(kernel="linear") rfe_sparse = RFE(estimator=clf_sparse, n_features_to_select=4, step=0.1) rfe_sparse.fit(X_sparse, y) X_r_sparse = rfe_sparse.transform(X_sparse)
assert_array_equal(X_r, iris.data)
iris = load_iris() score = cross_val_score(rfe, iris.data, iris.target) assert_greater(score.min(), .7)
selector = RFE(estimator, step=0.01) sel = selector.fit(X, y) assert_equal(sel.support_.sum(), n_features // 2)
selector = RFE(estimator, step=0.20) sel = selector.fit(X, y) assert_equal(sel.support_.sum(), n_features // 2)
selector = RFE(estimator, step=5) sel = selector.fit(X, y) assert_equal(sel.support_.sum(), n_features // 2)
f, p = f_oneway(X.astype(np.float), y) assert_array_almost_equal(f, fint, decimal=4) assert_array_almost_equal(p, pint, decimal=4)
X, y = make_classification(n_samples=200, n_features=20, n_informative=3, n_redundant=2, n_repeated=0, n_classes=8, n_clusters_per_class=1, flip_y=0.0, class_sep=10, shuffle=False, random_state=0)
X, y = make_regression(n_samples=200, n_features=20, n_informative=5, shuffle=False, random_state=0)
rng = np.random.RandomState(0) X = rng.rand(10, 20) y = np.arange(10).astype(np.int)
X, y = make_classification(n_samples=200, n_features=20, n_informative=3, n_redundant=2, n_repeated=0, n_classes=8, n_clusters_per_class=1, flip_y=0.0, class_sep=10, shuffle=False, random_state=0)
X, y = make_classification(n_samples=200, n_features=20, n_informative=3, n_redundant=2, n_repeated=0, n_classes=8, n_clusters_per_class=1, flip_y=0.0, class_sep=10, shuffle=False, random_state=0)
assert_equal(X_r2inv.getnnz(), X_r.getnnz())
X, y = make_classification(n_samples=200, n_features=20, n_informative=3, n_redundant=2, n_repeated=0, n_classes=8, n_clusters_per_class=1, flip_y=0.0, class_sep=10, shuffle=False, random_state=0)
X, y = make_classification(n_samples=20, n_features=10, shuffle=False, random_state=0)
X, y = make_classification(n_samples=20, n_features=10, shuffle=False, random_state=0)
X, y = make_classification(n_samples=200, n_features=20, n_informative=3, n_redundant=2, n_repeated=0, n_classes=8, n_clusters_per_class=1, flip_y=0.0, class_sep=10, shuffle=False, random_state=0)
X, y = make_regression(n_samples=200, n_features=20, n_informative=5, shuffle=False, random_state=0)
assert_array_equal(X_2.astype(bool), univariate_filter.inverse_transform(X_r.astype(bool)))
X, y = make_regression(n_samples=200, n_features=20, n_informative=5, shuffle=False, random_state=0)
X, y = make_regression(n_samples=200, n_features=20, n_informative=5, shuffle=False, random_state=0, noise=10)
X, y = make_regression(n_samples=200, n_features=20, n_informative=5, shuffle=False, random_state=0, noise=10)
def single_fdr(alpha, n_informative, random_state): X, y = make_regression(n_samples=150, n_features=20, n_informative=n_informative, shuffle=False, random_state=random_state, noise=10)
false_discovery_rate = np.mean([single_fdr(alpha, n_informative, random_state) for random_state in range(30)]) assert_greater_equal(alpha, false_discovery_rate)
if false_discovery_rate != 0: assert_greater(false_discovery_rate, alpha / 10)
X, y = make_regression(n_samples=200, n_features=20, n_informative=5, shuffle=False, random_state=0)
X0 = np.array([[10000, 9999, 9998], [1, 1, 1]]) y = [0, 1]
X_train = np.array([[0, 0, 0], [1, 1, 1]]) y_train = [0, 1]
X = [[0, 1, 0], [0, -1, -1], [0, .5, .5]] y = [1, 0, 1]
x = np.array([0, 1, 1, 0, 0]) y = np.array([1, 0, 0, 0, 1])
mean = np.zeros(2)
sigma_1 = 1 sigma_2 = 10 corr = 0.5 cov = np.array([ [sigma_1**2, corr * sigma_1 * sigma_2], [corr * sigma_1 * sigma_2, sigma_2**2] ])
I_theory = (np.log(sigma_1) + np.log(sigma_2) - 0.5 * np.log(np.linalg.det(cov)))
for n_neighbors in [3, 5, 7]: I_computed = _compute_mi(x, y, False, False, n_neighbors) assert_almost_equal(I_computed, I_theory, 1)
for n_neighbors in [3, 5, 7]: I_computed = _compute_mi(x, y, True, False, n_neighbors) assert_almost_equal(I_computed, I_theory, 1)
n_samples = 100 x = np.random.uniform(size=n_samples) > 0.5
mi = mutual_info_classif(X, y, discrete_features=True) assert_array_equal(np.argsort(-mi), np.array([0, 2, 1]))
for X in [data, csr_matrix(data)]: X = VarianceThreshold(threshold=.4).fit_transform(X) assert_equal((len(data), 1), X.shape)
sample_weight = np.ones(y.shape) sample_weight[y == 1] *= 100
model = SelectFromModel(clf, prefit=False) model.fit(data, y) assert_array_equal(model.transform(data), X_transform)
model = SelectFromModel(clf, prefit=True) assert_raises(ValueError, model.fit, data, y)
est.fit(data, y) threshold = 0.5 * np.mean(est.feature_importances_) mask = est.feature_importances_ > threshold assert_array_equal(X_transform, data[:, mask])
model.threshold = 1.0 assert_greater(X_transform.shape[1], model.transform(data).shape[1])
scores = as_float_array(scores, copy=True) scores[np.isnan(scores)] = np.finfo(scores.dtype).min return scores
f = np.asarray(f).ravel() prob = special.fdtrc(dfbn, dfwn, f) return f, prob
chisq = f_obs chisq -= f_exp chisq **= 2 chisq /= f_exp chisq = chisq.sum(axis=0) return chisq, special.chdtrc(k - 1, chisq)
corr = safe_sparse_dot(y, X) corr /= row_norms(X.T) corr /= norm(y)
mask[np.argsort(scores, kind="mergesort")[-self.k:]] = 1 return mask
possible_params = selector._get_param_names() possible_params.remove('score_func') selector.set_params(**{possible_params[0]: self.param})
if ((X.data if sparse else X) < 0).any(): raise ValueError("Entries of X must be non-negative.")
exp_doc_topic = np.exp(_dirichlet_expectation_2d(doc_topic_distr))
suff_stats = np.zeros(exp_topic_word_distr.shape) if cal_sstats else None
exp_doc_topic_d = exp_doc_topic[idx_d, :].copy() exp_topic_word_d = exp_topic_word_distr[:, ids]
for _ in xrange(0, max_iters): last_d = doc_topic_d
norm_phi = np.dot(exp_doc_topic_d, exp_topic_word_d) + EPS
if cal_sstats: norm_phi = np.dot(exp_doc_topic_d, exp_topic_word_d) + EPS suff_stats[:, ids] += np.outer(exp_doc_topic_d, cnts / norm_phi)
self.components_ = self.random_state_.gamma( init_gamma, init_var, (self.n_topics, n_features))
self.exp_dirichlet_component_ = np.exp( _dirichlet_expectation_2d(self.components_))
random_state = self.random_state_ if random_init else None
doc_topics, sstats_list = zip(*results) doc_topic_distr = np.vstack(doc_topics)
suff_stats = np.zeros(self.components_.shape) for sstats in sstats_list: suff_stats += sstats suff_stats *= self.exp_dirichlet_component_
_, suff_stats = self._e_step(X, cal_sstats=True, random_init=True, parallel=parallel)
self.exp_dirichlet_component_ = np.exp( _dirichlet_expectation_2d(self.components_)) self.n_batch_iter_ += 1 return
if not hasattr(self, 'components_'): self._init_latent_vars(n_features)
doc_topic_distr /= doc_topic_distr.sum(axis=1)[:, np.newaxis] return doc_topic_distr
score += _loglikelihood(doc_topic_prior, doc_topic_distr, dirichlet_doc_topic, self.n_topics)
if sub_sampling: doc_ratio = float(self.total_samples) / n_samples score *= doc_ratio
score += _loglikelihood(topic_word_prior, self.components_, dirichlet_component_, n_features)
nsqrt = sqrt(n_samples) llconst = n_features * log(2. * np.pi) + n_components var = np.var(X, axis=0)
if self.n_components == 0: return np.diag(1. / self.noise_variance_) if self.n_components == n_features: return linalg.inv(self.get_covariance())
if not hasattr(self, 'n_samples_seen_'): self.n_samples_seen_ = 0 self.mean_ = .0 self.var_ = .0
col_mean, col_var, n_total_samples = \ _incremental_mean_and_var(X, last_mean=self.mean_, last_variance=self.var_, last_sample_count=self.n_samples_seen_)
U, S, V = randomized_svd(X, n_components, random_state=random_state) W, H = np.zeros(U.shape), np.zeros(V.shape)
x_p_nrm, y_p_nrm = norm(x_p), norm(y_p) x_n_nrm, y_n_nrm = norm(x_n), norm(y_n)
if m_p > m_n: u = x_p / x_p_nrm v = y_p / y_p_nrm sigma = m_p else: u = x_n / x_n_nrm v = y_n / y_n_nrm sigma = m_n
if norm(grad * np.logical_or(grad < 0, H > 0)) < tol: break
tolW = max(0.001, tol) * np.sqrt(init_grad) tolH = tolW
proj_grad_W = squared_norm(gradW * np.logical_or(gradW < 0, W > 0)) proj_grad_H = squared_norm(gradH * np.logical_or(gradH < 0, H > 0))
if l2_reg != 0.: HHt.flat[::n_components + 1] += l2_reg if l1_reg != 0.: XHt -= l1_reg
permutation = np.asarray(permutation, dtype=np.intp) return _update_cdnmf_fast(W, HHt, XHt, permutation)
Ht = check_array(H.T, order='C') X = check_array(X, accept_sparse='csr')
U *= sqrt(X.shape[0])
U *= S[:self.n_components_]
if self.n_components is None: n_components = X.shape[1] else: n_components = self.n_components
self.mean_ = np.mean(X, axis=0) X -= self.mean_
U, V = svd_flip(U, V)
explained_variance_ = (S ** 2) / n_samples total_var = explained_variance_.sum() explained_variance_ratio_ = explained_variance_ / total_var
if n_components < min(n_features, n_samples): self.noise_variance_ = explained_variance_[n_components:].mean() else: self.noise_variance_ = 0.
self.mean_ = np.mean(X, axis=0) X -= self.mean_
U, S, V = randomized_svd(X, n_components=n_components, n_iter=self.iterated_power, flip_sign=True, random_state=random_state)
self.mean_ = np.mean(X, axis=0) X -= self.mean_ if self.n_components is None: n_components = X.shape[1] else: n_components = self.n_components
import warnings import numpy as np from scipy import linalg
return np.dot(np.dot(u * (1. / np.sqrt(s)), u.T), W)
for j in range(n_components): w = w_init[j, :].copy() w /= np.sqrt((w ** 2).sum())
lim = max(abs(abs(np.diag(fast_dot(W1, W.T))) - 1)) W = W1 if lim < tol: break
def _logcosh(x, fun_args=None):
X = check_array(X, copy=whiten, dtype=FLOAT_DTYPES).T
X_mean = X.mean(axis=-1) X -= X_mean[:, np.newaxis]
u, d, _ = linalg.svd(X, full_matrices=False)
X1 *= np.sqrt(p)
copy_cov = False cov = np.dot(dictionary, X.T)
lasso_lars = LassoLars(alpha=alpha, fit_intercept=False, verbose=verbose, normalize=False, precompute=gram, fit_path=False) lasso_lars.fit(dictionary.T, X.T, Xy=cov) new_code = lasso_lars.coef_
lars = Lars(fit_intercept=False, verbose=verbose, normalize=False, precompute=gram, n_nonzero_coefs=int(regularization), fit_path=False) lars.fit(dictionary.T, X.T, Xy=cov) new_code = lars.coef_
if code.ndim == 1: code = code[np.newaxis, :] return code
code = np.empty((n_samples, n_components)) slices = list(gen_even_slices(n_samples, _get_n_jobs(n_jobs)))
alpha = float(alpha) random_state = check_random_state(random_state)
dictionary = np.array(dictionary, order='F')
ii = -1
current_cost = 0.5 * residuals + alpha * np.sum(np.abs(code)) errors.append(current_cost)
if dE < tol * errors[-1]: if verbose == 1: print("") elif verbose: print("--- Convergence reached after %d iterations" % ii) break
alpha = float(alpha) random_state = check_random_state(random_state)
ii = iter_offset - 1
dictionary = _update_dict(dictionary, B, A, verbose=verbose, random_state=random_state)
if callback is not None: callback(locals())
X = check_array(X) n_samples, n_features = X.shape
if sp.issparse(X) and X.getformat() not in ["csr", "csc"]: X = X.tocsr()
Sigma = Sigma[::-1] U, VT = svd_flip(U[:, ::-1], VT[::-1])
K = self._centerer.fit_transform(K)
if self.eigen_solver == 'auto': if K.shape[0] > 200 and n_components < 10: eigen_solver = 'arpack' else: eigen_solver = 'dense' else: eigen_solver = self.eigen_solver
indices = self.lambdas_.argsort()[::-1] self.lambdas_ = self.lambdas_[indices] self.alphas_ = self.alphas_[:, indices]
if self.remove_zero_eig or self.n_components is None: self.alphas_ = self.alphas_[:, self.lambdas_ > 0] self.lambdas_ = self.lambdas_[self.lambdas_ > 0]
if self.n_components_ == 0: return np.eye(n_features) / self.noise_variance_ if self.n_components_ == n_features: return linalg.inv(self.get_covariance())
assert_array_almost_equal(comp_a[:9], comp_r[:9]) assert_array_almost_equal(comp_a[9:], comp_r[9:], decimal=2)
tsvd = TruncatedSVD(n_components=52, random_state=42) Xt = tsvd.fit_transform(X) Xinv = tsvd.inverse_transform(Xt) assert_array_almost_equal(Xinv, Xdense, decimal=1)
for svd_10, svd_20 in svds_10_v_20: assert_array_almost_equal( svd_10.explained_variance_ratio_, svd_20.explained_variance_ratio_[:10], decimal=5, )
for svd_10, svd_20 in svds_10_v_20: assert_greater( svd_20.explained_variance_ratio_.sum(), svd_10.explained_variance_ratio_.sum(), )
for svd in svds: assert_array_less(0.0, svd.explained_variance_ratio_)
for svd in svds: assert_array_less(svd.explained_variance_ratio_.sum(), 1.0)
for svd_sparse, svd_dense in svds_sparse_v_dense: assert_array_almost_equal(svd_sparse.explained_variance_ratio_, svd_dense.explained_variance_ratio_)
inv = not callable(kernel)
assert_not_equal(X_fit_transformed.size, 0)
X_pred_transformed = kpca.transform(X_pred) assert_equal(X_pred_transformed.shape[1], X_fit_transformed.shape[1])
if inv: X_pred2 = kpca.inverse_transform(X_pred_transformed) assert_equal(X_pred2.shape, X_pred.shape)
state = np.random.RandomState(0) X = state.rand(10, 10) kpca = KernelPCA(random_state=state).fit(X) transformed1 = kpca.transform(X)
X_pred_transformed = kpca.transform(X_pred) assert_equal(X_pred_transformed.shape[1], X_fit_transformed.shape[1])
kpca = KernelPCA() Xt = kpca.fit_transform(X) assert_equal(Xt.shape, (3, 0))
X, y = make_circles(n_samples=400, factor=.3, noise=.05, random_state=0)
train_score = Perceptron().fit(X, y).score(X, y) assert_less(train_score, 0.8)
kpca = KernelPCA(kernel="rbf", n_components=2, fit_inverse_transform=True, gamma=2.) X_kpca = kpca.fit_transform(X)
train_score = Perceptron().fit(X_kpca, y).score(X_kpca, y) assert_equal(train_score, 1.0)
X = iris.data
cov = pca.get_covariance() precision = pca.get_precision() assert_array_almost_equal(np.dot(cov, precision), np.eye(X.shape[1]), 12)
pca = PCA(svd_solver='full') pca.fit(X) assert_almost_equal(pca.explained_variance_ratio_.sum(), 1.0, 3)
X = iris.data d = X.shape[1]
for n_comp in np.arange(1, d): pca = PCA(n_components=n_comp, svd_solver='arpack', random_state=0)
cov = pca.get_covariance() precision = pca.get_precision() assert_array_almost_equal(np.dot(cov, precision), np.eye(d), 12)
X = iris.data
for n_comp in np.arange(1, X.shape[1]): pca = PCA(n_components=n_comp, svd_solver='randomized', random_state=0)
cov = pca.get_covariance() precision = pca.get_precision() assert_array_almost_equal(np.dot(cov, precision), np.eye(X.shape[1]), 12)
n_components = 10
rng = np.random.RandomState(0) n_samples = 100 n_features = 80 n_components = 30 rank = 50
assert_greater(X.std(axis=0).std(), 43.8)
X_whitened = pca.fit_transform(X_.copy()) assert_equal(X_whitened.shape, (n_samples, n_components)) X_whitened2 = pca.transform(X_) assert_array_almost_equal(X_whitened, X_whitened2)
assert_almost_equal(X_unwhitened.std(axis=0).std(), 74.1, 1)
@ignore_warnings def test_explained_variance(): rng = np.random.RandomState(0) n_samples = 100 n_features = 80
X_pca = pca.transform(X) assert_array_almost_equal(pca.explained_variance_, np.var(X_pca, axis=0))
X = datasets.make_classification(n_samples, n_features, n_informative=n_features-2, random_state=rng)[0]
rng = np.random.RandomState(0) n, p = 50, 3
rng = np.random.RandomState(0) n, p = 50, 3
pca = PCA(n_components=2, whiten=True, svd_solver=solver) pca.fit(X) ll2 = pca.score(X) assert_true(ll1 > ll2)
pca = PCA(n_components=50) pca.fit(X) pca_test = PCA(n_components=50, svd_solver='full') pca_test.fit(X) assert_array_almost_equal(pca.components_, pca_test.components_)
A = np.abs(random_state.randn(30, 10)) NMF(n_components=15, random_state=0, tol=1e-2).fit(A)
from scipy.sparse import csc_matrix
A = np.abs(random_state.randn(10, 10)) A[:, 2 * np.arange(5)] = 0
Y = np.dot(U, V)
spca_lasso = SparsePCA(n_components=3, method='cd', random_state=0, alpha=alpha) spca_lasso.fit(Y) assert_array_almost_equal(spca_lasso.components_, spca_lars.components_)
rng = np.random.RandomState(0)
spca_lasso = MiniBatchSparsePCA(n_components=3, method='cd', alpha=alpha, random_state=0).fit(Y) assert_array_almost_equal(spca_lasso.components_, spca_lars.components_)
def g_test(x): return x ** 3, (3 * x ** 2).mean(axis=-1)
if whiten: assert_almost_equal(s_, np.dot(np.dot(mixing_, k_), m))
ica = FastICA(n_components=1, whiten=False, random_state=0) assert_warns(UserWarning, ica.fit, m) assert_true(hasattr(ica, 'mixing_'))
rng = np.random.RandomState(0)
mixing = rng.randn(6, 2) m = np.dot(mixing, s)
assert_almost_equal(s_, np.dot(np.dot(mixing_, k_), m))
if not add_noise: assert_almost_equal(np.dot(s1_, s1) / n_samples, 1, decimal=3) assert_almost_equal(np.dot(s2_, s2) / n_samples, 1, decimal=3)
if n_components == X.shape[1]: assert_array_almost_equal(X, X2)
X = iris.data batch_size = X.shape[0] // 3 ipca = IncrementalPCA(n_components=2, batch_size=batch_size) pca = PCA(n_components=2) pca.fit_transform(X)
Yt = IncrementalPCA(n_components=2).fit(X).transform(Xt)
Yt /= np.sqrt((Yt ** 2).sum())
assert_almost_equal(np.abs(Yt[0][0]), 1., 1)
rng = np.random.RandomState(1999) n, p = 50, 3
ipca = IncrementalPCA(n_components=2, batch_size=10).fit(X) Y = ipca.transform(X) Y_inverse = ipca.inverse_transform(Y) assert_almost_equal(X, Y_inverse, decimal=3)
X = [[0, 1], [1, 0]] for n_components in [-1, 0, .99, 3]: assert_raises(ValueError, IncrementalPCA(n_components, batch_size=10).fit, X)
rng = np.random.RandomState(1999) n, p = 50, 3
X = iris.data
rng = np.random.RandomState(1999) n_samples = 100 n_features = 3 X = rng.randn(n_samples, n_features) + 5 * rng.rand(1, n_features)
from sklearn.externals.six.moves import cStringIO as StringIO import sys
@ignore_warnings def test_factor_analysis(): rng = np.random.RandomState(0) n_samples, n_features, n_components = 20, 5, 3
W = rng.randn(n_components, n_features) h = rng.randn(n_samples, n_components) noise = rng.gamma(1, size=n_features) * rng.randn(n_samples, n_features)
X = np.dot(h, W) + noise
scov = np.cov(X, rowvar=0., bias=1.)
n_topics, X = _build_sparse_mtx() prior = 1. / n_topics lda_1 = LatentDirichletAllocation(n_topics=n_topics, doc_topic_prior=prior, topic_word_prior=prior, random_state=0) lda_2 = LatentDirichletAllocation(n_topics=n_topics, random_state=0)
rng = np.random.RandomState(0) n_topics, X = _build_sparse_mtx() lda = LatentDirichletAllocation(n_topics=n_topics, evaluate_every=1, learning_method='batch', random_state=rng) lda.fit(X)
top_idx = set(component.argsort()[-3:][::-1]) assert_true(tuple(sorted(top_idx)) in correct_idx_grps)
top_idx = set(component.argsort()[-3:][::-1]) assert_true(tuple(sorted(top_idx)) in correct_idx_grps)
top_idx = set(component.argsort()[-3:][::-1]) assert_true(tuple(sorted(top_idx)) in correct_idx_grps)
X = np.ones((5, 10))
X = -np.ones((5, 10)) lda = LatentDirichletAllocation() regex = r"^Negative values in data passed" assert_raises_regexp(ValueError, regex, lda.fit, X)
n_topics, X = _build_sparse_mtx() lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=10, random_state=0) distr = lda.fit_transform(X) perplexity_1 = lda.perplexity(X, distr, sub_sampling=False)
normalizer = proba_k.sum(axis=1)[:, np.newaxis] normalizer[normalizer == 0.0] = 1.0 proba_k /= normalizer
self.centroids_ = np.empty((n_classes, n_features), dtype=np.float64) nk = np.zeros(n_classes)
raise ValueError( "kd_tree algorithm does not support callable metric '%s'" % metric)
return self.metric == 'precomputed'
n_neighbors += 1
neigh_ind = neigh_ind[ sample_range, np.argsort(dist[sample_range, neigh_ind])]
if return_distance: dist, neigh_ind = result else: neigh_ind = result
dup_gr_nbrs = np.all(sample_mask, axis=1) sample_mask[:, 0][dup_gr_nbrs] = False
if X is not None: X = check_array(X, accept_sparse='csr') n_samples1 = X.shape[0] else: n_samples1 = self._fit_X.shape[0]
if mode == 'connectivity': A_data = np.ones(n_samples1 * n_neighbors) A_ind = self.kneighbors(X, n_neighbors, return_distance=False)
neigh_ind = np.empty(n_samples, dtype='object') neigh_ind[:] = neigh_ind_list
if return_distance: dist, neigh_ind = results else: neigh_ind = results
self._choose_algorithm(self.algorithm, self.metric)
out = np.packbits((projected > 0).astype(int)).view(dtype=HASH_DTYPE) return out.reshape(projected.shape[0], -1)
return np.empty(0, dtype=np.int), np.empty(0, dtype=float)
left_mask = np.tril(np.ones((tri_size, tri_size), dtype=int))[:, 1:] right_mask = left_mask[::-1, ::-1]
n_candidates = 0 candidate_set = set() min_candidates = self.n_candidates * self.n_estimators while (max_depth > self.min_hash_match and (n_candidates < min_candidates or len(candidate_set) < n_neighbors)):
self.hash_functions_ = [] self.trees_ = [] self.original_indices_ = []
bin_queries = np.asarray([hasher.transform(X)[:, 0] for hasher in self.hash_functions_]) bin_queries = np.rollaxis(bin_queries, 1)
depths = [_find_longest_prefix_match(tree, tree_queries, MAX_HASH_SIZE, self._left_mask, self._right_mask) for tree, tree_queries in zip(self.trees_, np.rollaxis(bin_queries, 1))]
n_samples = 12 n_features = 2 n_iter = 10 rng = np.random.RandomState(42) X = rng.rand(n_samples, n_features)
assert_raises(ValueError, lshf.kneighbors, X[0])
assert_equal(neighbors.shape[1], n_neighbors)
assert_raises(ValueError, lshf.radius_neighbors, X[0])
query = X[rng.randint(0, n_samples)].reshape(1, -1)
mean_dist = np.mean(pairwise_distances(query, X, metric='cosine')) neighbors = lshf.radius_neighbors(query, radius=mean_dist, return_distance=False)
distances, neighbors = lshf.radius_neighbors(query, radius=mean_dist, return_distance=True) assert_array_less(distances[0], mean_dist)
n_queries = 5 queries = X[rng.randint(0, n_samples, n_queries)] distances, neighbors = lshf.radius_neighbors(queries, return_distance=True)
assert_equal(distances.shape, (n_queries,)) assert_equal(distances.dtype, object) assert_equal(neighbors.shape, (n_queries,)) assert_equal(neighbors.dtype, object)
sorted_dists_exact = np.sort(distances_exact[0]) sorted_dists_approx = np.sort(distances_approx[0])
assert_true(np.all(np.less_equal(sorted_dists_exact, sorted_dists_approx)))
nnbrs = NearestNeighbors(algorithm='brute', metric='cosine').fit(X)
lsfh = LSHForest(min_hash_match=0, n_candidates=n_points).fit(X)
query = [[1., 0.]]
dists = pairwise_distances(query, X, metric='cosine').ravel()
assert_almost_equal(dists[0], 0, decimal=5)
assert_almost_equal(dists[1], 1 - np.cos(np.pi / 4))
assert_almost_equal(dists[2], 1)
assert_almost_equal(dists[3], 2, decimal=5)
exact_dists, exact_idx = nnbrs.radius_neighbors(query, radius=1) approx_dists, approx_idx = lsfh.radius_neighbors(query, radius=1)
n_samples = 12 n_features = 2 n_iter = 10 rng = np.random.RandomState(42) X = rng.rand(n_samples, n_features)
assert_true(np.all(np.diff(distances[0]) >= 0))
n_samples = 12 n_features = 2 n_estimators = 5 rng = np.random.RandomState(42) X = rng.rand(n_samples, n_features)
n_samples = 12 n_samples_partial_fit = 3 n_features = 2 rng = np.random.RandomState(42) X = rng.rand(n_samples, n_features) X_partial_fit = rng.rand(n_samples_partial_fit, n_features)
ignore_warnings(lshf.partial_fit)(X) assert_array_equal(X, lshf._fit_X)
assert_raises(ValueError, lshf.partial_fit, np.random.randn(n_samples_partial_fit, n_features - 1))
assert_equal(lshf._fit_X.shape[0], n_samples + n_samples_partial_fit) assert_equal(len(lshf.original_indices_[0]), n_samples + n_samples_partial_fit) assert_equal(len(lshf.trees_[1]), n_samples + n_samples_partial_fit)
lshf = LSHForest(min_hash_match=32) ignore_warnings(lshf.fit)(X_train)
lshf = LSHForest(min_hash_match=31) ignore_warnings(lshf.fit)(X_train)
n_samples_sizes = [5, 10, 20] n_features = 3 rng = np.random.RandomState(42)
iris = datasets.load_iris() perm = rng.permutation(iris.target.size) iris.data = iris.data[perm] iris.target = iris.target[perm]
digits = datasets.load_digits() perm = rng.permutation(digits.target.size) digits.data = digits.data[perm] digits.target = digits.target[perm]
neighbors.kneighbors_graph = ignore_warnings(neighbors.kneighbors_graph) neighbors.radius_neighbors_graph = ignore_warnings( neighbors.radius_neighbors_graph)
with np.errstate(divide='ignore'): retval = 1. / dist return retval ** 2
X = rng.rand(n_samples, n_features)
X = rng.random_sample((10, 3))
nbrs_X = neighbors.NearestNeighbors(n_neighbors=3) nbrs_X.fit(X) dist_X, ind_X = getattr(nbrs_X, method)(Y)
dist_X, ind_X = getattr(nbrs_X, method)(None) dist_D, ind_D = getattr(nbrs_D, method)(None) assert_array_almost_equal(dist_X, dist_D) assert_array_almost_equal(ind_X, ind_D)
assert_raises(ValueError, getattr(nbrs_D, method), X)
rng = np.random.RandomState(random_state)
knn.fit(X, y_str) y_pred = knn.predict(X[:n_test_pts] + epsilon) assert_array_equal(y_pred, y_str[:n_test_pts])
rng = check_random_state(0) n_features = 2 n_samples = 40 n_output = 3
rnn_mo = neighbors.RadiusNeighborsClassifier(weights=weights, algorithm=algorithm) rnn_mo.fit(X_train, y_train) y_pred_mo = rnn_mo.predict(X_test)
rng = check_random_state(0) n_features = 5 n_samples = 50 n_output = 3
knn_mo = neighbors.KNeighborsClassifier(weights=weights, algorithm=algorithm) knn_mo.fit(X_train, y_train) y_pred_mo = knn_mo.predict(X_test)
y_pred_proba_mo = knn_mo.predict_proba(X_test) assert_equal(len(y_pred_proba_mo), n_output)
rng = check_random_state(0) n_features = 5 n_samples = 40 n_output = 4
X = np.array([[0, 1], [1.01, 1.], [2, 0]])
A = neighbors.kneighbors_graph(X, 1, mode='connectivity', include_self=True) assert_array_equal(A.toarray(), np.eye(A.shape[0]))
rng = np.random.RandomState(seed) X = rng.randn(10, 10) Xcsr = csr_matrix(X)
X = np.array([[0, 1], [1.01, 1.], [2, 0]])
rng = np.random.RandomState(seed) X = rng.randn(10, 10) Xcsr = csr_matrix(X)
assert_raises(ValueError, neighbors.NearestNeighbors, algorithm='blah')
V = rng.rand(n_features, n_features) VI = np.dot(V, V.T)
if (algorithm == 'kd_tree' and metric not in neighbors.KDTree.valid_metrics): assert_raises(ValueError, neighbors.NearestNeighbors, algorithm=algorithm, metric=metric, metric_params=metric_params) continue
dist_array = pairwise_distances(X).flatten() np.sort(dist_array) radius = dist_array[15]
for algorithm in ALGORITHMS:
assert_array_almost_equal(dist1, dist2)
bt1_pyfunc = BallTree(X, metric=dist_func, leaf_size=1, p=2)
simultaneous_sort(dist, ind)
i = np.argsort(dist2, axis=1) row_ind = np.arange(n_rows)[:, None] dist2 = dist2[row_ind, i] ind2 = ind2[row_ind, i]
self.X1_bool = self.X1.round(0) self.X2_bool = self.X2.round(0)
euclidean_pkl = pickle.loads(pickle.dumps(euclidean)) pyfunc_pkl = pickle.loads(pickle.dumps(pyfunc))
assert_array_almost_equal(dist1, dist2)
simultaneous_sort(dist, ind)
i = np.argsort(dist2, axis=1) row_ind = np.arange(n_rows)[:, None] dist2 = dist2[row_ind, i] ind2 = ind2[row_ind, i]
kde = KernelDensity(bandwidth, kernel=kernel).fit(X) samp = kde.sample(100) assert_equal(X.shape, samp.shape)
nbrs = NearestNeighbors(n_neighbors=1).fit(X) dist, ind = nbrs.kneighbors(X, return_distance=True)
assert np.all(dist < 5 * bandwidth)
rng = np.random.RandomState(0)
X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]
clf = NearestCentroid() clf.fit(X, y) assert_array_equal(clf.predict(T), true_result)
clf = NearestCentroid() clf.fit(X_csr, y) assert_array_equal(clf.predict(T_csr), true_result)
clf = NearestCentroid() clf.fit(X_csr, y) assert_array_equal(clf.predict(T), true_result)
clf = NearestCentroid() clf.fit(X, y) assert_array_equal(clf.predict(T_csr), true_result)
clf = NearestCentroid() clf.fit(X_csr.tocoo(), y) assert_array_equal(clf.predict(T_csr.tolil()), true_result)
obj = NearestCentroid() obj.fit(iris.data, iris.target) score = obj.score(iris.data, iris.target) s = pickle.dumps(obj)
self._validate_params(self.n_features, self.input_type) return self
if patches.shape[-1] == 1: return patches.reshape((n_patches, p_h, p_w)) else: return patches
img[i, j] /= float(min(i + 1, p_h, i_h - i) * min(j + 1, p_w, i_w - j))
if stop_words is not None: tokens = [w for w in tokens if w not in stop_words]
text_document = self._white_spaces.sub(" ", text_document)
text_document = self._white_spaces.sub(" ", text_document)
self._get_hasher().fit(X, y=y) return self
fit_transform = transform
vocabulary = defaultdict() vocabulary.default_factory = vocabulary.__len__
continue
vocabulary = dict(vocabulary) if not vocabulary: raise ValueError("empty vocabulary; perhaps the documents only" " contain stop words")
self._validate_vocabulary() max_df = self.max_df min_df = self.min_df max_features = self.max_features
_, X = self._count_vocab(raw_documents, fixed_vocab=True) if self.binary: X.data.fill(1) return X
X = X.tocsr()
X = np.asmatrix(X)
df += int(self.smooth_idf) n_samples += int(self.smooth_idf)
idf = np.log(float(n_samples) / df) + 1.0 self._idf_diag = sp.spdiags(idf, diags=0, m=n_features, n=n_features)
X = sp.csr_matrix(X, copy=copy)
X = sp.csr_matrix(X, dtype=np.float64, copy=copy)
X = X * self._idf_diag
return self._tfidf.transform(X, copy=False)
hasher = FeatureHasher() hasher.set_params(n_features=np.inf) assert_raises(TypeError, hasher.fit)
X = FeatureHasher().transform([{'foo': 0}]) assert_equal(X.data.shape, (0,))
np.testing.assert_array_equal(grad_x.data[grad_x.data > 0], grad_y.data[grad_y.data > 0])
mask = np.ones((size, size), dtype=np.int16) A = grid_to_graph(n_x=size, n_y=size, n_z=size, mask=mask) assert_true(connected_components(A)[0] == 1)
from scipy import misc face = misc.face(gray=True)
from scipy import misc face = misc.face(gray=True)
from scipy import misc face = misc.face(gray=True)
images = np.zeros((3,) + face.shape) images[0] = face images[1] = face + 1 images[2] = face + 2 return images
assert_array_equal(X.A, v.transform(iter(D) if iterable else D).A)
v_1 = DictVectorizer().fit([d_sorted]) v_2 = DictVectorizer().fit([d_shuffled])
a = '\xe0\xe1\xe2\xe3\xe4\xe5\xe7\xe8\xe9\xea\xeb' expected = 'aaaaaaceeee' assert_equal(strip_accents_unicode(a), expected)
a = "this is \xe0 test" expected = 'this is a test' assert_equal(strip_accents_unicode(a), expected)
a = '\xe0\xe1\xe2\xe3\xe4\xe5\xe7\xe8\xe9\xea\xeb' expected = 'aaaaaaceeee' assert_equal(strip_accents_ascii(a), expected)
a = "this is \xe0 test" expected = 'this is a test' assert_equal(strip_accents_ascii(a), expected)
text = "J'ai mang\xe9 du kangourou ce midi, c'\xe9tait pas tr\xeas bon." text_bytes = text.encode('utf-8')
wa = CountVectorizer(ngram_range=(1, 2), encoding='ascii').build_analyzer() assert_raises(UnicodeDecodeError, wa, text_bytes)
v.fit(["to be or not to be", "and me too", "and so do you"]) assert False, "we shouldn't get here"
assert_array_almost_equal((tfidf ** 2).sum(axis=1), [1., 1., 1.])
assert_array_almost_equal((tfidf ** 2).sum(axis=1), [1., 1., 1.])
X = [[1, 1, 0], [1, 1, 0], [1, 0, 0]] tr = TfidfTransformer(smooth_idf=False, norm='l2')
train_data = iter(ALL_FOOD_DOCS[:-1]) test_data = [ALL_FOOD_DOCS[-1]] n_train = len(ALL_FOOD_DOCS) - 1
v2 = CountVectorizer(vocabulary=v1.vocabulary_)
for v in (v1, v2): counts_test = v.transform(test_data) if hasattr(counts_test, 'tocsr'): counts_test = counts_test.tocsr()
assert_false("the" in vocabulary)
assert_false("copyright" in vocabulary)
tfidf_test = t1.transform(counts_test).toarray() assert_equal(tfidf_test.shape, (len(test_data), len(v1.vocabulary_)))
t2 = TfidfTransformer(norm='l1', use_idf=False) tf = t2.fit(counts_train).transform(counts_train).toarray() assert_equal(t2.idf_, None)
t3 = TfidfTransformer(use_idf=True) assert_raises(ValueError, t3.transform, counts_train)
X = [[1, 1, 5], [1, 1, 0]] t3.fit(X) X_incompt = [[1, 3], [1, 3]] assert_raises(ValueError, t3.transform, X_incompt)
assert_array_almost_equal(np.sum(tf, axis=1), [1.0] * n_train)
train_data = iter(ALL_FOOD_DOCS[:-1]) tv = TfidfVectorizer(norm='l1')
tfidf_test2 = tv.transform(test_data).toarray() assert_array_almost_equal(tfidf_test, tfidf_test2)
v3 = CountVectorizer(vocabulary=None) assert_raises(ValueError, v3.transform, train_data)
v3.set_params(strip_accents='ascii', lowercase=False) assert_equal(v3.build_preprocessor(), strip_accents_ascii)
v3.set_params(strip_accents='_gabbledegook_', preprocessor=None) assert_raises(ValueError, v3.build_preprocessor)
v3.set_params = '_invalid_analyzer_type_' assert_raises(ValueError, v3.build_analyzer)
for i in range(X.shape[0]): assert_almost_equal(np.linalg.norm(X[0].data, 2), 1.0)
ngrams_nnz = X.nnz assert_true(ngrams_nnz > token_nnz) assert_true(ngrams_nnz < 2 * token_nnz)
assert_true(np.min(X.data) > 0) assert_true(np.max(X.data) < 1)
for i in range(X.shape[0]): assert_almost_equal(np.linalg.norm(X[0].data, 1), 1.0)
assert_raises(ValueError, cv.get_feature_names)
vectorizer = vec_factory(max_df=0.6, max_features=4) vectorizer.fit(ALL_FOOD_DOCS) assert_equal(set(vectorizer.vocabulary_), expected_vocabulary) assert_equal(vectorizer.stop_words_, expected_stop_words)
assert_equal(7, counts_1.max()) assert_equal(7, counts_3.max()) assert_equal(7, counts_None.max())
vect = CountVectorizer(analyzer='char', max_df=1.0, binary=True, dtype=np.float32) X_sparse = vect.fit_transform(test_data) assert_equal(X_sparse.dtype, np.float32)
vect = HashingVectorizer(analyzer='char', non_negative=True, binary=True, norm=None, dtype=np.float64) X = vect.transform(test_data) assert_equal(X.dtype, np.float64)
data = JUNK_FOOD_DOCS + NOTJUNK_FOOD_DOCS
target = [-1] * len(JUNK_FOOD_DOCS) + [1] * len(NOTJUNK_FOOD_DOCS)
train_data, test_data, target_train, target_test = train_test_split( data, target, test_size=.2, random_state=0)
grid_search = GridSearchCV(pipeline, parameters, n_jobs=1)
pred = grid_search.fit(train_data, target_train).predict(test_data) assert_array_equal(pred, target_test)
assert_equal(grid_search.best_score_, 1.0) best_vectorizer = grid_search.best_estimator_.named_steps['vect'] assert_equal(best_vectorizer.ngram_range, (1, 1))
data = JUNK_FOOD_DOCS + NOTJUNK_FOOD_DOCS
target = [-1] * len(JUNK_FOOD_DOCS) + [1] * len(NOTJUNK_FOOD_DOCS)
train_data, test_data, target_train, target_test = train_test_split( data, target, test_size=.1, random_state=0)
grid_search = GridSearchCV(pipeline, parameters, n_jobs=1)
pred = grid_search.fit(train_data, target_train).predict(test_data) assert_array_equal(pred, target_test)
data = JUNK_FOOD_DOCS + NOTJUNK_FOOD_DOCS
target = [-1] * len(JUNK_FOOD_DOCS) + [1] * len(NOTJUNK_FOOD_DOCS)
assert_equal(X_counted.nnz, X_hashed.nnz)
assert_array_equal(np.sort(X_counted.data), np.sort(X_hashed.data))
message = "np.nan is an invalid document, expected byte or unicode string." exception = ValueError
v = TfidfVectorizer(binary=True, use_idf=False, norm=None) assert_true(v.binary)
X = [X] if isinstance(X, Mapping) else X
values = []
X = check_array(X, accept_sparse=['csr', 'csc']) n_samples = X.shape[0]
from .metrics import r2_score return r2_score(y, self.predict(X), sample_weight=sample_weight, multioutput='uniform_average')
self.steps = tosequence(steps) transforms = estimators[:-1] estimator = estimators[-1]
return getattr(self.steps[0][1], '_pairwise', False)
return transformer.transform(X) * transformer_weights[name]
n_samples, self.n_features_ = X.shape is_classification = isinstance(self, ClassifierMixin)
y = np.reshape(y, (-1, 1))
max_depth = ((2 ** 31) - 1 if self.max_depth is None else self.max_depth) max_leaf_nodes = (-1 if self.max_leaf_nodes is None else self.max_leaf_nodes)
if self.min_weight_fraction_leaf != 0. and sample_weight is not None: min_weight_leaf = (self.min_weight_fraction_leaf * np.sum(sample_weight)) else: min_weight_leaf = 0.
if self.presort == 'auto' and issparse(X): presort = False elif self.presort == 'auto': presort = True
if max_leaf_nodes < 0: builder = DepthFirstTreeBuilder(splitter, min_samples_split, min_samples_leaf, min_weight_leaf, max_depth) else: builder = BestFirstTreeBuilder(splitter, min_samples_split, min_samples_leaf, min_weight_leaf, max_depth, max_leaf_nodes)
if isinstance(self, ClassifierMixin): if self.n_outputs_ == 1: return self.classes_.take(np.argmax(proba, axis=1), axis=0)
else: if self.n_outputs_ == 1: return proba[:, 0]
s, v = 0.75, 0.9 c = s * v m = v - c
if tree.n_outputs == 1: value = tree.value[node_id][0, :] else: value = tree.value[node_id]
labels = (label == 'root' and node_id == 0) or label == 'all'
if node_ids: if labels: node_string += 'node ' node_string += characters[0] + str(node_id) + characters[4]
if node_string[-2:] == '\\n': node_string = node_string[:-2] if node_string[-5:] == '<br/>': node_string = node_string[:-5]
if max_depth is None or depth <= max_depth:
out_file.write(', fillcolor="#C0C0C0"')
out_file.write('%d -> %d ;\n' % (parent, node_id))
ranks = {'leaves': []} colors = {'bounds': None}
if isinstance(decision_tree, _tree.Tree): recurse(decision_tree, 0, criterion="impurity") else: recurse(decision_tree.tree_, 0, criterion=decision_tree.criterion)
clf = DecisionTreeClassifier(max_depth=3, min_samples_split=2, criterion="gini", random_state=2) clf.fit(X, y)
clf = DecisionTreeClassifier(max_depth=2, min_samples_split=2, criterion="gini", random_state=2) clf = clf.fit(X, y2, sample_weight=w)
clf = DecisionTreeRegressor(max_depth=3, min_samples_split=2, criterion="mse", random_state=2) clf.fit(X, y)
clf = DecisionTreeClassifier(max_depth=3) clf.fit(X, y_degraded)
clf = DecisionTreeClassifier(max_depth=3, min_samples_split=2) clf.fit(X, y)
out = StringIO() assert_raises(IndexError, export_graphviz, clf, out, feature_names=[])
out = StringIO() assert_raises(IndexError, export_graphviz, clf, out, class_names=[])
boston = datasets.load_boston() perm = rng.permutation(boston.target.size) boston.data = boston.data[perm] boston.target = boston.target[perm]
for name, Tree in CLF_TREES.items(): clf = Tree(random_state=0)
y = np.zeros((10, 10)) y[:5, :5] = 1 y[5:, 5:] = 1
X = np.arange(10000)[:, np.newaxis] y = np.arange(10000)
X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]] y = [1, 1, 1, 1, 1, 1]
X, y = datasets.make_classification(n_samples=2000, n_features=10, n_informative=3, n_redundant=0, n_repeated=0, shuffle=False, random_state=0)
clf = DecisionTreeClassifier() clf.feature_importances_
est = TreeEstimator(max_features=10) assert_raises(ValueError, est.fit, X, y)
for name, TreeEstimator in CLF_TREES.items(): est = TreeEstimator() assert_raises(NotFittedError, est.predict_proba, X)
est = TreeEstimator() y2 = y[:-1] assert_raises(ValueError, est.fit, X, y2)
Xf = np.asfortranarray(X) est = TreeEstimator() est.fit(Xf, y) assert_almost_equal(est.predict(T), true_result)
est = TreeEstimator() assert_raises(NotFittedError, est.predict, T)
est.fit(X, y) t = np.asarray(T) assert_raises(ValueError, est.predict, t[:, 1:])
Xt = np.array(X).T
est = TreeEstimator() assert_raises(NotFittedError, est.apply, T)
for max_leaf_nodes, name in product((None, 1000), ALL_TREES.keys()): TreeEstimator = ALL_TREES[name]
est = TreeEstimator(min_samples_split=10, max_leaf_nodes=max_leaf_nodes, random_state=0) est.fit(X, y) node_samples = est.tree_.n_node_samples[est.tree_.children_left != -1]
est = TreeEstimator(min_samples_split=0.2, max_leaf_nodes=max_leaf_nodes, random_state=0) est.fit(X, y) node_samples = est.tree_.n_node_samples[est.tree_.children_left != -1]
X = np.asfortranarray(iris.data.astype(tree._tree.DTYPE)) y = iris.target
for max_leaf_nodes, name in product((None, 1000), ALL_TREES.keys()): TreeEstimator = ALL_TREES[name]
leaf_weights = node_weights[node_weights != 0] assert_greater_equal( np.min(leaf_weights), total_weight * est.min_weight_fraction_leaf, "Failed with {0} " "min_weight_fraction_leaf={1}".format( name, est.min_weight_fraction_leaf))
for name in ALL_TREES: yield check_min_weight_fraction_leaf, name, "iris"
for name in SPARSE_TREES: yield check_min_weight_fraction_leaf, name, "multilabel", True
for name, TreeClassifier in CLF_TREES.items(): clf = TreeClassifier(random_state=0) clf.fit(X, y)
unbalanced_X = iris.data[:125] unbalanced_y = iris.target[:125] sample_weight = compute_sample_weight("balanced", unbalanced_y)
for (name, TreeEstimator), dtype in product(ALL_TREES.items(), [np.float64, np.float32]): est = TreeEstimator(random_state=0)
X = np.asarray(iris.data, dtype=dtype) y = iris.target assert_array_equal(est.fit(X, y).predict(X), y)
X = np.ascontiguousarray(iris.data, dtype=dtype) y = iris.target assert_array_equal(est.fit(X, y).predict(X), y)
X = csr_matrix(iris.data, dtype=dtype) y = iris.target assert_array_equal(est.fit(X, y).predict(X), y)
X = csc_matrix(iris.data, dtype=dtype) y = iris.target assert_array_equal(est.fit(X, y).predict(X), y)
X = np.arange(100)[:, np.newaxis] y = np.ones(100) y[:50] = 0.0
X = np.arange(200)[:, np.newaxis] y = np.zeros(200) y[50:100] = 1 y[100:200] = 2 X[100:200, 0] = 200
X = iris.data y = iris.target
X = np.arange(100)[:, np.newaxis] y = np.ones(100) y[:50] = 0.0
TreeClassifier = CLF_TREES[name] _y = np.vstack((y, np.array(y) * 2)).T
clf = TreeClassifier(class_weight='the larch', random_state=0) assert_raises(ValueError, clf.fit, X, y) assert_raises(ValueError, clf.fit, X, _y)
clf = TreeClassifier(class_weight=1, random_state=0) assert_raises(ValueError, clf.fit, X, _y)
clf = TreeClassifier(class_weight=[{-1: 0.5, 1: 1.}], random_state=0) assert_raises(ValueError, clf.fit, X, _y)
huge = 2 ** (n_bits + 1) clf = DecisionTreeClassifier(splitter='best', max_leaf_nodes=huge) assert_raises(Exception, clf.fit, X, y)
huge = 2 ** (n_bits - 1) - 1 clf = DecisionTreeClassifier(splitter='best', max_leaf_nodes=huge) assert_raises(MemoryError, clf.fit, X, y)
if dataset in ["digits", "boston"]: n_samples = X.shape[0] // 5 X = X[:n_samples] X_sparse = X_sparse[:n_samples] y = y[:n_samples]
d = TreeEstimator(random_state=0, max_depth=max_depth).fit(X, y) s = TreeEstimator(random_state=0, max_depth=max_depth).fit(X_sparse, y)
for tree, dataset in product(REG_TREES, ["boston", "reg_small"]): if tree in SPARSE_TREES: yield (check_sparse_input, tree, dataset, 2)
n_samples = n_features samples = np.arange(n_samples)
X_sparse_test = X_sparse_test.copy()
assert_greater((X_sparse.data == 0.).sum(), 0) assert_greater((X_sparse_test.data == 0.).sum(), 0)
d = TreeEstimator(random_state=0, max_depth=max_depth).fit(X, y) s = TreeEstimator(random_state=0, max_depth=max_depth).fit(X_sparse, y)
est = TreeEstimator(random_state=0) est.fit(X, y, sample_weight=sample_weight) assert_equal(est.tree_.max_depth, 1)
leaves = est.apply(X) leave_indicator = [node_indicator[i, j] for i, j in enumerate(leaves)] assert_array_almost_equal(leave_indicator, np.ones(shape=n_samples))
all_leaves = est.tree_.children_left == TREE_LEAF assert_array_almost_equal(np.dot(node_indicator, all_leaves), np.ones(shape=n_samples))
max_depth = node_indicator.sum(axis=1).max() assert_less_equal(est.tree_.max_depth, max_depth)
for name in ALL_TREES: yield (check_no_sparse_y_support, name)
from abc import ABCMeta, abstractmethod
self.kernel = kernel self.gamma = gamma self.n_neighbors = n_neighbors
self.alpha = alpha
graph_matrix = self._build_graph()
classes = np.unique(y) classes = (classes[classes != -1]) self.classes_ = classes
self.label_distributions_ = np.zeros((n_samples, n_classes)) for label in classes: self.label_distributions_[y == label, classes == label] = 1
self.label_distributions_ = np.multiply( clamp_weights, self.label_distributions_) + y_static remaining_iter -= 1
transduction = self.classes_[np.argmax(self.label_distributions_, axis=1)] self.transduction_ = transduction.ravel() self.n_iter_ = self.max_iter - remaining_iter return self
super(LabelSpreading, self).__init__(kernel=kernel, gamma=gamma, n_neighbors=n_neighbors, alpha=alpha, max_iter=max_iter, tol=tol, n_jobs=n_jobs)
for i, file_path in enumerate(file_paths): if i % 1000 == 0: logger.info("Loading face #%05d / %05d", i + 1, n_faces)
img = imread(file_path) if img.ndim is 0: raise RuntimeError("Failed to read the image file %s, " "Please make sure that libjpeg is installed" % file_path)
face = face.mean(axis=2)
m = Memory(cachedir=lfw_home, compress=6, verbose=0) load_func = m.cache(_fetch_lfw_people)
faces, target, target_names = load_func( data_folder_path, resize=resize, min_faces_per_person=min_faces_per_person, color=color, slice_=slice_)
return Bunch(data=faces.reshape(len(faces), -1), images=faces, target=target, target_names=target_names, DESCR="LFW faces dataset")
m = Memory(cachedir=lfw_home, compress=6, verbose=0) load_func = m.cache(_fetch_lfw_pairs)
pairs, target, target_names = load_func( index_file_path, data_folder_path, resize=resize, color=color, slice_=slice_)
return Bunch(data=pairs.reshape(len(pairs), -1), pairs=pairs, target=target, target_names=target_names, DESCR="'%s' segment of the LFW pairs dataset" % subset)
random_state = check_random_state(random_state) r = random_state.randint(0, n_samples_abnormal, 3377) abnormal_samples = abnormal_samples[r] abnormal_targets = abnormal_targets[r]
s = data[:, 11] == 1 data = np.c_[data[s, :11], data[s, 12:]] target = target[s]
dir_suffix = "-py3"
dir_suffix = ""
permutation = _find_permutation(sample_id_bis, sample_id) y = y[permutation, :]
categories = np.empty(N_CATEGORIES, dtype=object) for k in category_names.keys(): categories[category_names[k]] = k
order = np.argsort(categories) categories = categories[order] y = sp.csr_matrix(y[:, order])
from urllib2 import urlopen PY2 = True
from urllib.request import urlopen PY2 = False
names = F.readline().strip().split(',')
names = F.readline().decode('ascii').strip().split(',')
xgrid = np.arange(xmin, xmax, batch.grid_size) ygrid = np.arange(ymin, ymax, batch.grid_size)
extra_params = dict(x_left_lower_corner=-94.8, Nx=1212, y_left_lower_corner=-56.05, Ny=1592, grid_size=0.05) dtype = np.int16
from urllib2 import HTTPError from urllib2 import quote from urllib2 import urlopen
from urllib.error import HTTPError from urllib.parse import quote from urllib.request import urlopen
dataname = mldata_filename(dataname)
data_home = get_data_home(data_home=data_home) data_home = join(data_home, 'mldata') if not exists(data_home): os.makedirs(data_home)
with open(filename, 'rb') as matlab_file: matlab_dict = io.loadmat(matlab_file, struct_as_record=True)
col_names = [str(descr[0]) for descr in matlab_dict['mldata_descr_ordering'][0]]
if isinstance(target_name, numbers.Integral): target_name = col_names[target_name] if isinstance(data_name, numbers.Integral): data_name = col_names[data_name]
if len(col_names) == 1: data_name = col_names[0] dataset['data'] = matlab_dict[data_name] else: for name in col_names: dataset[name] = matlab_dict[name]
logger.warning("Download was incomplete, downloading again.") os.remove(archive_path)
data_lst = np.array(data.data, dtype=object) data_lst = data_lst[indices] data.data = data_lst.tolist()
data_train = fetch_20newsgroups(data_home=data_home, subset='train', categories=None, shuffle=True, random_state=12, remove=remove)
X_train = X_train.astype(np.float64) X_test = X_test.astype(np.float64) normalize(X_train, copy=False) normalize(X_test, copy=False)
else: with closing(_gen_open(f)) as f: actual_dtype, data, ind, indptr, labels, query = \ _load_svmlight_file(f, dtype, multilabel, zero_based, query_id)
if not multilabel: labels = frombuffer_empty(labels, np.float64) data = frombuffer_empty(data, actual_dtype) indices = frombuffer_empty(ind, np.intc)
import urllib2 urlopen = urllib2.urlopen
import urllib.request urlopen = urllib.request.urlopen
MODULE_DOCS = __doc__
filenames = np.array(filenames) target = np.array(target)
feature_names=feature_names[:-1], DESCR=descr_text)
X = np.zeros((n_samples, n_features)) y = np.zeros(n_samples, dtype=np.int)
X[:, :n_informative] = generator.randn(n_samples, n_informative)
stop = 0 for k, centroid in enumerate(centroids): start, stop = stop, stop + n_samples_per_cluster[k]
if n_useless > 0: X[:, -n_useless:] = generator.randn(n_samples, n_useless)
if flip_y >= 0.0: flip_mask = generator.rand(n_samples) < flip_y y[flip_mask] = generator.randint(n_classes, size=flip_mask.sum())
if shift is None: shift = (2 * generator.rand(n_features) - 1) * class_sep X += shift
X, y = util_shuffle(X, y, random_state=generator)
indices = np.arange(n_features) generator.shuffle(indices) X[:, :] = X[:, indices]
y_size = n_classes + 1 while (not allow_unlabeled and y_size == 0) or y_size > n_classes: y_size = generator.poisson(n_labels)
n_words = 0 while n_words == 0: n_words = generator.poisson(length)
if len(y) == 0: words = generator.randint(n_features, size=n_words) return words, y
X = generator.randn(n_samples, n_features)
X = make_low_rank_matrix(n_samples=n_samples, n_features=n_features, effective_rank=effective_rank, tail_strength=tail_strength, random_state=generator)
ground_truth = np.zeros((n_features, n_targets)) ground_truth[:n_informative, :] = 100 * generator.rand(n_informative, n_targets)
if noise > 0.0: y += generator.normal(scale=noise, size=y.shape)
if shuffle: X, y = util_shuffle(X, y, random_state=generator)
singular_ind = np.arange(n, dtype=np.float64)
D = generator.randn(n_features, n_components) D /= np.sqrt(np.sum((D ** 2), axis=0))
Y = np.dot(D, X)
permutation = random_state.permutation(dim) aux = aux[permutation].T[permutation] chol += aux prec = np.dot(chol.T, chol)
d = np.diag(prec).reshape(1, prec.shape[0]) d = 1. / np.sqrt(d)
X = generator.multivariate_normal(mean, cov * np.identity(n_features), (n_samples,))
idx = np.argsort(np.sum((X - mean[np.newaxis, :]) ** 2, axis=1)) X = X[idx, :]
step = n_samples // n_classes
from urllib2 import urlopen
from urllib.request import urlopen
MODULE_DOCS = __doc__
columns_index = [8, 7, 2, 3, 4, 5, 6, 1, 0] cal_housing = cal_housing[:, columns_index] joblib.dump(cal_housing, filepath, compress=6)
data[:, 2] /= data[:, 5]
data[:, 3] /= data[:, 5]
data[:, 5] = data[:, 4] / data[:, 5]
target = target / 100000.0
data_home = get_data_home(data_home=DATA_HOME) assert_equal(data_home, DATA_HOME) assert_true(os.path.exists(data_home))
clear_data_home(data_home=data_home) assert_false(os.path.exists(data_home))
data_home = get_data_home(data_home=DATA_HOME) assert_true(os.path.exists(data_home))
class_sep = 1e6 make = partial(make_classification, class_sep=class_sep, n_redundant=0, n_repeated=0, flip_y=0, shift=0, scale=1, shuffle=False)
signs = np.sign(X) signs = signs.view(dtype='|S{0}'.format(signs.strides[0])) unique_signs, cluster_index = np.unique(signs, return_inverse=True)
X2, Y2, p_c, p_w_c = make_multilabel_classification( n_samples=25, n_features=20, n_classes=3, random_state=0, allow_unlabeled=allow_unlabeled, return_distributions=True)
assert_almost_equal(np.std(y - np.dot(X, c)), 1.0, decimal=1)
assert_almost_equal(np.std(y - np.dot(X, c)), 1.0, decimal=1)
assert_equal(len(data2cats.filenames), len(data2cats.target)) assert_equal(len(data2cats.filenames), len(data2cats.data))
raise SkipTest("Test too slow.")
global tmpdir tmpdir = tempfile.mkdtemp() os.makedirs(os.path.join(tmpdir, 'mldata'))
if tmpdir is not None: shutil.rmtree(tmpdir)
x = sp.arange(6).reshape(2, 3) datasets.mldata.urlopen = mock_mldata_urlopen({dataname: {'x': x}})
dset = fetch_mldata(dataname, transpose_data=False, data_home=tmpdir) assert_equal(dset.data.shape, (3, 2))
assert_true(sp.issparse(X1)) assert_true(sp.issparse(Y1)) assert_equal(60915113, X1.data.size) assert_equal(2606875, Y1.data.size)
data2 = fetch_rcv1(shuffle=True, subset='train', random_state=77, download_if_missing=False) X2, Y2 = data2.data, data2.target s2 = data2.sample_id
assert_array_equal(np.sort(s1[:23149]), np.sort(s2))
some_sample_ids = (2286, 3274, 14042) for sample_id in some_sample_ids: idx1 = s1.tolist().index(sample_id) idx2 = s2.tolist().index(sample_id)
assert_equal(X.indptr.shape[0], 7) assert_equal(X.shape[0], 6) assert_equal(X.shape[1], 21) assert_equal(y.shape[0], 6)
X[0, 2] *= 2 assert_equal(X[0, 2], 5)
assert_array_equal(y, [1, 2, 3, 4, 1, 2])
X1, y1 = load_svmlight_file(datafile)
assert_equal(X.indptr.shape[0], 7) assert_equal(X.shape[0], 6) assert_equal(X.shape[1], 22)
assert_raises(ValueError, load_svmlight_file, datafile, n_features=20)
os.remove(tmp.name)
os.remove(tmp.name)
load_svmlight_file(.42)
X_sliced = X_sparse[np.arange(X_sparse.shape[0])] y_sliced = y_sparse[np.arange(y_sparse.shape[0])]
y = y.T
assert_array_almost_equal( X_dense.astype(dtype), X2_dense, 4) assert_array_almost_equal( y_dense.astype(dtype), y2, 4)
assert_array_almost_equal( X_dense.astype(dtype), X2_dense, 15) assert_array_almost_equal( y_dense.astype(dtype), y2, 15)
utf8_comment = b("It is true that\n\xc2\xbd\xc2\xb2 = \xc2\xbc") f = BytesIO() assert_raises(UnicodeDecodeError, dump_svmlight_file, X, y, f, comment=utf8_comment)
assert_equal(lfw_people.images.shape, (10, 62, 47)) assert_equal(lfw_people.data.shape, (10, 2914))
assert_array_equal(lfw_people.target, [2, 0, 1, 0, 2, 0, 2, 1, 1, 2])
expected_classes = ['Abdelatif Smith', 'Abhati Kepler', 'Onur Lopez'] assert_array_equal(lfw_people.target_names, expected_classes)
lfw_people = fetch_lfw_people(data_home=SCIKIT_LEARN_DATA, resize=None, slice_=None, color=True, download_if_missing=False) assert_equal(lfw_people.images.shape, (17, 250, 250, 3))
assert_equal(lfw_pairs_train.pairs.shape, (10, 2, 62, 47))
assert_array_equal(lfw_pairs_train.target, [1, 1, 1, 1, 1, 0, 0, 0, 0, 0])
expected_classes = ['Different persons', 'Same person'] assert_array_equal(lfw_pairs_train.target_names, expected_classes)
lfw_pairs_train = fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA, resize=None, slice_=None, color=True, download_if_missing=False) assert_equal(lfw_pairs_train.pairs.shape, (10, 2, 250, 250, 3))
assert_array_equal(lfw_pairs_train.target, [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]) assert_array_equal(lfw_pairs_train.target_names, expected_classes)
lines = '\n'.join(l.rstrip(' ') for l in lines.split('\n')) return lines
init = getattr(cls.__init__, 'deprecated_original', cls.__init__) if init is object.__init__: return []
return self
self.fit(X) return self.labels_
if y is None: return self.fit(X, **fit_params).transform(X) else: return self.fit(X, y, **fit_params).transform(X)
y_type = y_type.pop()
if (y_type not in ["binary", "multiclass", "multilabel-indicator"]): raise ValueError("{0} is not supported".format(y_type))
ind = np.logical_and(y_pred < n_labels, y_true < n_labels) y_pred = y_pred[ind] y_true = y_true[ind] sample_weight = sample_weight[ind]
score[pred_or_true == 0.0] = 1.0
result[mask] = 0.0
axis0 = 'sample' axis1 = 'label' if average == 'samples': axis0, axis1 = axis1, axis0
return (0., 0., 0., 0)
true_and_pred = y_true.multiply(y_pred) tp_sum = count_nonzero(true_and_pred, axis=sum_axis, sample_weight=sample_weight) pred_sum = count_nonzero(y_pred, axis=sum_axis, sample_weight=sample_weight) true_sum = count_nonzero(y_true, axis=sum_axis, sample_weight=sample_weight)
tp = y_true == y_pred tp_bins = y_true[tp] if sample_weight is not None: tp_bins_weights = np.asarray(sample_weight)[tp] else: tp_bins_weights = None
true_sum = pred_sum = tp_sum = np.zeros(len(labels))
indices = np.searchsorted(sorted_labels, labels[:n_labels]) tp_sum = tp_sum[indices] true_sum = true_sum[indices] pred_sum = pred_sum[indices]
Y = np.clip(y_pred, eps, 1 - eps)
if not isinstance(Y, np.ndarray): raise ValueError("y_pred should be an array of floats.")
if Y.ndim == 1: Y = Y[:, np.newaxis] if Y.shape[1] == 1: Y = np.append(1 - Y, Y, axis=1)
Y /= Y.sum(axis=1)[:, np.newaxis] loss = -(T * np.log(Y)).sum(axis=1)
pred_decision = column_or_1d(pred_decision) pred_decision = np.ravel(pred_decision)
losses[losses <= 0] = 0 return np.average(losses, weights=sample_weight)
return comb(n, 2, exact=1)
sum_comb_c = sum(comb2(n_c) for n_c in contingency.sum(axis=1)) sum_comb_k = sum(comb2(n_k) for n_k in contingency.sum(axis=0))
return -np.sum((pi / pi_sum) * (np.log(pi) - log(pi_sum)))
assert_almost_equal(s, 2.0/3.0)
dataset = datasets.load_iris() X = dataset.data
n_clusters_range = [2, 10, 50, 90] n_samples = 100 n_runs = 10
assert expected_mutual_information(np.array([[70000]]), 70000) <= 1
intra_clust_dists = np.ones(distances.shape[0], dtype=distances.dtype)
inter_clust_dists = np.inf * intra_clust_dists
mask = labels == curr_label current_distances = distances[mask]
n_samples_curr_lab = np.sum(mask) - 1 if n_samples_curr_lab != 0: intra_clust_dists[mask] = np.sum( current_distances[:, mask], axis=1) / n_samples_curr_lab
for other_label in unique_labels: if other_label != curr_label: other_mask = labels == other_label other_distances = np.mean( current_distances[:, other_mask], axis=1) inter_clust_dists[mask] = np.minimum( inter_clust_dists[mask], other_distances)
multioutput = None
multioutput = None
return output_scores
avg_weights = None
if isinstance(y_pred, list): y_pred = np.vstack(p for p in y_pred).T
r2_scorer = make_scorer(r2_score) mean_squared_error_scorer = make_scorer(mean_squared_error, greater_is_better=False) mean_absolute_error_scorer = make_scorer(mean_absolute_error, greater_is_better=False) median_absolute_error_scorer = make_scorer(median_absolute_error, greater_is_better=False)
accuracy_scorer = make_scorer(accuracy_score) f1_scorer = make_scorer(f1_score)
roc_auc_scorer = make_scorer(roc_auc_score, greater_is_better=True, needs_threshold=True) average_precision_scorer = make_scorer(average_precision_score, needs_threshold=True) precision_scorer = make_scorer(precision_score) recall_scorer = make_scorer(recall_score)
log_loss_scorer = make_scorer(log_loss, greater_is_better=False, needs_proba=True)
adjusted_rand_scorer = make_scorer(adjusted_rand_score)
order = np.lexsort((y, x)) x, y = x[order], y[order]
area = area.dtype.type(area)
y_true = (y_true == pos_label)
desc_score_indices = np.argsort(y_score, kind="mergesort")[::-1] y_score = y_score[desc_score_indices] y_true = y_true[desc_score_indices] if sample_weight is not None: weight = sample_weight[desc_score_indices] else: weight = 1.
distinct_value_indices = np.where(np.logical_not(isclose( np.diff(y_score), 0)))[0] threshold_idxs = np.r_[distinct_value_indices, y_true.size - 1]
last_ind = tps.searchsorted(tps[-1]) sl = slice(last_ind, None, -1) return np.r_[precision[sl], 1], np.r_[recall[sl], 0], thresholds[sl]
tps = np.r_[0, tps] fps = np.r_[0, fps] thresholds = np.r_[thresholds[0] + 1, thresholds]
0.416...
out += 1. continue
loss[i] = np.dot(true_at_reversed_rank.cumsum(), false_at_reversed_rank)
loss[np.logical_or(n_positives == 0, n_positives == n_labels)] = 0.
average_weight = score_weight score_weight = None not_average_axis = 0
if average is not None: return np.average(score, weights=average_weight) else: return score
distances.flat[::distances.shape[0] + 1] = 0.0
indices = np.empty(X.shape[0], dtype=np.intp) values = np.empty(X.shape[0]) values.fill(np.infty)
min_indices = d_chunk.argmin(axis=1) min_values = d_chunk[np.arange(chunk_x.stop - chunk_x.start), min_indices]
S = cosine_similarity(X, Y) S *= -1 S += 1 return S
PAIRWISE_DISTANCE_FUNCTIONS = { 'cityblock': manhattan_distances, 'cosine': cosine_distances, 'euclidean': euclidean_distances, 'l2': euclidean_distances, 'l1': manhattan_distances, 'manhattan': manhattan_distances,
return func(X, Y, **kwds)
out = out + out.T
for i in range(X.shape[0]): x = X[i] out[i, i] = metric(x, x, **kwds)
from ..gaussian_process.kernels import Kernel as GPKernel
assert_raises(ValueError, pairwise_distances, X, Y, metric="blah")
S = func(np.array([[1]], dtype='int'), metric='precomputed') assert_equal('f', S.dtype.kind)
S = func([[1]], metric='precomputed') assert_true(isinstance(S, np.ndarray))
if make_data is csr_matrix: assert_raises(type(exc), func, X, metric=metric, n_jobs=2, **kwds) continue else: raise
assert_equal(pairwise_distances([[1]], metric=lambda x, y: 5)[0, 0], 5)
K = rbf_kernel(np.atleast_2d(x), np.atleast_2d(y), **kwds) return K
K1 = pairwise_kernels(X, Y=X, metric=metric, **kwds) K2 = rbf_kernel(X, Y=X, **kwds) assert_array_almost_equal(K1, K2)
Y = rng.random_sample((3, 4)) assert_raises(ValueError, paired_distances, X, Y)
X = [[0], [1]] Y = [[-1], [2]]
D, E = pairwise_distances_argmin_min(X, Y, metric=minkowski, metric_kwargs={"p": 2}) assert_array_almost_equal(D, [0, 1]) assert_array_almost_equal(E, [1., 1.])
rng = np.random.RandomState(0) X = rng.randn(97, 149) Y = rng.randn(111, 149)
X = [[0]] Y = [[1], [2]] D = euclidean_distances(X, Y) assert_array_almost_equal(D, [[1., 2.]])
X = [[0], [0]] Y = [[1], [2]] D = paired_euclidean_distances(X, Y) assert_array_almost_equal(D, [1., 2.])
X = [[0], [0]] Y = [[1], [2]] D = paired_manhattan_distances(X, Y) assert_array_almost_equal(D, [1., 2.])
assert_raises(ValueError, chi2_kernel, [[0, -1]]) assert_raises(ValueError, chi2_kernel, [[0, -1]], [[-1, -1]]) assert_raises(ValueError, chi2_kernel, [[0, 1]], [[-1, -1]])
assert_raises(ValueError, chi2_kernel, [[0, 1]], [[.2, .2, .6]])
assert_raises(ValueError, chi2_kernel, csr_matrix(X), csr_matrix(Y)) assert_raises(ValueError, additive_chi2_kernel, csr_matrix(X), csr_matrix(Y))
assert_array_almost_equal(K.flat[::6], [linalg.norm(x) ** 2 for x in X])
assert_array_almost_equal(K.flat[::6], np.ones(5))
assert_array_almost_equal(np.diag(K), np.ones(5))
assert_true(np.all(K > 0)) assert_true(np.all(K - np.diag(np.diag(K)) < 1))
s = X.shape if len(s) > 1: return tuple(tuplify(row) for row in X) else: return tuple(r for r in X)
XA_checked, XB_checked = check_pairwise_arrays(XA, XB) assert_equal(XA_checked.dtype, np.float32) assert_equal(XB_checked.dtype, np.float32)
XA_checked, XB_checked = check_pairwise_arrays(XA.astype(np.float), XB) assert_equal(XA_checked.dtype, np.float) assert_equal(XB_checked.dtype, np.float)
XA_checked, XB_checked = check_pairwise_arrays(XA, XB.astype(np.float)) assert_equal(XA_checked.dtype, np.float) assert_equal(XB_checked.dtype, np.float)
METRIC_UNDEFINED_BINARY = [ "samples_f0.5_score", "samples_f1_score", "samples_f2_score", "samples_precision_score", "samples_recall_score", "coverage_error",
METRIC_UNDEFINED_MULTICLASS = [ "brier_score_loss", "matthews_corrcoef_score", ]
METRIC_UNDEFINED_BINARY_MULTICLASS = set(METRIC_UNDEFINED_BINARY).union( set(METRIC_UNDEFINED_MULTICLASS))
METRICS_WITH_AVERAGING = [ "precision_score", "recall_score", "f1_score", "f2_score", "f0.5_score" ]
THRESHOLDED_METRICS_WITH_AVERAGING = [ "roc_auc_score", "average_precision_score", ]
METRICS_WITH_POS_LABEL = [ "roc_curve",
"weighted_f0.5_score", "weighted_f1_score", "weighted_f2_score", "weighted_precision_score", "weighted_recall_score",
METRICS_WITH_NORMALIZE_OPTION = [ "accuracy_score", "jaccard_similarity_score", "zero_one_loss", ]
THRESHOLDED_MULTILABEL_METRICS = [ "log_loss", "unnormalized_log_loss",
MULTILABELS_METRICS = [ "accuracy_score", "unnormalized_accuracy_score", "hamming_loss", "jaccard_similarity_score", "unnormalized_jaccard_similarity_score", "zero_one_loss", "unnormalized_zero_one_loss",
MULTIOUTPUT_METRICS = [ "mean_absolute_error", "mean_squared_error", "r2_score", "explained_variance_score" ]
SYMMETRIC_METRICS = [ "accuracy_score", "unnormalized_accuracy_score", "hamming_loss", "jaccard_similarity_score", "unnormalized_jaccard_similarity_score", "zero_one_loss", "unnormalized_zero_one_loss",
NOT_SYMMETRIC_METRICS = [ "explained_variance_score", "r2_score", "confusion_matrix",
METRICS_WITHOUT_SAMPLE_WEIGHT = [ "cohen_kappa_score",
"median_absolute_error",
random_state = check_random_state(0) y_true = random_state.randint(0, 2, size=(20, )) y_pred = random_state.randint(0, 2, size=(20, ))
assert_equal(set(SYMMETRIC_METRICS).union( NOT_SYMMETRIC_METRICS, THRESHOLDED_METRICS, METRIC_UNDEFINED_BINARY_MULTICLASS), set(ALL_METRICS))
for name in SYMMETRIC_METRICS: metric = ALL_METRICS[name] assert_almost_equal(metric(y_true, y_pred), metric(y_pred, y_true), err_msg="%s is not symmetric" % name)
assert_almost_equal(metric(y1_1d, y2_list), measure, err_msg="%s is not representation invariant " "with mix np-array-1d and list" % name)
if (name not in (MULTIOUTPUT_METRICS + THRESHOLDED_MULTILABEL_METRICS + MULTILABELS_METRICS)): assert_raises(ValueError, metric, y1_row, y2_row)
random_state = check_random_state(0) y1 = random_state.randint(0, 2, size=(20, )) y2 = random_state.randint(0, 2, size=(20, ))
metric_str = metric if name in METRICS_WITH_POS_LABEL: metric_str = partial(metric_str, pos_label=pos_label_str)
metric_str = metric if name in METRICS_WITH_POS_LABEL: metric_str = partial(metric_str, pos_label=pos_label_str)
metric = ALL_METRICS[name]
for i, j in product([0, 1], repeat=2): metric([i], [j])
continue
n_classes = 4 n_samples = 50
y1 += [0]*n_classes y2 += [0]*n_classes
if isinstance(metric, partial): metric.__module__ = 'tmp' metric.__name__ = name
assert_almost_equal(metric(y1_sparse_indicator, y2_sparse_indicator), measure, err_msg="%s failed representation invariance " "between dense and sparse indicator " "formats." % name)
random_state = check_random_state(0) y_true = random_state.randint(0, 2, size=(n_samples, )) y_pred = random_state.randint(0, 2, size=(n_samples, ))
n_classes = 4 n_samples = 100
_, y_true = make_multilabel_classification(n_features=1, n_classes=n_classes, random_state=0, allow_unlabeled=True, n_samples=n_samples) _, y_pred = make_multilabel_classification(n_features=1, n_classes=n_classes, random_state=1, allow_unlabeled=True, n_samples=n_samples)
y_true += [0]*n_classes y_pred += [0]*n_classes
label_measure = metric(y_true, y_pred, average=None) assert_array_almost_equal(label_measure, [metric(y_true_binarize[:, i], y_pred_binarize[:, i]) for i in range(n_classes)])
micro_measure = metric(y_true, y_pred, average="micro") assert_almost_equal(micro_measure, metric(y_true_binarize.ravel(), y_pred_binarize.ravel()))
macro_measure = metric(y_true, y_pred, average="macro") assert_almost_equal(macro_measure, np.mean(label_measure))
weights = np.sum(y_true_binarize, axis=0, dtype=int)
binary_metric = (lambda y_true, y_score, average="macro": _average_binary_score( precision_score, y_true, y_score, average)) _check_averaging(binary_metric, y_true, y_pred, y_true_binarize, y_pred_binarize, is_multilabel=True)
for scaling in [2, 0.3]: assert_almost_equal( weighted_score, metric(y1, y2, sample_weight=sample_weight * scaling), err_msg="%s sample_weight is not invariant " "under scaling" % name)
assert_raises(Exception, metric, y1, y2, sample_weight=np.hstack([sample_weight, sample_weight]))
X_mm, y_mm, y_ml_mm, ESTIMATORS = None, None, None, None shutil.rmtree(TEMP_FOLDER)
for name, scorer in SCORERS.items(): repr(scorer)
scores = cross_val_score(EstimatorWithFit(), [[1], [2], [3]], [1, 0, 1], scoring=DummyScorer()) assert_array_equal(scores, 1)
f = lambda *args: 0 assert_raises(ValueError, make_scorer, f, needs_threshold=True, needs_proba=True)
scorer = make_scorer(fbeta_score, beta=2) score1 = scorer(clf, X_test, y_test) score2 = fbeta_score(y_test, clf.predict(X_test), beta=2) assert_almost_equal(score1, score2)
unpickled_scorer = pickle.loads(pickle.dumps(scorer)) score3 = unpickled_scorer(clf, X_test, y_test) assert_almost_equal(score1, score3)
repr(fbeta_score)
X, y = make_multilabel_classification(allow_unlabeled=False, random_state=0) X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
estimator = _make_estimators(X_train, y_train, y_ml_train)
for name in SCORERS.keys(): yield check_scorer_memmap, name
dataset = datasets.load_iris()
X, y = X[y < 2], y[y < 2]
rng = np.random.RandomState(0) X = np.c_[X, rng.randn(n_samples, 200 * n_features)]
clf = svm.SVC(kernel='linear', probability=True, random_state=0) probas_pred = clf.fit(X[:half], y[:half]).predict_proba(X[half:])
probas_pred = probas_pred[:, 1]
y_true, _, probas_pred = make_prediction(binary=True) expected_auc = _auc(y_true, probas_pred)
y_true, _, probas_pred = make_prediction(binary=True) fpr, tpr, thresholds = roc_curve(y_true, probas_pred)
assert_array_almost_equal(tpr, tpr_correct, decimal=2) assert_equal(fpr.shape, tpr.shape) assert_equal(fpr.shape, thresholds.shape)
dataset = datasets.load_digits() X = dataset['data'] y = dataset['target']
clf = ensemble.RandomForestClassifier(n_estimators=100, random_state=0)
train, test = slice(None, None, 2), slice(1, None, 2) probas_pred = clf.fit(X[train], y[train]).predict_proba(X[test])
fpr, tpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=False) assert_equal(thresholds.size, np.unique(np.round(thresholds, 2)).size)
y_true, _, probas_pred = make_prediction(binary=False)
y_true, _, probas_pred = make_prediction(binary=True)
y_true, pred, probas_pred = make_prediction(binary=True)
assert_raises(ValueError, auc, [0.0, 0.5, 1.0], [0.1, 0.2])
assert_raises(ValueError, auc, [0.0], [0.1])
assert_raises(ValueError, auc, [1.0, 0.0, 0.5], [0.0, 0.0, 0.0])
y_true = rng.randint(0, 3, size=10) assert_raise_message(ValueError, "multiclass format is not supported", roc_auc_score, y_true, y_pred)
y_true[np.where(y_true == 0)] = -1 y_true_copy = y_true.copy() _test_precision_recall_curve(y_true, probas_pred) assert_array_equal(y_true_copy, y_true)
assert_raises(ValueError, precision_recall_curve, [0, 1, 2], [[0.0], [1.0], [1.0]])
y_true, _, probas_pred = make_prediction(binary=True)
y_true = np.zeros((1, n_labels)) assert_equal(lrap_score(y_true, y_score), 1.) assert_equal(lrap_score(y_true, y_score_ties), 1.)
y_true = np.ones((1, n_labels)) assert_equal(lrap_score(y_true, y_score), 1.) assert_equal(lrap_score(y_true, y_score_ties), 1.)
assert_almost_equal(lrap_score([[1], [0], [1], [0]], [[0.5], [0.5], [0.5], [0.5]]), 1.)
for n_labels in range(2, 10): y_score = np.ones((1, n_labels))
for n_labels in range(2, 10): y_score = n_labels - (np.arange(n_labels).reshape((1, n_labels)) + 1)
unique_rank, inv_rank = np.unique(y_score[i], return_inverse=True) n_ranks = unique_rank.size rank = n_ranks - inv_rank
corr_rank = np.bincount(rank, minlength=n_ranks + 1).cumsum() rank = corr_rank[rank]
n_ranked_above = sum(rank[r] <= rank[label] for r in relevant)
score[i] += n_ranked_above / rank[label]
y_score = sparse_random_matrix(n_components=y_true.shape[0], n_features=y_true.shape[1], random_state=random_state)
dataset = datasets.load_iris()
X, y = X[y < 2], y[y < 2]
rng = np.random.RandomState(0) X = np.c_[X, rng.randn(n_samples, 200 * n_features)]
clf = svm.SVC(kernel='linear', probability=True, random_state=0) probas_pred = clf.fit(X[:half], y[:half]).predict_proba(X[half:])
probas_pred = probas_pred[:, 1]
y1 = np.array([[0, 1, 1], [1, 0, 1]]) y2 = np.array([[0, 0, 1], [1, 0, 1]])
y_true, y_pred, _ = make_prediction(binary=True)
for average in ['macro', 'weighted', 'micro']: assert_not_equal(recall_13(average=average), recall_all(average=average))
rng = check_random_state(404) y_pred = rng.rand(10)
y_true = rng.randint(0, 3, size=10) assert_raise_message(ValueError, "multiclass format is not supported", average_precision_score, y_true, y_pred)
assert_raises(ValueError, precision_recall_fscore_support, y_true, y_pred, beta=0.0)
assert_raises(ValueError, precision_recall_fscore_support, y_true, y_pred, pos_label=2, average='macro')
assert_raises(ValueError, precision_recall_fscore_support, [0, 1, 2], [1, 2, 0], average='mega')
y_true, y_pred, _ = make_prediction(binary=True)
y1 = np.append(y1, [2] * 4) y2 = np.append(y2, [2] * 4) assert_equal(cohen_kappa_score(y1, y2, labels=[0, 1]), kappa)
assert_almost_equal(matthews_corrcoef(y_true, y_true), 1.0)
y_true_inv = ["b" if i == "a" else "a" for i in y_true]
mcc = assert_warns_message(RuntimeWarning, 'invalid value encountered', matthews_corrcoef, [0, 0, 0, 0], [0, 0, 0, 0])
assert_almost_equal(mcc, 0.)
mcc = assert_warns_message(RuntimeWarning, 'invalid value encountered', matthews_corrcoef, y_true, rng.randint(-100, 100) * np.ones(20, dtype=int))
assert_almost_equal(mcc, 0.)
mask = [1] * 10 + [0] * 10 assert_raises(AssertionError, assert_almost_equal, matthews_corrcoef(y_1, y_2, sample_weight=mask), 0.)
y_true, y_pred, _ = make_prediction(binary=False)
ps = precision_score(y_true, y_pred, pos_label=1, average='micro') assert_array_almost_equal(ps, 0.53, 2)
p, r, f, s = precision_recall_fscore_support(y_true, y_pred, pos_label=None, average='weighted')
y_true, y_pred, _ = make_prediction(binary=False)
cm = confusion_matrix(y_true, y_pred) assert_array_equal(cm, [[19, 4, 1], [4, 3, 24], [0, 2, 18]])
y_true, y_pred, _ = make_prediction(binary=False)
cm = confusion_matrix(y_true, y_pred, labels=[0, 1]) assert_array_equal(cm, [[19, 4], [4, 3]])
cm = confusion_matrix(y_true, y_pred, labels=[2, 1]) assert_array_equal(cm, [[18, 2], [24, 3]])
iris = datasets.load_iris() y_true, y_pred, _ = make_prediction(dataset=iris, binary=False)
iris = datasets.load_iris() y_true, y_pred, _ = make_prediction(dataset=iris, binary=False)
y1 = np.array([[0, 1, 1], [1, 0, 1]]) y2 = np.array([[0, 0, 1], [1, 0, 1]])
assert_equal(hamming_loss(y1[0], y2[0]), sp_hamming(y1[0], y2[0]))
y1 = np.array([[0, 1, 1], [1, 0, 1]]) y2 = np.array([[0, 0, 1], [1, 0, 1]])
y_true = [1, 2, 3, 3] y_pred = [1, 2, 3, 1] y_true_bin = [0, 1, 1] y_pred_bin = [0, 1, 0]
assert_no_warnings(metric, y_true_bin, y_pred_bin)
y_true *= 2 y_pred *= 2 loss = log_loss(y_true, y_pred, normalize=False) assert_almost_equal(loss, 0.6904911 * 6, decimal=6)
error = mean_absolute_error(y_true, y_pred) assert_almost_equal(error, (1. / 3 + 2. / 3 + 2. / 3) / 4.)
evecs /= np.apply_along_axis(np.linalg.norm, 0, evecs)
std = Xc.std(axis=0) std[std == 0] = 1. fac = 1. / (n_samples - n_classes)
X = np.sqrt(fac) * (Xc / std) U, S, V = linalg.svd(X, full_matrices=False)
scalings = (V[:rank] / std).T / S[:rank]
prob /= prob.sum(axis=1).reshape((prob.shape[0], -1)) return prob
if len(self.classes_) == 2: return dec_func[:, 1] - dec_func[:, 0] return dec_func
likelihood = np.exp(values - values.max(axis=1)[:, np.newaxis]) return likelihood / likelihood.sum(axis=1)[:, np.newaxis]
probas_ = self.predict_proba(X) return np.log(probas_)
kl_divergence = 2.0 * np.dot(P, np.log(P / Q))
if len(P.shape) == 2: P = squareform(P) kl_divergence = 2.0 * np.dot(P, np.log(P / Q))
degrees_of_freedom = max(self.n_components - 1.0, 1) n_samples = X.shape[0] k = min(n_samples - 1, int(3. * self.perplexity + 1))
neighbors_nn = np.argsort(distances, axis=1)[:, :k]
bt = BallTree(X) distances_nn, neighbors_nn = bt.query(X, k=k + 1) neighbors_nn = neighbors_nn[:, 1:]
X_embedded = 1e-4 * random_state.randn(n_samples, self.n_components)
opt_args['objective_error'] = objective_error opt_args['kwargs']['angle'] = self.angle opt_args['kwargs']['verbose'] = self.verbose
P *= self.early_exaggeration
self.n_iter_final = it
P /= self.early_exaggeration opt_args['n_iter'] = self.n_iter opt_args['it'] = it + 1 params, error, it = _gradient_descent(obj_func, params, **opt_args)
X = random_state.rand(n_samples * n_components) X = X.reshape((n_samples, n_components))
n_components = init.shape[1] if n_samples != init.shape[0]: raise ValueError("init matrix should be of shape (%d, %d)" % (n_samples, n_components)) X = init
dis = euclidean_distances(X)
dis_flat_w = dis_flat[sim_flat != 0]
stress = ((dis.ravel() - disparities.ravel()) ** 2).sum() / 2
graph = graph.tocsr()
n_connected_components, _ = connected_components(graph) return n_connected_components == 1
return _graph_connected_component(graph, 0).sum() == graph.shape[0]
if drop_first: n_components = n_components + 1
laplacian = _set_diag(laplacian, 1, norm_laplacian)
self.affinity_matrix_ = 0.5 * (self.affinity_matrix_ + self.affinity_matrix_.T) return self.affinity_matrix_
sim = np.array([[0, 5, 9, 4], [5, 0, 2, 2], [3, 2, 0, 1], [4, 2, 1, 0]])
sim = np.array([[0, 5, 9, 4], [5, 0, 2, 2], [4, 2, 1, 0]])
sim = np.array([[0, 5, 3, 4], [5, 0, 2, 2], [3, 2, 0, 1], [4, 2, 1, 0]])
N_per_side = 5 Npts = N_per_side ** 2 n_neighbors = Npts - 1
X = np.array(list(product(range(N_per_side), repeat=2)))
G = neighbors.kneighbors_graph(X, n_neighbors, mode='distance').toarray()
N_per_side = 5 Npts = N_per_side ** 2 n_neighbors = Npts - 1
X = np.array(list(product(range(N_per_side), repeat=2)))
rng = np.random.RandomState(0) noise = 0.1 * rng.randn(Npts, 1) X = np.concatenate((X, noise), 1)
G = neighbors.kneighbors_graph(X, n_neighbors, mode='distance').toarray()
G_iso = neighbors.kneighbors_graph(clf.embedding_, n_neighbors, mode='distance').toarray()
reconstruction_error = np.linalg.norm(K - K_iso) / Npts assert_almost_equal(reconstruction_error, clf.reconstruction_error())
X, y = datasets.samples_generator.make_s_curve(n_samples, random_state=0)
iso = manifold.Isomap(n_components, 2) X_iso = iso.fit_transform(X)
rng = np.random.RandomState(0) noise = noise_scale * rng.randn(*X.shape) X_iso2 = iso.transform(X + noise)
assert_less(np.sqrt(np.mean((X_iso - X_iso2) ** 2)), 2 * noise_scale)
def test_barycenter_kneighbors_graph(): X = np.array([[0, 1], [1.01, 1.], [2, 0]])
rng = np.random.RandomState(2)
noise = rng.randn(*X.shape) / 100 X_reembedded = clf.transform(X + noise) assert_less(linalg.norm(X_reembedded - clf.embedding_), tol)
def test_lle_init_parameters(): X = np.random.rand(5, 3)
class ObjectiveSmallGradient: def __init__(self): self.it = -1
for k in np.linspace(80, n_samples, 10): k = int(k)
random_state = check_random_state(0)
random_state = check_random_state(0)
X = random_state.randn(100, 2) assert_equal(trustworthiness(X, 5.0 + X / 10.0), 1.0)
tsne = TSNE(early_exaggeration=0.99) assert_raises_regexp(ValueError, "early_exaggeration .*", tsne.fit_transform, np.array([[0.0]]))
tsne = TSNE(n_iter=199) assert_raises_regexp(ValueError, "n_iter .*", tsne.fit_transform, np.array([[0.0]]))
tsne = TSNE(metric="precomputed") assert_raises_regexp(ValueError, ".* square distance matrix", tsne.fit_transform, np.array([[0.0], [1.0]]))
m = "'init' must be 'pca', 'random' or a NumPy array" assert_raises_regexp(ValueError, m, TSNE, init="not available")
random_state = check_random_state(0) tsne = TSNE(verbose=2) X = random_state.randn(5, 2)
random_state = check_random_state(0) tsne = TSNE(metric="chebyshev") X = random_state.randn(5, 2) tsne.fit_transform(X)
angle = 0.0 perplexity = 10 n_samples = 100 for n_components in [2, 3]: n_features = 5 degrees_of_freedom = float(n_components - 1.0)
Xs = []
Xs.append(np.array([[1, 0.0003817754041], [2, 0.0003817753750]], dtype=np.float32))
Xs.append(np.array([[0.0003817754041, 1.0], [0.0003817753750, 2.0]], dtype=np.float32))
assert_equal(_barnes_hut_tsne.test_index2offset(), 1) assert_equal(_barnes_hut_tsne.test_index_offset(), 1)
for i in range(len(group) - 1): connections.append((group[i], group[i + 1]))
component_2 = _graph_connected_component(affinity, p[stop - 1]) assert_equal(component_2.sum(), component_size) assert_array_equal(component_1, component_2)
affinity[0, n_sample + 1] = 1 affinity[n_sample + 1, 0] = 1 affinity.flat[::2 * n_sample + 1] = 0 affinity = 0.5 * (affinity + affinity.T)
try: from pyamg import smoothed_aggregation_solver except ImportError: raise SkipTest("pyamg not available.")
se = SpectralEmbedding(n_components=1, affinity="precomputed", random_state=np.random.RandomState(seed), eigen_solver="<unknown>") assert_raises(ValueError, se.fit, S)
se = SpectralEmbedding(n_components=1, affinity="<unknown>", random_state=np.random.RandomState(seed)) assert_raises(ValueError, se.fit, S)
laplacian, dd = graph_laplacian(sims, normed=False, return_diag=True) _, diffusion_map = eigh(laplacian) embedding_2 = diffusion_map.T[:n_components] * dd embedding_2 = _deterministic_vector_sign_flip(embedding_2).T
for i, A in enumerate(Z.transpose(0, 2, 1)):
if use_svd: U = svd(Gi, full_matrices=0)[0] else: Ci = np.dot(Gi, Gi.T) U = eigh(Ci)[1][:, ::-1]
V = np.zeros((N, n_neighbors, n_neighbors)) nev = min(d_in, n_neighbors) evals = np.zeros([N, nev])
use_svd = (n_neighbors > d_in)
reg = 1E-3 * evals.sum(1)
rho = evals[:, n_components:].sum(1) / evals[:, :n_components].sum(1) eta = np.median(rho)
M = np.zeros((N, N), dtype=np.float64) for i in range(N): s_i = s_range[i]
Vi = V[i, :, n_neighbors - s_i:] alpha_i = np.linalg.norm(Vi.sum(0)) / np.sqrt(s_i)
h = alpha_i * np.ones(s_i) - np.dot(Vi.T, np.ones(n_neighbors))
if use_svd: v = svd(Xi, full_matrices=True)[0] else: Ci = np.dot(Xi, Xi.T) v = eigh(Ci)[1][:, ::-1]
param_grid = [param_grid]
product = partial(reduce, operator.mul) return sum(product(len(v) for v in p.values()) if p else 1 for p in self.param_grid)
for sub_grid in self.param_grid: if not sub_grid: if ind == 0: return {} else: ind -= 1 continue
keys, values_lists = zip(*sorted(sub_grid.items())[::-1]) sizes = [len(v_list) for v_list in values_lists] total = np.product(sizes)
ind -= total
all_lists = np.all([not hasattr(v, "rvs") for v in self.param_distributions.values()]) rnd = check_random_state(self.random_state)
param_grid = ParameterGrid(self.param_distributions) grid_size = len(param_grid)
n_fits = len(out)
best = sorted(grid_scores, key=lambda x: x.mean_validation_score, reverse=True)[0] self.best_params_ = best.parameters self.best_score_ = best.mean_validation_score
pass
n_samples_per_label = np.bincount(labels)
indices = np.argsort(n_samples_per_label)[::-1] n_samples_per_label = n_samples_per_label[indices]
n_samples_per_fold = np.zeros(self.n_folds)
label_to_fold = np.zeros(len(unique_labels))
for label_index, weight in enumerate(n_samples_per_label): lightest_fold = np.argmin(n_samples_per_fold) n_samples_per_fold[lightest_fold] += weight label_to_fold[indices[label_index]] = lightest_fold
labels = np.array(labels, copy=True) unique_labels = np.unique(labels) for i in unique_labels: yield labels == i
permutation = rng.permutation(n_samples) ind_test = permutation[:n_test] ind_train = permutation[n_test:(n_test + n_train)] yield ind_train, ind_test
raise ValueError("Invalid value for test_size: %r" % test_size)
raise ValueError("Invalid value for train_size: %r" % train_size)
raise ValueError("Cannot use a custom kernel function. " "Precompute the kernel matrix instead.")
if self._is_training_data(X): return 2. - float(self.train_sizes) / self.n_max_train_sizes else: return float(self.train_sizes) / self.n_max_train_sizes
scores = cross_val_score(clf, X, y2) assert_array_equal(scores, clf.score(X, y2))
multioutput_y = np.column_stack([y2, y2[::-1]]) scores = cross_val_score(clf, X_sparse, multioutput_y) assert_array_equal(scores, clf.score(X_sparse, multioutput_y))
scores = cross_val_score(clf, X_sparse, multioutput_y) assert_array_equal(scores, clf.score(X_sparse, multioutput_y))
list_check = lambda x: isinstance(x, list) clf = CheckingClassifier(check_X=list_check) scores = cross_val_score(clf, X.tolist(), y2.tolist())
X_3d = X[:, :, np.newaxis] clf = MockClassifier(allow_nd=True) scores = cross_val_score(clf, X_3d, y2)
X, y = make_classification(n_samples=20, n_classes=2, random_state=0)
svm = SVC(kernel="precomputed") assert_raises(ValueError, cross_val_score, svm, X, y)
assert_raises(ValueError, cross_val_score, svm, linear_kernel.tolist(), y)
scores = cross_val_score(reg, X, y, cv=5) assert_array_almost_equal(scores, [0.94, 0.97, 0.97, 0.99, 0.92], 2)
def custom_score(y_true, y_pred): return (((y_true == y_pred).sum() - (y_true != y_pred).sum()) / y_true.shape[0])
y = np.mod(np.arange(len(y)), 3)
preds2 = np.zeros_like(y) for train, test in cv.split(X, y): est.fit(X[train], y[train]) preds2[test] = est.predict(X[test])
predictions = cross_val_predict(clf, X, y) assert_equal(predictions.shape, (150,))
predictions = cross_val_predict(clf, X_sparse, multioutput_y) assert_equal(predictions.shape, (150, 2))
predictions = cross_val_predict(clf, X_sparse, multioutput_y) assert_array_equal(predictions.shape, (150, 2))
list_check = lambda x: isinstance(x, list) clf = CheckingClassifier(check_X=list_check) predictions = cross_val_predict(clf, X.tolist(), y.tolist())
estimator = MockImprovingEstimator(1) assert_raises(ValueError, learning_curve, estimator, X, y, exploit_incremental_learning=True)
for train, test in kfold.split(X, y): est.fit(X[train], y[train]) expected_predictions[test] = func(X[test])
empty = ParameterGrid({}) assert_equal(len(empty), 1) assert_equal(list(empty), [{}]) assert_grid_iter_equals_getitem(empty) assert_raises(IndexError, lambda: empty[1])
grid_search.score(X, y) grid_search.predict_proba(X) grid_search.decision_function(X) grid_search.transform(X)
grid_search.scoring = 'sklearn' assert_raises(ValueError, grid_search.fit, X, y)
grid_search_no_score.fit(X, y)
assert_equal(grid_search_no_score.best_params_, grid_search.best_params_) assert_equal(grid_search.score(X, y), grid_search_no_score.score(X, y))
grid_search_no_score = GridSearchCV(clf_no_score, {'C': Cs}) assert_raise_message(TypeError, "no scoring", grid_search_no_score.fit, [[1]])
assert_true(score_auc < 1.0) assert_true(score_accuracy < 1.0) assert_not_equal(score_auc, score_accuracy)
rng = np.random.RandomState(0)
gs.fit(X, y)
clf = MockClassifier() grid_search = GridSearchCV(clf, {'foo_param': [1]}) grid_search.fit(X, y) assert_true(hasattr(grid_search, "grid_scores_"))
X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)
X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)
X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)
K_train = np.dot(X_[:180], X_[:180].T) y_train = y_[:180]
K_test = np.dot(X_[180:], X_[:180].T) y_test = y_[180:]
assert_raises(ValueError, cv.fit, K_train.tolist(), y_train)
X = np.arange(100).reshape(10, 10) y = np.array([0] * 5 + [1] * 5)
X = np.arange(100).reshape(10, 10) y = np.array([0] * 5 + [1] * 5)
X = np.arange(100).reshape(10, 10) y = np.array([0] * 5 + [1] * 5)
types = [(MockDataFrame, MockDataFrame)] try: from pandas import Series, DataFrame types.append((DataFrame, Series)) except ImportError: pass
grid_search = GridSearchCV(km, param_grid=dict(n_clusters=[2, 3, 4])) grid_search.fit(X) assert_equal(grid_search.best_params_["n_clusters"], 4)
X, y = make_classification(n_samples=200, n_features=100, n_informative=3, random_state=0)
sorted_grid_scores = list(sorted(search.grid_scores_, key=lambda x: x.mean_validation_score)) best_score = sorted_grid_scores[-1].mean_validation_score assert_equal(search.best_score_, best_score)
clf = MockClassifier() grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]}, refit=True) grid_search.fit(X, y)
assert all(np.all(this_point.cv_validation_scores == 0.0) for this_point in gs.grid_scores_ if this_point.parameters['parameter'] == FailingClassifier.FAILING_PARAMETER)
gs = GridSearchCV(clf, [{'parameter': [0, 1, 2]}], scoring='accuracy', refit=False, error_score='raise')
assert_raises(ValueError, gs.fit, X, y)
assert_equal(n_splits[i], cv.get_n_splits(X, y, labels))
assert_equal(cv_repr, repr(cv))
train, test = set(train), set(test)
assert_equal(train.intersection(test), set())
assert_equal(train.union(test), set(range(n_samples)))
if expected_n_iter is not None: assert_equal(cv.get_n_splits(X, y, labels), expected_n_iter) else: expected_n_iter = cv.get_n_splits(X, y, labels)
assert_equal(iterations, expected_n_iter) if n_samples is not None: assert_equal(collected_test_samples, set(range(n_samples)))
assert_raises(ValueError, next, KFold(4).split(X1))
y = np.array([3, 3, -1, -1, 3])
with warnings.catch_warnings(): warnings.simplefilter("ignore") check_cv_coverage(skf_3, X2, y, labels=None, expected_n_iter=3)
y = np.array([3, 3, -1, -1, 2])
assert_raises(TypeError, KFold, n_folds=4, shuffle=None)
X1 = np.ones(18) kf = KFold(3) check_cv_coverage(kf, X1, y=None, labels=None, expected_n_iter=3)
X2 = np.ones(17) kf = KFold(3) check_cv_coverage(kf, X2, y=None, labels=None, expected_n_iter=3)
assert_equal(5, KFold(5).get_n_splits(X2))
X2 = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]
assert_equal(5, StratifiedKFold(5).get_n_splits(X, y))
X = np.ones(17) y = [0] * 3 + [1] * 14
kf = KFold(3) kf2 = KFold(3, shuffle=True, random_state=0) kf3 = KFold(3, shuffle=True, random_state=1)
assert_not_equal(len(np.intersect1d(tr_a, tr_b)), len(tr1))
all_folds[te2] = 1
assert_equal(sum(all_folds), 300)
assert_raises(ValueError, next, StratifiedShuffleSplit(3, 0.2).split(X, y))
assert_raises(ValueError, next, StratifiedShuffleSplit(train_size=2).split(X, y)) assert_raises(ValueError, next, StratifiedShuffleSplit(test_size=2).split(X, y))
n_folds = 5 n_iter = 1000
y = [0, 1, 2, 3] * 3 + [4, 5] * 5 X = np.ones_like(y)
repr(slo)
assert_equal(slo.get_n_splits(X, y, labels=l), n_iter)
assert_equal(l[train].size + l[test].size, l.size)
assert_array_equal(np.intersect1d(train, test), [])
assert_equal(3, LeavePLabelOut(n_labels=2).get_n_splits(X, y, labels)) assert_equal(3, LeaveOneLabelOut().get_n_splits(X, y, labels))
np.testing.assert_equal(list(KFold(3).split(X)), list(cv.split(X)))
np.testing.assert_equal(list(cv), list(wrapped_old_skf.split()))
assert_equal(len(cv), wrapped_old_skf.get_n_splits())
n_labels = 15 n_samples = 1000 n_folds = 5
assert_equal(len(folds), len(labels)) for i in np.unique(folds): assert_greater_equal(tolerance, abs(sum(folds == i) - ideal_n_labels_per_fold))
for label in np.unique(labels): assert_equal(len(np.unique(folds[labels == label])), 1)
folds = np.zeros(n_samples) for i, (_, test) in enumerate(lkf.split(X, y, labels)): folds[test] = i
assert_equal(len(folds), len(labels)) for i in np.unique(folds): assert_greater_equal(tolerance, abs(sum(folds == i) - ideal_n_labels_per_fold))
rng = np.random.RandomState(0)
fit_params = fit_params if fit_params is not None else {} fit_params = dict([(k, _index_param_value(X, v, train)) for k, v in fit_params.items()])
if not callable(getattr(estimator, method)): raise AttributeError('{} not implemented in estimator' .format(method))
predictions = [pred_block_i for pred_block_i, _ in prediction_blocks] test_indices = np.concatenate([indices_i for _, indices_i in prediction_blocks])
if sp.issparse(predictions[0]): predictions = sp.vstack(predictions, format=predictions[0].format) else: predictions = np.concatenate(predictions) return predictions[inv_test_indices]
fit_params = fit_params if fit_params is not None else {} fit_params = dict([(k, _index_param_value(X, v, train)) for k, v in fit_params.items()])
return v
cv_iter = list(cv_iter) scorer = check_scoring(estimator, scoring=scoring)
train_sizes_abs = _translate_train_sizes(train_sizes, n_max_training_samples) n_unique_ticks = train_sizes_abs.shape[0] if verbose > 0: print("[learning_curve] Training set sizes: " + str(train_sizes_abs))
X_train, y_train, sw_train = \ X[:n_samples], y[:n_samples], sample_weight[:n_samples] X_test, y_test = X[n_samples:], y[n_samples:]
clf = MultinomialNB().fit(X_train, y_train, sample_weight=sw_train) prob_pos_clf = clf.predict_proba(X_test)[:, 1]
assert_greater(brier_score_loss(y_test, prob_pos_clf), brier_score_loss(y_test, prob_pos_pc_clf))
pc_clf.fit(this_X_train, y_train + 1, sample_weight=sw_train) prob_pos_pc_clf_relabeled = pc_clf.predict_proba(this_X_test)[:, 1] assert_array_almost_equal(prob_pos_pc_clf, prob_pos_pc_clf_relabeled)
pc_clf.fit(this_X_train, 2 * y_train - 1, sample_weight=sw_train) prob_pos_pc_clf_relabeled = pc_clf.predict_proba(this_X_test)[:, 1] assert_array_almost_equal(prob_pos_pc_clf, prob_pos_pc_clf_relabeled)
clf_base_regressor = CalibratedClassifierCV(Ridge()) clf_base_regressor.fit(X_train, y_train) clf_base_regressor.predict(X_test)
clf_invalid_method = CalibratedClassifierCV(clf, method="foo") assert_raises(ValueError, clf_invalid_method.fit, X_train, y_train)
clf_base_regressor = \ CalibratedClassifierCV(RandomForestRegressor(), method="sigmoid") assert_raises(RuntimeError, clf_base_regressor.fit, X_train, y_train)
msg = "LinearSVC does not support sample_weight." assert_warns_message( UserWarning, msg, calibrated_clf.fit, X_train, y_train, sample_weight=sw_train) probs_with_sw = calibrated_clf.predict_proba(X_test)
calibrated_clf.fit(X_train, y_train) probs_without_sw = calibrated_clf.predict_proba(X_test)
clf = LinearSVC() X, y_idx = make_blobs(n_samples=100, n_features=2, random_state=42, centers=3, cluster_std=3.0)
target_names = np.array(['a', 'b', 'c']) y = target_names[y_idx]
clf = MultinomialNB() clf.fit(X_train, y_train, sw_train) prob_pos_clf = clf.predict_proba(X_test)[:, 1]
assert_raises(ValueError, _SigmoidCalibration().fit, np.vstack((exF, exF)), exY)
assert_raises(ValueError, calibration_curve, [1.1], [-0.1], normalize=False)
X2 = rng.randint(5, size=(6, 100)) y2 = np.array([1, 1, 2, 2, 3, 3])
assert_raises(ValueError, GaussianNB().partial_fit, X, y, classes=[0, 1])
sw = np.ones(6) clf = GaussianNB().fit(X, y) clf_sw = GaussianNB().fit(X, y, sw)
ind = rng.randint(0, X.shape[0], 20) sample_weight = np.bincount(ind, minlength=X.shape[0])
clf.fit(X, y) assert_raises(ValueError, clf.partial_fit, np.hstack((X, X)), y)
clf = MultinomialNB() assert_raises(ValueError, clf.fit, -X, y2) y_pred = clf.fit(X, y2).predict(X)
y_pred_proba = clf.predict_proba(X) y_pred_log_proba = clf.predict_log_proba(X) assert_array_almost_equal(np.log(y_pred_proba), y_pred_log_proba, 8)
clf3 = MultinomialNB() clf3.partial_fit(X, y2, classes=np.unique(y2))
for cls in [BernoulliNB, MultinomialNB, GaussianNB]: assert_raises(ValueError, cls().fit, X2, y2[:-1])
clf = cls().fit(X2, y2) assert_raises(ValueError, clf.predict, X2[:, :-1])
assert_raises(ValueError, cls().partial_fit, X2, y2[:-1], classes=np.unique(y2))
assert_raises(ValueError, cls().partial_fit, X2, y2)
clf = cls() clf.partial_fit(X2, y2, classes=np.unique(y2)) assert_raises(ValueError, clf.partial_fit, X2, y2, classes=np.arange(42))
assert_raises(ValueError, clf.partial_fit, X2[:, :-1], y2)
assert_raises(ValueError, clf.predict, X2[:, :-1])
X_bernoulli = [[1, 100, 0], [0, 1, 0], [0, 100, 1]] X_multinomial = [[0, 1], [1, 3], [4, 0]]
yield check_sample_weight_multiclass, cls
X = [[1, 0, 0], [1, 1, 1]]
scores = cross_val_score(MultinomialNB(alpha=10), X, y, cv=10) assert_greater(scores.mean(), 0.86)
scores = cross_val_score(BernoulliNB(alpha=10), X > 4, y, cv=10) assert_greater(scores.mean(), 0.83)
scores = cross_val_score(GaussianNB(), X, y, cv=10) assert_greater(scores.mean(), 0.77)
clf = BernoulliNB(alpha=1.0) clf.fit(X, Y)
assert_array_equal(clf.feature_log_prob_, (num - denom))
Y = np.array([0, 0, 0, 1])
clf = BernoulliNB(alpha=1.0) clf.fit(X, Y)
class_prior = np.array([0.75, 0.25]) assert_array_almost_equal(np.exp(clf.class_log_prior_), class_prior)
X_test = np.array([[0, 1, 1, 0, 0, 1]])
unnorm_predict_proba = np.array([[0.005183999999999999, 0.02194787379972565]]) predict_proba = unnorm_predict_proba / np.sum(unnorm_predict_proba) assert_array_almost_equal(clf.predict_proba(X_test), predict_proba)
class MyEstimator(BaseEstimator):
buggy = Buggy() buggy.a = 2 assert_raises(RuntimeError, clone, buggy)
clf = MyEstimator(empty=np.array([])) clf2 = clone(clf) assert_array_equal(clf.empty, clf2.empty)
clf = MyEstimator(empty=np.nan) clf2 = clone(clf)
my_estimator = MyEstimator() str(my_estimator)
est = DeprecatedAttributeEstimator(a=1)
estimators = [DecisionTreeClassifier(max_depth=2), DecisionTreeRegressor(max_depth=2)] sets = [datasets.load_iris(), datasets.load_boston()]
assert_raises(Exception, getattr(delegator, method), delegator_data.fit_args[0])
getattr(delegator, method)(delegator_data.fit_args[0])
if self._is_training_data(X): return 2. - float(self.train_sizes) / self.n_max_train_sizes else: return float(self.train_sizes) / self.n_max_train_sizes
estimator = MockImprovingEstimator(1) assert_raises(ValueError, learning_curve, estimator, X, y, exploit_incremental_learning=True)
pipe.set_params(svc__a=0.1) assert_equal(clf.a, 0.1) assert_equal(clf.b, None) repr(pipe)
clf = SVC() filter1 = SelectKBest(f_classif) pipe = Pipeline([('anova', filter1), ('svc', clf)])
assert_raises(ValueError, Pipeline, [('svc', SVC()), ('svc', SVC())])
pipe.set_params(svc__C=0.1) assert_equal(clf.C, 0.1) repr(pipe)
assert_raises(ValueError, pipe.set_params, anova__C=0.1)
pipe2 = clone(pipe) assert_false(pipe.named_steps['svc'] is pipe2.named_steps['svc'])
params = pipe.get_params(deep=True) params2 = pipe2.get_params(deep=True)
params.pop('svc') params.pop('anova') params2.pop('svc') params2.pop('anova') assert_equal(params, params2)
pipe = Pipeline([('cls', LinearRegression())])
error_msg = ('Invalid parameter %s for estimator %s. ' 'Check the list of available parameters ' 'with `estimator.get_params().keys()`.')
assert_raise_message(ValueError, error_msg % ("fake", pipe), pipe.set_params, fake__estimator='nope')
predict = pipe.predict(X) assert_equal(predict.shape, (n_samples,))
iris = load_iris() scaler = StandardScaler() km = KMeans(random_state=0)
scaled = scaler.fit_transform(iris.data) separate_pred = km.fit_predict(scaled)
assert_array_almost_equal(X_transformed[:, :-1], svd.fit_transform(X)) assert_array_equal(X_transformed[:, -1], select.fit_transform(X, y).ravel())
fs.set_params(select__k=2) assert_equal(fs.fit_transform(X, y).shape, (X.shape[0], 4))
iris = load_iris() X = iris.data pca = PCA(n_components=2, svd_solver='full') pipeline = Pipeline([('pca', pca)])
iris = load_iris() X = iris.data y = iris.target transft = TransfT() pipeline = Pipeline([('mock', transft)])
X_trans = pipeline.fit_transform(X, y) X_trans2 = transft.fit(X, y).transform(X) assert_array_almost_equal(X_trans, X_trans2)
X = JUNK_FOOD_DOCS
X_transformed_parallel2 = fs_parallel2.fit_transform(X) assert_array_equal( X_transformed.toarray(), X_transformed_parallel2.toarray() )
X_transformed_parallel2 = fs_parallel2.transform(X) assert_array_equal( X_transformed.toarray(), X_transformed_parallel2.toarray() )
empty = ParameterGrid({}) assert_equal(len(empty), 1) assert_equal(list(empty), [{}]) assert_grid_iter_equals_getitem(empty) assert_raises(IndexError, lambda: empty[1])
grid_search.score(X, y) grid_search.predict_proba(X) grid_search.decision_function(X) grid_search.transform(X)
grid_search.scoring = 'sklearn' assert_raises(ValueError, grid_search.fit, X, y)
grid_search_no_score.fit(X, y)
assert_equal(grid_search_no_score.best_params_, grid_search.best_params_) assert_equal(grid_search.score(X, y), grid_search_no_score.score(X, y))
grid_search_no_score = GridSearchCV(clf_no_score, {'C': Cs}) assert_raise_message(TypeError, "no scoring", grid_search_no_score.fit, [[1]])
clf = MockClassifier() grid_search = GridSearchCV(clf, {'foo_param': [1]}) grid_search.fit(X, y) assert_true(hasattr(grid_search, "grid_scores_"))
X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)
X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)
X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)
K_train = np.dot(X_[:180], X_[:180].T) y_train = y_[:180]
K_test = np.dot(X_[180:], X_[:180].T) y_test = y_[180:]
assert_raises(ValueError, cv.fit, K_train.tolist(), y_train)
X = np.arange(100).reshape(10, 10) y = np.array([0] * 5 + [1] * 5)
X = np.arange(100).reshape(10, 10) y = np.array([0] * 5 + [1] * 5)
X = np.arange(100).reshape(10, 10) y = np.array([0] * 5 + [1] * 5)
types = [(MockDataFrame, MockDataFrame)] try: from pandas import Series, DataFrame types.append((DataFrame, Series)) except ImportError: pass
grid_search = GridSearchCV(km, param_grid=dict(n_clusters=[2, 3, 4])) grid_search.fit(X) assert_equal(grid_search.best_params_["n_clusters"], 4)
X, y = make_classification(n_samples=200, n_features=100, n_informative=3, random_state=0)
sorted_grid_scores = list(sorted(search.grid_scores_, key=lambda x: x.mean_validation_score)) best_score = sorted_grid_scores[-1].mean_validation_score assert_equal(search.best_score_, best_score)
clf = MockClassifier() grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]}, refit=True) grid_search.fit(X, y)
assert all(np.all(this_point.cv_validation_scores == 0.0) for this_point in gs.grid_scores_ if this_point.parameters['parameter'] == FailingClassifier.FAILING_PARAMETER)
gs = GridSearchCV(clf, [{'parameter': [0, 1, 2]}], scoring='accuracy', refit=False, error_score='raise')
assert_raises(ValueError, gs.fit, X, y)
from __future__ import print_function
estimators = all_estimators(include_meta_estimators=True)
assert_greater(len(estimators), 0)
yield check_parameters_default_constructible, name, Estimator
elif (name in CROSS_DECOMPOSITION or name in ['LinearSVC', 'LogisticRegression']): continue
yield (check_non_transformer_estimators_n_iter, name, estimator, 'Multi' in name)
with ignore_warnings(): estimator = Estimator()
external_solver = ['Isomap', 'KernelPCA', 'LocallyLinearEmbedding', 'RandomizedLasso', 'LogisticRegressionCV']
with ignore_warnings(): yield check_transformer_n_iter, name, estimator
if issubclass(Estimator, ProjectedGradientNMF): with ignore_warnings(): yield check_get_params_invariance, name, Estimator else: yield check_get_params_invariance, name, Estimator
for random_matrix in all_random_matrix: yield check_input_size_random_matrix, random_matrix yield check_size_generated, random_matrix yield check_zero_mean_and_unit_norm, random_matrix
n_components = 100 n_features = 1000 A = gaussian_random_matrix(n_components, n_features, random_state=0)
n_components = 100 n_features = 500
def test_sparse_random_projection_transformer_invalid_density(): for RandomProjection in all_SparseRandomProjection: assert_raises(ValueError, RandomProjection(density=1.1).fit, data)
original_distances = original_distances[non_identical]
projected_distances = projected_distances[non_identical]
assert_less(distances_ratio.max(), 1 + eps) assert_less(1 - eps, distances_ratio.min())
rp = SparseRandomProjection(n_components=10, dense_output=True, random_state=0) rp.fit(data) assert isinstance(rp.transform(data), np.ndarray)
rp = SparseRandomProjection(n_components=10, dense_output=False, random_state=0) rp = rp.fit(data) assert isinstance(rp.transform(data), np.ndarray)
assert sp.issparse(rp.transform(sparse_data))
assert_equal(rp.n_components, 'auto') assert_equal(rp.n_components_, 110)
projected_2 = rp.transform(data) assert_array_equal(projected_1, projected_2)
rp2 = RandomProjection(random_state=0, eps=0.5) projected_3 = rp2.fit_transform(data) assert_array_equal(projected_1, projected_3)
assert_raises(ValueError, rp.transform, data[:, 1:5])
X, y = datasets.make_regression(n_targets=1) X_train, y_train = X[:50], y[:50] X_test, y_test = X[50:], y[50:]
rgr = MultiOutputRegressor(GradientBoostingRegressor(random_state=0)) rgr.fit(X, y, w)
multi_target_forest.fit(X, y)
for i in range(3):
svc = LinearSVC(random_state=0) multi_class_svc = OneVsRestClassifier(svc) multi_target_svc = MultiOutputClassifier(multi_class_svc)
for i in range(3):
X_ = X[:, np.newaxis, :] Y_ = Y[np.newaxis, :, :]
kernel = (large_kernel.sum(axis=2))
transform = AdditiveChi2Sampler(sample_steps=3) X_trans = transform.fit_transform(X) Y_trans = transform.transform(Y)
Y_neg = Y.copy() Y_neg[0, 0] = -1 assert_raises(ValueError, transform.transform, Y_neg)
transform = AdditiveChi2Sampler(sample_steps=4) assert_raises(ValueError, transform.fit, X)
sample_steps_available = [1, 2, 3] for sample_steps in sample_steps_available:
transform = AdditiveChi2Sampler(sample_steps=sample_steps) assert_equal(transform.sample_interval, None)
transform.fit(X) assert_not_equal(transform.sample_interval_, None)
sample_interval = 0.3 transform = AdditiveChi2Sampler(sample_steps=4, sample_interval=sample_interval) assert_equal(transform.sample_interval, sample_interval) transform.fit(X) assert_equal(transform.sample_interval_, sample_interval)
c = 0.03 X_c = (X + c)[:, np.newaxis, :] Y_c = (Y + c)[np.newaxis, :, :]
transform = SkewedChi2Sampler(skewedness=c, n_components=1000, random_state=42) X_trans = transform.fit_transform(X) Y_trans = transform.transform(Y)
Y_neg = Y.copy() Y_neg[0, 0] = -1 assert_raises(ValueError, transform.transform, Y_neg)
gamma = 10. kernel = rbf_kernel(X, Y, gamma=gamma)
rbf_transform = RBFSampler(gamma=gamma, n_components=1000, random_state=42) X_trans = rbf_transform.fit_transform(X) Y_trans = rbf_transform.transform(Y) kernel_approx = np.dot(X_trans, Y_trans.T)
rnd = np.random.RandomState(0) X = rnd.uniform(size=(10, 4))
X_transformed = Nystroem(n_components=X.shape[0]).fit_transform(X) K = rbf_kernel(X) assert_array_almost_equal(np.dot(X_transformed, X_transformed.T), K)
rng = np.random.RandomState(0) X = rng.rand(10, 20)
rnd = np.random.RandomState(37) X = rnd.uniform(size=(10, 4))
rnd = np.random.RandomState(42) n_samples = 10 X = rnd.uniform(size=(n_samples, 4))
assert_equal(_top_import_error, None)
log_proba = clf.predict_log_proba(X)
assert_array_equal(np.log(proba[k]), log_proba[k])
est = DummyRegressor() est.fit(X_learn, y_learn) y_pred_learn = est.predict(X_learn) y_pred_test = est.predict(X_test)
est = DummyRegressor(strategy="median") est.fit(X_learn, y_learn) y_pred_learn = est.predict(X_learn) y_pred_test = est.predict(X_test)
est = DummyRegressor(strategy="quantile", quantile=0.5) est.fit(X_learn, y_learn) y_pred_learn = est.predict(X_learn) y_pred_test = est.predict(X_test)
est = DummyRegressor(strategy="quantile", quantile=0.8) est.fit(X_learn, y_learn) y_pred_learn = est.predict(X_learn) y_pred_test = est.predict(X_test)
constants = random_state.randn(5)
est = DummyRegressor(strategy="constant", constant=constants) est.fit(X_learn, y_learn) y_pred_learn = est.predict(X_learn) y_pred_test = est.predict(X_test)
est = DummyRegressor(strategy='mean') est.fit(X, y)
version = sys.version_info if version[0] == 3: if version[1] == 3: reload = None else: from importlib import reload
X1 = np.array([[-2, ], [-1, ], [-1, ], [1, ], [1, ], [2, ]], dtype='f')
y4 = np.array([1, 1, 1, 1, 1, 1, 1, 1, 2])
y_pred1 = clf.fit(X1, y).predict(X1) assert_array_equal(y_pred1, y, 'solver %s' % solver)
y_pred3 = clf.fit(X, y3).predict(X) assert_true(np.any(y_pred3 != y3), 'solver %s' % solver)
clf = LinearDiscriminantAnalysis(priors=[0.5, 0.5]) clf.fit(X, y)
n_features = 2 n_classes = 2 n_samples = 1000 X, y = make_blobs(n_samples=n_samples, n_features=n_features, centers=n_classes, random_state=11)
assert_array_almost_equal(clf_lda_svd.explained_variance_ratio_, clf_lda_eigen.explained_variance_ratio_[:tested_length])
means = np.array([[0, 0, -1], [0, 2, 0], [0, -2, 0], [0, 0, 5]])
clf = LinearDiscriminantAnalysis(solver="svd").fit(X, y) means_transformed = clf.transform(means)
assert_almost_equal(np.cov(clf.transform(scatter).T), np.eye(2))
assert_almost_equal(np.abs(np.dot(d1[:2], [1, 0])), 1.0)
assert_almost_equal(np.abs(np.dot(d2[:2], [0, 1])), 1.0)
assert_equal(clf.fit(x, y).score(x, y), 1.0, 'using covariance: %s' % solver)
clf = QuadraticDiscriminantAnalysis() y_pred = clf.fit(X6, y6).predict(X6) assert_array_equal(y_pred, y6)
y_pred1 = clf.fit(X7, y6).predict(X7) assert_array_equal(y_pred1, y6)
assert_true(np.any(y_pred3 != y7))
assert_raises(ValueError, clf.fit, X6, y4)
clf = QuadraticDiscriminantAnalysis().fit(X6, y6) assert_true(not hasattr(clf, 'covariances_'))
clf = QuadraticDiscriminantAnalysis(store_covariances=True).fit(X6, y6) assert_true(hasattr(clf, 'covariances_'))
clf = QuadraticDiscriminantAnalysis() with ignore_warnings(): y_pred = clf.fit(X2, y6).predict(X2) assert_true(np.any(y_pred != y6))
clf = QuadraticDiscriminantAnalysis(reg_param=0.01) with ignore_warnings(): clf.fit(X2, y6) y_pred = clf.predict(X2) assert_array_equal(y_pred, y6)
clf = QuadraticDiscriminantAnalysis(reg_param=0.1) with ignore_warnings(): clf.fit(X5, y5) y_pred5 = clf.predict(X5) assert_array_equal(y_pred5, y5)
reload(sklearn.lda) return sklearn.lda
reload(sklearn.qda) return sklearn.qda
x = np.dot(x, np.arange(x.shape[1] ** 2).reshape(x.shape[1], x.shape[1]))
y = np.array([0.0, 1.1, 2.0, 3.0]) msg = type_of_target(y) assert_raise_message(ValueError, msg, check_classification_targets, y)
assert_greater(np.mean(pred == iris.target), .9)
Y_proba = clf_sprs.predict_proba(X_test)
pred = Y_proba > .5 assert_array_equal(pred, Y_pred_sprs.toarray())
X = np.ones((10, 2)) X[:5, :] = 0
y = np.zeros((10, 3)) y[5:, 0] = 1 y[:, 1] = 1 y[:, 2] = 1
y = np.zeros((10, 2))
clf = OneVsRestClassifier(base_clf).fit(X, Y) y_pred = clf.predict([[0, 0, 4]])[0] assert_array_equal(y_pred, [0, 0, 1])
clf = OneVsRestClassifier(base_clf).fit(X, Y) y_pred = clf.predict([[3, 0, 0]])[0] assert_equal(y_pred, 1)
decision_only = OneVsRestClassifier(svm.SVR()).fit(X_train, Y_train) assert_raises(AttributeError, decision_only.predict_proba, X_test)
decision_only = OneVsRestClassifier(svm.SVC(probability=False)) decision_only.fit(X_train, Y_train) assert_raises(AttributeError, decision_only.predict_proba, X_test)
pred = Y_proba > .5 assert_array_equal(pred, Y_pred)
decision_only = OneVsRestClassifier(svm.SVR()).fit(X_train, Y_train) assert_raises(AttributeError, decision_only.predict_proba, X_test)
pred = np.array([l.argmax() for l in Y_proba]) assert_false((pred - Y_pred).any())
ovr = OneVsRestClassifier(LinearSVC(random_state=0)) assert_raises(ValueError, lambda x: ovr.coef_, None)
ovr = OneVsRestClassifier(DecisionTreeClassifier()) ovr.fit(iris.data, iris.target) assert_raises(AttributeError, lambda x: ovr.coef_, None)
votes = np.zeros((n_samples, n_classes))
assert_array_equal(votes, np.round(decisions))
assert_true(set(votes[:, class_idx]).issubset(set([0., 1., 2.])))
votes = np.round(ovo_decision) normalized_confidences = ovo_decision - votes
assert_array_equal(votes[0, :], 1) assert_array_equal(np.argmax(votes[1:], axis=1), ovo_prediction[1:]) assert_equal(ovo_prediction[0], normalized_confidences[0].argmax())
X = np.array([[1, 2], [2, 1], [-2, 1], [-2, -1]]) y_ref = np.array([2, 0, 1, 2])
X = np.eye(4) y = np.array(['a', 'b', 'c', 'd'])
is_increasing = assert_no_warnings(check_increasing, x, y) assert_true(is_increasing)
is_increasing = assert_no_warnings(check_increasing, x, y) assert_true(is_increasing)
is_increasing = assert_no_warnings(check_increasing, x, y) assert_false(is_increasing)
is_increasing = assert_no_warnings(check_increasing, x, y) assert_false(is_increasing)
is_increasing = assert_warns_message(UserWarning, "interval", check_increasing, x, y)
ir = IsotonicRegression() assert_array_equal(ir.fit_transform(np.ones(len(x)), y), np.mean(y))
y = np.array([10, 9, 10, 7, 6, 6.1, 5]) x = np.arange(len(y))
is_increasing = y_[0] < y_[-1] assert_false(is_increasing)
y = np.array([5, 6.1, 6, 7, 10, 9, 10]) x = np.arange(len(y))
is_increasing = y_[0] < y_[-1] assert_true(is_increasing)
y = np.array([3, 7, 5, 9, 8, 7, 10]) x = np.arange(len(y))
ir = IsotonicRegression(increasing='auto', out_of_bounds="raise") ir.fit(x, y)
assert_raises(ValueError, ir.predict, [min(x) - 10, max(x) + 10])
y = np.array([3, 7, 5, 9, 8, 7, 10]) x = np.arange(len(y))
ir = IsotonicRegression(increasing='auto', out_of_bounds="clip") ir.fit(x, y)
y = np.array([3, 7, 5, 9, 8, 7, 10]) x = np.arange(len(y))
ir = IsotonicRegression(increasing='auto', out_of_bounds="nan") ir.fit(x, y)
y1 = ir.predict([min(x) - 10, max(x) + 10]) assert_equal(sum(np.isnan(y1)), 2)
y = np.array([3, 7, 5, 9, 8, 7, 10]) x = np.arange(len(y))
ir = IsotonicRegression(increasing='auto', out_of_bounds="xyz")
assert_raises(ValueError, ir.fit, x, y)
y = np.array([3, 7, 5, 9, 8, 7, 10]) x = np.arange(len(y))
ir = IsotonicRegression(increasing='auto', out_of_bounds="raise")
ir.fit(x, y) ir.out_of_bounds = "xyz" assert_raises(ValueError, ir.transform, x)
ir = IsotonicRegression(increasing='auto', out_of_bounds="clip") ir.fit(x, y)
rng = np.random.RandomState(42)
regression = IsotonicRegression() n_samples = 50 x = np.linspace(-3, 3, n_samples) y = x + rng.uniform(size=n_samples)
w = rng.uniform(size=n_samples) w[5:8] = 0 regression.fit(x, y, sample_weight=w)
regression.fit(x, y, sample_weight=w)
weights[rng.rand(n_samples) < 0.1] = 0
X_train_fit, y_train_fit = slow_model._build_y(X_train, y_train, sample_weight=weights, trim_duplicates=False) slow_model._build_f(X_train_fit, y_train_fit)
fast_model.fit(X_train, y_train, sample_weight=weights)
ir = IsotonicRegression() copy.copy(ir)
y = np.arange(10) % 3
train, test = set(train), set(test)
assert_equal(train.intersection(test), set())
assert_equal(train.union(test), set(range(n_samples)))
if expected_n_iter is not None: assert_equal(len(cv), expected_n_iter) else: expected_n_iter = len(cv)
assert_equal(iterations, expected_n_iter) if n_samples is not None: assert_equal(collected_test_samples, set(range(n_samples)))
assert_raises(ValueError, cval.KFold, 3, 4)
y = [3, 3, -1, -1, 3]
check_cv_coverage(cv, expected_n_iter=3, n_samples=len(y))
y = [3, 3, -1, -1, 2]
assert_raises(ValueError, cval.KFold, 2.5, 2)
assert_raises(ValueError, cval.KFold, 5, 1.5) assert_raises(ValueError, cval.StratifiedKFold, y, 1.5)
kf = cval.KFold(300, 3) check_cv_coverage(kf, expected_n_iter=3, n_samples=300)
kf = cval.KFold(17, 3) check_cv_coverage(kf, expected_n_iter=3, n_samples=17)
splits = iter(cval.KFold(4, 2)) train, test = next(splits) assert_array_equal(test, [0, 1]) assert_array_equal(train, [2, 3])
kf = cval.KFold(300, 3, shuffle=True, random_state=0) ind = np.arange(300)
n_labels = 15 n_samples = 1000 n_folds = 5
assert_equal(len(folds), len(labels)) for i in np.unique(folds): assert_greater_equal(tolerance, abs(sum(folds == i) - ideal_n_labels_per_fold))
for label in np.unique(labels): assert_equal(len(np.unique(folds[labels == label])), 1)
assert_equal(len(folds), len(labels)) for i in np.unique(folds): assert_greater_equal(tolerance, abs(sum(folds == i) - ideal_n_labels_per_fold))
for label in np.unique(labels): assert_equal(len(np.unique(folds[labels == label])), 1)
for train, test in cval.LabelKFold(labels, n_folds=n_folds): assert_equal(len(np.intersect1d(labels[train], labels[test])), 0)
labels = np.array([1, 1, 1, 2, 2]) assert_raises(ValueError, cval.LabelKFold, labels, n_folds=3)
assert_raises(ValueError, cval.StratifiedShuffleSplit, y, 3, 0.2)
assert_raises(ValueError, cval.StratifiedShuffleSplit, y, 3, 2) assert_raises(ValueError, cval.StratifiedShuffleSplit, y, 3, 3, 2)
assert_raises(ValueError, cval.StratifiedShuffleSplit, y, train_size=2) assert_raises(ValueError, cval.StratifiedShuffleSplit, y, test_size=2)
n_folds = 5 n_iter = 1000
labels = [0, 1, 2, 3] * 3 + [4, 5] * 5
repr(slo)
assert_equal(len(slo), n_iter)
assert_equal(y[train].size + y[test].size, y.size)
assert_array_equal(np.intersect1d(train, test), [])
scores = cval.cross_val_score(clf, X, y) assert_array_equal(scores, clf.score(X, y))
scores = cval.cross_val_score(clf, X_sparse, X) assert_array_equal(scores, clf.score(X_sparse, X))
scores = cval.cross_val_score(clf, X_sparse, X) assert_array_equal(scores, clf.score(X_sparse, X))
list_check = lambda x: isinstance(x, list) clf = CheckingClassifier(check_X=list_check) scores = cval.cross_val_score(clf, X.tolist(), y.tolist())
X_3d = X[:, :, np.newaxis] clf = MockClassifier(allow_nd=True) scores = cval.cross_val_score(clf, X_3d, y)
svm = SVC(kernel="precomputed") assert_raises(ValueError, cval.cross_val_score, svm, X, y)
assert_raises(ValueError, cval.cross_val_score, svm, linear_kernel.tolist(), y)
X_df = MockDataFrame(X) X_train, X_test = cval.train_test_split(X_df) assert_true(isinstance(X_train, MockDataFrame)) assert_true(isinstance(X_test, MockDataFrame))
def custom_score(y_true, y_pred): return (((y_true == y_pred).sum() - (y_true != y_pred).sum()) / y_true.shape[0])
y = np.mod(np.arange(len(y)), 3)
ss = cval.ShuffleSplit(10, random_state=21) assert_array_equal(list(a for a, b in ss), list(a for a, b in ss))
preds2 = np.zeros_like(y) for train, test in cv: est.fit(X[train], y[train]) preds2[test] = est.predict(X[test])
predictions = cval.cross_val_predict(clf, X, y) assert_equal(predictions.shape, (10,))
predictions = cval.cross_val_predict(clf, X_sparse, X) assert_equal(predictions.shape, (10, 2))
predictions = cval.cross_val_predict(clf, X_sparse, X) assert_array_equal(predictions.shape, (10, 2))
list_check = lambda x: isinstance(x, list) clf = CheckingClassifier(check_X=list_check) predictions = cval.cross_val_predict(clf, X.tolist(), y.tolist())
cv = list(check_cv(cv, X, y, classifier=is_classifier(estimator))) scorer = check_scoring(estimator, scoring=scoring)
train_sizes_abs = _translate_train_sizes(train_sizes, n_max_training_samples) n_unique_ticks = train_sizes_abs.shape[0] if verbose > 0: print("[learning_curve] Training set sizes: " + str(train_sizes_abs))
assert_array_equal(X, Xdigits)
X = csr_matrix(Xdigits[:4])
rbm1.random_state = 42 d_score = rbm1.score_samples(X) rbm1.random_state = 42 s_score = rbm1.score_samples(lil_matrix(X)) assert_almost_equal(d_score, s_score)
with np.errstate(under='ignore'): rbm1.score_samples([np.arange(1000) * 100])
X = X_digits_binary[:100] y = y_digits_binary[:100]
mlp.n_iter_ = 0 mlp.learning_rate_ = 0.1
mlp.n_layers_ = 3
mlp._coef_grads = [0] * (mlp.n_layers_ - 1) mlp._intercept_grads = [0] * (mlp.n_layers_ - 1)
def loss_grad_fun(t): return mlp._loss_grad_lbfgs(t, X, Y, activations, deltas, coef_grads, intercept_grads)
for X, y in classification_datasets: X_train = X[:150] y_train = y[:150] X_test = X[150:]
for X, y in classification_datasets: X = X y = y mlp = MLPClassifier(algorithm='sgd', max_iter=100, random_state=1, tol=0, alpha=1e-5, learning_rate_init=0.2)
X = Xboston y = yboston
mlp.fit(X, y)
X = [[3, 2], [1, 6]] y = [1, 0]
assert_raises(ValueError, MLPClassifier( algorithm='sgd').partial_fit, X, y, classes=[2])
assert_false(hasattr(MLPClassifier(algorithm='l-bfgs'), 'partial_fit'))
X = [[3, 2], [1, 6]] y = [1, 0] clf = MLPClassifier
X = X_digits_binary[:50] y = y_digits_binary[:50]
X = X_digits_multi[:10] y = y_digits_multi[:10]
for i in range(self.n_layers_ - 1): activations[i + 1] = safe_sparse_dot(activations[i], self.coefs_[i]) activations[i + 1] += self.intercepts_[i]
if (i + 1) != (self.n_layers_ - 1): activations[i + 1] = hidden_activation(activations[i + 1])
if with_output_activation: output_activation = ACTIVATIONS[self.out_activation_] activations[i + 1] = output_activation(activations[i + 1])
activations = self._forward_pass(activations)
last = self.n_layers_ - 2
diff = y - activations[-1] deltas[last] = -diff
coef_grads, intercept_grads = self._compute_loss_grad( last, n_samples, activations, deltas, coef_grads, intercept_grads)
self.n_iter_ = 0 self.t_ = 0 self.n_outputs_ = y.shape[1]
self.n_layers_ = len(layer_units)
self.coefs_ = [] self.intercepts_ = []
init_bound = np.sqrt(2. / (fan_in + fan_out))
raise ValueError("Unknown activation function %s" % self.activation)
hidden_layer_sizes = self.hidden_layer_sizes if not hasattr(hidden_layer_sizes, "__iter__"): hidden_layer_sizes = [hidden_layer_sizes] hidden_layer_sizes = list(hidden_layer_sizes)
self._validate_hyperparameters() if np.any(np.array(hidden_layer_sizes) <= 0): raise ValueError("hidden_layer_sizes must be > 0, got %s." % hidden_layer_sizes)
if y.ndim == 1: y = y.reshape((-1, 1))
self._initialize(y, layer_units)
activations = [X] activations.extend(np.empty((batch_size, n_fan_out)) for n_fan_out in layer_units[1:]) deltas = [np.empty_like(a_layer) for a_layer in activations]
if self.algorithm in _STOCHASTIC_ALGOS: self._fit_stochastic(X, y, activations, deltas, coef_grads, intercept_grads, layer_units, incremental)
elif self.algorithm == 'l-bfgs': self._fit_lbfgs(X, y, activations, deltas, coef_grads, intercept_grads, layer_units) return self
self._coef_indptr = [] self._intercept_indptr = [] start = 0
for i in range(self.n_layers_ - 1): n_fan_in, n_fan_out = layer_units[i], layer_units[i + 1]
for i in range(self.n_layers_ - 1): end = start + layer_units[i + 1] self._intercept_indptr.append((start, end)) start = end
packed_coef_inter = _pack(self.coefs_, self.intercepts_)
grads = coef_grads + intercept_grads self._optimizer.update_params(grads)
self._update_no_improvement_count(early_stopping, X_val, y_val)
self._optimizer.iteration_ends(self.t_)
self.coefs_ = self._best_coefs self.intercepts_ = self._best_intercepts
self.validation_scores_.append(self.score(X_val, y_val))
last_valid_score = self.validation_scores_[-1]
hidden_layer_sizes = self.hidden_layer_sizes if not hasattr(hidden_layer_sizes, "__iter__"): hidden_layer_sizes = [hidden_layer_sizes] hidden_layer_sizes = list(hidden_layer_sizes)
activations = [X]
self._forward_pass(activations, with_output_activation=False) y_pred = activations[-1]
self._check_params()
X, y = check_X_y(X, y, multi_output=True, y_numeric=True) self.y_ndim_ = y.ndim if y.ndim == 1: y = y[:, np.newaxis]
n_samples, n_features = X.shape _, n_targets = y.shape
self._check_params(n_samples)
X = check_array(X) n_eval, _ = X.shape n_samples, n_features = self.X.shape n_samples_y, n_targets = self.y.shape
self._check_params(n_samples)
X = (X - self.X_mean) / self.X_std
y = np.zeros(n_eval) if eval_MSE: MSE = np.zeros(n_eval)
dx = manhattan_distances(X, Y=self.X, sum_over_features=False) f = self.regr(X) r = self.corr(self.theta_, dx).reshape(n_eval, n_samples)
y_ = np.dot(f, self.beta) + np.dot(r, self.gamma)
y = (self.y_mean + self.y_std * y_).reshape(n_eval, n_targets)
u = linalg.solve_triangular(self.G.T, np.dot(self.Ft.T, rt) - f.T, lower=True)
u = np.zeros((n_targets, n_eval))
MSE[MSE < 0.] = 0.
theta = self.theta_
reduced_likelihood_function_value = - np.inf par = {}
n_samples = self.X.shape[0] D = self.D ij = self.ij F = self.F
try: C = linalg.cholesky(R, lower=True) except linalg.LinAlgError: return reduced_likelihood_function_value, par
beta = linalg.solve_triangular(G, np.dot(Q.T, Yt))
beta = np.array(self.beta0)
detR = (np.diag(C) ** (2. / n_samples)).prod()
best_optimal_theta = [] best_optimal_rlf_value = [] best_optimal_par = []
if self.optimizer == 'Welch' and self.theta0.size == 1: self.optimizer = 'fmin_cobyla'
theta0 = self.theta0
theta0, thetaL, thetaU = self.theta0, self.thetaL, self.thetaU corr = self.corr verbose = self.verbose
self.optimizer = 'fmin_cobyla' self.verbose = False
self.theta0, self.thetaL, self.thetaU = theta0, thetaL, thetaU self.corr = corr self.optimizer = 'Welch' self.verbose = verbose
if self.beta0 is not None: self.beta0 = np.atleast_2d(self.beta0) if self.beta0.shape[1] != 1: self.beta0 = self.beta0.T
self.theta0 = np.atleast_2d(self.theta0) lth = self.theta0.size
self.verbose = bool(self.verbose)
self.normalize = bool(self.normalize)
if self.optimizer not in self._optimizer_types: raise ValueError("optimizer should be one of %s" % self._optimizer_types)
self.random_start = int(self.random_start)
def obj_func(theta, eval_gradient=True): if eval_gradient: lml, grad = self.log_marginal_likelihood( theta, eval_gradient=True) return -lml, -grad else: return -self.log_marginal_likelihood(theta)
optima = [self._constrained_optimization(obj_func, self.kernel_.theta, self.kernel_.bounds)]
K = self.kernel_(self.X_train_)
var_f_star = self.kernel_.diag(X) - np.einsum("ij,ij->j", v, v)
Z, (pi, W_sr, L, b, a) = \ self._posterior_mode(K, return_temporaries=True)
d_Z = np.empty(theta.shape[0])
s_2 = -0.5 * (np.diag(K) - np.einsum('ij, ij -> j', C, C)) \
s_1 = .5 * a.T.dot(C).dot(a) - .5 * R.T.ravel().dot(C.ravel())
return np.mean( [estimator.log_marginal_likelihood( theta[n_dims * i:n_dims * (i + 1)]) for i, estimator in enumerate(estimators)])
return self
setattr(self, hyperparameter.name, np.exp(theta[i:i + hyperparameter.n_elements])) i += hyperparameter.n_elements
K = squareform(K) np.fill_diagonal(K, 1)
return K, np.empty((X.shape[0], X.shape[0], 0))
K = squareform(K) np.fill_diagonal(K, 1)
K_gradient = np.empty((X.shape[0], X.shape[0], 0)) return K, K_gradient
if not self.hyperparameter_length_scale.fixed: length_scale_gradient = \ dists * K / (self.length_scale ** 2 * base) length_scale_gradient = length_scale_gradient[:, :, np.newaxis]
if not self.hyperparameter_length_scale.fixed: length_scale_gradient = \ 4 / self.length_scale**2 * sin_of_arg**2 * K length_scale_gradient = length_scale_gradient[:, :, np.newaxis]
if not self.hyperparameter_periodicity.fixed: periodicity_gradient = \ 4 * arg / self.length_scale**2 * cos_of_arg \ * sin_of_arg * K periodicity_gradient = periodicity_gradient[:, :, np.newaxis]
return np.apply_along_axis(self, 1, X)[:, 0]
for i, hyperparameter in enumerate(kernel.hyperparameters): assert_equal(theta[i], np.log(getattr(kernel, hyperparameter.name)))
for i, hyperparameter in enumerate(kernel.hyperparameters): theta[i] = np.log(42) kernel.theta = theta assert_almost_equal(getattr(kernel, hyperparameter.name), 42)
assert_almost_equal((RBF(2.0) + 1.0)(X), (1.0 + RBF(2.0))(X))
assert_almost_equal((3.0 * RBF(2.0))(X), (RBF(2.0) * 3.0)(X))
assert_not_equal(id(attr_value), id(attr_value_cloned))
if kernel != kernel_white: K1 = kernel(X) K2 = pairwise_kernels(X, metric=kernel) assert_array_almost_equal(K1, K2)
K1 = kernel(X, Y) K2 = pairwise_kernels(X, Y, metric=kernel) assert_array_almost_equal(K1, K2)
index = 0 params = kernel.get_params() for hyperparameter in kernel.hyperparameters: if hyperparameter.bounds is "fixed": continue size = hyperparameter.n_elements
index = 0
all_corr = ['absolute_exponential', 'squared_exponential', 'cubic', 'linear']
gp = GaussianProcess(corr='absolute_exponential', theta0=1e-4, thetaL=1e-12, thetaU=1e-2, nugget=1e-2, optimizer='Welch', regr="linear", random_state=0)
assert_greater(gpc.log_marginal_likelihood(gpc.kernel_.theta), gpc.log_marginal_likelihood(kernel.theta))
assert_almost_equal(np.diag(y_cov), np.exp(kernel.theta[0]), 5)
gpr = GaussianProcessRegressor(kernel=kernel) gpr.fit(X, y_norm) gpr_norm = GaussianProcessRegressor(kernel=kernel, normalize_y=True) gpr_norm.fit(X, y)
y_pred, y_pred_std = gpr.predict(X2, return_std=True) y_pred = y_mean + y_pred y_pred_norm, y_pred_std_norm = gpr_norm.predict(X2, return_std=True)
kernel = RBF(length_scale=1.0)
assert_almost_equal(y_std_1d, y_std_2d) assert_almost_equal(y_cov_1d, y_cov_2d)
for kernel in kernels: gpr = GaussianProcessRegressor(kernel=kernel, normalize_y=True) gpr.fit(X, y)
assert_greater(gpr.log_marginal_likelihood(gpr.kernel_.theta), gpr.log_marginal_likelihood(gpr.kernel.theta))
if self.normalize_y: self.y_train_mean = np.mean(y, axis=0) y = y - self.y_train_mean else: self.y_train_mean = np.zeros(1)
def obj_func(theta, eval_gradient=True): if eval_gradient: lml, grad = self.log_marginal_likelihood( theta, eval_gradient=True) return -lml, -grad else: return -self.log_marginal_likelihood(theta)
optima = [(self._constrained_optimization(obj_func, self.kernel_.theta, self.kernel_.bounds))]
K = self.kernel_(self.X_train_) K[np.diag_indices_from(K)] += self.alpha
y_train = self.y_train_ if y_train.ndim == 1: y_train = y_train[:, np.newaxis]
log_likelihood_gradient_dims = \ 0.5 * np.einsum("ijl,ijk->kl", tmp, K_gradient) log_likelihood_gradient = log_likelihood_gradient_dims.sum(-1)
return True
return True
check_ortho(Wx, "x weights are not orthogonal") check_ortho(Wy, "y weights are not orthogonal")
check_ortho(T, "x scores are not orthogonal") check_ortho(U, "y scores are not orthogonal")
pls_ca = pls_.PLSCanonical(n_components=X.shape[1]) pls_ca.fit(X, Y)
x_weights_sign_flip = pls_ca.x_weights_ / x_weights
assert_array_almost_equal(x_rotations_sign_flip, x_weights_sign_flip) assert_array_almost_equal(np.abs(x_rotations_sign_flip), 1, 4) assert_array_almost_equal(np.abs(x_weights_sign_flip), 1, 4)
pls_2 = pls_.PLSRegression(n_components=X.shape[1]) pls_2.fit(X, Y)
assert_array_almost_equal(x_loadings_sign_flip, x_weights_sign_flip, 4) assert_array_almost_equal(np.abs(x_loadings_sign_flip), 1, 4) assert_array_almost_equal(np.abs(x_weights_sign_flip), 1, 4)
check_ortho(pls_ca.x_weights_, "x weights are not orthogonal") check_ortho(pls_ca.y_weights_, "y weights are not orthogonal")
check_ortho(pls_ca.x_scores_, "x scores are not orthogonal") check_ortho(pls_ca.y_scores_, "y scores are not orthogonal")
d = load_linnerud() X = d.data Y = d.target
model1 = clf.fit(X, Y[:, 0]).coef_ model2 = clf.fit(X, Y[:, :1]).coef_ assert_array_almost_equal(model1, model2)
X1[:, -1] = 1.0
clf.set_params(scale=True) X_score, Y_score = clf.fit_transform(X_s, Y_s) assert_array_almost_equal(X_s_score, X_score) assert_array_almost_equal(Y_s_score, Y_score)
from distutils.version import LooseVersion from sklearn.utils.extmath import svd_flip
pinv2_args = {'check_finite': False}
while True: if mode == "B": if X_pinv is None: X_pinv = linalg.pinv2(X, **pinv2_args) x_weights = np.dot(X_pinv, y_score)
x_weights = np.dot(X.T, y_score) / np.dot(y_score.T, y_score)
y_weights = np.dot(Y.T, x_score) / np.dot(x_score.T, x_score)
X -= self.x_mean_ X /= self.x_std_ Ypred = np.dot(X, self.coef_) return Ypred + self.y_mean_
axis = uniques
axis = np.linspace(emp_percentiles[0, col], emp_percentiles[1, col], num=grid_resolution, endpoint=True)
if feature_names is None: feature_names = [str(i) for i in range(gbrt.n_features)] elif isinstance(feature_names, np.ndarray): feature_names = feature_names.tolist()
for i in fxs: l.append(feature_names[i]) names.append(l)
pd_result = Parallel(n_jobs=n_jobs, verbose=verbose)( delayed(partial_dependence)(gbrt, fxs, X=X, grid_resolution=grid_resolution, percentiles=percentiles) for fxs in features)
if 2 in pdp_lim: Z_level = np.linspace(*pdp_lim[2], num=8)
ax.xaxis.set_major_locator(MaxNLocator(nbins=6, prune='lower')) tick_formatter = ScalarFormatter() tick_formatter.set_powerlimits((-3, 4)) ax.xaxis.set_major_formatter(tick_formatter)
self.n_classes = np.unique(y).shape[0] if self.n_classes == 2: self.n_classes = 1
self.n_classes = 1
terminal_regions = tree.apply(X)
masked_terminal_regions = terminal_regions.copy() masked_terminal_regions[~sample_mask] = -1
for leaf in np.where(tree.children_left == TREE_LEAF)[0]: self._update_terminal_region(tree, masked_terminal_regions, leaf, X, y, residual, y_pred[:, k], sample_weight)
y_pred[:, k] += (learning_rate * tree.value[:, 0, 0].take(terminal_regions, axis=0))
y_pred[:, k] += learning_rate * tree.predict(X).ravel()
super(BinomialDeviance, self).__init__(1)
Y = np.zeros((y.shape[0], self.K), dtype=np.float64) for k in range(self.K): Y[:, k] = y == k
super(ExponentialLoss, self).__init__(1)
print(('%10s ' + '%16s ' * (len(header_fields) - 1)) % tuple(header_fields))
self.verbose_mod = 1 self.start_time = time() self.begin_at_stage = begin_at_stage
self.verbose_mod *= 10
sample_weight = sample_weight * sample_mask.astype(np.float64)
self.estimators_[i, k] = tree
if self.n_classes_ > 1: max_features = max(1, int(np.sqrt(self.n_features))) else: max_features = self.n_features
if self.subsample < 1.0: self.oob_improvement_ = np.zeros((self.n_estimators), dtype=np.float64)
total_n_estimators = self.n_estimators if total_n_estimators < self.estimators_.shape[0]: raise ValueError('resize with smaller n_estimators %d < %d' % (total_n_estimators, self.estimators_[0]))
if hasattr(self, 'oob_improvement_'): self.oob_improvement_.resize(total_n_estimators) else: self.oob_improvement_ = np.zeros((total_n_estimators,), dtype=np.float64)
if not self.warm_start: self._clear_state()
self._init_state()
self.init_.fit(X, y, sample_weight)
y_pred = self.init_.predict(X) begin_at_stage = 0
if presort == 'auto' and issparse(X): presort = False elif presort == 'auto': presort = True
if self.min_weight_fraction_leaf != 0. and sample_weight is not None: min_weight_leaf = (self.min_weight_fraction_leaf * np.sum(sample_weight)) else: min_weight_leaf = 0.
i = begin_at_stage for i in range(begin_at_stage, self.n_estimators):
if do_oob: sample_mask = _random_sample_mask(n_samples, n_inbag, random_state) old_oob_score = loss_(y[~sample_mask], y_pred[~sample_mask], sample_weight[~sample_mask])
y_pred = self._fit_stage(i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)
raise NotImplementedError()
score = self._init_decision_function(X) predict_stages(self.estimators_, X, self.learning_rate, score) return score
yield dec
return y
n_estimators, n_classes = self.estimators_.shape leaves = np.zeros((X.shape[0], n_estimators, n_classes))
yield dec
n_samples, n_features = X.shape max_features = ensemble.max_features
estimators = [] estimators_samples = [] estimators_features = []
if bootstrap_features: features = random_state.randint(0, n_features, max_features) else: features = sample_without_replacement(n_features, max_features, random_state=random_state)
if support_sample_weight: if sample_weight is None: curr_sample_weight = np.ones((n_samples,)) else: curr_sample_weight = sample_weight.copy()
else: if bootstrap: indices = random_state.randint(0, n_samples, max_samples) else: indices = sample_without_replacement(n_samples, max_samples, random_state=random_state)
predictions = estimator.predict(X[:, features])
X, y = check_X_y(X, y, ['csr', 'csc'])
n_samples, self.n_features_ = X.shape y = self._validate_y(y)
self._validate_estimator()
if not isinstance(max_samples, (numbers.Integral, np.integer)): max_samples = int(max_samples * X.shape[0])
self.estimators_ = [] self.estimators_samples_ = [] self.estimators_features_ = []
n_jobs, n_estimators, starts = _partition_estimators(n_more_estimators, self.n_jobs) total_n_estimators = sum(n_estimators)
if self.warm_start and len(self.estimators_) > 0: random_state.randint(MAX_INT, size=len(self.estimators_))
return column_or_1d(y, warn=True)
X = check_array(X, accept_sparse=['csr', 'csc'])
n_jobs, n_estimators, starts = _partition_estimators(self.n_estimators, self.n_jobs)
proba = sum(all_proba) / self.n_estimators
X = check_array(X, accept_sparse=['csr', 'csc'])
n_jobs, n_estimators, starts = _partition_estimators( self.n_estimators, self.n_jobs)
log_proba = all_log_proba[0]
X = check_array(X, accept_sparse=['csr', 'csc'])
n_jobs, n_estimators, starts = _partition_estimators(self.n_estimators, self.n_jobs)
decisions = sum(all_decisions) / self.n_estimators
X = check_array(X, accept_sparse=['csr', 'csc'])
n_jobs, n_estimators, starts = _partition_estimators(self.n_estimators, self.n_jobs)
y_hat = sum(all_y_hat) / self.n_estimators
X = check_array(X, accept_sparse="csc", dtype=DTYPE) y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None) if issparse(X): X.sort_indices()
n_samples, self.n_features_ = X.shape
y = np.reshape(y, (-1, 1))
self._validate_estimator()
self.estimators_ = []
random_state.randint(MAX_INT, size=len(self.estimators_))
self.estimators_.extend(trees)
if hasattr(self, "classes_") and self.n_outputs_ == 1: self.n_classes_ = self.n_classes_[0] self.classes_ = self.classes_[0]
return y, None
X = self._validate_X_predict(X)
n_jobs, _, _ = _partition_estimators(self.n_estimators, self.n_jobs)
all_proba = Parallel(n_jobs=n_jobs, verbose=self.verbose, backend="threading")( delayed(parallel_helper)(e, 'predict_proba', X, check_input=False) for e in self.estimators_)
proba = all_proba[0]
X = self._validate_X_predict(X)
n_jobs, _, _ = _partition_estimators(self.n_estimators, self.n_jobs)
all_y_hat = Parallel(n_jobs=n_jobs, verbose=self.verbose, backend="threading")( delayed(parallel_helper)(e, 'predict', X, check_input=False) for e in self.estimators_)
y_hat = sum(all_y_hat) / len(self.estimators_)
X = check_array(X, accept_sparse=['csc'], ensure_2d=False) if issparse(X): X.sort_indices()
self.base_estimator = base_estimator self.n_estimators = n_estimators self.estimator_params = estimator_params
self.estimators_ = []
n_jobs = min(_get_n_jobs(n_jobs), n_estimators)
n_estimators_per_job = (n_estimators // n_jobs) * np.ones(n_jobs, dtype=np.int) n_estimators_per_job[:n_estimators % n_jobs] += 1 starts = np.cumsum(n_estimators_per_job)
boston = datasets.load_boston() perm = rng.permutation(boston.target.size) boston.data = boston.data[perm] boston.target = boston.target[perm]
iris = datasets.load_iris() perm = rng.permutation(iris.target.size) iris.data = iris.data[perm] iris.target = iris.target[perm]
clf = GradientBoostingClassifier(loss=loss, n_estimators=10, random_state=1, presort=presort)
assert_raises(ValueError, lambda: GradientBoostingClassifier().feature_importances_)
assert_raises(ValueError, lambda X, y: GradientBoostingClassifier( loss='deviance').fit(X, y), X, [0, 0, 0, 0])
X, y = datasets.make_hastie_10_2(n_samples=12000, random_state=1)
X, y = datasets.make_friedman1(n_samples=1200, random_state=random_state, noise=1.0) X_train, y_train = X[:200], y[:200] X_test, y_test = X[200:], y[200:]
X, y = datasets.make_friedman2(n_samples=1200, random_state=random_state) X_train, y_train = X[:200], y[:200] X_test, y_test = X[200:], y[200:]
X, y = datasets.make_friedman3(n_samples=1200, random_state=random_state) X_train, y_train = X[:200], y[:200] X_test, y_test = X[200:], y[200:]
clf = GradientBoostingClassifier(n_estimators=100, random_state=1)
y_proba = clf.predict_proba(T) assert_true(np.all(y_proba >= 0.0)) assert_true(np.all(y_proba <= 1.0))
y_pred = clf.classes_.take(y_proba.argmax(axis=1), axis=0) assert_array_equal(y_pred, true_result)
clf = GradientBoostingClassifier(n_estimators=100, random_state=1) assert_raises(ValueError, clf.fit, X, y + [0, 1])
clf = GradientBoostingClassifier(n_estimators=100, random_state=1) clf.fit(X, y)
clf = GradientBoostingRegressor(n_estimators=100, random_state=1, max_features=0) assert_raises(ValueError, clf.fit, X, y)
X, y = datasets.make_hastie_10_2(n_samples=12000, random_state=1)
X, y = datasets.make_hastie_10_2(n_samples=12000, random_state=1) _, n_features = X.shape
for y in clf.staged_predict(X_test): assert_equal(y.shape, y_pred.shape)
for y_pred in clf.staged_predict(X_test): assert_equal(y_test.shape, y_pred.shape)
for staged_proba in clf.staged_predict_proba(X_test): assert_equal(y_test.shape[0], staged_proba.shape[0]) assert_equal(2, staged_proba.shape[1])
rng = np.random.RandomState(0) X = rng.uniform(size=(10, 3))
continue
clf = GradientBoostingClassifier(n_estimators=100, random_state=1)
clf = GradientBoostingClassifier(n_estimators=100, random_state=1)
assert_raises(ValueError, clf.fit, X, np.ones(len(X)))
clf_quantile = GradientBoostingRegressor(n_estimators=100, loss='quantile', max_depth=4, alpha=0.5, random_state=7)
clf = GradientBoostingClassifier(n_estimators=100, random_state=1)
clf = GradientBoostingClassifier(n_estimators=100, random_state=1)
clf = GradientBoostingClassifier(n_estimators=100, random_state=1)
assert_warns(DataConversionWarning, clf.fit, X, y_) assert_array_equal(clf.predict(T), true_result) assert_equal(100, len(clf.estimators_))
clf = GradientBoostingClassifier(n_estimators=100, random_state=1, subsample=1.0) clf.fit(X, y) assert_raises(AttributeError, lambda: clf.oob_improvement_)
assert_equal(10 + 9, n_lines)
assert_equal(100, n_lines)
X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1) for Cls in [GradientBoostingRegressor, GradientBoostingClassifier]: est = Cls(n_estimators=200, max_depth=1) est.fit(X, y)
X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1) for Cls in [GradientBoostingRegressor, GradientBoostingClassifier]: est = Cls(n_estimators=300, max_depth=1) est.fit(X, y)
assert_equal(est.estimators_[0, 0].max_depth, 1) for i in range(1, 11): assert_equal(est.estimators_[-i, 0].max_depth, 2)
X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1) for Cls in [GradientBoostingRegressor, GradientBoostingClassifier]: est = Cls(n_estimators=100, max_depth=1) est.fit(X, y)
X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1) for Cls in [GradientBoostingRegressor, GradientBoostingClassifier]: est = Cls(n_estimators=100, max_depth=1) est.fit(X, y)
assert_array_equal(est.oob_improvement_[-10:] == 0.0, np.zeros(10, dtype=np.bool))
X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)
from sklearn.tree._tree import TREE_LEAF X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1) k = 4
from sklearn.tree._tree import TREE_LEAF k = 4
X = iris.data y = np.array(iris.target) est = GradientBoostingClassifier(n_estimators=20, max_depth=1, random_state=1, init=ZeroEstimator()) est.fit(X, y)
X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1) all_estimators = [GradientBoostingRegressor, GradientBoostingClassifier]
clf = GradientBoostingClassifier(loss='exponential', n_estimators=100, random_state=1)
y_pred = clf.classes_.take(y_proba.argmax(axis=1), axis=0) assert_array_equal(y_pred, true_result)
ensemble = BaggingClassifier(base_estimator=Perceptron(), n_estimators=3)
iris = load_iris() perm = rng.permutation(iris.target.size) iris.data = iris.data[perm] iris.target = iris.target[perm]
boston = load_boston() perm = rng.permutation(boston.target.size) boston.data = boston.data[perm] boston.target = boston.target[perm]
sparse_classifier = BaggingClassifier( base_estimator=CustomSVC(decision_function_shape='ovr'), random_state=1, **params ).fit(X_train_sparse, y_train) sparse_results = getattr(sparse_classifier, f)(X_test_sparse)
rng = check_random_state(0) X_train, X_test, y_train, y_test = train_test_split(boston.data[:50], boston.target[:50], random_state=rng)
sparse_classifier = BaggingRegressor( base_estimator=CustomSVR(), random_state=1, **params ).fit(X_train_sparse, y_train) sparse_results = sparse_classifier.predict(X_test_sparse)
dense_results = BaggingRegressor( base_estimator=CustomSVR(), random_state=1, **params ).fit(X_train, y_train).predict(X_test)
rng = check_random_state(0) X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=rng)
ensemble = BaggingRegressor(base_estimator=DecisionTreeRegressor(), max_samples=1.0, bootstrap=False, random_state=rng).fit(X_train, y_train)
ensemble = BaggingRegressor(base_estimator=DecisionTreeRegressor(), max_samples=1.0, bootstrap=True, random_state=rng).fit(X_train, y_train)
rng = check_random_state(0) X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=rng)
rng = check_random_state(0) X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=rng)
ensemble = BaggingClassifier(base_estimator=DecisionTreeClassifier(), random_state=rng).fit(X_train, y_train)
ensemble = BaggingClassifier(base_estimator=LogisticRegression(), random_state=rng, max_samples=5).fit(X_train, y_train)
rng = check_random_state(0) X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=rng)
assert_warns(UserWarning, BaggingClassifier(base_estimator=base_estimator, n_estimators=1, bootstrap=True, oob_score=True, random_state=rng).fit, X_train, y_train)
rng = check_random_state(0) X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=rng)
assert_warns(UserWarning, BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=1, bootstrap=True, oob_score=True, random_state=rng).fit, X_train, y_train)
rng = check_random_state(0) X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=rng)
X, y = iris.data, iris.target base = DecisionTreeClassifier()
assert_false(hasattr(BaggingClassifier(base).fit(X, y), 'decision_function'))
rng = check_random_state(0)
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=rng)
ensemble.set_params(n_jobs=1) y1 = ensemble.predict_proba(X_test) ensemble.set_params(n_jobs=2) y2 = ensemble.predict_proba(X_test) assert_array_almost_equal(y1, y2)
ensemble = BaggingClassifier(SVC(decision_function_shape='ovr'), n_jobs=3, random_state=0).fit(X_train, y_train)
rng = check_random_state(0)
X, y = iris.data, iris.target y[y == 2] = 1
parameters = {'n_estimators': (1, 2), 'base_estimator__C': (1, 2)}
rng = check_random_state(0)
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=rng)
X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=rng)
X, y = make_hastie_10_2(n_samples=20, random_state=1)
X, y = make_hastie_10_2(n_samples=20, random_state=1) X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=43)
X_train += 1.
X, y = make_hastie_10_2(n_samples=20, random_state=1) X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=43)
X, y = make_hastie_10_2(n_samples=20, random_state=1) clf = BaggingClassifier(n_estimators=5, warm_start=True, oob_score=True) assert_raises(ValueError, clf.fit, X, y)
iris = datasets.load_iris() X, y = iris.data[:, 1:3], iris.target
rng = np.random.RandomState(0)
X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]
iris = datasets.load_iris() perm = rng.permutation(iris.target.size) iris.data, iris.target = shuffle(iris.data, iris.target, random_state=rng)
boston = datasets.load_boston() boston.data, boston.target = shuffle(boston.data, boston.target, random_state=rng)
class MockEstimator(object): def predict_proba(self, X): assert_array_equal(X.shape, probs.shape) return probs mock = MockEstimator()
assert_array_equal(np.argmin(samme_proba, axis=1), [2, 0, 0, 2]) assert_array_equal(np.argmax(samme_proba, axis=1), [0, 1, 1, 1])
clf = AdaBoostRegressor(random_state=0) clf.fit(X, y_regr) assert_array_equal(clf.predict(T), y_t_regr)
classes = np.unique(iris.target) clf_samme = prob_samme = None
clf_samme.algorithm = "SAMME.R" assert_array_less(0, np.abs(clf_samme.predict_proba(iris.data) - prob_samme))
clf = AdaBoostRegressor(random_state=0) clf.fit(boston.data, boston.target) score = clf.score(boston.data, boston.target) assert score > 0.85
rng = np.random.RandomState(0) iris_weights = rng.randint(10, size=iris.target.shape) boston_weights = rng.randint(10, size=boston.target.shape)
for alg in ['SAMME', 'SAMME.R']: clf = AdaBoostClassifier(algorithm=alg, n_estimators=10) clf.fit(iris.data, iris.target, sample_weight=iris_weights)
clf = AdaBoostRegressor(n_estimators=10, random_state=0) clf.fit(boston.data, boston.target, sample_weight=boston_weights)
import pickle
X, y = datasets.make_classification(n_samples=2000, n_features=10, n_informative=3, n_redundant=0, n_repeated=0, shuffle=False, random_state=1)
assert_raises(ValueError, AdaBoostClassifier(learning_rate=-1).fit, X, y_class)
from sklearn.ensemble import RandomForestClassifier from sklearn.svm import SVC
clf = AdaBoostClassifier(RandomForestClassifier()) clf.fit(X, y_regr)
y = np.ravel(y)
sparse_classifier = AdaBoostClassifier( base_estimator=CustomSVC(probability=True), random_state=1, algorithm="SAMME" ).fit(X_train_sparse, y_train)
dense_classifier = AdaBoostClassifier( base_estimator=CustomSVC(probability=True), random_state=1, algorithm="SAMME" ).fit(X_train, y_train)
sparse_results = sparse_classifier.predict(X_test_sparse) dense_results = dense_classifier.predict(X_test) assert_array_equal(sparse_results, dense_results)
sparse_results = sparse_classifier.decision_function(X_test_sparse) dense_results = dense_classifier.decision_function(X_test) assert_array_equal(sparse_results, dense_results)
sparse_results = sparse_classifier.predict_log_proba(X_test_sparse) dense_results = dense_classifier.predict_log_proba(X_test) assert_array_equal(sparse_results, dense_results)
sparse_results = sparse_classifier.predict_proba(X_test_sparse) dense_results = dense_classifier.predict_proba(X_test) assert_array_equal(sparse_results, dense_results)
sparse_results = sparse_classifier.score(X_test_sparse, y_test) dense_results = dense_classifier.score(X_test, y_test) assert_array_equal(sparse_results, dense_results)
sparse_results = sparse_classifier.staged_decision_function( X_test_sparse) dense_results = dense_classifier.staged_decision_function(X_test) for sprase_res, dense_res in zip(sparse_results, dense_results): assert_array_equal(sprase_res, dense_res)
sparse_results = sparse_classifier.staged_predict(X_test_sparse) dense_results = dense_classifier.staged_predict(X_test) for sprase_res, dense_res in zip(sparse_results, dense_results): assert_array_equal(sprase_res, dense_res)
sparse_results = sparse_classifier.staged_predict_proba(X_test_sparse) dense_results = dense_classifier.staged_predict_proba(X_test) for sprase_res, dense_res in zip(sparse_results, dense_results): assert_array_equal(sprase_res, dense_res)
sparse_results = sparse_classifier.staged_score(X_test_sparse, y_test) dense_results = dense_classifier.staged_score(X_test, y_test) for sprase_res, dense_res in zip(sparse_results, dense_results): assert_array_equal(sprase_res, dense_res)
types = [i.data_type_ for i in sparse_classifier.estimators_]
sparse_classifier = AdaBoostRegressor( base_estimator=CustomSVR(), random_state=1 ).fit(X_train_sparse, y_train)
dense_classifier = dense_results = AdaBoostRegressor( base_estimator=CustomSVR(), random_state=1 ).fit(X_train, y_train)
sparse_results = sparse_classifier.predict(X_test_sparse) dense_results = dense_classifier.predict(X_test) assert_array_equal(sparse_results, dense_results)
sparse_results = sparse_classifier.staged_predict(X_test_sparse) dense_results = dense_classifier.staged_predict(X_test) for sprase_res, dense_res in zip(sparse_results, dense_results): assert_array_equal(sprase_res, dense_res)
bd = BinomialDeviance(2)
est = LogOddsEstimator() assert_raises(ValueError, est.fit, None, np.array([1]))
rng = check_random_state(13) X = rng.rand(100, 2) sample_weight = np.ones(100) reg_y = rng.rand(100)
continue
assert_array_equal(out, sw_out)
p = np.zeros((y.shape[0], k), dtype=np.float64) for i in range(k): p[:, i] = y == i
iris = datasets.load_iris() rng = check_random_state(0) perm = rng.permutation(iris.target.size) iris.data = iris.data[perm] iris.target = iris.target[perm]
boston = datasets.load_boston() perm = rng.permutation(boston.target.size) boston.data = boston.data[perm] boston.target = boston.target[perm]
hastie_X, hastie_y = datasets.make_hastie_10_2(n_samples=20, random_state=1) hastie_X = hastie_X.astype(np.float32)
leaf_indices = clf.apply(X) assert_equal(leaf_indices.shape, (len(X), clf.n_estimators))
ForestClassifier = FOREST_CLASSIFIERS[name]
ForestRegressor = FOREST_REGRESSORS[name]
r = FOREST_REGRESSORS[name](random_state=0) assert_false(hasattr(r, "classes_")) assert_false(hasattr(r, "n_classes_"))
X_new = assert_warns( DeprecationWarning, est.transform, X, threshold="mean") assert_less(0 < X_new.shape[1], X.shape[1])
importances = est.feature_importances_ est.set_params(n_jobs=2) importances_parrallel = est.feature_importances_ assert_array_almost_equal(importances, importances_parrallel)
coef = 1. / (binomial(k, n_features) * (n_features - k))
for B in combinations(features, k): for b in product(*[values[B[j]] for j in range(k)]): mask_b = np.ones(n_samples, dtype=np.bool)
true_importances = np.zeros(n_features)
clf = ExtraTreesClassifier(n_estimators=500, max_features=1, criterion="entropy", random_state=0).fit(X, y)
assert_almost_equal(entropy(y), sum(importances)) assert_less(np.abs(true_importances - importances).mean(), 0.01)
yield check_oob_score, name, csc_matrix(iris.data), iris.target
yield check_oob_score, name, iris.data, iris.target * 2 + 1
yield check_oob_score, name, csc_matrix(boston.data), boston.target, 50
assert_raises(ValueError, ForestEstimator(oob_score=True, bootstrap=False).fit, X, y)
for name in FOREST_CLASSIFIERS: yield check_gridsearch, name
ForestClassifier = FOREST_CLASSIFIERS[name]
clf = ForestClassifier(random_state=0).fit(X, y)
_y = np.vstack((y, np.array(y) * 2)).T clf = ForestClassifier(random_state=0).fit(X, _y)
hasher = RandomTreesEmbedding(n_estimators=10, sparse_output=False) X, y = datasets.make_circles(factor=0.5) X_transformed = hasher.fit_transform(X)
assert_equal(type(X_transformed), np.ndarray)
assert_array_equal(X_transformed_sparse.toarray(), X_transformed_dense)
@ignore_warnings def test_random_hasher(): hasher = RandomTreesEmbedding(n_estimators=30, random_state=1) X, y = datasets.make_circles(factor=0.5) X_transformed = hasher.fit_transform(X)
hasher = RandomTreesEmbedding(n_estimators=30, random_state=1) assert_array_equal(hasher.fit(X).transform(X).toarray(), X_transformed.toarray())
X = rng.randint(0, 4, size=(1000, 1)) y = rng.rand(1000) n_trees = 500
ForestEstimator = FOREST_ESTIMATORS[name] est = ForestEstimator(max_depth=1, max_leaf_nodes=4, n_estimators=1, random_state=0).fit(X, y) assert_greater(est.estimators_[0].tree_.max_depth, 1)
ForestEstimator = FOREST_ESTIMATORS[name]
assert_raises(ValueError, ForestEstimator(min_samples_leaf=-1).fit, X, y) assert_raises(ValueError, ForestEstimator(min_samples_leaf=0).fit, X, y)
leaf_count = node_counts[node_counts != 0] assert_greater(np.min(leaf_count), 4, "Failed with {0}".format(name))
leaf_count = node_counts[node_counts != 0] assert_greater(np.min(leaf_count), len(X) * 0.25 - 1, "Failed with {0}".format(name))
ForestEstimator = FOREST_ESTIMATORS[name] rng = np.random.RandomState(0) weights = rng.rand(X.shape[0]) total_weight = np.sum(weights)
for frac in np.linspace(0, 0.5, 6): est = ForestEstimator(min_weight_fraction_leaf=frac, n_estimators=1, random_state=0) if "RandomForest" in name: est.bootstrap = False
leaf_weights = node_weights[node_weights != 0] assert_greater_equal( np.min(leaf_weights), total_weight * est.min_weight_fraction_leaf, "Failed with {0} " "min_weight_fraction_leaf={1}".format( name, est.min_weight_fraction_leaf))
X = np.asarray(iris.data, dtype=dtype) y = iris.target assert_array_equal(est.fit(X, y).predict(X), y)
X = np.ascontiguousarray(iris.data, dtype=dtype) y = iris.target assert_array_equal(est.fit(X, y).predict(X), y)
X = csr_matrix(iris.data, dtype=dtype) y = iris.target assert_array_equal(est.fit(X, y).predict(X), y)
X = csc_matrix(iris.data, dtype=dtype) y = iris.target assert_array_equal(est.fit(X, y).predict(X), y)
X = coo_matrix(iris.data, dtype=dtype) y = iris.target assert_array_equal(est.fit(X, y).predict(X), y)
ForestClassifier = FOREST_CLASSIFIERS[name]
ForestClassifier = FOREST_CLASSIFIERS[name] _y = np.vstack((y, np.array(y) * 2)).T
clf = ForestClassifier(class_weight='the larch', random_state=0) assert_raises(ValueError, clf.fit, X, y) assert_raises(ValueError, clf.fit, X, _y)
clf = ForestClassifier(class_weight='auto', warm_start=True, random_state=0) assert_warns(UserWarning, clf.fit, X, y) assert_warns(UserWarning, clf.fit, X, _y)
clf = ForestClassifier(class_weight=1, random_state=0) assert_raises(ValueError, clf.fit, X, _y)
clf = ForestClassifier(class_weight=[{-1: 0.5, 1: 1.}], random_state=0) assert_raises(ValueError, clf.fit, X, _y)
X, y = hastie_X, hastie_y ForestEstimator = FOREST_ESTIMATORS[name] clf = ForestEstimator(n_estimators=5, max_depth=1, warm_start=False, random_state=1) clf.fit(X, y)
X, y = hastie_X, hastie_y ForestEstimator = FOREST_ESTIMATORS[name] clf = ForestEstimator(n_estimators=5, max_depth=3, warm_start=True, random_state=1) clf.fit(X, y)
assert_array_equal(clf.apply(X), clf_2.apply(X))
X, y = hastie_X, hastie_y ForestEstimator = FOREST_ESTIMATORS[name] clf = ForestEstimator(n_estimators=15, max_depth=3, warm_start=False, random_state=1, bootstrap=True, oob_score=True) clf.fit(X, y)
clf_3 = ForestEstimator(n_estimators=15, max_depth=3, warm_start=True, random_state=1, bootstrap=True, oob_score=False) clf_3.fit(X, y) assert_true(not(hasattr(clf_3, 'oob_score_')))
iris = load_iris() perm = rng.permutation(iris.target.size) iris.data = iris.data[perm] iris.target = iris.target[perm]
boston = load_boston() perm = rng.permutation(boston.target.size) boston.data = boston.data[perm] boston.target = boston.target[perm]
sparse_classifier = IsolationForest( n_estimators=10, random_state=1, **params).fit(X_train_sparse) sparse_results = sparse_classifier.predict(X_test_sparse)
dense_classifier = IsolationForest( n_estimators=10, random_state=1, **params).fit(X_train) dense_results = dense_classifier.predict(X_test)
rng = check_random_state(2) X = 0.3 * rng.randn(120, 2) X_train = np.r_[X + 2, X - 2] X_train = X[:100]
clf = IsolationForest(max_samples=100, random_state=rng).fit(X_train)
y_pred = - clf.decision_function(X_test)
assert_greater(roc_auc_score(y_test, y_pred), 0.98)
X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1], [6, 3], [-4, 7]]
clf = IsolationForest(random_state=rng, contamination=0.25) clf.fit(X) decision_func = - clf.decision_function(X) pred = clf.predict(X)
assert_greater(np.min(decision_func[-2:]), np.max(decision_func[:-2])) assert_array_equal(pred, 6 * [1] + 2 * [-1])
boston = datasets.load_boston()
iris = datasets.load_iris()
clf = GradientBoostingClassifier(n_estimators=10, random_state=1) clf.fit(X, y)
assert pdp.shape == (1, 4) assert axes[0].shape[0] == 4
X_ = np.asarray(X) grid = np.unique(X_[:, 0]) pdp_2, axes = partial_dependence(clf, [0], grid=grid)
clf = GradientBoostingClassifier(n_estimators=10, random_state=1) clf.fit(iris.data, iris.target)
clf = GradientBoostingRegressor(n_estimators=10, random_state=1) clf.fit(boston.data, boston.target)
clf = GradientBoostingClassifier(n_estimators=10, random_state=1) clf.fit(X, y)
assert_raises(ValueError, partial_dependence, {}, [0], X=X)
assert_raises(ValueError, partial_dependence, GradientBoostingClassifier(), [0], X=X)
grid = np.random.rand(10, 2, 1) assert_raises(ValueError, partial_dependence, clf, [0], grid=grid)
clf = GradientBoostingRegressor(n_estimators=10, random_state=1) clf.fit(boston.data, boston.target)
fig, axs = plot_partial_dependence(clf, boston.data, ['CRIM', 'ZN', ('CRIM', 'ZN')], grid_resolution=grid_resolution, feature_names=boston.feature_names)
clf = GradientBoostingClassifier(n_estimators=10, random_state=1)
assert_raises(ValueError, plot_partial_dependence, clf, X, [0])
assert_raises(ValueError, plot_partial_dependence, {}, X, [0])
assert_raises(ValueError, plot_partial_dependence, clf, X, [-1])
assert_raises(ValueError, plot_partial_dependence, clf, X, [100])
assert_raises(ValueError, plot_partial_dependence, clf, X, ['foobar'])
assert_raises(ValueError, plot_partial_dependence, clf, X, [{'foo': 'bar'}])
clf = GradientBoostingClassifier(n_estimators=10, random_state=1) clf.fit(iris.data, iris.target)
target = iris.target_names[iris.target] clf = GradientBoostingClassifier(n_estimators=10, random_state=1) clf.fit(iris.data, target)
assert_raises(ValueError, plot_partial_dependence, clf, iris.data, [0, 1], label='foobar', grid_resolution=grid_resolution)
assert_raises(ValueError, plot_partial_dependence, clf, iris.data, [0, 1], grid_resolution=grid_resolution)
bootstrap=bootstrap, bootstrap_features=False, n_estimators=n_estimators, max_samples=max_samples, max_features=max_features, n_jobs=n_jobs, random_state=random_state, verbose=verbose)
X = check_array(X, accept_sparse=['csc'], ensure_2d=False) if issparse(X): X.sort_indices()
n_samples = X.shape[0]
X = self.estimators_[0]._validate_X_predict(X, check_input=True) n_samples = X.shape[0]
return 0.5 - scores
if self.learning_rate <= 0: raise ValueError("learning_rate must be greater than zero")
sample_weight = np.empty(X.shape[0], dtype=np.float64) sample_weight[:] = 1. / X.shape[0]
sample_weight = sample_weight / sample_weight.sum(dtype=np.float64)
if sample_weight.sum() <= 0: raise ValueError( "Attempting to fit with a non-positive " "weighted number of samples.")
self._validate_estimator()
self.estimators_ = [] self.estimator_weights_ = np.zeros(self.n_estimators, dtype=np.float64) self.estimator_errors_ = np.ones(self.n_estimators, dtype=np.float64)
sample_weight, estimator_weight, estimator_error = self._boost( iboost, X, y, sample_weight)
if sample_weight is None: break
if estimator_error == 0: break
if sample_weight_sum <= 0: break
sample_weight /= sample_weight_sum
proba[proba < np.finfo(proba.dtype).eps] = np.finfo(proba.dtype).eps log_proba = np.log(proba)
if self.algorithm not in ('SAMME', 'SAMME.R'): raise ValueError("algorithm %s is not supported" % self.algorithm)
return super(AdaBoostClassifier, self).fit(X, y, sample_weight)
incorrect = y_predict != y
estimator_error = np.mean( np.average(incorrect, weights=sample_weight, axis=0))
if estimator_error <= 0: return sample_weight, 1., 0.
estimator_weight = (-1. * self.learning_rate * (((n_classes - 1.) / n_classes) * inner1d(y_coding, np.log(y_predict_proba))))
if not iboost == self.n_estimators - 1: sample_weight *= np.exp(estimator_weight * ((sample_weight > 0) | (estimator_weight < 0)))
incorrect = y_predict != y
estimator_error = np.mean( np.average(incorrect, weights=sample_weight, axis=0))
if estimator_error <= 0: return sample_weight, 1., 0.
estimator_weight = self.learning_rate * ( np.log((1. - estimator_error) / estimator_error) + np.log(n_classes - 1.))
if not iboost == self.n_estimators - 1: sample_weight *= np.exp(estimator_weight * incorrect * ((sample_weight > 0) | (estimator_weight < 0)))
pred = sum(_samme_proba(estimator, n_classes, X) for estimator in self.estimators_)
current_pred = _samme_proba(estimator, n_classes, X)
proba = sum(_samme_proba(estimator, n_classes, X) for estimator in self.estimators_)
current_proba = _samme_proba(estimator, n_classes, X)
return super(AdaBoostRegressor, self).fit(X, y, sample_weight)
estimator.fit(X[bootstrap_idx], y[bootstrap_idx]) y_predict = estimator.predict(X)
estimator_error = (sample_weight * error_vect).sum()
return sample_weight, 1., 0.
if len(self.estimators_) > 1: self.estimators_.pop(-1) return None, None, None
estimator_weight = self.learning_rate * np.log(1. / beta)
predictions = np.array([ est.predict(X) for est in self.estimators_[:limit]]).T
sorted_idx = np.argsort(predictions, axis=1)
weight_cdf = self.estimator_weights_[sorted_idx].cumsum(axis=1) median_or_above = weight_cdf >= 0.5 * weight_cdf[:, -1][:, np.newaxis] median_idx = median_or_above.argmax(axis=1)
return predictions[np.arange(X.shape[0]), median_estimators]
import subprocess
import sklearn
config.set_options(ignore_setup_xxx_py=True, assume_default_configuration=True, delegate_options_to_subpackages=True, quiet=True)
try: from setuptools import setup except ImportError: from distutils.core import setup
generate_cython()
try: WindowsError except NameError: WindowsError = None
current_hash = get_hash_tuple(full_header_path, full_cython_path, full_gen_file_path)
hashes[clean_path(full_cython_path)] = current_hash
save_hashes(hashes, HASH_FILE)
pr_url = os.environ.get('CI_PULL_REQUEST') if not pr_url: exit("not a pull request")
msg = "no doc impacting files detected:\n" + u"\n".join(filenames) exit(msg, skip=True)
global custom_data_home custom_data_home = tempfile.mkdtemp() makedirs(join(custom_data_home, 'mldata')) globs['custom_data_home'] = custom_data_home return globs
greet = Word( alphas ) + "," + Word( alphas ) + "!"
return str(obj)
#~ asList = False
if not formatted: indent = "" nextLevelIndent = "" nl = ""
if f.__call__.func_code.co_flags & STAR_ARGS: return f numargs = f.__call__.func_code.co_argcount if hasattr(f.__call__,"im_self"): numargs -= 1
pass
pass
pass
_exprArgCache = {} def resetCache(): ParserElement._exprArgCache.clear() resetCache = staticmethod(resetCache)
self.returnString = matchString self.name = "'%s'" % self.returnString self.errmsg = "Expected " + self.name #self.myException.msg = self.errmsg
quoteChar = quoteChar.strip() if len(quoteChar) == 0: warnings.warn("quoteChar cannot be the empty string",SyntaxWarning,stacklevel=2) raise SyntaxError()
ret = ret[self.quoteCharLen:-self.endQuoteCharLen]
if self.escChar: ret = re.sub(self.escCharReplacePattern,"\g<1>",ret)
if self.escQuote: ret = ret.replace(self.escQuote, self.endQuoteChar)
else: if self.exprs: raise maxException else: raise ParseException(instring, loc, "no defined alternatives to match", self)
if adjacent: self.leaveWhitespace() self.adjacent = adjacent self.skipWhitespace = True self.joinString = joinString
tflat = _flatten(t.asList()) rep << And( [ Literal(tt) for tt in tflat ] )
return MatchFirst( [ parseElementClass(sym) for sym in symbols ] )
if not isinstance(opExpr, Optional): opExpr = Optional(opExpr) matchExpr = FollowedBy(opExpr.expr + thisExpr) + Group( opExpr + thisExpr )
cStyleComment = Regex(r"/\*(?:[^*]*\*+)+?/").setName("C style comment")
from urllib2 import Request, build_opener
from urllib.request import Request, build_opener
}
request.add_header('User-Agent', 'OpenAnything/1.0') html_content = opener.open(request).read() open(html_filename, 'wb').write(html_content)
languages_data_folder = sys.argv[1] dataset = load_files(languages_data_folder)
docs_train, docs_test, y_train, y_test = train_test_split( dataset.data, dataset.target, test_size=0.5)
print(metrics.classification_report(y_test, y_predicted, target_names=dataset.target_names))
cm = metrics.confusion_matrix(y_test, y_predicted) print(cm)
movie_reviews_data_folder = sys.argv[1] dataset = load_files(movie_reviews_data_folder, shuffle=False) print("n_samples: %d" % len(dataset.data))
docs_train, docs_test, y_train, y_test = train_test_split( dataset.data, dataset.target, test_size=0.25, random_state=None)
print(metrics.classification_report(y_test, y_predicted, target_names=dataset.target_names))
cm = metrics.confusion_matrix(y_test, y_predicted) print(cm)
languages_data_folder = sys.argv[1] dataset = load_files(languages_data_folder)
docs_train, docs_test, y_train, y_test = train_test_split( dataset.data, dataset.target, test_size=0.5)
vectorizer = TfidfVectorizer(ngram_range=(1, 3), analyzer='char', use_idf=False)
clf = Pipeline([ ('vec', vectorizer), ('clf', Perceptron()), ])
clf.fit(docs_train, y_train)
y_predicted = clf.predict(docs_test)
print(metrics.classification_report(y_test, y_predicted, target_names=dataset.target_names))
cm = metrics.confusion_matrix(y_test, y_predicted) print(cm)
movie_reviews_data_folder = sys.argv[1] dataset = load_files(movie_reviews_data_folder, shuffle=False) print("n_samples: %d" % len(dataset.data))
docs_train, docs_test, y_train, y_test = train_test_split( dataset.data, dataset.target, test_size=0.25, random_state=None)
pipeline = Pipeline([ ('vect', TfidfVectorizer(min_df=3, max_df=0.95)), ('clf', LinearSVC(C=1000)), ])
print(grid_search.grid_scores_)
y_predicted = grid_search.predict(docs_test)
print(metrics.classification_report(y_test, y_predicted, target_names=dataset.target_names))
cm = metrics.confusion_matrix(y_test, y_predicted) print(cm)
try: import gen_rst except: pass
templates_path = ['templates']
autosummary_generate = True
source_suffix = '.rst'
#source_encoding = 'utf-8'
plot_gallery = True
master_doc = 'index'
project = u('scikit-learn') copyright = u('2010 - 2016, scikit-learn developers (BSD License)')
import sklearn version = sklearn.__version__ release = sklearn.__version__
#today = '' #today_fmt = '%B %d, %Y'
#unused_docs = []
exclude_trees = ['_build', 'templates', 'includes']
#default_role = None
add_function_parentheses = False
#add_module_names = True
#show_authors = False
pygments_style = 'sphinx'
#modindex_common_prefix = []
html_theme = 'scikit-learn'
html_theme_options = {'oldversion': False, 'collapsiblesidebar': True, 'google_analytics': True, 'surveybanner': False, 'sprintbanner': True}
html_theme_path = ['themes']
#html_title = None
html_short_title = 'scikit-learn'
html_logo = 'logos/scikit-learn-logo-small.png'
html_favicon = 'logos/favicon.ico'
html_static_path = ['images']
#html_last_updated_fmt = '%b %d, %Y'
#html_use_smartypants = True
#html_sidebars = {}
#html_additional_pages = {}
html_domain_indices = False
html_use_index = False
#html_split_index = False
#html_show_sourcelink = True
#html_use_opensearch = ''
#html_file_suffix = ''
htmlhelp_basename = 'scikit-learndoc'
#latex_paper_size = 'letter'
#latex_font_size = '10pt'
latex_documents = [('index', 'user_guide.tex', u('scikit-learn user guide'), u('scikit-learn developers'), 'manual'), ]
latex_logo = "logos/scikit-learn-logo.png"
#latex_use_parts = False
#latex_appendices = []
latex_domain_indices = False
app.add_javascript('js/copybutton.js') app.connect('autodoc-process-docstring', generate_example_rst)
linkcode_resolve = make_linkcode_resolve('sklearn', u'https://github.com/scikit-learn/' 'scikit-learn/blob/{revision}/' '{package}/{path}#L{lineno}')
execfile
import matplotlib matplotlib.use('Agg')
pass
subdict_str = _select_block(dict_str[pos:], '{', '}') value = _parse_dict_recursive(subdict_str) pos_tmp = pos + len(subdict_str)
if hasattr(searchindex, 'decode'): searchindex = searchindex.decode('UTF-8')
query = 'objects:' pos = searchindex.find(query) if pos < 0: raise ValueError('"objects:" not found in search index')
sindex = get_data(searchindex_url) filenames, objects = parse_sphinx_searchindex(sindex)
comb_name = comb_name.decode('utf-8', 'replace')
link = self._get_link(cobj) self._link_cache[full_name] = link
return None
link = link.replace('\\', '/')
link = link[3:]
print(file=ex_file) print('Examples using ``%s``' % backref, file=ex_file) print('%s--' % ('-' * len(backref)), file=ex_file) print(file=ex_file)
DOCMODULES = ['sklearn', 'matplotlib', 'numpy', 'scipy']
img.thumbnail((width_sc, height_sc), Image.ANTIALIAS)
short_name = '.'.join(parts[:(i + 1)]) break
attrs.append(node.id) self.accessed_names.add('.'.join(reversed(attrs)))
self.visit(node)
full_name = self.imported_names[local_name] + remainder yield name, full_name
figure_list = []
my_stdout = my_stdout.replace( my_globals['__doc__'], '')
make_thumbnail('images/no_image.png', thumb_file, 200, 140)
return
doc_resolvers = {} doc_resolvers['sklearn'] = SphinxDocLinkResolver(app.builder.outdir, relative=True)
link_pattern = '<a href="%s">%s</a>' orig_pattern = '<span class="n">%s</span>' period = '<span class="o">.</span>'
names = sorted(str_repl, key=len, reverse=True)
app.connect('build-finished', embed_code_links)
pass
def _str_header(self, name, symbol='`'): return ['.. rubric:: ' + name, '']
out += ['.. autosummary::', ''] out += autosum
app.add_domain(NumpyPythonDomain) app.add_domain(NumpyCDomain)
from docutils.statemachine import ViewList self.content = ViewList(lines, self.content.parent)
try: from StringIO import StringIO except: from io import StringIO
class_name = class_name.encode('utf-8')
n_samples_range = np.logspace(1, 9, 9)
eps_range = np.linspace(0.01, 0.99, 100)
n_samples_range = np.logspace(2, 6, 5) colors = plt.cm.Blues(np.linspace(0.3, 1.0, len(n_samples_range)))
data = fetch_20newsgroups_vectorized().data[:500]
nonzero = dists != 0 dists = dists[nonzero]
predicted = cross_val_predict(lr, boston.data, y, cv=10)
weight = X[rows][:, cols].sum() cut = (X[row_complement][:, cols].sum() + X[rows][:, col_complement].sum()) return cut / weight
from __future__ import print_function
('subjectbody', SubjectBodyExtractor()),
('union', FeatureUnion( transformer_list=[
('subject', Pipeline([ ('selector', ItemSelector(key='subject')), ('tfidf', TfidfVectorizer(min_df=50)), ])),
('body_stats', Pipeline([ ('selector', ItemSelector(key='body')),
transformer_weights={ 'subject': 0.8, 'body_bow': 0.5, 'body_stats': 1.0, },
('svc', SVC(kernel='linear')),
X = 5 * rng.rand(10000, 1) y = np.sin(X).ravel()
y[::5] += 3 * (0.5 - rng.rand(X.shape[0]/5))
plt.figure()
plt.figure()
X = X[:, np.newaxis]
rng = np.random.RandomState(0) X, y = make_regression(n_samples=20, n_features=1, random_state=0, noise=4.0, bias=100.0)
colors = ['r-', 'b-', 'y-', 'm-']
plt.figure(figsize=(20, 6))
iris = datasets.load_iris()
y = iris.target colors = "bry"
idx = np.arange(X.shape[0]) np.random.seed(13) np.random.shuffle(idx) X = X[idx] y = y[idx]
mean = X.mean(axis=0) std = X.std(axis=0) X = (X - mean) / std
xmin, xmax = plt.xlim() ymin, ymax = plt.ylim() coef = clf.coef_ intercept = clf.intercept_
print("--- Dense matrices")
print("--- Sparse matrices")
diabetes = datasets.load_diabetes()
diabetes_X = diabetes.data[:, np.newaxis, 2]
diabetes_X_train = diabetes_X[:-20] diabetes_X_test = diabetes_X[-20:]
diabetes_y_train = diabetes.target[:-20] diabetes_y_test = diabetes.target[-20:]
regr = linear_model.LinearRegression()
regr.fit(diabetes_X_train, diabetes_y_train)
plt.scatter(diabetes_X_test, diabetes_y_test, color='black') plt.plot(diabetes_X_test, regr.predict(diabetes_X_test), color='blue', linewidth=3)
clf = ARDRegression(compute_score=True) clf.fit(X, y)
X, Y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60)
clf = SGDClassifier(loss="hinge", alpha=0.01, n_iter=200, fit_intercept=True) clf.fit(X, Y)
xx = np.linspace(-1, 5, 10) yy = np.linspace(-1, 5, 10)
X /= np.sqrt(np.sum(X ** 2, axis=0))
m_log_alphas = -np.log10(model.alphas_)
m_log_alphas = -np.log10(model.cv_alphas_)
np.random.seed(0) n_samples, n_features = 100, 100
clf = BayesianRidge(compute_score=True) clf.fit(X, y)
n_features = 501 n_relevant_features = 3 noise_level = .2 coef_min = .2 n_samples = 25 block_size = n_relevant_features
coef = np.zeros(n_features) coef[:n_relevant_features] = coef_min + rng.rand(n_relevant_features)
alpha_grid, scores_path = lasso_stability_path(X, y, random_state=42, eps=0.05)
plt.xlim(0, 100) plt.legend(loc='best') plt.title('Feature selection scores - Mutual incoherence: %.1f' % mi)
clf = linear_model.LogisticRegression(C=1e5) clf.fit(X, y)
y = (y > 4).astype(np.int)
print("training score : %.3f (%s)" % (clf.score(X, y), multi_class))
xmin, xmax = plt.xlim() ymin, ymax = plt.ylim() coef = clf.coef_ intercept = clf.intercept_
X = 1. / (np.arange(1, 11) + np.arange(0, 10)[:, np.newaxis]) y = np.ones(10)
y_noisy = y + 0.05 * np.random.randn(len(y))
np.random.seed(42)
y += 0.01 * np.random.normal((n_samples,))
n_samples = X.shape[0] X_train, y_train = X[:n_samples / 2], y[:n_samples / 2] X_test, y_test = X[n_samples / 2:], y[n_samples / 2:]
from sklearn.linear_model import Lasso
from sklearn.linear_model import ElasticNet
model = linear_model.LinearRegression() model.fit(X, y)
model_ransac = linear_model.RANSACRegressor(linear_model.LinearRegression()) model_ransac.fit(X, y) inlier_mask = model_ransac.inlier_mask_ outlier_mask = np.logical_not(inlier_mask)
line_X = np.arange(-5, 5) line_y = model.predict(line_X[:, np.newaxis]) line_y_ransac = model_ransac.predict(line_X[:, np.newaxis])
print("Estimated coefficients (true, normal, RANSAC):") print(coef, model.coef_, model_ransac.estimator_.coef_)
x_plot = np.linspace(0, 10, 100)
X = x[:, np.newaxis] X_plot = x_plot[:, np.newaxis]
iris = datasets.load_iris()
logreg.fit(X, Y)
Z = Z.reshape(xx.shape) plt.figure(1, figsize=(4, 3)) plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)
n_samples = 200
clf = GridSearchCV(clf, {'anova__percentile': [5, 10, 20]}, cv=cv)
shutil.rmtree(cachedir, ignore_errors=True)
fig = plt.figure(fignum, figsize=(4, 3)) plt.clf() ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
y_pred = KMeans(n_clusters=2, random_state=random_state).fit_predict(X)
X_varied, y_varied = make_blobs(n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state) y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_varied)
X_filtered = np.vstack((X[y == 0][:500], X[y == 1][:100], X[y == 2][:10])) y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_filtered)
n_runs = 5
n_init_range = np.array([1, 5, 10, 15, 20])
n_samples_per_center = 100 grid_size = 3 scale = 0.1 n_clusters = grid_size ** 2
n_samples = 1500 noise = 0.05 X, _ = make_swiss_roll(n_samples, noise) X[:, 1] *= .5
from sklearn.neighbors import kneighbors_graph connectivity = kneighbors_graph(X, n_neighbors=10, include_self=False)
try: face = sp.face(gray=True) except AttributeError: from scipy import misc face = misc.face(gray=True)
face = sp.misc.imresize(face, 0.10) / 255.
connectivity = grid_to_graph(*face.shape)
print("Compute structured hierarchical clustering...") st = time.time()
img = circle1 + circle2 + circle3 + circle4
mask = img.astype(bool)
graph = image.img_to_graph(img, mask=mask)
graph.data = np.exp(-graph.data / graph.data.std())
labels = spectral_clustering(graph, n_clusters=4, eigen_solver='arpack') label_im = -np.ones(mask.shape) label_im[mask] = labels
img = circle1 + circle2 mask = img.astype(bool) img = img.astype(float)
china = load_sample_image("china.jpg")
china = np.array(china, dtype=np.float64) / 255
w, h, d = original_shape = tuple(china.shape) assert d == 3 image_array = np.reshape(china, (w * h, d))
centers = [[1, 1], [-1, -1], [1, -1]] X, labels_true = make_blobs(n_samples=300, centers=centers, cluster_std=0.5, random_state=0)
af = AffinityPropagation(preference=-50).fit(X) cluster_centers_indices = af.cluster_centers_indices_ labels = af.labels_
import matplotlib.pyplot as plt from itertools import cycle
pca = PCA(n_components=n_digits).fit(data) bench_k_means(KMeans(init=pca.components_, n_clusters=n_digits, n_init=1), name="PCA-based", data=data) print(79 * '_')
Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])
centers = [[1, 1], [-1, -1], [1, -1]] X, labels_true = make_blobs(n_samples=750, centers=centers, cluster_std=0.4, random_state=0)
db = DBSCAN(eps=0.3, min_samples=10).fit(X) core_samples_mask = np.zeros_like(db.labels_, dtype=bool) core_samples_mask[db.core_sample_indices_] = True labels = db.labels_
n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
import matplotlib.pyplot as plt
n_features = 2000 t = np.pi * np.linspace(0, 1, n_features)
additional_noise[np.abs(additional_noise) < .997] = 0
try: face = sp.face(gray=True) except AttributeError: from scipy import misc face = misc.face(gray=True)
face = sp.misc.imresize(face, 0.10) / 255.
graph = image.img_to_graph(face)
beta = 5 eps = 1e-6 graph.data = np.exp(-beta * graph.data / graph.data.std()) + eps
N_REGIONS = 25
knn_graph = kneighbors_graph(X, 30, include_self=False)
X = StandardScaler().fit_transform(X)
bandwidth = cluster.estimate_bandwidth(X, quantile=0.3)
connectivity = kneighbors_graph(X, n_neighbors=10, include_self=False) connectivity = 0.5 * (connectivity + connectivity.T)
from scipy import misc face = misc.face(gray=True)
face_compressed = np.choose(labels, values) face_compressed.shape = face.shape
plt.figure(1, figsize=(3, 2.2)) plt.imshow(face, cmap=plt.cm.gray, vmin=vmin, vmax=256)
plt.figure(2, figsize=(3, 2.2)) plt.imshow(face_compressed, cmap=plt.cm.gray, vmin=vmin, vmax=vmax)
regular_values = np.linspace(0, 256, n_clusters + 1) regular_labels = np.searchsorted(regular_values, face) - 1
centers = [[1, 1], [-1, -1], [1, -1]] X, _ = make_blobs(n_samples=10000, centers=centers, cluster_std=0.6)
bandwidth = estimate_bandwidth(X, quantile=0.2, n_samples=500)
import matplotlib.pyplot as plt from itertools import cycle
print("Computing embedding") X_red = manifold.SpectralEmbedding(n_components=2).fit_transform(X) print("Done.")
X, y = make_blobs(n_samples=500, n_features=2, centers=4, cluster_std=1, center_box=(-10.0, 10.0), shuffle=True,
fig, (ax1, ax2) = plt.subplots(1, 2) fig.set_size_inches(18, 7)
clusterer = KMeans(n_clusters=n_clusters, random_state=10) cluster_labels = clusterer.fit_predict(X)
silhouette_avg = silhouette_score(X, cluster_labels) print("For n_clusters =", n_clusters, "The average silhouette_score is :", silhouette_avg)
sample_silhouette_values = silhouette_samples(X, cluster_labels)
ith_cluster_silhouette_values = \ sample_silhouette_values[cluster_labels == i]
ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
ax1.axvline(x=silhouette_avg, color="red", linestyle="--")
centers = clusterer.cluster_centers_ ax2.scatter(centers[:, 0], centers[:, 1], marker='o', c="white", alpha=1, s=200)
np.random.seed(0)
different = (mbk_means_labels == 4) ax = fig.add_subplot(1, 3, 3)
colors_ = cycle(colors.cnames.keys())
birch_models = [Birch(threshold=1.7, n_clusters=None), Birch(threshold=1.7, n_clusters=100)] final_step = ['without global clustering', 'with global clustering']
labels = birch_model.labels_ centroids = birch_model.subcluster_centers_ n_clusters = np.unique(labels).size print("n_clusters : %d" % n_clusters)
n_samples = 200 outliers_fraction = 0.25 clusters_separation = [0, 1, 2]
n_samples = 60 n_features = 20
emp_cov = np.dot(X.T, X) / n_samples
plt.figure(figsize=(10, 6)) plt.subplots_adjust(left=0.02, right=0.98)
robust_cov = MinCovDet().fit(X)
emp_cov = EmpiricalCovariance().fit(X)
fig = plt.figure() plt.subplots_adjust(hspace=-.1, wspace=.4, top=.95, bottom=.05)
coloring_matrix = np.random.normal(size=(n_features, n_features)) X_train = np.dot(base_X_train, coloring_matrix) X_test = np.dot(base_X_test, coloring_matrix)
shrinkages = np.logspace(-2, 0, 30) negative_logliks = [-ShrunkCovariance(shrinkage=s).fit(X_train).score(X_test) for s in shrinkages]
real_cov = np.dot(coloring_matrix.T, coloring_matrix) emp_cov = empirical_covariance(X_train) loglik_real = -log_likelihood(emp_cov, linalg.inv(real_cov))
tuned_parameters = [{'shrinkage': shrinkages}] cv = GridSearchCV(ShrunkCovariance(), tuned_parameters) cv.fit(X_train)
lw = LedoitWolf() loglik_lw = lw.fit(X_train).score(X_test)
oa = OAS() loglik_oa = oa.fit(X_train).score(X_test)
n_samples = 80 n_features = 5 repeat = 10
for i, n_outliers in enumerate(range_n_outliers): for j in range(repeat):
mcd = MinCovDet().fit(X) err_loc_mcd[i, j] = np.sum(mcd.location_ ** 2) err_cov_mcd[i, j] = mcd.error_norm(np.eye(n_features))
err_loc_emp_full[i, j] = np.sum(X.mean(0) ** 2) err_cov_emp_full[i, j] = EmpiricalCovariance().fit(X).error_norm( np.eye(n_features))
r = 0.1 real_cov = toeplitz(r ** np.arange(n_features)) coloring_matrix = cholesky(real_cov)
plt.scatter(X_test[:, 0], X_test[:, 1], s=80, facecolors='none', zorder=10)
std_error = scores_std / np.sqrt(n_folds)
plt.fill_between(alphas, scores + std_error, scores - std_error, alpha=0.2)
standard_scaler = StandardScaler() Xtr_s = standard_scaler.fit_transform(X_train) Xte_s = standard_scaler.transform(X_test)
from sklearn.neighbors import KNeighborsClassifier
w = clf.coef_[0] a = -w[0] / w[1]
if n_features > 1: X = np.hstack([X, np.random.randn(n_samples, n_features - 1)]) return X, y
import matplotlib.pyplot as plt
from sklearn import datasets, svm, metrics
digits = datasets.load_digits()
n_samples = len(digits.images) data = digits.images.reshape((n_samples, -1))
classifier = svm.SVC(gamma=0.001)
classifier.fit(data[:n_samples / 2], digits.target[:n_samples / 2])
expected = digits.target[n_samples / 2:] predicted = classifier.predict(data[n_samples / 2:])
Z = Z.reshape(xx.shape) ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)
fignum = 1
for kernel in ('linear', 'poly', 'rbf'): clf = svm.SVC(kernel=kernel, gamma=2) clf.fit(X, Y)
plt.figure(fignum, figsize=(4, 3)) plt.clf()
clf = svm.SVC(kernel='linear') clf.fit(X, Y)
w = clf.coef_[0] a = -w[0] / w[1] xx = np.linspace(-5, 5) yy = a * xx - (clf.intercept_[0]) / w[1]
b = clf.support_vectors_[0] yy_down = a * xx + (b[1] - a * b[0]) b = clf.support_vectors_[-1] yy_up = a * xx + (b[1] - a * b[0])
plt.plot(xx, yy, 'k-') plt.plot(xx, yy_down, 'k--') plt.plot(xx, yy_up, 'k--')
iris = datasets.load_iris()
y = iris.target
titles = ['SVC with linear kernel', 'LinearSVC (linear kernel)', 'SVC with RBF kernel', 'SVC with polynomial (degree 3) kernel']
plt.subplot(2, 2, i + 1) plt.subplots_adjust(wspace=0.4, hspace=0.4)
Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)
n_samples = 100 n_features = 300
X_1, y_1 = datasets.make_classification(n_samples=n_samples, n_features=n_features, n_informative=5, random_state=1)
plt.figure(fignum, figsize=(9, 10))
grid = GridSearchCV(clf, refit=False, param_grid=param_grid, cv=ShuffleSplit(train_size=train_size, n_iter=250, random_state=1)) grid.fit(X, y) scores = [x[1] for x in grid.grid_scores_]
xx, yy = np.meshgrid(np.linspace(-4, 5, 500), np.linspace(-4, 5, 500))
clf_weights = svm.SVC() clf_weights.fit(X, y, sample_weight=sample_weight_last_ten)
X = np.sort(5 * np.random.rand(40, 1), axis=0) y = np.sin(X).ravel()
y[::5] += 3 * (0.5 - np.random.rand(8))
fignum = 1
for name, penalty in (('unreg', 1), ('reg', 0.05)):
w = clf.coef_[0] a = -w[0] / w[1] xx = np.linspace(-5, 5) yy = a * xx - (clf.intercept_[0]) / w[1]
margin = 1 / np.sqrt(np.sum(clf.coef_ ** 2)) yy_down = yy + a * margin yy_up = yy - a * margin
Z = Z.reshape(XX.shape) plt.figure(fignum, figsize=(4, 3)) plt.pcolormesh(XX, YY, Z, cmap=plt.cm.Paired)
clf = svm.SVC(kernel='linear', C=1.0) clf.fit(X, y)
wclf = svm.SVC(kernel='linear', class_weight={1: 10}) wclf.fit(X, y)
iris = datasets.load_iris()
Y = iris.target
clf = svm.SVC(kernel=my_kernel) clf.fit(X, Y)
Z = Z.reshape(xx.shape) plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)
clf = svm.NuSVC() clf.fit(X, Y)
Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape)
Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape)
scores = [x[1] for x in grid.grid_scores_] scores = np.array(scores).reshape(len(C_range), len(gamma_range))
Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape)
score_means = list() score_stds = list() percentiles = (1, 3, 6, 10, 15, 20, 30, 40, 60, 80, 100)
this_scores = cross_val_score(clf, X, y, n_jobs=1) score_means.append(this_scores.mean()) score_stds.append(this_scores.std())
skf = StratifiedKFold(n_folds=4) train_index, test_index = next(iter(skf.split(iris.data, iris.target)))
estimators = dict((cov_type, GaussianMixture(n_components=n_classes, covariance_type=cov_type, max_iter=20, random_state=0)) for cov_type in ['spherical', 'diag', 'tied', 'full'])
estimator.means_init = np.array([X_train[y_train == i].mean(axis=0) for i in range(n_classes)])
estimator.fit(X_train)
for n, color in enumerate(colors): data = X_test[y_test == n] plt.scatter(data[:, 0], data[:, 1], marker='x', color=color)
np.random.seed(0)
shifted_gaussian = np.random.randn(n_samples, 2) + np.array([20, 20])
C = np.array([[0., -0.7], [3.5, .7]]) stretched_gaussian = np.dot(np.random.randn(n_samples, 2), C)
X_train = np.vstack([shifted_gaussian, stretched_gaussian])
clf = mixture.GaussianMixture(n_components=2, covariance_type='full') clf.fit(X_train)
if not np.any(Y_ == i): continue plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1], .8, color=color)
angle = np.arctan(u[1] / u[0])
n_samples = 500
n_samples = 500
gmm = mixture.GaussianMixture(n_components=n_components, covariance_type=cv_type) gmm.fit(X) bic.append(gmm.bic(X)) if bic[-1] < lowest_bic: lowest_bic = bic[-1] best_gmm = gmm
angle = np.arctan2(w[0][1], w[0][0])
if not np.any(Y_ == i): continue plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1], .8, color=color)
angle = np.arctan(u[1] / u[0])
n_samples = 100
np.random.seed(0) X = np.zeros((n_samples, 2)) step = 4. * np.pi / n_samples
X, y = samples_generator.make_classification( n_features=20, n_informative=3, n_redundant=0, n_classes=4, n_clusters_per_class=2)
anova_filter = SelectKBest(f_regression, k=3) clf = svm.SVC(kernel='linear')
digits = load_digits() X = digits.images.reshape((len(digits.images), -1)) y = digits.target
plt.matshow(ranking, cmap=plt.cm.Blues) plt.colorbar() plt.title("Ranking of pixels with RFE") plt.show()
iris = datasets.load_iris()
E = np.random.uniform(0, 0.1, size=(len(iris.data), 20))
X = np.hstack((iris.data, E)) y = iris.target
clf = svm.SVC(kernel='linear') clf.fit(X, y)
iris = datasets.load_iris() X = iris.data y = iris.target n_classes = np.unique(y).size
random = np.random.RandomState(seed=0) E = random.normal(size=(len(X), 2200))
X = np.c_[X, E]
X, y = make_classification(n_samples=1000, n_features=25, n_informative=3, n_redundant=2, n_repeated=0, n_classes=8, n_clusters_per_class=1, random_state=0)
svc = SVC(kernel="linear") rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(2), scoring='accuracy') rfecv.fit(X, y)
boston = load_boston() X, y = boston['data'], boston['target']
clf = LassoCV()
sfm = SelectFromModel(clf, threshold=0.25) sfm.fit(X, y) n_features = sfm.transform(X).shape[1]
while n_features > 2: sfm.threshold += 0.1 X_transform = sfm.transform(X) n_features = X_transform.shape[1]
rng = np.random.RandomState(42) S = rng.standard_t(1.5, size=(20000, 2)) S[:, 0] *= 2.
face = sp.face(gray=True)
face = face / 255
dataset = fetch_olivetti_faces(shuffle=True, random_state=rng) faces = dataset.data
faces_centered = faces - faces.mean(axis=0)
faces_centered -= faces_centered.mean(axis=1).reshape(n_samples, -1)
estimators = [ ('Eigenfaces - RandomizedPCA', decomposition.RandomizedPCA(n_components=n_components, whiten=True), True),
X_homo = X + sigma * rng.randn(n_samples, n_features)
sigmas = sigma * rng.rand(n_features) + sigma / 2. X_hetero = X + rng.randn(n_samples, n_features) * sigmas
np.random.seed(0) n_samples = 2000 time = np.linspace(0, 8, n_samples)
ica = FastICA(n_components=3)
assert np.allclose(X, np.dot(S_, A_.T) + ica.mean_)
pca = PCA(n_components=3)
y = np.linspace(0, resolution - 1, resolution) first_quarter = y < resolution / 4 y[first_quarter] = 3. y[np.logical_not(first_quarter)] = -1.
print('explained variance ratio (first two components): %s' % str(pca.explained_variance_ratio_))
import numpy as np import matplotlib.pyplot as plt from scipy.stats import norm from sklearn.neighbors import KernelDensity
ax[0, 0].hist(X[:, 0], bins=bins, fc='#AAAAFF', normed=True) ax[0, 0].text(-3.5, 0.31, "Histogram")
X_plot = np.linspace(-6, 6, 1000)[:, None] X_src = np.zeros((1, 1))
iris = datasets.load_iris()
y = iris.target
cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF']) cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])
clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights) clf.fit(X, y)
Z = Z.reshape(xx.shape) plt.figure() plt.pcolormesh(xx, yy, Z, cmap=cmap_light)
import numpy as np import matplotlib.pyplot as plt from sklearn import neighbors
y[::5] += 1 * (0.5 - np.random.rand(8))
n_neighbors = 5
n_samples_min = int(1e3) n_samples_max = int(1e5) n_features = 100 n_centers = 100 n_queries = 100 n_steps = 6 n_iter = 5
n_samples_values = np.logspace(np.log10(n_samples_min), np.log10(n_samples_max), n_steps).astype(np.int)
rng = np.random.RandomState(42) all_data, _ = make_blobs(n_samples=n_samples_max + n_queries, n_features=n_features, centers=n_centers, shuffle=True, random_state=0) queries = all_data[:n_queries] index_data = all_data[n_queries:]
average_times_exact = [] average_times_approx = [] std_times_approx = [] accuracies = [] std_accuracies = [] average_speedups = [] std_speedups = []
query = queries[[rng.randint(0, n_queries)]]
iris = datasets.load_iris()
y = iris.target
cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF']) cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])
Z = Z.reshape(xx.shape) plt.figure() plt.pcolormesh(xx, yy, Z, cmap=cmap_light)
try: from mpl_toolkits.basemap import Basemap basemap = True except ImportError: basemap = False
data = fetch_species_distributions() species_names = ['Bradypus Variegatus', 'Microryzomys Minutus']
fig = plt.figure() fig.subplots_adjust(left=0.05, right=0.95, wspace=0.05)
Z = -9999 + np.zeros(land_mask.shape[0]) Z[land_mask] = np.exp(kde.score_samples(xy)) Z = Z.reshape(X.shape)
levels = np.linspace(0, Z.max(), 25) plt.contourf(X, Y, Z, levels=levels, cmap=plt.cm.Reds)
n_samples = 10000 n_features = 100 n_queries = 30 rng = np.random.RandomState(42)
n_estimators_values = [1, 5, 10, 20, 30, 40, 50] accuracies_trees = np.zeros(len(n_estimators_values), dtype=float)
digits = load_digits() data = digits.data
pca = PCA(n_components=15, whiten=False) data = pca.fit_transform(digits.data)
params = {'bandwidth': np.logspace(-1, 1, 20)} grid = GridSearchCV(KernelDensity(), params) grid.fit(data)
kde = grid.best_estimator_
new_data = kde.sample(44, random_state=0) new_data = pca.inverse_transform(new_data)
new_data = new_data.reshape((4, 11, -1)) real_data = digits.data[:44].reshape((4, 11, -1))
n_classes = 3 plot_colors = "bry" plot_step = 0.02
iris = load_iris()
X = iris.data[:, pair] y = iris.target
clf = DecisionTreeClassifier().fit(X, y)
plt.subplot(2, 3, pairidx + 1)
node_depth = np.zeros(shape=n_nodes) is_leaves = np.zeros(shape=n_nodes, dtype=bool)
if (children_left[node_id] != children_right[node_id]): stack.append((children_left[node_id], parent_depth + 1)) stack.append((children_right[node_id], parent_depth + 1)) else: is_leaves[node_id] = True
sample_ids = [0, 1] common_nodes = (node_indicator.toarray()[sample_ids].sum(axis=0) == len(sample_ids))
import numpy as np from sklearn.tree import DecisionTreeRegressor import matplotlib.pyplot as plt
regr_1 = DecisionTreeRegressor(max_depth=2) regr_2 = DecisionTreeRegressor(max_depth=5) regr_1.fit(X, y) regr_2.fit(X, y)
X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis] y_1 = regr_1.predict(X_test) y_2 = regr_2.predict(X_test)
pred_entropies = stats.distributions.entropy( lp_model.label_distributions_.T)
uncertainty_index = uncertainty_index = np.argsort(pred_entropies)[-5:]
delete_indices = np.array([])
delete_index, = np.where(unlabeled_indices == image_index) delete_indices = np.concatenate((delete_indices, delete_index))
n_samples = 200 X, y = make_circles(n_samples=n_samples, shuffle=False) outer, inner = 0, 1 labels = -np.ones(n_samples) labels[0] = outer labels[-1] = inner
label_spread = label_propagation.LabelSpreading(kernel='knn', alpha=1.0) label_spread.fit(X, labels)
y_train = np.copy(y) y_train[unlabeled_set] = -1
lp_model = label_propagation.LabelSpreading(gamma=0.25, max_iter=5) lp_model.fit(X, y_train) predicted_labels = lp_model.transduction_[unlabeled_set] true_labels = y[unlabeled_set]
pred_entropies = stats.distributions.entropy(lp_model.label_distributions_.T)
uncertainty_index = np.argsort(pred_entropies)[-10:]
f = plt.figure(figsize=(7, 5)) for index, image_index in enumerate(uncertainty_index): image = images[image_index]
h = .02
titles = ['Label Spreading 30% data', 'Label Spreading 50% data', 'Label Spreading 100% data', 'SVC with rbf kernel']
plt.subplot(2, 2, i + 1) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, cmap=plt.cm.Paired) plt.axis('off')
colors = [color_map[y] for y in y_train] plt.scatter(X[:, 0], X[:, 1], c=colors, cmap=plt.cm.Paired)
iris = datasets.load_iris()
RANDOM_SEED = np.random.randint(2 ** 10)
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')
n_samples, h, w = lfw_people.images.shape
X = lfw_people.data n_features = X.shape[1]
y = lfw_people.target target_names = lfw_people.target_names n_classes = target_names.shape[0]
X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.25, random_state=42)
n_components = 150
return '__file__' in globals()
throughputs = benchmark_throughputs(configuration) plot_benchmark_throughput(throughputs, configuration)
np.random.seed(0)
from matplotlib.finance import quotes_historical_yahoo as quotes_historical_yahoo_ochl
d1 = datetime.datetime(2003, 1, 1) d2 = datetime.datetime(2008, 1, 1)
variation = close - open
edge_model = covariance.GraphLassoCV()
X = variation.copy().T X /= X.std(axis=0) edge_model.fit(X)
node_position_model = manifold.LocallyLinearEmbedding( n_components=2, eigen_solver='dense', n_neighbors=6)
plt.scatter(embedding[0], embedding[1], s=100 * d ** 2, c=labels, cmap=plt.cm.spectral)
for index, (name, label, (x, y)) in enumerate( zip(names, labels, embedding.T)):
rgr_ridge = Ridge(alpha=0.2) rgr_ridge.fit(proj_operator, proj.ravel()) rec_l2 = rgr_ridge.coef_.reshape(l, l)
rgr_lasso = Lasso(alpha=0.001) rgr_lasso.fit(proj_operator, proj.ravel()) rec_l1 = rgr_lasso.coef_.reshape(l, l)
import Tkinter as Tk
self.fitted = False
self.refit()
return '__file__' in globals()
vectorizer = HashingVectorizer(decode_error='ignore', n_features=2 ** 18, non_negative=True)
data_stream = stream_reuters_documents()
test_stats = {'n_test': 0, 'n_test_pos': 0}
minibatch_size = 1000
minibatch_iterators = iter_minibatches(data_stream, minibatch_size) total_vect_time = 0.0
for i, (X_train_text, y_train) in enumerate(minibatch_iterators):
cls.partial_fit(X_train, y_train, classes=all_classes)
plt.figure() fig = plt.gcf() cls_runtime = [] for cls_name, stats in sorted(cls_stats.items()): cls_runtime.append(stats['total_fit_time'])
try: from mpl_toolkits.basemap import Basemap basemap = True except ImportError: basemap = False
pts = pts[pts['species'] == species_name] bunch['pts_%s' % label] = pts
data = fetch_species_distributions()
xgrid, ygrid = construct_grids(data)
X, Y = np.meshgrid(xgrid, ygrid[::-1])
land_reference = data.coverages[6]
for i, species in enumerate([BV_bunch, MM_bunch]): print("_" * 80) print("Modeling distribution of species '%s'" % species.name)
mean = species.cov_train.mean(axis=0) std = species.cov_train.std(axis=0) train_cover_std = (species.cov_train - mean) / std
Z = np.ones((data.Ny, data.Nx), dtype=np.float64)
idx = np.where(land_reference > -9999) coverages_land = data.coverages[:, idx[0], idx[1]].T
plt.contourf(X, Y, Z, levels=levels, cmap=plt.cm.Reds) plt.colorbar(format='%.2f')
X, redirects, index_map = get_adjacency_matrix( redirects_filename, page_links_filename, limit=5000000) names = dict((i, name) for name, i in iteritems(index_map))
data = fetch_olivetti_faces() targets = data.target
n_faces = 5 rng = check_random_state(4) face_ids = rng.randint(test.shape[0], size=(n_faces, )) test = test[face_ids, :]
image_shape = (64, 64)
estimator = RandomForestRegressor(random_state=0, n_estimators=100) score = cross_val_score(estimator, X_full, y_full).mean() print("Score with the complete dataset = %.2f" % score)
import matplotlib.pyplot as plt import numpy as np from time import time
from sklearn import datasets, svm, pipeline from sklearn.kernel_approximation import (RBFSampler, Nystroem) from sklearn.decomposition import PCA
digits = datasets.load_digits(n_class=9)
n_samples = len(digits.data) data = digits.data / 16. data -= data.mean(axis=0)
data_train, targets_train = data[:n_samples / 2], digits.target[:n_samples / 2]
data_test, targets_test = data[n_samples / 2:], digits.target[n_samples / 2:] #data_test = scaler.transform(data_test)
kernel_svm = svm.SVC(gamma=.2) linear_svm = svm.LinearSVC()
plt.figure(figsize=(8, 8)) accuracy = plt.subplot(211) timescale = plt.subplot(212)
accuracy.plot([64, 64], [0.7, 1], label="n_features")
pca = PCA(n_components=8).fit(data_train)
for i, clf in enumerate((kernel_svm, nystroem_approx_svm, fourier_approx_svm)): plt.subplot(1, 3, i + 1) Z = clf.predict(flat_grid)
Z = Z.reshape(grid.shape[:-1]) plt.contourf(multiples, multiples, Z, cmap=plt.cm.Paired) plt.axis('off')
plt.scatter(X[:, 0], X[:, 1], c=targets_train, cmap=plt.cm.Paired)
pca.fit(X_digits)
for X, y in datasets: X = StandardScaler().fit_transform(X) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)
Z = Z.reshape(xx.shape) ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)
digits = datasets.load_digits() X = np.asarray(digits.data, 'float32') X, Y = nudge_dataset(X, digits.target)
logistic = linear_model.LogisticRegression() rbm = BernoulliRBM(random_state=0, verbose=True)
rbm.learning_rate = 0.06 rbm.n_iter = 20 rbm.n_components = 100 logistic.C = 6000.0
classifier.fit(X_train, Y_train)
logistic_classifier = linear_model.LogisticRegression(C=100.0) logistic_classifier.fit(X_train, Y_train)
X, y = mnist.data / 255., mnist.target X_train, X_test = X[:60000], X[60000:] y_train, y_test = y[:60000], y[60000:]
mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, alpha=1e-4, algorithm='sgd', verbose=10, tol=1e-4, random_state=1, learning_rate_init=.1)
Axes3D
n_neighbors = 10 n_samples = 1000
random_state = check_random_state(0) p = random_state.rand(n_samples) * (2 * np.pi - 0.55) t = random_state.rand(n_samples) * np.pi
fig = plt.figure(figsize=(15, 8)) plt.suptitle("Manifold Learning with %i points, %i neighbors" % (1000, n_neighbors), fontsize=14)
ax.view_init(40, -10)
methods = ['standard', 'ltsa', 'hessian', 'modified'] labels = ['LLE', 'LTSA', 'Hessian LLE', 'Modified LLE']
continue
print("Computing random projection") rp = random_projection.SparseRandomProjection(n_components=2, random_state=42) X_projected = rp.fit_transform(X) plot_embedding(X_projected, "Random Projection of the digits")
print("Computing Spectral embedding") embedder = manifold.SpectralEmbedding(n_components=2, random_state=0, eigen_solver="arpack") t0 = time() X_se = embedder.fit_transform(X)
print("Computing t-SNE embedding") tsne = manifold.TSNE(n_components=2, init='pca', random_state=0) t0 = time() X_tsne = tsne.fit_transform(X)
X_true -= X_true.mean()
clf = PCA(n_components=2) X_true = clf.fit_transform(X_true)
Axes3D
from mpl_toolkits.mplot3d import Axes3D Axes3D
lr = LogisticRegression() gnb = GaussianNB() svc = LinearSVC(C=1.0) rfc = RandomForestClassifier(n_estimators=100)
clf = RandomForestClassifier(n_estimators=25) clf.fit(X_train_valid, y_train_valid) clf_probs = clf.predict_proba(X_test) score = log_loss(y_test, clf_probs)
X_train, X_test, y_train, y_test, sw_train, sw_test = \ train_test_split(X, y, sample_weight, test_size=0.9, random_state=42)
clf = GaussianNB()
clf_isotonic = CalibratedClassifierCV(clf, cv=2, method='isotonic') clf_isotonic.fit(X_train, y_train, sw_train) prob_pos_isotonic = clf_isotonic.predict_proba(X_test)[:, 1]
clf_sigmoid = CalibratedClassifierCV(clf, cv=2, method='sigmoid') clf_sigmoid.fit(X_train, y_train, sw_train) prob_pos_sigmoid = clf_sigmoid.predict_proba(X_test)[:, 1]
X, y = datasets.make_classification(n_samples=100000, n_features=20, n_informative=2, n_redundant=10, random_state=42)
isotonic = CalibratedClassifierCV(est, cv=2, method='isotonic')
sigmoid = CalibratedClassifierCV(est, cv=2, method='sigmoid')
lr = LogisticRegression(C=1., solver='lbfgs')
plot_calibration_curve(GaussianNB(), "Naive Bayes", 1)
plot_calibration_curve(LinearSVC(), "SVC", 2)
scores = cross_val_score(pipeline, X[:, np.newaxis], y, scoring="mean_squared_error", cv=10)
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')
pipeline = Pipeline([ ('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', SGDClassifier()), ])
parameters = { 'vect__max_df': (0.5, 0.75, 1.0), #'vect__max_features': (None, 5000, 10000, 50000),
grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)
n_samples_train, n_samples_test, n_features = 75, 150, 500 np.random.seed(0) coef = np.random.randn(n_features)
X_train, X_test = X[:n_samples_train], X[n_samples_train:] y_train, y_test = y[:n_samples_train], y[n_samples_train:]
enet.set_params(alpha=alpha_optim) coef_ = enet.fit(X, y).coef_
iris = datasets.load_iris() X = iris.data y = iris.target
colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal']) lw = 2
y = label_binarize(y, classes=[0, 1, 2]) n_classes = y.shape[1]
random_state = np.random.RandomState(0) n_samples, n_features = X.shape X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=random_state)
classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True, random_state=random_state)) y_score = classifier.fit(X_train, y_train).decision_function(X_test)
digits = load_digits() X, y = digits.data, digits.target
clf = RandomForestClassifier(n_estimators=20)
n_iter_search = 20 random_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=n_iter_search)
grid_search = GridSearchCV(clf, param_grid=param_grid) start = time() grid_search.fit(X, y)
iris = datasets.load_iris() X = iris.data y = iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
classifier = svm.SVC(kernel='linear', C=0.01) y_pred = classifier.fit(X_train, y_train).predict(X_test)
cm = confusion_matrix(y_test, y_pred) np.set_printoptions(precision=2) print('Confusion matrix, without normalization') print(cm) plt.figure() plot_confusion_matrix(cm)
iris = datasets.load_iris() X = iris.data y = iris.target X, y = X[y != 2], y[y != 2] n_samples, n_features = X.shape
random_state = np.random.RandomState(0) X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]
cv = StratifiedKFold(n_folds=6) classifier = svm.SVC(kernel='linear', probability=True, random_state=random_state)
digits = datasets.load_digits()
n_samples = len(digits.images) X = digits.images.reshape((n_samples, -1)) y = digits.target
X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.5, random_state=0)
iris = datasets.load_iris() X = iris.data y = iris.target
y = label_binarize(y, classes=[0, 1, 2]) n_classes = y.shape[1]
random_state = np.random.RandomState(0) n_samples, n_features = X.shape X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=0)
classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True, random_state=random_state)) y_score = classifier.fit(X_train, y_train).decision_function(X_test)
all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))
mean_tpr = np.zeros_like(all_fpr) for i in range(n_classes): mean_tpr += interp(all_fpr, fpr[i], tpr[i])
mean_tpr /= n_classes
cv = ShuffleSplit(n_iter=100, test_size=0.2, random_state=0)
pca = PCA(n_components=2)
selection = SelectKBest(k=1)
X_features = combined_features.fit(X, y).transform(X)
lim = 8
y = np.array(g(X) > 0, dtype=int)
k3 = 0.66**2 \ * RationalQuadratic(length_scale=1.2, alpha=0.78) k4 = 0.18**2 * RBF(length_scale=0.134) \
k3 = 0.5**2 * RationalQuadratic(length_scale=1.0, alpha=1.0) k4 = 0.1**2 * RBF(length_scale=0.1) \ + WhiteKernel(noise_level=0.1**2,
iris = datasets.load_iris()
plt.subplot(1, 2, i + 1)
X = 15 * rng.rand(100, 1) y = np.sin(X).ravel()
gp = GaussianProcessRegressor(kernel=kernel)
gp_fix = GaussianProcessClassifier(kernel=1.0 * RBF(length_scale=1.0), optimizer=None) gp_fix.fit(X[:train_size], y[:train_size])
X = np.atleast_2d([1., 3., 5., 6., 7., 8.]).T
y = f(X).ravel()
x = np.atleast_2d(np.linspace(0, 10, 1000)).T
kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2)) gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)
gp.fit(X, y)
y_pred, sigma = gp.predict(x, return_std=True)
X = np.linspace(0.1, 9.9, 20) X = np.atleast_2d(X).T
gp = GaussianProcessRegressor(kernel=kernel, alpha=(dy / y) ** 2, n_restarts_optimizer=10)
gp.fit(X, y)
y_pred, sigma = gp.predict(x, return_std=True)
l1 = np.random.normal(size=n) l2 = np.random.normal(size=n)
plsca = PLSCanonical(n_components=2) plsca.fit(X_train, Y_train) X_train_r, Y_train_r = plsca.transform(X_train, Y_train) X_test_r, Y_test_r = plsca.transform(X_test, Y_test)
Y = np.dot(X, B) + np.random.normal(size=n * q).reshape((n, q)) + 5
print("Estimated B") print(np.round(pls2.coef_, 1)) pls2.predict(X)
print("Estimated betas") print(np.round(pls1.coef_, 1))
bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), algorithm="SAMME", n_estimators=200)
import numpy as np import matplotlib.pyplot as plt from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import AdaBoostRegressor
regr_1 = DecisionTreeRegressor(max_depth=4)
y_1 = regr_1.predict(X) y_2 = regr_2.predict(X)
probas = [c.fit(X, y).predict_proba(X) for c in (clf1, clf2, clf3, eclf)]
class1_1 = [pr[0, 0] for pr in probas] class2_1 = [pr[0, 1] for pr in probas]
n_jobs = 1
data = fetch_olivetti_faces() X = data.images.reshape((len(data.images), -1)) y = data.target
print("Fitting ExtraTreesClassifier on faces data with %d cores..." % n_jobs) t0 = time() forest = ExtraTreesClassifier(n_estimators=1000, max_features=128, n_jobs=n_jobs, random_state=0)
plt.matshow(importances, cmap=plt.cm.hot) plt.title("Pixel importances with forests of trees") plt.show()
test_score = np.zeros((params['n_estimators'],), dtype=np.float64)
learning_rate = 1.
discrete_estimator_errors = bdt_discrete.estimator_errors_[:n_trees_discrete] real_estimator_errors = bdt_real.estimator_errors_[:n_trees_real] discrete_estimator_weights = bdt_discrete.estimator_weights_[:n_trees_discrete]
plt.subplots_adjust(wspace=0.25) plt.show()
X = np.atleast_2d(np.random.uniform(0, 10.0, size=100)).T X = X.astype(np.float32)
y = f(X).ravel()
xx = np.atleast_2d(np.linspace(0, 10, 1000)).T xx = xx.astype(np.float32)
y_upper = clf.predict(xx)
y_lower = clf.predict(xx)
y_pred = clf.predict(xx)
iris = datasets.load_iris() X = iris.data[:, [0, 2]] y = iris.target
cv_score = cv_estimate(3)
test_score = heldout_score(clf, X_test, y_test)
cumsum = -np.cumsum(clf.oob_improvement_)
oob_best_iter = x[np.argmin(cumsum)]
test_score -= test_score[0] test_best_iter = x[np.argmin(test_score)]
cv_score -= cv_score[0] cv_best_iter = x[np.argmin(cv_score)]
y_multirf = regr_multirf.predict(X_test) y_rf = regr_rf.predict(X_test)
X_train, X_train_lr, y_train, y_train_lr = train_test_split(X_train, y_train, test_size=0.5)
rt = RandomTreesEmbedding(max_depth=3, n_estimators=n_estimator, random_state=0)
y_pred_grd = grd.predict_proba(X_test)[:, 1] fpr_grd, tpr_grd, _ = roc_curve(y_test, y_pred_grd)
y_pred_rf = rf.predict_proba(X_test)[:, 1] fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_rf)
n_classes = 3 n_estimators = 30 plot_colors = "ryb" cmap = plt.cm.RdYlBu
iris = load_iris()
X = iris.data[:, pair] y = iris.target
idx = np.arange(X.shape[0]) np.random.seed(RANDOM_SEED) np.random.shuffle(idx) X = X[idx] y = y[idx]
mean = X.mean(axis=0) std = X.std(axis=0) X = (X - mean) / std
clf = clone(model) clf = model.fit(X, y)
plt.title(model_title)
X, y = make_classification(n_samples=1000, n_features=10, n_informative=3, n_redundant=0, n_repeated=0, n_classes=2, random_state=0, shuffle=False)
forest = ExtraTreesClassifier(n_estimators=250, random_state=0)
print("Feature ranking:")
X, y = make_circles(factor=0.5, random_state=0, noise=0.05)
hasher = RandomTreesEmbedding(n_estimators=10, random_state=0, max_depth=3) X_transformed = hasher.fit_transform(X)
pca = TruncatedSVD(n_components=2) X_reduced = pca.fit_transform(X_transformed)
nb = BernoulliNB() nb.fit(X_transformed, y)
trees = ExtraTreesClassifier(max_depth=3, n_estimators=10, random_state=0) trees.fit(X, y)
fig = plt.figure(figsize=(9, 8))
transformed_grid = hasher.transform(np.c_[xx.ravel(), yy.ravel()]) y_grid_pred = nb.predict_proba(transformed_grid)[:, 1]
y_grid_pred = trees.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]
X_train, X_test, y_train, y_test = train_test_split(cal_housing.data, cal_housing.target, test_size=0.2, random_state=1) names = cal_housing.feature_names
if __name__ == '__main__': main()
labels, y = np.unique(y, return_inverse=True)
test_deviance = np.zeros((params['n_estimators'],), dtype=np.float64)
test_deviance[i] = clf.loss_(y_test, y_pred)
clf = IsolationForest(max_samples=100, random_state=rng) clf.fit(X_train) y_pred_train = clf.predict(X_train) y_pred_test = clf.predict(X_test) y_pred_outliers = clf.predict(X_outliers)
estimators = [("Tree", DecisionTreeRegressor()), ("Bagging(Tree)", BaggingRegressor(DecisionTreeRegressor()))]
def f(x): x = x.ravel()
for n, (name, estimator) in enumerate(estimators): y_predict = np.zeros((n_test, n_repeat))
y_error = np.zeros(n_test)
X, y = make_classification(n_samples=500, n_features=25, n_clusters_per_class=1, n_informative=15, random_state=RANDOM_STATE)
error_rate = OrderedDict((label, []) for label, _ in ensemble_clfs)
min_estimators = 15 max_estimators = 175
oob_error = 1 - clf.oob_score_ error_rate[label].append((i, oob_error))
for label, clf_err in error_rate.items(): xs, ys = zip(*clf_err) plt.plot(xs, ys, label=label)
#categories = None
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')
hasher = HashingVectorizer(n_features=opts.n_features, stop_words='english', non_negative=True, norm=None, binary=False) vectorizer = make_pipeline(hasher, TfidfTransformer())
svd = TruncatedSVD(opts.n_components) normalizer = Normalizer(copy=False) lsa = make_pipeline(svd, normalizer)
plt.matshow(cm) plt.title('Confusion matrix of the %s classifier' % name) plt.colorbar()
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')
if opts.all_categories: categories = None else: categories = [ 'alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space', ]
y_train, y_test = data_train.target, data_test.target
if opts.use_hashing: feature_names = None else: feature_names = vectorizer.get_feature_names()
feature_names = [feature_names[i] for i in ch2.get_support(indices=True)]
results.append(benchmark(LinearSVC(loss='l2', penalty=penalty, dual=False, tol=1e-3)))
results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50, penalty=penalty)))
print('=' * 80) print("Elastic-Net penalty") results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50, penalty="elasticnet")))
print('=' * 80) print("NearestCentroid (aka Rocchio classifier)") results.append(benchmark(NearestCentroid()))
C = 1. fit_intercept = True tol = 1.0e-14
max_squared_sum = get_max_squared_sum(X) step_size = get_auto_step_size(max_squared_sum, alpha, "log", fit_intercept)
n = 23149 X_test = X[:n, :] y_test = y[:n] X = X[n:, :] y = y[n:]
scikit_classifier_results = [] scikit_regressor_results = []
tstart = datetime.now() clf = DecisionTreeClassifier() clf.fit(X, Y).predict(X) delta = (datetime.now() - tstart)
tstart = datetime.now() clf = DecisionTreeRegressor() clf.fit(X, Y).predict(X) delta = (datetime.now() - tstart)
memory = Memory(os.path.join(get_data_home(), 'covertype_benchmark_data'), mmap_mode='r')
eps = 1e-5 n, m = V.shape W, H = _initialize_nmf(V, r, init, random_state=0)
ax.plot_surface(X, Y, Z, rstride=8, cstride=8, alpha=0.3, color=c) ax.plot([1], [1], [1], color=c, label=label)
t_start = datetime.now() clf.fit(X) delta = (datetime.now() - t_start) time_to_fit = compute_time(t_start, delta)
t_start = datetime.now() clf.transform(X) delta = (datetime.now() - t_start) time_to_transform = compute_time(t_start, delta)
op = optparse.OptionParser() op.add_option("--n-times", dest="n_times", default=5, type=int, help="Benchmark results are average over n_times experiments")
n_nonzeros = int(opts.ratio_nonzeros * opts.n_features)
transformers = {}
gaussian_matrix_params = { "n_components": opts.n_components, "random_state": opts.random_seed } transformers["GaussianRandomProjection"] = \ GaussianRandomProjection(**gaussian_matrix_params)
sparse_matrix_params = { "n_components": opts.n_components, "random_state": opts.random_seed, "density": opts.density, "eps": opts.eps, }
time_fit = collections.defaultdict(list) time_transform = collections.defaultdict(list)
idx = np.arange(n_train) np.random.seed(13) np.random.shuffle(idx) X_train = X_train[idx] y_train = y_train[idx]
y += 0.01 * np.random.normal((n_samples,))
return X[:n_samples], X[n_samples:]
n_samples = [int(1e3), int(1e4), int(1e5), int(1e6)] n_features = int(1e2) n_queries = 100 n_neighbors = 10
plt.figure() plt.legend(legend_rects, legend_labels, loc='upper left')
plt.figure() plt.legend(legend_rects, legend_labels, loc='upper left')
enable_spectral_norm = False
MAX_MEMORY = np.int(2e9)
CIFAR_FOLDER = "./cifar-10-batches-py/" SVHN_FOLDER = "./SVHN/"
U, mu, V = fbpca.pca(X, n_comps, raw=True, n_iter=n_iter, l=n_oversamples+n_comps) call_time = time() - t0
value = sp.sparse.linalg.svds(A, k=1, return_singular_vectors=False)
if not args.show_plot: print(n, np.mean(time_per_iteration))
ax.plot_surface(X, Y, Z.T, cstride=1, rstride=1, color=c, alpha=0.8)
#ax.plot([1], [1], [1], color=c, label=label)
plot_batch_errors(all_errors, n_components, batch_sizes, data)
X = faces.data[:5000] n_samples, h, w = faces.images.shape n_features = X.shape[1]
memory = Memory(os.path.join(get_data_home(), 'mnist_benchmark_data'), mmap_mode='r')
X = X / 255
tstart = time() clf = factory(alpha=alpha).fit(X, Y) delta = (time() - tstart)
import matplotlib.pyplot as plt
ax.plot_surface(X, Y, Z, rstride=8, cstride=8, alpha=0.3, color=c) ax.plot([1], [1], [1], color=c, label=label)
import time
s = (y != 4) X = X[s, :] y = y[s] y = (y != 1).astype(int)
s = (y == 2) + (y == 4) X = X[s, :] y = y[s] y = (y != 2).astype(int)
t_start = datetime.now() sampling(n_population, n_samples) delta = (datetime.now() - t_start) time = compute_time(t_start, delta) return time
op = optparse.OptionParser() op.add_option("--n-times", dest="n_times", default=5, type=int, help="Benchmark results are average over n_times experiments")
sampling_algorithm = {}
sampling_algorithm["python-core-sample"] = \ lambda n_population, n_sample: \ random.sample(xrange(n_population), n_sample)
sampling_algorithm["custom-auto"] = \ lambda n_population, n_samples, random_state=None: \ sample_without_replacement(n_population, n_samples, method="auto", random_state=random_state)
sampling_algorithm["custom-tracking-selection"] = \ lambda n_population, n_samples, random_state=None: \ sample_without_replacement(n_population, n_samples, method="tracking_selection", random_state=random_state)
sampling_algorithm["custom-reservoir-sampling"] = \ lambda n_population, n_samples, random_state=None: \ sample_without_replacement(n_population, n_samples, method="reservoir_sampling", random_state=random_state)
sampling_algorithm["custom-pool"] = \ lambda n_population, n_samples, random_state=None: \ sample_without_replacement(n_population, n_samples, method="pool", random_state=random_state)
sampling_algorithm["numpy-permutation"] = \ lambda n_population, n_sample: \ np.random.permutation(n_population)[:n_sample]
sampling_algorithm = dict((key, value) for key, value in sampling_algorithm.items() if key in selected_algorithm)
time = {} n_samples = np.linspace(start=0, stop=opts.n_population, num=opts.n_steps).astype(np.int)
col = np.argmax(self.marked[row] == 2) if self.marked[row, col] != 2: col = -1 return col
self.row_uncovered[:] = True self.col_uncovered[:] = True
return (hasattr(x, '__len__') or hasattr(x, 'shape') or hasattr(x, '__array__'))
return parameter in signature(estimator.fit).parameters
return getattr(obj, methodname)(*args, **kwargs)
if isinstance(estimator, DBSCAN): return if "random_state" in estimator.get_params(): estimator.set_params(random_state=random_state)
if os.environ.get('TRAVIS') == "true": raise SkipTest("This test needs to be skipped on Travis")
return struct.calcsize('P') * 8 == 32
return (p.name != 'self' and p.kind != p.VAR_KEYWORD and p.kind != p.VAR_POSITIONAL)
x = _ravel(x) return np.dot(x, x)
sign, ld = np.linalg.slogdet(A) if not sign > 0: return -np.inf return ld
pass
self.extra = extra
def __init__(self, msg, eigenvalues, eigenvectors): ArpackError.__init__(self, -1, {-1: msg}) self.eigenvalues = eigenvalues self.eigenvectors = eigenvectors
input_type = X.format if sp.issparse(X) else type(X) err = "Expected a CSR or CSC sparse matrix, got %s." % input_type raise TypeError(err)
if rank < n_negative: return data[rank] if rank - n_negative < n_zeros: return 0 return data[rank - n_zeros]
jll = self._joint_log_likelihood(X) return self.classes_[np.argmax(jll, axis=1)]
check_is_fitted(self, "classes_") X = check_array(X, accept_sparse='csr') return (safe_sparse_dot(X, self.feature_log_prob_.T) + self.class_log_prior_)
random_state = check_random_state(self.random_state) return gaussian_random_matrix(n_components, n_features, random_state=random_state)
func.__doc__ = doc
__import__(name) return sys.modules[name]
setattr(_MovedItems, move.name, move)
try: delattr(_MovedItems, name) except AttributeError: try: del moves.__dict__[name] except KeyError: raise AttributeError("no such move, %r" % (name,))
return iter(getattr(d, _iterkeys)(**kw))
return iter(getattr(d, _itervalues)(**kw))
return iter(getattr(d, _iteritems)(**kw))
return iter(getattr(d, _iterlists)(**kw))
return meta("NewBase", bases, {})
if sys.platform.startswith('win'): return max(0, t - .1) else: return t
def __init__(self, *args): self.args = args
return _get_backing_memmap(a) is not None
try: if os.path.exists(folder_path): shutil.rmtree(folder_path) except WindowsError: warnings.warn("Failed to clean temporary folder: %s" % folder_path)
modules, funcname = get_func_name(func) modules.append(funcname) return os.path.join(*modules)
return _load_output(self._output_dir, _get_func_fullname(self.func), timestamp=self.timestamp, metadata=self.metadata, mmap_mode=self.mmap_mode, verbose=self.verbose)
shutil.rmtree(self._output_dir, ignore_errors=True)
func_code_h = hash(getattr(self.func, '__code__', None)) return id(self.func), hash(self.func), func_code_h
if not self.dispatch_one_batch(self._original_iterator): self._iterating = False self._original_iterator = None
return 1 - (0.5 ** (1 / n_subsamples) * (n_samples - n_subsamples + 1) + n_subsamples - 1) / n_samples
raise NotImplementedError
X, _, X_offset, _, X_scale = _preprocess_data(X, y, fit_intercept, normalize=normalize) return X, y, X_offset, y, X_scale
return sparse.csr_matrix(self.coef_)
return self._decision_function(X)
return self._decision_function(X)
return self._decision_function(X)
check_is_fitted(self, 'estimator_') return self.estimator_.predict(X)
return self._decision_function(X)
return self._decision_function(X)
return self._residues
factory_class = SparseSGDClassifier
estimator.partial_fit(X, y, np.array((0, 1))) return estimator
if (not hasattr(estimator, "decision_function") and not hasattr(estimator, "predict_proba")): raise ValueError("The base estimator should implement " "decision_function or predict_proba!")
return self.label_binarizer_.y_type_.startswith('multilabel')
cond = np.logical_or(y == i, y == j) y = y[cond] y_binary = np.zeros_like(y) y_binary[y == j] = 1 return _partial_fit_binary(estimator, X[cond], y_binary)
if sp.issparse(X): variances = mean_variance_axis(X, axis=0)[1] else: variances = np.var(X, axis=0) return np.mean(variances) * tol
return self.fit(X).labels_
return euclidean_distances(X, self.cluster_centers_)
check_is_fitted(self, 'cluster_centers_') X = self._check_test_data(X) x_squared_norms = row_norms(X, squared=True) return -_labels_inertia(X, x_squared_norms, self.cluster_centers_)[1]
ind = self.subclusters_.index(subcluster) self.subclusters_[ind] = new_subcluster1 self.init_centroids_[ind] = new_subcluster1.centroid_ self.init_sq_norm_[ind] = new_subcluster1.sq_norm_ self.append_subcluster(new_subcluster2)
dot_product = -2 * np.dot(self.linear_sum_, self.centroid_) return sqrt( ((self.squared_sum_ + dot_product) / self.n_samples_) + self.sq_norm_)
self.fit_, self.partial_fit_ = True, False return self._fit(X)
leaf_ptr = self.dummy_leaf_.next_leaf_ leaves = [] while leaf_ptr is not None: leaves.append(leaf_ptr) leaf_ptr = leaf_ptr.next_leaf_ return leaves
_do_scale_test(scaled) assert_almost_equal(scaled.sum(axis=0).mean(), scaled.sum(axis=1).mean(), decimal=1)
current_leaf = birch_instance.dummy_leaf_.next_leaf_ while current_leaf: subclusters = current_leaf.subclusters_ for sc in subclusters: assert_greater_equal(threshold, sc.radius) current_leaf = current_leaf.next_leaf_
X = check_array(X, accept_sparse='csr', dtype=np.float64) self._check_parameters() self._fit(X)
X = check_array(X, accept_sparse=['csr', 'csc', 'coo'], ensure_min_features=2, estimator=self) return AgglomerativeClustering.fit(self, X.T, **params)
if self.store_precision: precision = self.precision_ else: precision = pinvh(self.covariance_) return precision
return self.transform(T)
y = column_or_1d(y, warn=True) _check_numpy_unicode_bug(y) self.classes_ = np.unique(y) return self
y = column_or_1d(y, warn=True) _check_numpy_unicode_bug(y) self.classes_, y = np.unique(y, return_inverse=True) return y
if value_to_mask == "NaN" or np.isnan(value_to_mask): return np.isnan(X) else: return X == value_to_mask
X = check_array(X, accept_sparse='csr') return self
copy = copy if copy is not None else self.copy X = check_array(X, accept_sparse='csr') return normalize(X, norm=self.norm, axis=1, copy=copy)
check_array(X, accept_sparse='csr') return self
copy = copy if copy is not None else self.copy return binarize(X, threshold=self.threshold, copy=copy)
K = check_array(K, dtype=FLOAT_DTYPES) n_samples = K.shape[0] self.K_fit_rows_ = np.sum(K, axis=0) / n_samples self.K_fit_all_ = self.K_fit_rows_.sum() / n_samples return self
self.fit_transform(X) return self
return _transform_selected(X, self._fit_transform, self.categorical_features, copy=True)
check_is_fitted(self, ["classes_", "calibrated_classifiers_"]) return self.classes_[np.argmax(self.predict_proba(X), axis=1)]
T = column_or_1d(T) return 1. / (1. + np.exp(self.a_ * T + self.b_))
return self.n_iter
return self.best_estimator_.predict(X)
return self.best_estimator_.predict_proba(X)
return self.best_estimator_.predict_log_proba(X)
return self.best_estimator_.decision_function(X)
return self.best_estimator_.transform(X)
return self.best_estimator_.transform(Xt)
covars = np.asarray(covars) _validate_covars(covars, self.covariance_type, self.n_components) self.covars_ = covars
logprob, responsibilities = self.score_samples(X) return responsibilities.argmax(axis=1)
return (-2 * self.score(X).sum() + self._n_parameters() * np.log(X.shape[0]))
return - 2 * self.score(X).sum() + 2 * self._n_parameters()
cv = np.tile(covars, (means.shape[0], 1, 1)) return _log_multivariate_normal_density_full(X, means, cv)
avg_X2 = np.dot(responsibilities.T, X * X) * norm avg_means2 = gmm.means_ ** 2 avg_X_means = gmm.means_ * weighted_X_sum * norm return avg_X2 - 2 * avg_X_means + avg_means2 + min_covar
cv = _covar_mstep_diag(*args) return np.tile(cv.mean(axis=1)[:, np.newaxis], (1, cv.shape[1]))
if np.any(np.less_equal(precision, 0.0)): raise ValueError("'%s precision' should be " "positive" % covariance_type)
for k, prec in enumerate(precisions): prec = _check_precision_matrix(prec, covariance_type)
return (-2 * self.score(X) * X.shape[0] + self._n_parameters() * np.log(X.shape[0]))
pass
pass
pass
pass
return self._estimate_log_prob(X) + self._estimate_log_weights()
pass
pass
q = (cdist(x, mu[np.newaxis], "mahalanobis", VI=A) ** 2).reshape(-1) return q
for test_index in self._iter_test_indices(): test_mask = self._empty_mask() test_mask[test_index] = True yield test_mask
raise NotImplementedError
if len(locs) != n: return False hit = np.zeros(n, bool) hit[locs] = True if not np.all(hit): return False return True
avg_score = [] for train, test in cv: estimator.fit(X[train], y[train]) avg_score.append(scorer(estimator, X[test], y[test])) return np.mean(avg_score)
return SelectKBest(chi2, k=k)
est = RandomForestClassifier() transformer = SelectFromModel(estimator=est) transformer.fit(data, y) assert_true(transformer.estimator is est)
X = check_array(X, accept_sparse='csr') check_non_negative(X, whom) return X
X = self._check_non_neg_array(X, "LatentDirichletAllocation.score") doc_topic_distr = self.transform(X) score = self._approx_bound(X, doc_topic_distr, sub_sampling=False) return score
return sqrt(squared_norm(x))
return np.dot(X.ravel(), Y.ravel())
sqrt_n = np.sqrt(len(x)) return (sqrt_n - np.linalg.norm(x, 1) / norm(x)) / (sqrt_n - 1)
check_is_fitted(self, 'n_components_') return np.dot(W, self.components_)
self._fit(X) return self
self._fit(X, compute_sources=False) return self
return self
self.fit_transform(X) return self
X = check_array(X, accept_sparse='csr') return safe_sparse_dot(X, self.components_.T)
if include_self: query = X._fit_X else: query = None return query
return np.sum(self.score_samples(X))
left_index = np.searchsorted(tree, bin_X & left_mask) right_index = np.searchsorted(tree, bin_X | right_mask, side='right') return left_index, right_index
def __init__(self, n_components=8, random_state=None): super(GaussianRandomProjectionHash, self).__init__( n_components=n_components, random_state=random_state)
out = np.empty(len(list_of_arrays), dtype=object) out[:] = list_of_arrays return out
return d.iteritems() if hasattr(d, "iteritems") else d.items()
return self
nkfd_form = unicodedata.normalize('NFKD', s) return nkfd_form.encode('ASCII', 'ignore').decode('ASCII')
return re.compile(r"<([^>]+)>", flags=re.UNICODE).sub(" ", s)
if self.tokenizer is not None: return self.tokenizer token_pattern = re.compile(self.token_pattern) return lambda doc: token_pattern.findall(doc)
return _check_stop_list(self.stop_words)
return self
if sp.isspmatrix_csr(X): return bincount(X.indices, minlength=X.shape[1]) else: return np.diff(sp.csc_matrix(X, copy=False).indptr)
self.fit_transform(raw_documents) return self
self._check_vocabulary() return [t for t, i in sorted(six.iteritems(self.vocabulary_), key=itemgetter(1))]
return array.array(str("i"))
X = super(TfidfVectorizer, self).fit_transform(raw_documents) self._tfidf.fit(X) return self
return self.feature_names_
Xt = X for name, transform in self.steps[:-1]: Xt = transform.transform(Xt) return self.steps[-1][-1].predict(Xt)
Xt = X for name, transform in self.steps[:-1]: Xt = transform.transform(Xt) return self.steps[-1][-1].predict_proba(Xt)
Xt = X for name, transform in self.steps[:-1]: Xt = transform.transform(Xt) return self.steps[-1][-1].decision_function(Xt)
Xt = X for name, transform in self.steps[:-1]: Xt = transform.transform(Xt) return self.steps[-1][-1].predict_log_proba(Xt)
Xt = X for name, transform in self.steps: Xt = transform.transform(Xt) return Xt
transformers = Parallel(n_jobs=self.n_jobs)( delayed(_fit_one_transformer)(trans, X, y) for name, trans in self.transformer_list) self._update_transformer_list(transformers) return self
return np.abs(y_truth - y_prediction).sum() > tol
probas = self.predict_proba(X) return self.classes_[np.argmax(probas, axis=1)].ravel()
scaled = face - face.min() scaled /= scaled.max() return scaled
return fetch_lfw_people(download_if_missing=download_if_missing, **kwargs)
dataname = dataname.lower().replace(' ', '-') return re.sub(r'[().]', '', dataname)
_before, _blankline, after = text.partition('\n\n') return after
good_lines = [line for line in text.split('\n') if not _QUOTE_RE.search(line)] return '\n'.join(good_lines)
data_home = get_data_home(data_home) shutil.rmtree(data_home)
for path in [DATA_HOME, LOAD_FILES_ROOT]: _remove_dir(path)
if os.path.isdir(SCIKIT_LEARN_DATA): shutil.rmtree(SCIKIT_LEARN_DATA) if os.path.isdir(SCIKIT_LEARN_EMPTY_DATA): shutil.rmtree(SCIKIT_LEARN_EMPTY_DATA)
return self.rows_, self.columns_
indices = self.get_indices(i) return tuple(len(i) for i in indices)
from .utils.validation import check_array data = check_array(data, accept_sparse='csr') row_ind, col_ind = self.get_indices(i) return data[row_ind[:, np.newaxis], col_ind]
pass
return getattr(estimator, "_estimator_type", None) == "classifier"
return ""
return estimator.score(*args, **kwargs)
pass
def fit(self, X, y): return self
def fit(self, X, y): return self def score(self, X, y): return 1.0
def fit(self, X, y): self.y = y return self def predict(self, X): return self.y
def __call__(self, est, X, y): return 1
precision recall f1-score support a 0.83 0.79 0.81 24 b 0.33 0.10 0.15 31 c 0.42 0.90 0.57 20
return np.log(self.predict_proba(X))
d = self._decision_function(X) y_pred = self.classes_.take(d.argmax(1)) return y_pred
self._fit_transform(X) return self
self._fit_transform(X) return self.embedding_
return self.n_iter
return self.best_estimator_.predict(X)
return self.best_estimator_.predict_proba(X)
return self.best_estimator_.predict_log_proba(X)
return self.best_estimator_.decision_function(X)
return self.best_estimator_.transform(X)
return self.best_estimator_.transform(Xt)
for test_index in self._iter_test_indices(X, y, labels): test_mask = np.zeros(_num_samples(X), dtype=np.bool) test_mask[test_index] = True yield test_mask
raise NotImplementedError
def __repr__(self): return _build_repr(self)
@property def score(self): raise AttributeError
if len(indices) != n_samples: return False hit = np.zeros(n_samples, bool) hit[indices] = True if not np.all(hit): return False return True
clf = GaussianNB(priors=np.array([-1., 2.])) assert_raises(ValueError, clf.fit, X, y)
def __init__(self, *vargs): pass
@property def score(self): raise AttributeError
return logistic_sigmoid(X, out=X)
return np.tanh(X, out=X)
np.clip(X, 0, np.finfo(X.dtype).max, out=X) return X
tmp = X - X.max(axis=1)[:, np.newaxis] np.exp(tmp, out=X) X /= X.sum(axis=1)[:, np.newaxis] return X
p = safe_sparse_dot(v, self.components_.T) p += self.intercept_hidden_ return expit(p, out=p)
updates = self._get_updates(grads) for param, update in zip(self.params, updates): param += update
pass
if verbose: print(msg + " Stopping.") return True
if self.lr_schedule == 'invscaling': self.learning_rate = (float(self.learning_rate_init) / (time_step + 1) ** self.power_t)
return np.hstack([l.ravel() for l in coefs_ + intercepts_])
coef_grads[layer] = safe_sparse_dot(activations[layer].T, deltas[layer]) coef_grads[layer] += (self.alpha * self.coefs_[layer]) coef_grads[layer] /= n_samples intercept_grads[layer] = np.mean(deltas[layer], 0) return coef_grads, intercept_grads
check_is_fitted(self, "coefs_") y_scores = self.decision_function(X) y_scores = ACTIVATIONS[self.out_activation_](y_scores) return self.label_binarizer_.inverse_transform(y_scores)
check_is_fitted(self, "coefs_") y_pred = self._decision_scores(X) if y_pred.shape[1] == 1: return y_pred.ravel() return y_pred
check_is_fitted(self, ["classes_", "n_classes_"]) X = check_array(X) return self.base_estimator_.predict(X)
cloned = clone(self) cloned.theta = theta return cloned
return self.theta.shape[0]
r = [] for attr, value in sorted(self.__dict__.items()): if attr.startswith("hyperparameter_"): r.append(value) return r
return True
return dict(kernels=self.kernels)
k_dims = self.k1.n_dims for i, kernel in enumerate(self.kernels): kernel.theta = theta[i * k_dims:(i + 1) * k_dims]
return np.vstack([kernel.bounds for kernel in self.kernels])
return np.all([kernel.is_stationary() for kernel in self.kernels])
k1_dims = self.k1.n_dims self.k1.theta = theta[:k1_dims] self.k2.theta = theta[k1_dims:]
return self.k1.is_stationary() and self.k2.is_stationary()
self.kernel.theta = theta
return self.kernel.bounds
return self.kernel.is_stationary()
return False
return self.metric in ["rbf"]
for kernel in kernels: K_call_diag = np.diag(kernel(X)) K_diag = kernel.diag(X) assert_almost_equal(K_call_diag, K_diag, 5)
for kernel in kernels: if not kernel.is_stationary(): continue K = kernel(X, X + 1) assert_almost_equal(K[0, 0], np.diag(K))
for kernel in kernels: gpc = GaussianProcessClassifier(kernel=kernel).fit(X, y) assert_almost_equal(gpc.log_marginal_likelihood(gpc.kernel_.theta), gpc.log_marginal_likelihood(), 7)
for kernel in kernels: gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y) assert_equal(gpr.log_marginal_likelihood(gpr.kernel_.theta), gpr.log_marginal_likelihood())
scale = 0.5
raise NotImplementedError()
pred = pred.ravel() return 2.0 * (y - pred > 0.0) - 1.0
return y - expit(pred.ravel())
return y - np.nan_to_num(np.exp(pred[:, k] - logsumexp(pred, axis=1)))
if self.estimators_ is None or len(self.estimators_) == 0: raise NotFittedError("Estimator not fitted, call `fit`" " before making predictions`.")
score = self.decision_function(X) decisions = self.loss_._score_to_decision(score) return self.classes_.take(decisions, axis=0)
X = check_array(X, dtype=DTYPE, order="C") return self._decision_function(X).ravel()
return np.asarray([clf.predict_proba(X) for clf in self.estimators_])
return sum(estimator.decision_function(X[:, features]) for estimator, features in zip(estimators, estimators_features))
return sum(estimator.predict(X[:, features]) for estimator, features in zip(estimators, estimators_features))
super(BaggingClassifier, self)._validate_estimator( default=DecisionTreeClassifier())
super(BaggingRegressor, self)._validate_estimator( default=DecisionTreeRegressor())
random_instance = check_random_state(random_state) sample_indices = random_instance.randint(0, n_samples, n_samples) return sample_indices
sample_indices = _generate_sample_indices(random_state, n_samples) sample_counts = bincount(sample_indices, minlength=n_samples) unsampled_mask = sample_counts == 0 indices_range = np.arange(n_samples) unsampled_indices = indices_range[unsampled_mask] return unsampled_indices
return self.estimators_[index]
return iter(self.estimators_)
if i == 9: return True else: return False
def fit(self, X, y): super(CustomSVC, self).fit(X, y) self.data_type_ = type(X) return self
def fit(self, X, y): super(CustomSVR, self).fit(X, y) self.data_type_ = type(X) return self
super(CustomSVC, self).fit(X, y, sample_weight=sample_weight) self.data_type_ = type(X) return self
super(CustomSVR, self).fit(X, y, sample_weight=sample_weight) self.data_type_ = type(X) return self
super(AdaBoostRegressor, self)._validate_estimator( default=DecisionTreeRegressor(max_depth=3))
path = path.replace(os.sep, '/') if path.startswith('./'): path = path[2:] return path
def __init__( self, parseElementList ): self.parseElementTrace = parseElementList def __str__( self ): return "RecursiveGrammarException: %s" % self.parseElementTrace
return self.__tokdict.keys()
return [(k,self[k]) for k in self.__tokdict.keys()]
return [ v[-1][0] for v in self.__tokdict.values() ]
out = [] for res in self.__toklist: if isinstance(res,ParseResults): out.append( res.asList() ) else: out.append( res ) return out
return dict( self.items() )
self.parseAction += map(self._normalizeParseActionArgs, list(fns)) self.callDuringTry = self.callDuringTry or ("callDuringTry" in kwargs and kwargs["callDuringTry"]) return self
return NotAny( self )
def __init__( self ): super(Empty,self).__init__() self.name = "Empty" self.mayReturnEmpty = True self.mayIndexError = False
def __init__( self, expr ): super(Group,self).__init__( expr ) self.saveAsList = True def postParse( self, instring, loc, tokenlist ): return [ tokenlist ]
return [ tt.upper() for tt in map(_ustr,t) ]
return [ tt.lower() for tt in map(_ustr,t) ]
return _makeTags( tagStr, False )
return _makeTags( tagStr, True )
return CaselessPreservingLiteral(char)
return textwrap.dedent("\n".join(lines)).split("\n")
return sorted(iteritems(d), key=operator.itemgetter(1), reverse=True)
for observer in self.observers: observer.update(event, self)
self.observers.append(observer)
if self.fitted: self.fit()
cs = self.ax.scatter(support_vectors[:, 0], support_vectors[:, 1], s=80, edgecolors="k", facecolors="none") self.contours.append(cs)
k = redirects.get(k, k) return index_map.setdefault(k, len(index_map))
return nt_uri[SHORTNAME_SLICE]
return len(np.unique(X.nonzero()[1]))
return (tok.lower() for tok in re.findall(r"\w+", doc))
freq = defaultdict(int) for tok in tokens(doc): freq[tok] += 1 return freq
return s if len(s) <= 80 else s[:77] + "..."