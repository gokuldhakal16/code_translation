this is an exclusive index used in the case that the template doesn't  end with a new line
an exclusive index in the case that the file didn't end with a  newline.
disable pragma can be at the start of the preceding line
matches output of jshint for simplicity
For parse rules, don't filter them because the comment could be a  part of the parse issue to begin with.
No uncommented code to search.
if we find another starting delim, consider this unparseable
Finds the line start index of the first uncommented line, including the current line.  No uncommented lines.  Current line is uncommented, so return original start_index.  Return start of first uncommented line.
No line comment delimeter, so this acts as a no-op.
check rules that shouldn't use concatenation
add final expression
If we can't find a starting quote, let's assume that what  we considered the end quote is really the start quote.
Attribute(expr value, identifier attr, expr_context ctx)
Store the caller, or left-hand-side node of the initial  format() call.
found Text() or HTML() call in arguments passed to format()
Do not continue processing child nodes of this format() node.
If format call has nested Text() or HTML(), then the caller,  or left-hand-side of the format() call, must be a call to  Text() or HTML().
skip this linter code (i.e. safe_template_linter.py)
Check rules specific to .py files only  Note that in template files, the scope is different, so you can make  different assumptions.  check format() rules that can be run on outer-most format() calls
check illegal concatenation and interpolation  check rules parse with regex
Finds Python blocks such as <% ... %>, skipping other Mako start tags  such as <%def> and <%page>.
Example: finds "| n, h}" when given "${x | n, h}"
get media type (e.g. get text/javascript from  type="text/javascript")
If start of mako expression is commented out, skip it.
for parsing error, restart search right after the start of the  current expression
restart search after the current expression
pylint: disable=line-too-long
Run as a django test suite
!/usr/bin/python  -*- coding: utf-8 -*-  Testing encoding on second line does not cause violation
Print violations if the lengths are different.
switch to JavaScript context
switch to JavaScript context
Line comment with ' to throw off parser
uncacheable. a list, for instance.  better to not cache than blow up.
no merge commit!
A bogus GitHub address, look up their GitHub name in  people.yaml
user passed in a custom date, so we need to parse it
This should not clear the directories list
Use RequireJS optimized storage
Redirect to the test_root folder within the repo
Store the static files under test root so that they don't overwrite existing static assets
Disable uglify when tests are running (used by build.js).  1. Uglify is by far the slowest part of the build process  2. Having full source code makes debugging tests easier for developers
SERVICE_VARIANT specifies name of the variant used, which decides what JSON  configuration files are read during startup.
CONFIG_ROOT specifies the directory where the JSON configuration  files are expected to be found. If not specified, use the project  directory.
CONFIG_PREFIX specifies the prefix of the JSON configuration files,  based on the service variant. If no variant is use, don't use a  prefix.
Don't use a connection pool, since connections are dropped by ELB.
For the Result Store, use the django cache named 'celery'
When the broker is behind an ELB, use a heartbeat to refresh the  connection and to detect if it has been dropped.
Each worker should only fetch one message at a time
STATIC_ROOT specifies the directory where static files are  collected
DEFAULT_COURSE_ABOUT_IMAGE_URL specifies the default image to show for courses that don't provide one
MEDIA_ROOT specifies the directory where user-uploaded files are stored.
For displaying on the receipt. At Stanford PLATFORM_NAME != MERCHANT_NAME, but PLATFORM_NAME is a fine default
Social media links for the page footer
Set the names of cookies shared with the marketing site  These have the same cookie domain as the session, which in production  usually includes subdomains.
Currency
Payment Report Settings
We can run smaller jobs on the low priority queue. See note above for why  we have to reset the value here.
Theme overrides
Marketing link overrides
Mobile store URL overrides
Timezone overrides
Additional installed apps
git repo loading  environment
Event Tracking
SSL external authentication settings
Video Caching. Pairing country codes with CDN URLs.  Example: {'CN': 'http://api.xuetangx.com/edx/video?s3_url='}
Credit notifications settings
Determines whether the CSRF token can be transported on  unencrypted channels. It is set to False here for backward compatibility,  but it is highly recommended that this is True for enviroments accessed  by end users.
Field overrides. To use the IDDE feature, add  'courseware.student_field_overrides.IndividualStudentOverrideProvider'.
Disabling querystring auth instructs Boto to exclude the querystring parameters (e.g. signature, access key) it  normally appends to every returned URL.
Specific setting for the File Upload Service to store media in a bucket.
If there is a database called 'read_replica', you can use the use_read_replica_if_available  function in util/query.py, which is useful for very large database reads
Datadog for events!
Analytics dashboard server
Analytics data source
Analytics Dashboard
Mailchimp New User List
Zendesk
API Key for inbound requests from Notifier service
upload limits
Student identity verification settings
Grades download
financial reports
Prefix for uploads of example-based assessment AI classifiers  This can be used to separate uploads for different environments  within the same S3 bucket.
The reduced session expiry time during the third party login pipeline. (Value in seconds)
third_party_auth config moved to ConfigurationModels. This is for data migration only:
The following can be used to integrate a custom login form with third_party_auth.  It should be a dict where the key is a word passed via ?auth_entry=, and the value is a  dict with an arbitrary 'secret_key' and a 'url'.
REGISTRATION CODES DISPLAY INFORMATION
Which access.py permission names to check;  We default this to the legacy permission 'see_exists'.
Enrollment API Cache Timeout
Use ElasticSearch as the search engine herein
Facebook app
For more info on this, see the notes in common.py
this setting specify which backend to be used when pulling microsite specific configuration  this setting specify which backend to be used when loading microsite specific templates  TTL for microsite database template cache
Course Content Bookmarks Settings
Offset for pk of courseware.StudentModuleHistoryExtended
Cutoff date for granting audit certificates
The extended StudentModule history table
Mobile App Version Upgrade config
The display name of the platform to be used in templates/emails/etc.  Shows up in the platform footer, eg "(c) COPYRIGHT_YEAR"
Features
for consistency in user-experience, keep the value of the following 3 settings  in sync with the corresponding ones in cms/envs/common.py
discussion home panel, which includes a subscription on/off setting for discussion digest emails.  this should remain off in production until digest notifications are online.
extrernal access methods  Even though external_auth is in common, shib assumes the LMS views / urls, so it should only be enabled  in LMS
This flag disables the requirement of having to agree to the TOS for users registering  with Shib.  Feature was requested by Stanford's office of general counsel
Toggles OAuth2 authentication provider
Allows to enable an API endpoint to serve XBlock view, used for example by external applications.  See jquey-xblock: https://github.com/edx-solutions/jquery-xblock
Allows to configure the LMS to provide CORS headers to serve requests from other domains
Can be turned off if course lists need to be hidden. Effects views and templates.
Enables ability to restrict enrollment in specific courses by the user account login method
enable analytics server.  WARNING: THIS SHOULD ALWAYS BE SET TO FALSE UNDER NORMAL  LMS OPERATION. See analytics.py for details about what  this does.
Flip to True when the YouTube iframe API breaks (again)
Give a UI to show a student's submission history in a problem by the  Staff Debug tool.
Provide a UI to allow users to submit feedback from the LMS (left-hand help modal)
Turn on a page that lets staff enter Python code to be run in the  sandbox, for testing whether it's enabled properly.
Enable URL that shows information about the status of variuous services
Toggle to indicate use of the Stanford theming system
Don't autoplay videos for students
Enable instructor dash to submit background tasks
Enable instructor to assign individual due dates  Note: In order for this feature to work, you must also add  'courseware.student_field_overrides.IndividualStudentOverrideProvider' to  the setting FIELD_OVERRIDE_PROVIDERS, in addition to setting this flag to  True.
Enable Custom Courses for EdX
Toggle to enable certificates of courses on dashboard
for load testing
Toggle the availability of the shopping cart page
Toggle storing detailed billing information
Enable flow for payments for course registration (DIFFERENT from verified student flow)
Enable the display of cosmetic course price display (set in course advanced settings)
Automatically approve student identity verification attempts
Disable instructor dash buttons for downloading course data  when enrollment exceeds this number
Grade calculation started from the new instructor dashboard will write  grades CSV files to S3 and give links for downloads.
whether to use password policy enforcement or not
Give course staff unrestricted access to grade downloads (if set to False,  only edX superusers can perform the downloads)
Turn off account locking if failed login attempts exceeds a limit
Hide any Personally Identifiable Information from application logs
Toggles the embargo functionality, which blocks users from  the site or courses based on their location.
Whether the Wiki subsystem should be accessible via the direct /wiki/ paths. Setting this to True means  that people can submit content and modify the Wiki in any arbitrary manner. We're leaving this as True in the  defaults, so that we maintain current behavior
Turn on/off Microsites feature
Turn on third-party auth. Disabled for now because full implementations are not yet available. Remember to syncdb  if you enable this; we don't create tables by default.
Toggle to enable alternate urls for marketing links
Prevent concurrent logins per user
Turn on Advanced Security by default
When a logged in user goes to the homepage ('/') should the user be  redirected to the dashboard - this is default Open edX behavior. Set to  False to not redirect the user
When a user goes to the homepage ('/') the user see the  courses listed in the announcement dates order - this is default Open edX behavior.  Set to True to change the course sorting behavior by their start dates, latest first.
Expose Mobile REST API. Note that if you use this, you must also set  ENABLE_OAUTH2_PROVIDER to True
Enable the combined login/registration form
Enable organizational email opt-in
Show a section in the membership tab of the instructor dashboard  to allow an upload of a CSV file that contains a list of new accounts to create  and register for course.
Enable display of enrollment counts in instructor dash, analytics section
Show the mobile app links in the footer
Let students save and manage their annotations
Milestones application flag
Organizations application flag
Prerequisite courses feature flag
For easily adding modes to courses during acceptance testing
Courseware search feature
Dashboard search feature
log all information from cybersource callbacks
enable beacons for video timing statistics
enable beacons for lms onload event statistics
Toggle platform-wide course licensing
Certificates Web/HTML Views
Batch-Generated Certificates from Instructor Dashboard
Course discovery feature
Software secure fake page feature flag
Teams feature
Show video bumper in LMS
How many seconds to show the bumper again, default is 7 days:
Special Exams, aka Timed and Proctored Exams
Enable OpenBadge support. See the BADGR_* settings later in this file.
The block types to disable need to be specified in "x block disable config" in django admin.
Enable the max score cache to speed up grading
Enable LTI Provider feature.
Show Language selector.
Write new CSM history to the extended table.  This will eventually default to True and may be  removed since all installs should have the separate  extended history table.
Read from both the CSMH and CSMHE history tables.  This is the default, but can be disabled if all history  lives in the Extended table, saving the frontend from  making multiple queries.
Ignore static asset files on import which match this pattern
Used for A/B testing
If this is true, random scores will be generated for the purpose of debugging the profile graphs
comprehensive theming system
For geolocation ip database
Where to look for a status message
Hack to get required link URLs to password reset templates
Shoppingcart processor (detects if request.user has a cart)
Allows the open edX footer to be leveraged in Django Templates.
Online contextual help
Change 'debug' in your environment settings files - not here.
use the ratelimit backend to prevent brute force attacks
Configuration option for when we want to grab server error pages
FIXME: Should we be doing this truncation?
We're already logging events, and we don't want to capture user  names/passwords.  Heartbeat events are likely not interesting.
Import after sys.path fixup
These are the Mixins that should be added to every XBlock.  This should be moved into an XBlock Runtime/Application object  once the responsibility of XBlock creation is moved out of modulestore - cpennington
Allow any XBlock in the LMS
Paths to wrapper methods which should be applied to every XBlock's FieldData.
Path to a sandboxed Python executable.  None means don't bother.  User to run as in the sandbox.
Configurable limits.  How many CPU seconds can jailed code use?
Some courses are allowed to run unsafe code. This is a list of regexes, one  of them must match the course id for that course to run unsafe code.  For example:    COURSES_WITH_UNSAFE_CODE = [        r"Harvard/XY123.1/.*"    ]
Change DEBUG in your environment settings files, not here
CMS base
Site info  NOTE: Please set ALLOWED_HOSTS to some sane value, as we do not allow the default '*'
Platform mailing address
Get git revision of the current file
Not a git repository
Static content
User-uploaded content
these languages display right to left
Sourced from http://www.localeplanet.com/icu/ and wikipedia
Messages
Setting for PAID_COURSE_REGISTRATION, DOES NOT AFFECT VERIFIED STUDENTS
Members of this group are allowed to generate payment reports
Configure the LMS to use our stub EdxNotes implementation
The age at which a learner no longer requires parental consent, or None  if parental consent is never required.
URL for OpenEdX displayed in the footer
This is just a placeholder image.  Site operators can customize this with their organization's image.
Cache expiration for the version of the footer served  by the branding API.
Max age cache control header for the footer (controls browser caching).
Credit api notification cache timeout
Ignore deprecation warnings (so we don't clutter Jenkins builds/production)
Instead of SessionMiddleware, we use a more secure version  'django.contrib.sessions.middleware.SessionMiddleware',
Instead of AuthenticationMiddleware, we use a cached backed version  Enable SessionAuthenticationMiddleware in order to invalidate  user sessions after a password change.
Adds user tags to tracking events  Must go before TrackMiddleware, to get the context set up
CORS and CSRF
Allows us to set user preferences
Allows us to dark-launch particular languages.  Must be after LangPrefMiddleware, so ?preview-lang query params can override  user's language preference. ?clear-lang resets to user's language preference.
Detects user-requested locale from 'accept-language' header in http request.  Must be after DarkLangMiddleware.
catches any uncaught RateLimitExceptions and returns a 403 instead of a 500  needs to run after locale middleware (or anything that modifies the request context)
for expiring inactive sessions
use Django built in clickjacking protection
to redirected unenrolled students to the course info page
This must be last
Clickjacking protection can be enabled by setting this to 'DENY'
Platform for Privacy Preferences header
Setting that will only affect the edX version of django-pipeline until our changes are merged upstream
Don't wrap JavaScript as there is code that depends upon updating the global namespace
Specify the UglifyJS binary to use
Make some edX UI Toolkit utilities available in the global "edx" namespace
Finally load RequireJS and dependent vendor libraries
Ignore tests
Symlinks used by js-test-tool
The baseUrl to pass to the r.js optimizer, relative to STATIC_ROOT.
The name of the require.js script used by your project, relative to REQUIRE_BASE_URL.
A dictionary of standalone modules to build with almond.js.
Whether to run django-require in debug mode.
A tuple of files to exclude from the compilation result of r.js.
Celery's task autodiscovery won't find tasks nested in a tasks package.  Tasks are only registered when the module they are defined in is imported.
let logging work as configured:
Suffix used to construct 'from' email address for bulk emails.  A course-specific identifier is prepended.
Parameters for breaking down course enrollment into subtasks.
Initial delay used for retrying tasks.  Additional retries use  longer delays.  Value is in seconds.
Maximum number of retries per task for errors that are not related  to throttling.
Maximum number of retries per task for errors that are related to  throttling.  If this is not set, then there is no cap on such retries.
We want Bulk Email running on the high-priority queue, so we define the  routing key that points to it.  At the moment, the name is the same.
We also define a queue for smaller jobs so that large courses don't block  smaller emails (see BULK_EMAIL_JOB_SIZE_THRESHOLD setting)
For emails with fewer than these number of recipients, send them through  a different queue to avoid large courses blocking emails that are meant to be  sent to self and staff
Flag to indicate if individual email addresses should be logged as they are sent  a bulk email message.
Delay in seconds to sleep between individual mail messages being sent,  when a bulk email task is retried for rate-related reasons.  Choose this  value depending on the number of workers that might be sending email in  parallel, and what the SES rate is.
Minimum age for organization-wide email opt in
YouTube JavaScript API
URL to get YouTube metadata
Common views
History tables
Database-backed configuration
Monitor the status of services
Display status message to students
For asset pipelining
For content serving
Theming
Site configuration for theming and behavioral modification
Our courseware
Student support tools
External auth (OpenID, shib)
django-oauth2-provider (deprecated)
django-oauth-toolkit
Notifications were enabled, but only 11 people used it in three years. It  got tangled up during the Django 1.8 migration, so we are disabling it.  See TNL-3783 for details.
Discussion forums
Splash screen
Monitoring
User API
Shopping cart
Notification preferences setting
Different Course Modes
Enrollment API
Student Identity Verification
Dark-launching languages
Microsite configuration
RSS Proxy
Student Identity Reverification
Monitoring functionality
Course action state
Country list
edX Mobile API
Surveys
Course data caching
Old course structure API
Mailchimp Syncing
CORS and cross-domain CSRF
Credit courses
Course teams
Bookmarks
programs support
Self-paced course configuration
Credentials support
edx-milestones service
Gating of course content
Static i18n support
Review widgets
API access administration
Verified Track Content Cohorting
Learner's dashboard
Needed whether or not enabled, due to migrations
Migrations which are not in the standard module "migrations"
Forwards-compatibility with Django 1.7  It is highly recommended that you override this in any environment accessed by  end users
Verified Certificates
The names list controls the order of social media  links in the footer.
The footer URLs dictionary maps social footer names  to URLs defined in configuration.
These are URLs to the app store for mobile.
Default cache expiration for the cross-domain proxy HTML page.  This is a static page that can be iframed into an external page  to simulate cross-domain requests.
Optional setting to restrict registration / account creation to only emails  that match a regex in this list. Set to None to allow any email (default).
Be sure to set up images for course modes using the BadgeImageConfiguration model in the certificates app.  Do not add the trailing slash here.  Number of seconds to wait on the badging server when contacting it before giving up.
These keys are used for all of our asynchronous downloadable files, including  the ones that contain information other than grades.
By default, don't use a file prefix
Default File Upload Storage bucket and prefix. Used by the FileUpload Service.
edx-ora2
edxval
edX Proctoring
Organizations App (http://github.com/edx/edx-organizations)
First attempt to only find the module rather than actually importing it,  to avoid circular references - only try to import if it can't be found  by find_module, which doesn't work with import hooks
Empty by default
REGISTRATION CODES DISPLAY INFORMATION SUBTITUTIONS IN THE INVOICE ATTACHMENT
Country code overrides  Used by django-countries  Taiwan is specifically not translated to avoid it being translated as "Taiwan (Province of China)"
which access.py permission name to check in order to determine if a course is visible in  the course catalog. We default this to the legacy permission 'see_exists'.
which access.py permission name to check in order to determine if a course about page is  visible. We default this to the legacy permission 'see_exists'.
Enrollment API Cache Timeout
These tabs are currently disabled
CDN experiment/monitoring flags
Page onload event sampling rate (min 0.0, max 1.0)
The configuration visibility of account fields.  Default visibility level for accounts without a specified value  The value is one of: 'all_users', 'private'
The list of all fields that can be shared with other users  Not an actual field, but used to signal whether badges should be public.
The list of account fields that are always public
E-Commerce API Configuration
Reverification checkpoint name pattern
For the fields override feature  If using FEATURES['INDIVIDUAL_DUE_DATES'], you should add  'courseware.student_field_overrides.IndividualStudentOverrideProvider' to  this setting.
Modulestore-level field override providers. These field override providers don't  require student context.
Sets the maximum number of courses listed on the homepage  If set to None, all courses will be listed on the homepage
Initial delay used for retrying tasks.  Additional retries use longer delays.  Value is in seconds.
Maximum number of retries per task for errors that are not related  to throttling.
Dummy secret key for dev/test
Secret keys shared with credit providers.  Used to digitally sign credit requests (us --> provider)  and validate responses (provider --> us).  Each key in the dictionary is a credit provider ID, and  the value is the 32-character key.
Maximum age in seconds of timestamps we will accept  when a credit provider notifies us that a student has been approved  or denied for credit.
The Help link to the FAQ page about the credit
Default domain for the e-mail address associated with users who are created  via the LTI Provider feature. Note that the generated e-mail addresses are  not expected to be active; this setting simply allows administrators to  route any messages intended for LTI users to a common domain.
For help generating a key pair import and run `openedx.core.lib.rsa_key_utils.generate_rsa_key_pair()`
Credit notifications settings
This is an arbitrary hard limit.  The reason we introcuced this number is because we do not want the CCX  to compete with the MOOC.
Maximum and minimum length of answers, in characters, for the  financial assistance form
Course Content Bookmarks Settings
Identifier included in the User Agent from open edX mobile apps.
cache timeout in seconds for Mobile App Version Upgrade
Deprecated xblock types
Dafault site id to use in case there is no site that matches with the request headers.
Affiliate cookie tracking
https://stackoverflow.com/questions/2890146/how-to-force-pyyaml-to-load-strings-as-unicode-objects
SERVICE_VARIANT specifies name of the variant used, which decides what YAML  configuration files are read during startup.
CONFIG_ROOT specifies the directory where the YAML configuration  files are expected to be found. If not specified, use the project  directory.
CONFIG_PREFIX specifies the prefix of the YAML configuration files,  based on the service variant. If no variant is use, don't use a  prefix.
SSL external authentication settings
Don't use a connection pool, since connections are dropped by ELB.
For the Result Store, use the django cache named 'celery'
When the broker is behind an ELB, use a heartbeat to refresh the  connection and to detect if it has been dropped.
Each worker should only fetch one message at a time
Works around an Ansible bug
Delete keys from ENV_TOKENS so that when it's imported  into settings it doesn't override what was set above
Update the token dictionary directly into settings
NOTE, there's a bug in Django (http://bugs.python.org/issue18012) which necessitates this being a str()
We can run smaller jobs on the low priority queue. See note above for why  we have to reset the value here.
Additional installed apps
Works around an Ansible bug
Grades download
pretend we are behind some marketing site, we want to be able to assert that the Microsite config values override  this global setting
Mongo perf stats
HOSTNAME_MODULESTORE_DEFAULT_MAPPINGS defines, as dictionary of regex's, a set of mappings of HTTP request hostnames to  what the 'default' modulestore to use while processing the request  for example 'preview.edx.org' should use the draft modulestore
Dummy secret key for dev
Disable transaction management because we are using a worker. Views  that request a task and wait for the result will deadlock otherwise.
This patch disables the commit_on_success decorator during tests  in TestCase subclasses.
mongo connection settings
can't test start dates with this True, but on the other hand,  can test everything else :)
Enable this feature for course staff grade downloads, to enable acceptance tests
Toggles embargo on for testing
Enable a parental consent age limit for testing
Nose Test Runner
Local Directories  Want static files in the same dir for running on jenkins.
Where the content data is checked out.  This may not exist on jenkins.
Don't rely on a real staff grading backend
Avoid having to run collectstatic before the unit test suite  If we don't add these settings, then Django templates that can't  find pipelined assets will raise a ValueError.  http://stackoverflow.com/questions/12816941/unit-testing-with-django-pipeline
Don't use compression during tests
Create tables directly from apps' models. This can be removed once we upgrade  to Django 1.9, which allows setting MIGRATION_MODULES to None in order to skip migrations.
Make sure we test with the extended history table
This is the cache used for most things.  In staging/prod envs, the sessions also live here.
Dummy secret key for dev
hide ratelimit warnings while running tests
Ignore deprecation warnings (so we don't clutter Jenkins builds/production)  https://docs.python.org/2/library/warnings.htmlthe-warnings-filter  Change to "default" to see the first instance of each hit  or "error" to convert all into errors
Default to advanced security in common.py, so tests can reset here to use  a simpler security model
don't cache courses for testing
Enable fake payment processing page
Configure the payment processor to use the fake processing page  Since both the fake payment page and the shoppingcart app are using  the same settings, we can generate this randomly and guarantee  that they are using the same secret.
Verified Certificates
Strip out any static files that aren't in the repository root  so that the tests can run with only the edx-platform directory checked out  Handle both tuples and non-tuple directory definitions
These ports are carefully chosen so that if the browser needs to  access them, they will be available through the SauceLabs SSH tunnel
'django.contrib.auth.hashers.PBKDF2PasswordHasher',  'django.contrib.auth.hashers.PBKDF2SHA1PasswordHasher',  'django.contrib.auth.hashers.BCryptPasswordHasher',  'django.contrib.auth.hashers.CryptPasswordHasher',
Setting for the testing of Software Secure Result Callback
Enable EdxNotes for tests.
Enable teams feature for tests.
Enable courseware search for tests
Enable dashboard search for tests
Use MockSearchEngine as the search engine for test scenario
Enable the LTI provider feature for testing
ORGANIZATIONS
Financial assistance page
Disable the use of the plugin manager in the transformer registry for  better performant unit tests.
Set the default Oauth2 Provider Model so that migrations can run in  verbose mode
Enable debug so that static assets are served by Django
Set REQUIRE_DEBUG to false so that it behaves like production
Fetch static files out of the pipeline's static root
Needed for the reset database management command
Redirect to the test_root folder within the repo
Enable debug so that static assets are served by Django
Don't use compression during tests
Configure the LMS to use our stub XQueue implementation
Configure the LMS to use our stub EdxNotes implementation
Enable milestones app
Enable oauth authentication, which we test.
Enable pre-requisite course
Enable Course Discovery
Enable student notes
Enable teams feature
Enable custom content licensing
Use the auto_auth workflow for creating users and logging them in
Default to advanced security in common.py, so tests can reset here to use  a simpler security model
Enable courseware search for tests
Enable dashboard search for tests
Enable support for OpenBadges accomplishments
Use MockSearchEngine as the search engine for test scenario  Path at which to store the mock index
this secret key should be the same as cms/envs/bok_choy.py's
Make sure we test with the extended history table
Lastly, see if the developer has any local overrides.
Don't use S3 in devstack, fall back to filesystem
By default don't use a worker, execute tasks as if they were local functions
Set this to the dashboard URL in order to display the link from the  dashboard to the Analytics Dashboard.
ProfilingPanel has been intentionally removed for default devstack.py  runtimes for performance reasons. If you wish to re-enable it in your  local development environment, please create a new settings file  that imports and extends devstack.py.
Disable JavaScript compression in development
Whether to run django-require in debug mode.
Setting for overriding default filtering facets for Course discovery  COURSE_DISCOVERY_FILTERS = ["org", "language", "modes"]
Software secure fake page feature flag
Setting for the testing of Software Secure Result Callback
Skip enrollment start date filtering
See if the developer has any local overrides.
Lastly, run any migrations, if needed.
You need to start the server in debug mode,  otherwise the browser will not render the pages correctly
Output Django logs to a file
set root logger level
Use the auto_auth workflow for creating users and logging them in
Enable fake payment processing page
Enable special exams
Don't actually send any requests to Software Secure for student identity  verification.
HACK  Setting this flag to false causes imports to not load correctly in the lettuce python files  We do not yet understand why this occurs. Setting this to true is a stopgap measure
Include the lettuce app for acceptance testing, including the 'harvest' django-admin command
Where to run: local, saucelabs, or grid
See if the developer has any local overrides.
Use MockSearchEngine as the search engine for test scenario
Generate a random UUID so that different runs of acceptance tests don't break each other
We want to make sure that any new migrations are run  see https://groups.google.com/forum/!msg/django-developers/PWPj3etj3-U/kCl6pMsQYYoJ
Make the keyedcache startup warnings go away
Dummy secret key for dev
List of `university` landing pages to display, even though they may not  have an actual course with that org set
Organization that contain other organizations
By default don't use a worker, execute tasks as if they were local functions
If there's an environment variable set, grab it
Lastly, see if the developer has any local overrides.
Dummy secret key for dev
Import everything from .aws so that our settings are based on those.
You never migrate a read_replica
set the default Django settings module for the 'celery' program.
Using a string here means the worker will not have to  pickle the object when using Windows.
Uncomment the next two lines to enable the admin:
Use urlpatterns formatted as within the Django docs with first parameter "stuck" to the open parenthesis
Note: these are older versions of the User API that will eventually be  subsumed by api/user listed below.
Feedback Form endpoint
Enrollment API RESTful endpoints
Courseware search endpoints
Course content API
Course API
User API endpoints
Bookmarks API endpoints
Profile Images API endpoints
Video Abstraction Layer used to allow video teams to manage video assets  independently of courseware. https://github.com/edx/edx-val
Update session view
URLs for API access management
We need to explicitly include external Django apps that are not in LOCALE_PATHS.
sysadmin dashboard, to see what courses are loaded, to delete & load courses
Semi-static views (these need to be rendered and have the login bar, but don't change)
Press releases
Only enable URLs for those marketing links actually enabled in the  settings. Disable URLs by marking them as None.  Skip disabled URLs
These urls are enabled separately
The MKTG_URL_LINK_MAP key specifies the template filename  Append STATIC_TEMPLATE_VIEW_DEFAULT_FILE_EXTENSION if  no file extension was specified in the key
To allow theme templates to inherit from default templates,  prepend a standard prefix
Make the assumption that the URL we want is the lowercased  version of the map key
Multicourse wiki (Note: wiki urls must be above the courseware ones because of  the custom tab catch-all)
xblock View API  (unpublished) API that returns JSON with the HTML fragment and related resources  for the xBlock's requested view.
Survey associated with a course
Takes optional student_id for instructor use--shows profile as that student sees it.
For the instructor
LTI endpoints listing
Student account
Student profile
Student Notes
Embargo
Survey Djangoapp
enable automatic login
OAuth token exchange
Certificates
REST APIs
XDomain proxy
Access to courseware as an LTI provider
in debug mode, allow any template to be rendered (most useful for UX reference templates)
Custom error pages
display error page templates, for testing purposes
include into our URL patterns the HTTP REST API that comes with edx-proctoring.
This will make sure the app is always imported when  Django starts so that shared_task will use this app.
Patch the xml libs before anything else.
This application object is used by the development server  as well as any WSGI server configured to use this file.
To override the settings before executing the autostartup() for python-social-auth
Comprehensive theming needs to be set up before django startup,  because modifying django template paths after startup has no effect.
We currently use 2 template rendering engines, mako and django_templates,  and one of them (django templates), requires the directories be added  before the django.setup().
Mako requires the directories to be added after the django setup.
Initialize Segment analytics module by setting the write_key.
register InstructorService (for deleting student attempts and user staff access roles)
Workaround for setting THEME_NAME to an empty  string which is the default due to this ansible  bug: https://github.com/ansible/ansible/issues/4812
Calculate the location of the theme's files
Include the theme's templates in the template search paths
Namespace the theme's static files to 'themes/<theme_name>' to  avoid collisions with default edX static files
Include theme locale path for django translations lookup
from nose.tools import set_trace  set_trace()
if we have an org filter, only include results for this org filter
If we have a course filter we are ensuring that we only get those courses above
re-fetch video from draft store
re-fetch video from published store
metric_tag_fields are used by Datadog to record metrics about the model
initializable_fields are sent in POST requests
pylint: disable=unused-import
attempt to gracefully recover from a previous failure  to sync this user to the comments service.
automatically generate the stripped version of the text from the HTML markup:
create the task, then save it immediately:
Allowing null=True to support data migration from email->user.  We need to first create the 'user' column with some sort of default in order to run the data migration,  and given the unique index, 'null' is the best default value.
Defines the tag that must appear in a template, to indicate  the location where the email message body is to be inserted.
Substitute all %%-encoded keywords in the message body
finally, return the result, after wrapping long lines and without converting to an encoded byte array.
The course that these features are attached to.
Whether or not to enable instructor email
pylint: disable=no-member
boolean field 'enabled' inherited from parent ConfigurationModel
Turn off the action bar (we have no bulk actions)
Note that we get back a blank string in the Form for an empty 'name' field  we want those to be set to None in Python and NULL in the database
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-
Student module history table will fail this migration otherwise
Note this is not a perfect 1:1 backwards migration - targets can hold more information than to_option can.  We use the first valid value from targets, or 'myself' if none can be found
Student module history table will fail this migration otherwise
-*- coding: utf-8 -*-
Course should still be authorized (invalid attempt had no effect)
Munge course id
Validation shouldn't work
Validation shouldn't work
Munge course id - common  Validation shouldn't work
now inspect the database and make sure the blank name was stored as a NULL  Note this will throw an exception if it is not found
now inspect the database and make sure the whitespace only name was stored as a NULL  Note this will throw an exception if it is not found
now inspect the database and make sure the name is properly  stripped
now inspect the database and make sure the blank name was stored as a NULL  Note this will throw an exception if it is not found
try to submit form with the same name
load initial content (since we don't run migrations as part of tests):
Assert that self.student.email not in mail.to, outbox should only contain "myself" target
Response loads the whole instructor dashboard, so no need to explicitly  navigate to a particular email section  If this fails, it is likely because BulkEmailFlag.is_enabled() is set to False
load initial content (since we don't run migrations as part of tests):
We should get back a HttpResponseForbidden (status code 403)
Post the email to the instructor dashboard API
Post the email to the instructor dashboard API
Create a student with Unicode in their first & last names
Post the email to the instructor dashboard API
make display_name that's longer than 320 characters when encoded  to ascii and escaped, but shorter than 320 unicode characters
it's shorter than 320 characters when just encoded  escaping it brings it over that limit  when not escaped or encoded, it's well below 320 characters
Post the email to the instructor dashboard API
Post the email to the instructor dashboard API
load initial content (since we don't run migrations as part of tests):
Get the default template, which has name=None
Test that course is not authorized by default
Test that course is authorized by default, since auth is turned off
Use the admin interface to unauthorize the course
Now, course should STILL be authorized!
load initial content (since we don't run migrations as part of tests):
check return value
Test that celery handles permanent SMTPDataErrors by failing and not retrying.
Test that celery handles permanent SMTPDataErrors by failing and not retrying.
Test that celery handles permanent SMTPDataErrors by failing and not retrying.
Test that celery handles permanent SMTPDataErrors by failing and not retrying.
Test that celery handles permanent SMTPDataErrors by failing and not retrying.
Test bulk email with unicode characters in course image name
We also send email to the instructor:
Test that we retry upon hitting a 4xx error
(?i) is a regex for ignore case
Errors that an individual email is failing to be sent, and should just  be treated as a fail.
Exceptions that, if caught, should cause the task to be re-tried.  These errors will be caught a limited number of times before the task fails.
Errors that are known to indicate an inability to send any more emails,  and should therefore not be retried.  For example, exceeding a quota for emails.  Also, any SMTP errors that are not explicitly enumerated above.
Get inputs to use in this task from the entry.
Fetch the course object.
Get arguments that will be passed to every subtask.
if there are few enough emails, send them through a different queue  to avoid large courses blocking emails to self and staff
Update the InstructorTask object that is storing its progress.
If retrying, a RetryTaskError needs to be returned to Celery.  We assume that the the progress made before the retry condition  was encountered has already been updated before the retry call was made,  so we only log here.
return status in a form that can be serialized by Celery into JSON:
Only count the num_optout for the first time the optouts are calculated.  We assume that the number will not change on retries, and so we don't need  to calculate it each time.
For the email address, get the course.  Then make sure that it can be used  in an email address, by substituting a '_' anywhere a non-(ascii, period, or dash)  character appears.
If the encoded from_addr is longer than 320 characters, reformat,  but with the course name rather than course title.  Amazon SES's from address field appears to have a maximum length of 320.
It seems that this value is also escaped when set out to amazon, judging  from our logs
Get information from current task's request:
use the email from address in the CourseEmail, if it is present, otherwise compute it
use the CourseEmailTemplate that was associated with the CourseEmail
Define context values to use in all course emails:
Construct message content using templates and context:
Create email:
Pop the user that was emailed off the end of the list only once they have  successfully been processed.  (That way, if there were a failure that  needed to be retried, the user is still on the list.)
Increment the "retried_nomax" counter, update other counters with progress to date,  and set the state to RETRY:
Update counters with progress to date, counting unsent emails as failures,  and set the state to FAILURE:
All went well.  Update counters with progress to date,  and set the state to SUCCESS:  Successful completion is marked by an exception value of None.
Clean up at the end.
Skew the new countdown value by a random factor, so that not all  retries are deferred by the same amount.
we make sure that we update the InstructorTask with the current subtask status  *before* actually calling retry(), to be sure that there is no race  condition between this update and the update made by the retried task.
return match indexed by state
reloading based on commit_id is needed when running mutiple worker threads,  so that a given thread doesn't reload the same commit multiple times
The consumer may not exist, or its record may not have a guid
Search by consumer key instead of instance_guid. If there is no  consumer with a matching key, the LTI launch does not have permission  to access the content.
Add the instance_guid field to the model if it's not there already.
This is the first time that the user has been here. Create an account.
The user is not authenticated, or is logged in as somebody else.  Switch them to the LTI user
The random edx_user_id wasn't unique. Since 'created' is still  False, we will retry with a different random ID.
This shouldn't happen, since we've created edX accounts for any LTI  users by this point, but just in case we can return a 403.
LTI launch parameters that must be present for a successful launch
Check the LTI parameters, and return 400 if any required parameters are  missing
Get the consumer information from either the instance GUID or the consumer  key
Check the OAuth signature on the message
Create an edX account if the user identifed by the LTI launch doesn't have  one already, and log the edX account into the platform.
Store any parameters required by the outcome service in order to report  scores back later. We know that the consumer exists, since the record was  used earlier to verify the oauth signature.
return an HttpResponse object that contains the template and necessary context to render the courseware.
The oauthlib library assumes that headers are passed directly from the  request, but Django mangles them into its own format. The only header  that the library requires (for now) is 'Content-Type', so we  reconstruct just that one.
Both usage and course ID parameters are supplied in the LTI launch URL
Create a record of the outcome service if necessary
failed to send result. 'response' is None, so more detail will be  logged at the end of the method.
Pylint doesn't recognize members in the LXML module
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Always accept the OAuth signature
Make sure that our body hash is now part of the test...
Get all assignments involving the current problem for which the campus LMS  is expecting a grade. There may be many possible graded assignments, if  a problem has been added several times to a course at different  granularities (such as the unit or the vertical).
Django Rest Framework v3.1 requires that we pass the request to the serializer  so it can construct hyperlinks.  To avoid changing the interface of this object,  we retrieve the request from the request cache.
Save the primary key so we can load the full objects easily after we search  Don't save the membership relations in elasticsearch
add generally searchable content
Don't emit changed events when these fields change.
Even though sorting is done outside of the serializer, sort_order needs to be passed  to the serializer so that the paginated results indicate how they were sorted.
Paginate and serialize topic data  BulkTeamCountPaginatedTopicSerializer will add team counts to the topics in a single  bulk operation per page.
Django Rest Framework v3 requires that we pass the request  into the serializer's context if the serialize contains  hyperlink fields.
Instantiate the paginator and use it to paginate the queryset
Serialize the page
OAuth2Authentication must come first to return a 401 for unauthenticated users
Ensure the course exists
MySQL does case-insensitive order_by.
Translators: 'ordering' is a string describing a way  of ordering a list. For example, {ordering} may be  'name', indicating that the user wants to sort the  list by lower case name.
Ensure the course exists
This code is taken from within the GenericAPIViewpaginate_queryset method.  We need need access to the page outside of that method for our paginate_search_results method
Note: list() forces the queryset to be evualuated before delete()
Ensure the course exists
Translators: 'ordering' is a string describing a way  of ordering a list. For example, {ordering} may be  'name', indicating that the user wants to sort the  list by lower case name.
Ensure the course exists
This is ugly, but there is a really strange circular dependency that doesn't  happen anywhere else that I can't figure out how to avoid it :(
-*- coding: utf-8 -*-
will be assigned to self.client by default
Enroll in the course and log in
Check the query count on the dashboard with no teams
Create some teams
Add the user to the last team
Check the query count on the dashboard again
Create teams in both courses
Add user to a course one team
Check that list of user teams in course one is not empty, it is one now
This student is enrolled in both test courses and is a member of a team in each course, but is not on the  same team as student_enrolled.
Make this student have a public profile
This student is enrolled in the other course, but not yet a member of a team. This is to allow  course_2 to use a max_team_size of 1 without breaking other tests on course_1
Check that initially list of user teams in course one is empty
Add user to a course one team
Check that list of user teams in course one is not empty now
Check that list of user teams in course two is still empty
Check that teams with the same name have unique IDs.
Verify expected truncation behavior with names > 20 characters.
First add the privileged user to a team.
Verify the id (it ends with a unique hash, which is the same as the discussion_id).
Remove date_created and discussion_topic_id because they change between test runs
Since membership is its own list, we want to examine this separately.
Verify that the creating user gets added to the team.
Verify that staff do not automatically get added to a team  when they create one.
Use the default user which is already private because to year_of_birth is set
Extending test classes should specify their serializer class.
Assert that the first save in the setUp sets a value.
Verify that we only change the last activity_at when it doesn't  already exist.
a list of all supported mobile platforms
pylint: disable=no-member
Create list of courses with various expected courseware_access responses and corresponding expected codes
Enroll in all the courses
override implementation to use PATCH method.
save something so we have an initial date
old modification date so skip update
The arguments are optional, so if there's no argument just succeed
identifiers
dates
notification info
access info
Set requested profiles
Testing when video_profiles='mobile_low,mobile_high,youtube'
Testing when there is no mobile_low, and that mobile_high doesn't show
Testing where youtube is the default video over mobile_high
add user to this cohort
remove user from this cohort
un-cohorted user should see no videos
staff user sees all videos
to be consistent with other edx-platform clients, return the defaulted display name
Get encoded videos
Get highest priority video to populate backwards compatible field
Then fall back to VideoDescriptor fields for video URLs
Get duration/size, else default
Transcripts...
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
pylint: disable=no-member
login and enroll as the test user
login and enroll as another user
now login and call the API as the test user
set user's role in the course
call API
verify static URLs are replaced in the content returned by the API
verify static URLs remain in the underlying content
but shouldn't finish with any
but shouldn't finish with any
but shouldn't finish with any
course_handouts_module could be None if there are no handouts
Translators: This message means that the user could not be authenticated (that is, we could  not log them in for some reason - maybe they don't have permission, or their password was wrong)
Translators: this means everything happened successfully, yay!
Translators: Domain is an email domain, such as "@gmail.com"
Grab logging output for debugging imports
Remove handler hijacks
Translators: "Git Commit" is a computer command; see http://gitref.org/basic/commit
Set mongodb defaults even if it isn't defined in settings
Require staff if not going to specific course
Positional arguments
Make a course dir that will be replaced with a symlink  while we are at it.
Test with all three args (branch)
Test successful import from command
Checkout non existent branch
Checkout new branch  Validate that it is different than master
Attempt to check out the same branch again to validate branch choosing  works
Build repo dir
Get logger for checking strings in logs
Translators: This is an error message when they ask for a  particular version of a git repository and that version isn't  available from the remote source they specified
Translators: Error message shown when they have asked for a git  repository branch, a specific version within a repository, that  doesn't exist, or there is a problem changing to it.
Get XML logging logger and capture debug to parse results
Remove handler hijacks
store import-command-run output in mongo
using XML store
Using mongo store
Create git loaded course
Make sure we don't have any git hashes on the page
Now add the course and make sure it does match
Check that our earlier import has a log with a link to details
Simulate a lack of git import logs
Add user as staff in course team
pylint: disable=missing-docstring
Will also run default tests for IDTokens and UserInfo
CourseAccessHandler uses the application cache.
create another course in the DB that only global staff have access to
Use the anonymous ID without any course as unique identifier.  Note that this ID is derived using the value of the `SECRET_KEY`  setting, this means that users will have different sub  values for different deployments.
Calling UserPreference directly because it is not clear which user made the request.
If the user has no language specified, return the default one.
If values was specified, filter out other courses.
pylint: disable=missing-docstring  Check the application cache and update if not present. The application  cache is useful since there are calls to different endpoints in close  succession, for example the id_token and user_info endpoints.
Global staff have access to all courses. Filter courses for non-global staff.
Don't return list of courses unless they are requested as essential.
Don't return list of courses unless they are requested as essential.
Guess content type from file extension
check response with branding
this is a two-step form: first look up the data, then issue the refund.  first time through, set the hidden "confirmed" field to true and then redisplay the form  second time through, do the unenrollment/refund.
pylint: disable=wildcard-import
pylint: disable=missing-docstring
In order to check the user's permission, he/she needs to be logged in.
users without the permission can't access support
Log out then try to retrieve the page
Expect a redirect to the login page
Check that all the expected links appear on the index page.
Check that an empty initial filter is passed to the JavaScript client correctly.
`self` isn't available from within the DDT declaration, so  assign the course ID here
pylint: disable=missing-docstring
Json request data for metrics for entire course
Json request data for metrics for particular section
For listing students that opened a sub-section
For listing of students' grade per problem
For generating metrics data as a csv
Only instructor for this particular course can request this information
Only instructor for this particular course can request this information
Only instructor for this particular course can request this information
Used to limit the length of list displayed to the screen.
Loop through resultset building data for each problem
Build set of grade distributions for each problem that has student responses
Build set of total students attempting each problem
Aggregate query on studentmodule table for "opening a subsection" data
Build set of "opened" data for each subsection that has "opened" data
Loop through resultset building data for each problem
Retrieve course object down to problems
Student data is at the problem level
Construct label to display for this problem
Only problems in prob_grade_distrib have had a student submission.
Get max_grade, grade_distribution for this problem
Get problem_name for tooltip
Compute percent of students with this grade
Construct data to be sent to d3
Retrieve course object down to subsection
Iterate through sections, subsections
Construct data for each subsection to be sent to d3
Tooltip parameters for subsection in open_distribution view
Retrieve course object down to problems
Retrieve grade distribution for these problems
Construct data for each problem to be sent to d3
Restrict screen list length  Adding 1 so can tell if list is larger than MAX_SCREEN_LIST_LENGTH  without doing another select.
Remove the last item so list length is exactly MAX_SCREEN_LIST_LENGTH
Subsection name is everything after 3rd space in tooltip
Remove the last item so list length is exactly MAX_SCREEN_LIST_LENGTH
tooltips array is array of dicts for subsections and  array of array of dicts for problems.  Append to results offsetting 1 column to the right.
Append to results offsetting 1 column to the right.
Only 2 students in the list and response_max_exceeded is True
Only 2 students in the list and response_max_exceeded is True
Check response contains 1 line for each user +1 for the header
Check response contains 1 line for header, 1 line for Section and 1 line for Subsection
Check response contains 1 line for header, 1 line for Sections and 2 lines for problems
If you try to subscribe with too many users at once  the transaction times out on the mailchimp side.
send the updates in batches of a fixed size
reset segments
shuffle and split emails
to avoid overly verbose output, this is off by default
bail early if no beta testing is set up
The date is shown in the title, no need to display it again.
Return `true` if user is not enrolled in course
Show the summary if user enrollment is in which allow user to upsell
If this is an inheritable field and an override is set above,  then we want to return False here, so the field_data uses the  override and not the original value for this block.
Key used to share state. This is the XBlock usage_id
Internal state of the object
We use the student_id instead of username to avoid a database hop.  This can actually matter in cases where we're logging many of  these (e.g. on a broken progress page).
This should be populated from the modified field in StudentModule
Django will sometimes try to join to courseware_studentmodule  so just do an in query
If we turn off reading from multiple history tables, then we don't want to read from  StudentModuleHistory anymore, we believe that all history is in the Extended table.  we want to save later SQL queries on the model which allows us to prefetch
When the extended studentmodulehistory table exists, don't save  duplicate history into courseware_studentmodulehistory, just retain  data for reading.
The name of the field
The value of the field. Defaults to None dumped as json
The definition id for the module
The type of the module for these preferences
Check for content which needs to be completed  before the rest of the content is made available
Check for gated content
The user may not actually have to complete the entrance exam, if one is required
Only show required content, if there is required content  chapter.hide_from_toc is read-only (bool)
Skip the current chapter if a hide flag is tripped
skip the section if it is gated/hidden from the user
call into edx_proctoring subsystem  to get relevant proctoring information regarding this  level of the courseware  This will return None, if (user, course_id, content_id)  is not applicable
safety net in case something blows up in edx_proctoring  as this is just informational descriptions, it is better  to log and continue (which is safe) than to have it be an  unhandled exception
yes, user has proctoring context about  this level of the courseware  so add to the accordion data context
Something has gone terribly wrong, but still not letting it turn into a 500.
Bin score into range and increment stats
Cycle through the milestone fulfillment scenarios to see if any are now applicable  thanks to the updated grading information that was just submitted
Send a signal out to any listeners who are waiting for score change  events.
now bind the module to the new ModuleSystem instance and vice-versa
Build a list of wrapping functions that will be applied in order  to the Fragment content coming out of the xblocks that are about to be rendered.
Rewrite urls beginning in /static to point to course-specific content
Allow URLs of the form '/course/' refer to the root of multicourse directory    hierarchy of this course
pass position specified in URL to module through ModuleSystem
make an ErrorDescriptor -- assuming that the descriptor's system is ok
Test xqueue package, which we expect to be:    xpackage = {'xqueue_header': json.dumps({'lms_key':'secretkey',...}),                'xqueue_body'  : 'Message from grader'}
Transfer 'queuekey' from xqueue response header to the data.  This is required to use the interface defined by 'handle_ajax'
We go through the "AJAX" path  So far, the only dispatch from xqueue will be 'score_update'  Can ignore the return value--not used for xqueue_callback  Save any state that has changed to the underlying KeyValueStore
For blocks that are inherited from a content library, we add some additional metadata:
Either permissions just changed, or someone is trying to be clever  and load something they shouldn't have access to.
Check submitted files
Make a CourseKey from the course_id, raising a 404 upon parse error.
Gather metrics for New Relic so we can slice data in New Relic Insights
If we can't find the module, respond with a 404
For XModule-specific errors, we log the error and respond with an error message
If any other error occurred, re-raise it to trigger a 500 response
Check number of files submitted
Check file sizes
Compute grades using real division, with no integer truncation
check for subtree_edited_on because old XML courses doesn't have this attribute
For the moment, we have to get scorable_locations from field_data_cache  and not from scores_client, because scores_client is ignorant of things  in the submissions API. As a further refactoring step, submissions should  be hidden behind the ScoresClient.
This next complicated loop is just to collect the totaled_scores, which is  passed to the grader
If there are no problems that always have to be regraded, check to  see if any of our locations are in the scores from the submissions  API. If scores exist, we have to calculate grades for this section.
If we haven't seen a single problem in the section, we don't have  to grade it at all! We can assume 0%
We simply cannot grade a problem that is 12/0, because we might need it as a percentage
Grading policy might be overriden by a CCX, need to reset it
We round the grade here, to make sure that the grade is an whole percentage and  doesn't get displayed differently than it gets grades
way to get all RAW scores out to instructor  so grader can be double-checked
Possible grades, sorted in descending order of score
For the moment, we have to get scorable_locations from field_data_cache  and not from scores_client, because scores_client is ignorant of things  in the submissions API. As a further refactoring step, submissions should  be hidden behind the ScoresClient.
Check for gated content
Don't include chapters that aren't displayable (e.g. due to error)  Skip if the chapter is hidden
Skip if the section is hidden
If there is no weighting, or weighting can't be applied, return input.
These are not problems, and do not have a score
Problem may be an error module (if something in the problem builder failed)  In which case total might be None  add location to the max score cache
Grading calls problem rendering, which calls masquerading,  which checks session vars -- thus the empty session dict below.  It's not pretty, but untangling that is currently beyond the  scope of this feature.
Deliberately return a non-specific error message to avoid  leaking info about access control settings
Verify that the user is either enrolled in the course or a staff  member.  If user is not enrolled, raise UserNotEnrolled exception  that will be caught by middleware.
Use an empty cache
Use an empty cache
Sort courses by how far are they from they start day
This is fragile, but unfortunately the problem is that within the LMS we  can't use the reverse calls from the CMS
This is fragile, but unfortunately the problem is that within the LMS we  can't use the reverse calls from the CMS
it will be a Mongo performance boost, if you pass in a depth=3 argument here  as it will optimize round trips to the database to fetch all children for the current node
Remove due dates  Remove release dates for course content
Check key for validity
If we're getting user data, we expect that the key matches the  user we were constructed for.
If we're getting user data, we expect that the key matches the  user we were constructed for.
If save is successful on these fields, add it to  the list of successful saves
If we're getting user data, we expect that the key matches the  user we were constructed for.
If we're getting user data, we expect that the key matches the  user we were constructed for.
If we're getting user data, we expect that the key matches the  user we were constructed for.
Translators: 'Textbooks' refers to the tab in the course that leads to the course' textbooks
Translators: 'Discussion' refers to the tab in the courseware that leads to the discussion forums
Add in any dynamic tabs, i.e. those that are not persisted
No default class--want to complain if it doesn't find plugins for any  module.
print course
dircmp doesn't do recursive diffs.  diff = dircmp(course_dir, export_dir, ignore=[], hide=[])
load into a CorrectMap, as done in LoncapaProblem.__init__():
calculate score the way LoncapaProblem.get_score() works, by deferring to  CorrectMap's get_npoints implementation.
NOTE: if xml store owns these, it won't import them into mongo
HACK: add discussion ids to list of items to export (AN-6696)
When calculating inherited metadata, don't include existing  locally-defined metadata
If filename is '-' save to a temp file
-*- coding: utf-8 -*-
Just in case user is passed in as None, make them anonymous
delegate the work to type-specific functions.  (start with more specific types, then get more general)
NOTE: any descriptor access checkers need to go above this
Passing an unknown object here is a coding error, so rather than  returning a default, complain.
Courselike objects (e.g., course descriptors and CourseOverviews) have an attribute named `id`  which actually points to a CourseKey. Sigh.
If the user appears in CourseEnrollmentAllowed paired with the given course key,  they may enroll. Note that as dictated by the legacy database schema, the filter  call includes a `course_id` kwarg which requires a CourseKey.
Short-circuit the process, since there are no defined user partitions that are not  user_partitions used by the split_test module. The split_test module handles its own access  via updating the children of the split_test module.
use merged_group_access which takes group access on the block's  parents / ancestors into account  check for False in merged_access, which indicates that at least one  partition's group list excludes all students.
look up the user's group for each partition
finally: check that the user has a satisfactory group assignment  for each partition.
all checks passed.
Delegate to the descriptor
Use this sample rate for DataDog events.
If the state is the empty dict, then it has been deleted, and so  conformant UserStateClients should treat it as if it doesn't exist.
The rest of this method exists only to submit DataDog events.  Remove it once we're no longer interested in the data.
We do a find_or_create for every block (rather than re-using field objects  that were queried in get_many) so that if the score has  been changed by some other piece of the code, we don't overwrite  that score.
Anonymous users cannot be persisted to the database, so let's just use  what we have.
We just read this object, so we know that we can do an update
The rest of this method exists only to submit DataDog events.  Remove it once we're no longer interested in the data.  Record whether a state row has been created or updated.
Event to record number of fields sent in to set/set_many.
Event to record number of new fields set in set/set_many.
Event to record number of existing fields updated in set/set_many.
We just read this object, so we know that we can do an update
Event for the entire delete_many call.
If no history records exist, raise an error
If the state is serialized json, then load it
If the state is empty, then for the purposes of `get_history`, it has been  deleted, and so we list that entry as `None`.
Only display the requirements on learner dashboard for  credit and verified modes.
If the user needs to take an entrance exam to access this course, then we'll need  to send them to that specific course module before allowing them into other areas
check to see if there is a required survey that must be taken before  the user can access the course.
link to where the student should go to enroll in the course:  about page if there is not marketing site, SITE_NAME if there is
Get the URL of the user's last position in order to display the 'where you were last' message
Disable student view button if user is staff and  course is not yet visible to students.
Translators: This will look like '$50', where {currency_symbol} is a symbol such as '$' and {price} is a  numerical amount in that currency. Adjust this display as needed for your language.
Translators: This refers to the cost of the course. In this case, the course costs nothing so it is free.
In any other case redirect to the course about page.
Note: this is a flow for payment for course registration, not the Verified Certificate flow.
Find the minimum price for the course across all course modes
Used to provide context to message to student if enrollment not allowed
Register button should be disabled if one of the following is true:  - Student is already registered for course  - Course is already full  - Student cannot enroll in course
get prerequisite courses display names
Overview
check to see if there is a required survey that must be taken before  the user can access the course.
always allowed to see your own profile
Requesting access to a different student's profile  Check for ValueError if 'student_id' cannot be converted to integer.
The pre-fetching of groups is done to make auth checks not require an  additional DB lookup (this kills the Progress page in particular).
checking certificate generation configuration
If credit eligibility is not enabled or this is not a credit course,  short-circuit and return `None`.  This indicates that credit requirements  should NOT be displayed on the progress page.
This indicates that credit requirements should NOT be displayed on the progress page.
Credit requirement statuses for which user does not remain eligible to get credit.
If the user has *failed* any requirements (for example, if a photo verification is denied),  then the user is NOT eligible for credit.
Otherwise, the user may be eligible for credit, but the user has not  yet completed all the requirements.
Permission Denied if they don't have staff access and are trying to see  somebody else's submission history.
if there is no Survey associated with this course,  then redirect to the course instead
verify the user has access to the course, including enrollment check
get the block, which verifies whether the user has access to the block.
Simple sanity check that the session belongs to the user  submitting an FA request
Thrown if JSON parsing fails
Thrown if course key parsing fails
Thrown if fields are missing
The call to Zendesk failed. The frontend will display a  message to the user.
let it propagate
Set the user in the request to the effective user.
User may be trying to access a child that isn't live yet
Pre-fetch all descendant data
Bind section to user
default tab
Only save if position changed  Save this new position to the underlying KeyValueStore
manually twiddle the is_staff bit, if needed
staff should fail because password expired
if we reset the password, we should be able to log in
now retry with a different password
now use different one
now try again with the first one
should be rejected
now use different one
now we should be able to reuse the first one
try to reset password, it should fail
try to reset password, it should fail
try to reset password with a long enough password
Enroll in the course before trying to access pages
Search for items in the course
Try to load each item in the course
fix was to allow get_items() to take the course_id parameter
have a course which explicitly sets visibility in catalog to False
have a course which explicitly sets visibility in catalog and about to true
assert that test course display name is visible
assert that test course with 'visible_in_catalog' to True is showing up
assert that test course that is outside microsite is not visible
assert that a course that has visible_in_catalog=False is not visible
assert that footer template has been properly overriden on homepage
assert that the edX partners section is not in the HTML
assert that the edX partners tag line is not in the HTML
assert that test course display name IS NOT VISIBLE, since that is a Microsite only course
assert that test course that is outside microsite IS VISIBLE
assert that footer template has been properly overriden on homepage
Access the microsite dashboard and make sure the right courses appear
Now access the non-microsite dashboard and make sure the right courses appear
Create ccx coach account
assign role to coach
user have access as coach on ccx
user dont have access as coach on ccx
Enroll user
Assert access of a student
Enroll student to the course
Course key is not None
Enroll student to the course
Staff can always enroll even outside the open enrollment period
User should be able access after completing required course
Masquerade staff
Masquerade instructor
get a fresh user object that won't have any cached role information
checks staff role
checks staff role and enrollment data
This explicitly sets the user_tag for self.student to ``user_tag``
Assert we see the proper icon in the top display  And proper tooltips
Assert that we can see the data from the appropriate test condition
Data is html encoded, because it's inactive inside the  sequence until javascript is executed
We define problem compenents that we need but don't explicitly call elsewhere.  pylint: disable=unused-variable
Data is html encoded, because it's inactive inside the  sequence until javascript is executed
We define problem compenents that we need but don't explicitly call elsewhere.  pylint: disable=unused-variable
Now that we have the course, change the position and save, nothing should explode!
Factories are self documenting  pylint: disable=missing-docstring
pylint: disable=super-method-not-called
make sure we can access courseware immediately
then wait a bit and see if we get timed out
re-request, and we should get a redirect to login page
Check that registration button is present
this text appears in that course's about page  common/test/data/2014/about/overview.html
Get the about page again and make sure that the page says that the course is full
Try to enroll as well
Check that registration button is not present
Check that registration button is not present
Check that registration button is present
Setup enrollment period to be in future
Check that registration button is not present
course price is not visible ihe course_about page when the course  mode is not set to honor
note that we can't call self.enroll here since that goes through  the Django student views, which doesn't allow for enrollments  for paywalled courses
course price is visible ihe course_about page when the course  mode is set to honor and it's price is set
note that we can't call self.enroll here since that goes through  the Django student views, which doesn't allow for enrollments  for paywalled courses
Create ccx coach account
create ccx
50 percent exam score should be achieved.
This user helps to cover a discovered bug in the milestone fulfillment logic
make sure toc is locked before allowing user to skip entrance exam
Login as member of staff
assert staff has access to all toc
Mocking get_required_content with empty list to assume user has passed entrance exam
pylint: disable=protected-access
Assert that orphan sequential is root of the discussion module.
not allowed to be passed in (i.e. was throwing exception)
fix was to allow get_items() to take the course_id parameter
create tab
name is as expected
link is as expected
verify active page name
validate tab
check get and set methods
check to_json and from_json methods
check equality methods
return tab for any additional tests
Test render works okay
this text appears in the test course's tab  common/test/data/2014/tabs/8e4cce2b4aaf4ba28b1220804619e41f.html
login as instructor hit skip entrance exam api in instructor app
tab types that should appear only once
add the unique tab multiple times
create 1 book per textbook type
initialize the course tabs to a list of all valid tabs
enumerate the tabs with no user
test including non-empty collections
test not including empty collections
get tab by type
get tab by id
Data from YAML common/lib/xmodule/xmodule/templates/NAME/default.yaml  METADATA must be overwritten for every instance that uses it. Otherwise,  if we'll change it in the tests, it will be changed for all other instances  of parent class.
Turn off cache.
username = robot{0}, password = 'test'
Regression test; LOC-85
Now try to access with dark lang
Clearing the language should go back to site default
Create a registration for the user
Create a profile for the user
Create the test client
Url and site lang vars for tests to use
Regression test; LOC-87
Visit the front page; verify we see site default lang
Regression test; LOC-87
Upload english transcript.
Upload non-english transcript.
Upload english transcript.
Upload non-english transcript.
Test different language to ensure we are just ignoring it since we can't  translate with static fallback
No language
No filename in request.GET
no self.sub, self.youttube_1_0 exist, but no file in assets
no self.sub and no self.youtube_1_0, no non-en transcritps
Use toy course from XML
refresh the course from the modulestore so that it has children
reload the chapter from the store so its children information is updated
disable the visibility of the sections in the chapter
Verify staff has been enrolled to the given course
Set up the edxmako middleware for this request to create the RequestContext
Set up the edxmako middleware for this request to create the RequestContext
depreciated function
Since registration_price is set, it overrides the cosmetic_display_price and should be returned
Since registration_price is not set, cosmetic_display_price should be returned
Since both prices are not set, there is no price, thus "Free"
Toy course has no course end date or about/end_date blob
test_end has a course end date, no end_date HTML blob
test_about_blob_end_date has both a course end date and an end_date HTML blob.  HTML blob wins
log into a staff account
Tests that we do not get an "Invalid x" response when passing correct arguments to view
log into a staff account
log into a staff account
store state via the UserStateClient
Verify that the email opt-in checkbox appears, and that the expected  organization name is displayed.
Verify that the email opt-in checkbox does not appear
This is a static page, so just assert that it is returned correctly
Ensure that the user can only apply for assistance in  courses which have a verified mode which hasn't expired yet,  where the user is not already enrolled in verified mode
Middleware is not supported by the request factory. Simulate a  logged-in user by setting request.user manually.
Set up the edxmako middleware for this request to create the RequestContext
assertRedirects would be great here, but it forces redirections to be absolute URLs.
Same for setting the due date to None
hide due date completely
Set up the edxmako middleware for this request to create the RequestContext
Set up the edxmako middleware for this request to create the RequestContext
The start date is set in the set_up_course function above.
The start date is set in common/test/data/two_toys/policies/TT_2012_Fall/policy.json
Set up the edxmako middleware for this request to create the RequestContext
Test that malicious code does not appear in html
Create new course with respect to 'default_store'
Create a new course, a user which will not be enrolled in course, admin user for staff access
Create and enable Credit course
Configure a credit provider for the course
Add a single credit requirement (final grade)
Enable the feature, but do not enable it for this course
Enable the feature, but do not enable it for this course
Enable certificate generation for this course
when course certificate is not active
Enable the feature, but do not enable it for this course
Enable certificate generation for this course
If user has not grade then false will return
Mocking the grades.grade  If user has above passing marks then True will return
Mocking the grades.grade  If user has below passing marks then False will return
Mocking the grades.grade  If user's achieved passing marks are equal to the required passing  marks then it will return True
If try to access a course with valid key pattern then it will return  bad request code with course is not valid message
If try to access a course with invalid key pattern then 404 will return
We don't need actual children to test this.
Set up the edxmako middleware for this request to create the RequestContext
Set up the edxmako middleware for this request to create the RequestContext
Set up the edxmako middleware for this request to create the RequestContext
Referencing a non-existent VAL ID in courseware won't cause an error --  it'll just fall back to the values in the VideoDescriptor.
make sure the desktop_mp4 url is included as part of the alternative sources.
make sure the urls for the various encodings are included as part of the alternative sources.
test with and without edx_video_id specified.
the video is associated in VAL so no cache miss should ever happen but test retrieval in both contexts
The video is not in VAL so in contexts that do and don't allow cache misses we should always get a fallback
arbitrary constant
Check whether the user has been enrolled in the course.  There was a bug in which users would be automatically enrolled  with is_active=False (same as if they enrolled and immediately unenrolled).  This verifies that the user doesn't have *any* enrollment record.
Create ccx coach account
create ccx
this text appears in that course's course info page  common/test/data/2014/info/updates.html
should redirect
Check both that the user is created, and inactive
and now we try to activate  Now make sure that the user is now actually activated
format the response dictionary to be sent in the post request by adding the above prefix to each key
Tell Django to clean out all databases, not just default  arbitrary constant
re-fetch the course from the database so the object is up to date
if we don't already have a chapter create a new one
now that we've added the problem and section to the course  we fetch the course from the database so the object we are  dealing with has these additions
list of grade summaries for each section
get the first section that matches the url (there should only be one)
Tell Django to clean out all databases, not just default
names for the problem in the homeworks
Now fetch the state entry for that problem.  count how many state history entries there are
now click "show answer"
check that we don't have more state history entries
problem isn't in the cache
problem is in the cache
Verify that the submissions API was sent an anonymized student ID
Get both parts correct
Enable the course for credit
Configure a credit provider for the course
Add a single credit requirement (final grade)
Tell Django to clean out all databases, not just default
re-fetch the course from the database so the object is up to date
Tell Django to clean out all databases, not just default
define the correct and incorrect responses to this problem
re-fetch the course from the database so the object is up to date
define the correct and incorrect responses to this problem
re-fetch the course from the database so the object is up to date
define the correct and incorrect responses to this problem
re-fetch the course from the database so the object is up to date
Just make sure we can process this without errors.
Throw in a non-ASCII answer
Now make more submissions by our original user
We'll submit one problem, and then muck with the student_answers  dict inside its state to try different data types (str, int, float,  none)
If there's a StudentModule entry for content that no longer exists,  we just quietly ignore it (because we can't display a meaningful url  or name for it).
It should be empty (ignored)
Submit p1
Now fetch the StudentModule entry for p1 so we can corrupt its state
Submit p2
Now add the student to the specified group.
Group 1 will have 1 problem in the section, worth a total of 1 point.
Submit answers for problem in Section 1, which is visible to all students.
Grade percent is .63. Here is the calculation
Grade percent is .75. Here is the calculation
Group 1 will have 1 problem in the section, worth a total of 1 point.
Grade percent is .25. Here is the calculation.
Grade percent is .75. Here is the calculation.
check word cloud response for every user
1.
2.  Invcemental state per user.
Final state after all posts.
3.
4.
The courseware url should redirect, not 200
Shouldn't be able to get to the instructor pages
Now should be able to get to self.course, but not  self.test_course
Now should be able to get to self.course, but not  self.test_course
First, try with an enrolled student
shouldn't be able to get to anything except the light pages
Enroll in the classescan't see courseware otherwise.
should now be able to get to everything for self.course
and now should be able to load both
First, try with an enrolled student
Then, try as an instructor
Then, try as global staff
student user shouldn't see it
now the student should see it
however we want to make sure we persist the course_id
Pass `emit_signals=True` so that these courses are cached with CourseOverviews.
Request filtering for an org distinct from the designated microsite org.
Request filtering for an org matching the designated microsite org.
Test render works okay
Verify staff initially can see staff debug
Toggle masquerade to student
Toggle masquerade back to staff
Verify that staff initially can see "Show Answer".
Toggle masquerade to student
Toggle masquerade back to staff
Log in as staff, and check we can see the info page.
Answer correctly as the student, and check progress.
Log in as staff, and check the problem is unanswered.
Masquerade as the student, and check we can see the student state.
Temporarily override the student state.
Reload the page and check we see the student state again.
Become the staff user again, and check the problem is still unanswered.
Verify the student state did not change.
Log in as staff, and check we can see the info page.
Masquerade as the student, and check we can see the info page.
Verify that there is no masquerading group initially
Verify that the masquerading group is returned
Configure course as a credit course
Create a user and log in
Enroll the user in the course as "verified"
The user hasn't satisfied any of the credit requirements yet, but she  also hasn't failed any.
Mark the user as eligible for all requirements
Mark the user as having failed both requirements
Verify the requirements are shown only if the user is in a credit-eligible mode.
Clear the internal staticfiles caches, to get test isolation.
This string comes from footer.html  This string comes from header.html
Test that a theme adds itself to the staticfiles search path.
pylint: disable=missing-docstring
Create a released discussion module
Create a scheduled discussion module
Create a `self_paced` course and add a beta tester in it
Verify course is `self_paced` and course has start date but not section.
Verify that non-staff user do not have access to the course
Verify beta tester can access the course as well as the course sections
Only the released module should be visible when the course is instructor-paced.
pylint: disable=missing-docstring
Since field_data is responsible for attribute access, you'd  expect it to raise AttributeError. In fact, it raises KeyError,  so we check for that.
Tell Django to clean out all databases, not just default
We only record block state history in DjangoUserStateClient  when the block type is 'problem'
We're skipping these tests because the iter_all_by_block and iter_all_by_course  are not implemented in the DjangoXBlockUserStateClient
delete the gray/worm groups from the partitions now so we can test scenarios  for user whose group is missing.
add a staff user, whose access will be unconditional in spite of group access.
this test isn't valid unless block_accessed is a descendant of  block_specified.
Initially, "red_cat" user can't view the vertical.
Change the vertical's user_partitions value to the empty list. Now red_cat can view the vertical.
Finally, add back in a cohort user_partition
make sure we redirect to the course about page
Tell Django to clean out all databases, not just default
There should be only one query to load a single descriptor with a single user_state field
This should only read from the cache, not the database
This should only read from the cache, not the database
because we're patching the underlying save, we need to ensure the  fields are in the cache
Tell Django to clean out all databases, not just default
The descriptor has no fields, so FDC shouldn't send any queries
Each field is stored as a separate row in the table,  but we can query them in a single query
Each field is a separate row in the database, hence  a separate query
Construct a mock module for the modulestore to return
get the rendered HTML output which should have the rewritten link
See if the url got rewritten to the target link  note if the URL mapping changes then this assertion will break
Verify that handle ajax is called with the correct data
get_score_bucket calls error cases 'incorrect'
check that _unwrapped_field_data is the same as the original  _field_data, but now _field_data as been reset.  pylint: disable=protected-access, no-member
now bind this module to a few other students
_field_data should now be wrapped by LmsFieldData  pylint: disable=protected-access, no-member
the LmsFieldData should now wrap OverrideFieldData  pylint: disable=protected-access, no-member
the OverrideFieldData should point to the original unwrapped field_data  pylint: disable=protected-access, no-member
Construct a mock module for the modulestore to return
We can't use `name` as a kwarg to Mock to set the name attribute  because mock uses `name` to name the mock itself
we expect there not to be a 'proctoring' key in the dict
refresh cache after update
No matter what data goes in, there should only be one close-script tag.
pylint: disable=attribute-defined-outside-init
The "set" here is to work around the bug that load_classes returns duplicates for multiply-delcared classes.
Use the xblock_class's bind_for_student method
This value is set by observation, so that later changes to the student  id computation don't break old data
This value is set by observation, so that later changes to the student  id computation don't break old data
This value is set by observation, so that later changes to the student  id computation don't break old data
Bind the module to another student, which will remove "correct_map"  from the module's _field_data_cache and _dirty_fields.
pylint: disable=no-member
pylint: disable=attribute-defined-outside-init, no-member
Validate direct XModule access as well
Validate direct XModule access as well
Create a child of each block type for each user
pylint: disable=no-member
But we should still have five gradesets
Even though two will simply be empty
The rest will have grade information in them
add score to cache
push to remote cache
create a new cache with the same params, fetch from remote cache
see cache is populated
Tell Django to clean out all databases, not just default
pylint: disable=protected-access
DOM elements that appear in an xBlock,  but are excluded from the xBlock-only rendering.
The key used to store a user's course-level masquerade information in the Django session.  The value is a dict from course keys to CourseMasquerade objects.
The key used to store temporary XBlock field data in the Django session.  This is where field  data is stored to avoid modifying the state of the user we are masquerading as.
All parameters to this function must be named identically to the corresponding attribute.  If you remove or rename an attribute, also update the __setstate__() method to migrate  old data from users' sessions.
Sentinel object to mark deleted objects in the session cache
We can't simply delete the key from the session, since it might still exist in the kvs,  which we are not allowed to modify, so we mark it as deleted by setting it to  _DELETED_SENTINEL in the session.
Prevent jQuery menu animations from interferring with the clicks
Open the 2nd section
Click on the subsection to see the content
Click on the 2nd subsection to see the content
Generate the problem XML using capa.tests.response_xml_factory  Since this is just a placeholder, we always use multiple choice.
Add the problem
Wait for the problem to reload
error is shown
iframe is not presented
link is not presented
iframe is visible
To start with you should only have one window/tab
Give it a few seconds for the LTI window to appear
Verify the LTI window
inside iframe test content is presented
First clear the modulestore so we don't try to recreate  the same course twice  This also ensures that the necessary templates are loaded
Create the course  We always use the same org and display name,  but vary the course identifier (e.g. 600x or 191x)
create beta tester
Enroll the user in the course and log them in
You should now have 2 browser windows open, the original courseware and the LTI
For verification, iterate through the window titles and make sure that  both are there.
First clear the modulestore so we don't try to recreate  the same course twice  This also ensures that the necessary templates are loaded
Create the course  We always use the same org and display name,  but vary the course identifier (e.g. 600x or 191x)
Add a chapter to the course to contain problems
Create the course
Create the user
Do we really use these 3 w/ a different course than is in the scenario_dict? if so, why? If not,  then get rid of the override arg
Make sure that the problem has been completely rendered before  starting to input an answer.
Correct answer is any two integers that sum to 10
If we want an incorrect answer, then change  the second addend so they no longer sum to 10
The other response types use random data,  which would be difficult to check  We trade input value coverage in the other tests for  input type coverage in this test.
Create a problem item using our generated XML  We set rerandomize=always in the metadata so that the "Reset" button  will appear.
this is necessary due to naming requirement for this problem type
If the input element doesn't exist, fail immediately
Retrieve the input element
Remove any trailing spaces that may have been added
Ensure that the course has this problem type
Go to the one section in the factory-created course  which should be loaded with the correct problem
Set the fake xqueue server to always respond  correct/incorrect when asked to grade a problem
Change the answer on the page
Submit the problem
Wait for the problem to finish re-rendering
first scroll down so the loading mathjax button does not  cover up the Check button
Wait for the problem to finish re-rendering
The label text is changed by static/xmodule_js/src/capa/display.js  so give it some time to change on the page.
The problem progress is changed by  cms/static/xmodule_js/src/capa/display.js  so give it some time to render on the page.
Determine which selector(s) to look for based on correctness
At least one of the correct selectors should be present
As soon as we find the selector, break out of the loop
Expect that we found the expected selector
Ensure all events are written out to mongo before querying.
Importing signals is necessary to activate the course publish/delete signal handlers.
Course identifier (opaque_keys.edx.keys.CourseKey)
User object (django.contrib.auth.models.User)
Cached value of whether the user has staff access (bool/None)
The countdown=0 kwarg ensures the call occurs after the signal emitter  has finished all operations.
Default list of transformers for manipulating course block structures  based on the user's access to the course blocks.
compute merged value of visible_to_staff_only from all parents
set the merged value for this block  merge visible_to_staff_only from all parents and this block
Users with staff access bypass the Visibility check.
compute merged value of start date from all parents
set the merged value for this block  no parents so just use value on block or default
no value on this block so take value from parents
max of merged-start-from-all-parents and this block
Users with staff access bypass the Start Date check.
update selected
publish events for analytics
Check and remove all non-selected children from course  structure.
Create dict of child location to group_id, using the  group_id_to_child field on the split_test module.
Set group access for each child using its group_access  field so the user partitions transformer enforces it.
The UserPartitionTransformer will enforce group access, so  go ahead and remove all extraneous split_test modules.
First have the split test transformer setup its group access  data for each block.
Because user partitions are course-wide, only store data for  them on the root block.
If there are no user partitions, this transformation is a  no-op, so there is nothing to collect.
{ partition.id: set(IDs of groups that can access partition) }  If partition id is absent in this dict, no group access  restrictions exist for that partition.
Get the group_access value that is directly set on the xblock.  Do not get the inherited value since field inheritance doesn't  take a union of them for DAGs.
Running list of all groups that have access to this  block, computed as a "union" from all parent chains.  Set the default to universal access, for the case when  there are no parents.
Set the default to most restrictive as we iterate  through all the parent chains.
Group access for this partition as stored on the xblock
Compute this block's access by intersecting the block's  own access with the merged access from its parent chains.
Add this partition's access only if group restrictions  exist.
If the user is not assigned to a group for this partition,  deny access.
If the user belongs to one of the allowed groups for this  partition, then move and check the next partition.
Else, deny access.
The user has access for every partition, grant access.
Enroll user in course.
Set up users.
First remove the block from the course.  It would be re-added to the course if the course was  explicitly listed in parents.
Add this to block to each listed parent.
recursively call the children
build the course tree
add additional parents if the course is a DAG or built  linearly (without specifying 'children' values)
Tree formed by parent_map:         0      /     \     1       2    / \     / \   3   4   /   5        \ /         6  Note the parents must always have lower indices than their  children.
create the course
an ordered list of block locations, where the index  corresponds to the block's index in the parents_map.
create all other blocks in the course
verify given test user has access to expected blocks
verify staff has access to all blocks
compute access results of the block
compare with expected value
Set up groups
Set up user partitions and groups.
Build course.
Enroll user in course.
Set up cohorts.
Set up multiple user partitions and groups.
Set up cohorts.
universal access throughout
partitions 1 and 2 requiring membership in list of groups
parent inheritance    1 parent allows
1 parent denies
1 parent denies, 1 parent allows
intersect with parent    child denies, 1 parent allows
child allows, 1 parent allows, 1 parent denies
use the course as the block to test
update block access
convert merged_parents_list to _MergedGroupAccess objects
convert group_id to groups in user_partition_groups parameter
Build course.
Enroll user in course.
user was randomly assigned to one of the groups
course is not visible to staff only
course becomes visible to staff only
Mode a badge was awarded for. Included for legacy/migration purposes.
Actual max is 256KB, but need overhead for badge baking. This should be more than enough.
pylint: disable=no-member
Fall back to default, if there is one.
Use the standard Configuration Model Admin handler for this model.
Make this unique to the course, and down to 64 characters.  We don't do this to badges without issuing_component set for backwards compatibility.
Will be 64 characters.
Should be the hashed result of test_slug as the slug, and test_component as the component
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Can't preserve old badges without modes.
-*- coding: utf-8 -*-
We're not configured to make a badge for this course mode.
Badge already exists. Skip.
pylint: disable=no-member
pylint: disable=no-member
pylint: disable=no-member
pylint: disable=no-member
pylint: disable=no-member
pylint: disable=no-member
pylint: disable=no-member
We don't award badges until all three are set.  pylint: disable=no-member
Password defined by factory.
pylint: disable=no-member
pylint: disable=no-member
Also create a version of this badge under a different course.  Same badge class, but different user. Should not show up in the list.
Different badge class AND different user. Certainly shouldn't show up in the list!
pylint: disable=no-member
pylint: disable=no-member
pylint: disable=no-member
We might want to get all the matching course scoped badges to see how many courses  a user managed to get a specific award on.
Need to get all badges for the user.
Django won't let us use 'None' for querying a ForeignKey field. We have to use this special  'Empty' value to indicate we're looking only for badges without a course key set.
Run this twice to verify there wasn't a background creation of the badge.
Make sure wiki_plugin.py gets run.
Markdown 2.1.0 changed from 2.0.3. We try importing the new version first,  but import the 2.0.3 version if it fails
Needs to come before escape matching because \ is pretty important in LaTeX
Markdown 2.1.0 changed from 2.0.3. We try importing the new version first,  but import the 2.0.3 version if it fails
Override defaults with user settings
pylint: disable=no-member
pylint: disable=no-member
pylint: disable=no-member
pylint: disable=no-member
pylint: disable=no-member
pylint: disable=no-member
pylint: disable=no-member
We will create it in the next block
create it
Somehow we got a urlpath without an article. Just delete it and  recerate it.
Translators: this string includes wiki markup.  Leave the ** and the _ alone.
When not enrolled, we should get a 302
When not logged in, we should get a 302
and end up at the login page
This string comes from themes/red-theme/lms/templates/footer.html
Django-wiki expects article slug to be non-numerical. In case the  course number is numerical append an underscore.
Ancestors of /Phy101/Mechanics/Acceleration/ is a list of URLPaths  ['Root', 'Phy101', 'Mechanics']
See if we are able to view the course. If we are, redirect to it  Even though we came from the course, we can't see it. So don't worry about it.
we care only about requests to wiki urls
wiki pages are login required
This is a /courses/org/name/run/wiki request  HACK: django-wiki monkeypatches the reverse function to enable  urls to be rewritten
if a user is logged in, but not authorized to see a page,  we'll redirect them to the course about page  set the course onto here so that the wiki template can show the course navigation
Check to see if we don't allow top-level access to the wiki via the /wiki/xxxx/yyy/zzz URLs  this will help prevent people from writing pell-mell to the Wiki in an unstructured way
Temporarily remove the grant type to avoid triggering the super method's code that removes request.user.
Ensure the tokens get associated with the correct user since DOT does not normally  associate access tokens issued with the client_credentials grant to users.
Restore the original request attributes
Extracting order information
Extracting OrderItem information of unit_cost, list_price and status
if coupon is redeemed against the order, update the information in the order_item_dict
Extracting sale information
Report run date
For data extractions on the 'meta' field  the feature name should be in the format of 'meta.foo' where  'foo' is the keyname in the meta dictionary
now featch the requested meta fields
Note that we use student.course_groups.all() here instead of  student.course_groups.filter(). The latter creates a fresh query,  therefore negating the performance gain from prefetch_related().
Are we dealing with an "old-style" problem location?
we have to capture the redeemed_by value in the case of the downloading and spent registration  codes csv. In the case of active and generated registration codes the redeemed_by value will be None.   They have not been redeemed yet
Ensure that UsageKey.from_string returns a problem key that list_problem_responses can work with  (even when called with a dummy location):
Ensure that StudentModule.objects.filter returns a result set that list_problem_responses can work with  (this keeps us from having to create fixtures for this test):
Check if list_problem_responses called UsageKey.from_string to look up problem key:  Check if list_problem_responses called StudentModule.objects.filter to obtain relevant records:
This behaviour is somewhat inconsistent: None string fields  objects are converted to "None", but non-JSON serializable fields  are converted to an empty string.
get the updated item  get the redeemed coupon information
get the updated item
Create a paid course mode.
choices with a restricted domain, e.g. level_of_education  choices with a larger domain e.g. year_of_birth
to be set later
short name and display name (full) of the choices.
change none to no_data for valid json key  django does not properly count NULL values when using annotate Count  so      distribution['no_data'] = distribution.pop(None)  would always be 0.
Correctly count null values
now call the actual save method
first remove any answer the user might have done before
make sure the form is wrap in some outer single element  otherwise lxml can't parse it  NOTE: This wrapping doesn't change the ability to query it
adding the course_id where the end-user answered the survey question  since it didn't exist in the beginning, it is nullable
See if there is an answer stored for this user, form, field_name pair or not  this will allow for update cases. This does include an additional lookup,  but write operations will be relatively infrequent
Allow for update cases.
the result set from get_answers, has an outer key with the user_id  just remove that outer key to make the JSON payload simplier
support multi-SELECT form values, by string concatenating them with a comma separator
the URL we are supposed to redirect to is  in a hidden form field
scrub the answers to make sure nothing malicious from the user gets stored in  our database, e.g. JavaScript  only allow known input fields
The HTTP end-point for the payment processor.
-*- coding: utf-8 -*-
Create two accounts
is the SurveyForm html present in the HTML response?
however we want to make sure we persist the course_id
update
update with just a subset of the origin dataset
even though we have 2 users submitted answers  limit the result set to just 1
this will throw exception if not found, but a non existing survey name will  be trapped in the above is_survey_required_for_course() method
survey is required and it exists, let's see if user has answered the survey  course staff do not need to answer survey
this is here to support registering the signals in signals.py
if anything goes wrong rendering the receipt, it indicates a problem fetching order data.
don't assume the signal was fired with `send_robust`.  avoid blowing up other signal handlers by gracefully  trapping the Exception and logging an error.
this is a known limitation; commerce service does not presently  support the case of a non-superusers initiating a refund on  behalf of another user.
no other error is anticipated, so re-raise the Exception
at least one refundable order was found.
don't break, just log a warning
no refundable orders were found.
Copy the tags to avoid modifying the original list.
Remove duplicates
Encode the data to create a JSON payload
this is not presently supported with the external service.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
LMS utilizes User.user_is_active to indicate email verification, not whether an account is active. Sigh!
log the error, return silently
If there is no audit or honor course mode, this most likely  a Prof-Ed course. Return an error so that the JS redirects  to track selection.
Accept either honor or audit as an enrollment mode to  maintain backwards compatibility with existing courses
Make the API call
Pass data to the client to begin the payment flow.
The order was completed immediately because there is no charge.
Ignore events fired from UserFactory creation
Verify that an audit message was logged
Validate the response content
Set user's active flag
Set user's active flag
Place an order
Remove SKU from all course modes
Validate the response
Ensure that the user is not enrolled and that no calls were made to the E-Commerce API
The view should return an error status code
Set SKU to empty string for all modes.
Enroll user in the course
NOTE (CCB): Ideally, the course modes table should only contain data for courses that exist in  modulestore. If that is not the case, say for local development/testing, carry on without failure.
Override the verification deadline for the course (not the individual modes)
Django Rest Framework v3 requires that we provide a queryset.  Note that we're overriding `get_object()` below to return a `Course`  rather than a CourseMode, so this isn't really used.
There is nothing to pre-save. The default behavior changes the Course.id attribute from  a CourseKey to a string, which is not desired.
In cases where upgrade_deadline is None (e.g. the verified professional mode), allow a verification  deadline to be set anyway.
Sanity check: The API should return HTTP status 200 for updates
Sanity check: Ensure no verification deadline is set
Generate the expected data
Sanity check: The API should return HTTP status 200 for updates
Verify the course and modes are returned as JSON
Verify the verification deadline is updated
Now set the deadline to None
The existing CourseMode should have been removed.
Verify the display names are correct
fake an ecommerce api request.
a key will be missing; we will not expect the receipt page to handle a cybersource decision
Verify that the header navigation links are hidden for the edx.org version
Verify that the logged message contains comma-separated  key-value pairs ordered alphabetically by key.
override this in subclasses.
override this in subclasses, using one of httpretty's method constants
if skip_refund is set to True in the signal, we should not try to initiate a refund.
if the course_enrollment is not refundable, we should not try to initiate a refund.
no HTTP request/user: auth to commerce service as the unenrolled student.
HTTP user is another server (AnonymousUser): do not try to initiate a refund at all.
See class docstring for description of status states
Where we place the uploaded image files (e.g. S3 URLs)
Randomly generated UUID so that external services can post back the  results of checking a user's photo submission without use exposing actual  user IDs or something too easily guessable.
If the review was done by an internal staff member, mark who it was.
Mark the name of the service used to evaluate this attempt (e.g  Software Secure).
If status is "denied", this should contain text explaining why.
Non-required field. External services can add any arbitrary codes as time  goes on. We don't try to define an exhuastive list -- this is just  capturing it so that we can later query for the common problems.
This should only be one at the most, but just in case we create more  by mistake, we'll grab the most recently created one.
user_has_valid_or_pending does include 'approved', but if we are  here, we know that the attempt is still pending
If someone is denied their original verification attempt, they can try to reverify.
If there's no deadline, then return the most recently created verification
Otherwise, look for a verification that was in effect at the deadline,  preferring recent verifications.  If no such verification is found, implicitly return `None`
At any point prior to this, they can change their names via their  student dashboard. But at this point, we lock the value into the  attempt.
If someone approves an outdated version of this, the first one wins
Upload this to S3
Update our record fields
find the messages associated with this category
if we can't parse the message as JSON or the category doesn't  match one of our known categories, show a generic error
Override the receipt ID if one is provided.  This allow us to construct S3 keys to images submitted in previous attempts  (used for reverification, where we send a new face photo with the same photo ID  from a previous attempt).
If we're copying the photo ID image from a previous verification attempt,  then we need to send the old image data with the correct image key.
The system prefers to set this automatically based on default settings. But  if the field is set manually we want a way to indicate that so we don't  overwrite the manual setting of the field.
Maintain a history of changes to deadlines for auditing purposes
Endpoint for in-course reverification  Users are sent to this end-point from within courseware  to re-verify their identities by re-submitting face photos.
These steps can be skipped using the ?skip-first-step GET param
Messages  Depending on how the user entered reached the page,  we will display different text messaging.  For example, we show users who are upgrading  slightly different copy than users who are verifying  for the first time.
Deadline types
Parse the course key  The URL regex should guarantee that the key format is valid.
Verify that the course exists
Check whether the user has access to this course  based on country access rules.
Redirect the user to a more appropriate page if the  messaging won't make sense based on the user's  enrollment / payment / verification status.
If the user set a contribution amount on another page,  use that amount to pre-fill the price selection form.
Remember whether the user is upgrading  so we can fire an analytics event upon payment.
Determine the photo verification status
get available payment processors  transaction will be conducted via ecommerce service  transaction will be conducted using legacy shopping cart
If the user is already enrolled but hasn't yet paid,  then the "upgrade" messaging is more appropriate.
If the user is NOT enrolled, then send him/her  to the first time verification page.
If the student has paid, but not verified, redirect to the verification flow.
IIf the user is trying to pay, has activated their account, and the ecommerce service  is enabled redirect him to the ecommerce checkout page.
Redirect if necessary, otherwise implicitly return None
Retrieve all the modes at once to reduce the number of database queries
Retrieve the first mode that matches the following criteria:   * Unexpired   * Price > 0   * Not credit
Otherwise, find the first non credit expired paid mode
Otherwise, return None and so the view knows to respond with a 404.
The "make payment" step doubles as an intro step,  so if we're showing the payment step, hide the intro step.
return 'expiration_datetime' of latest photo verification if found,  otherwise implicitly return ''
Make an API call to create the order and retrieve the results
Pass the payment parameters directly from the API response.
if request.POST doesn't contain 'processor' then the service's default payment processor will be used.
(XCOM-214) To be removed after release.  the absence of this key in the POST payload indicates that the request was initiated from  a stale js client, which expects a response containing only the 'payment_form_data' part of  the payment data result.
Validate the POST parameters
If necessary, update the user's full name
Retrieve the image data  Validation ensures that we'll have a face image, but we may not have  a photo ID image if this is a reverification.
If we have a photo_id we do not want use the initial verification image.
Submit the attempt
Send a URL that the client can redirect to in order  to return to the checkpoint in the courseware.
Pull out the parameters we care about.
The face image is always required.
Decode face image data (used for both an initial and re-verification)
Decode the photo ID image data if it's provided
We will always have face image data, so upload the face image
Submit the attempt
We catch all exceptions and log them.  It would be much, much worse to roll back the transaction due to an uncaught  exception than to skip sending the notification email.
Catch exception if unable to add credit requirement  status for user
This is what we should be doing...     return HttpResponseBadRequest("Signature is invalid")
This is what we're doing until we can figure out why we disagree on sigs
Trigger ICRV email only if ICRV status emails config is enabled
emit the reverification event
Set the attempts status to 'must_retry' so that we can re-submit it
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Get-or-create the verification checkpoint
Avoid circular import
As a user skips the reverification it declines to fulfill the requirement so  requirement sets to declined.
Choose an uncommon number for the price so we can search for it on the page
Go to the course mode page, expecting a redirect to the intro step of the  payment flow (since this is a professional ed course). Otherwise, the student  would have the option to choose their track.
For professional ed courses, expect that the student is NOT enrolled  automatically in the course.
On the first page of the flow, verify that there's a button allowing the user  to proceed to the payment processor; this is the only action the user is allowed to take.
Longer string
Even though our AES key is only 32 bytes, RSA encryption will make it 256  bytes, and base64 encoding will blow that up to 344
This shouldn't happen if the student has been auto-enrolled,  but if they somehow end up on this page without enrolling,  treat them as if they need to pay
If unenrolled, treat them like they haven't paid at all  (we assume that they've gotten a refund or didn't pay initially)
Verify that the header navigation links are hidden for the edx.org version
We've already paid, and now we're trying to verify
Expect that *all* steps are displayed,  but we start after the payment step (because it's already completed).
These will be hidden from the user anyway since they're starting  after the payment step.
Already verified, so if we somehow end up here,  redirect immediately to the dashboard
Expect that *all* steps are displayed,  but we start at the payment confirmation step
These will be hidden from the user anyway since they're starting  after the payment step.  We're already including the payment  steps, so it's easier to include these as well.
Expect that *all* steps are displayed,  but we start on the first verify step
There are no other steps, so stay on the  payment confirmation step
If we've already paid, then the upgrade messaging  won't make much sense.  Redirect them to the  "verify later" page instead.
Already verified and paid, so redirect to the dashboard
Do NOT specify a contribution for the course in a session var.
Expect that the contribution amount is NOT pre-filled,
Specify a contribution amount for this course in the session
Expect that the contribution amount is pre-filled,
Set the upgrade deadline (course mode expiration) and verification deadline  to the same value.  This used to be the default when we used the expiration datetime  for BOTH values.
Need to be enrolled
Simulate paying for the course and enrolling
Enter the verification part of the flow  Expect that we are able to verify
Check that the mode selected is expired verified mode not the credit mode  because the direct enrollment to the credit mode is not allowed.
Enroll as verified (simulate purchasing the verified enrollment)
Simulate that we're embargoed from accessing this  course based on our IP address.
Set the course mode expiration (same as the "upgrade" deadline)
Set the verification deadline
setting a nonempty sku on the course will a trigger calls to  the ecommerce api to get payment processors.
ensure the mock api call was made.  NOTE: the following line  approximates the check - if the headers were empty it means  there was no last request.
mock out the payment processors endpoint
Call the function
Verify that an audit message was logged
Use the wrong HTTP method
Submit the photos
Verify that the attempt is created in the database
Verify that the user's name wasn't changed
Submit the photos, along with a name change
Check that the user's name was changed in the database
Error because invalid parameters, so no confirmation email  should be sent.
Mock the POST to Software Secure
Submit an initial verification attempt
Submit a face photo for re-verification
Verify that the initial attempt sent the same ID photo as the reverification attempt
Verify that the second attempt sent the updated face photo
Submit a new face photo and photo id for verification
Verify that the initial attempt sent a new ID photo for the reverification attempt
Missing face image parameter
Submit face image data, but not photo ID data.  Since the user doesn't have an initial verification attempt, this should fail
Create the initial verification attempt with some dummy  value set for field 'photo_id_key'
Now the request should succeed
Verify that photo submission confirmation email was sent
Verify that photo submission confirmation email was not sent
Verify that photo submission confirmation email was sent
Verify that ICRV status email was sent when config is enabled
Create the 'edx-reverification-block' in course tree
now check that '_send_email' method is called on result callback  with required parameters
Create checkpoint
Add a re-verification attempt
Add a re-verification attempt status for the user
User has a denied attempt, so can reverify
User has a verification attempt, but it's expired
Allow the student to reverify
User has submitted a verification attempt, but Software Secure has not yet responded
Can re-verify because an attempt has already been submitted.
Submitted attempt has been approved
Cannot reverify because the user is already verified.
Enroll the user in the default mode (honor) to emulate
mocking and patching for bi events
Retrieve a checkpoint that doesn't yet exist
Since the user has no initial verification and we're not sending the ID photo,  we should expect a 400 bad request
Check that the checkpoint status has been updated
Get the re-verification block to check the call made
Expect that the verification block is fetched
-*- coding: utf-8 -*-
The keys should be stored as Base64 strings, i.e. this should not explode
These should all fail because we're in the wrong starting state.
Now let's fill in some values so that we can pass the mark_ready() call
ready (can't approve or deny unless it's "submitted")
must_retry
submitted
approved
denied
Basic case, things go well.
We post, but Software Secure doesn't like what we send for some reason
We try to post, but run into an error (in this case a newtork connection error)
This user has no active at the moment...
Create an attempt and mark it ready...
A new user won't see this...
But if we create yet another one and mark it ready, it passes again.
And if we add yet another one with a later created time, we get that  one instead. We always want the most recent attempt marked ready()
We haven't marked attempt_3 ready yet, so attempt_2 still wins
Now we mark attempt_3 ready and expect it to come back
If it's any of these statuses, they don't have anything outstanding
Any of these, and we are. Note the benefit of the doubt we're giving  -- must_retry, and submitted both count until we hear otherwise
test for correct status when no error returned
test for when one has been created
create another one for the same user, make sure the right one is  returned
now delete the first one and verify that the denial is being handled  properly
Not active before the created date
Active immediately after created date
Active immediately before expiration date
Not active after the expiration date
No attempts in the query set, so should return None
Should also return None if no deadline specified
Make an attempt
Before the created date, should get no results
Immediately after the created date, should get the attempt
If no deadline specified, should return first available
Immediately after the expiration date, should not get the attempt
Create a second attempt in the same window
Now we should get the newer attempt
No initial verification for the user
Make an initial verification with 'photo_id_key'
Check that method 'get_initial_verification' returns the correct  initial verification attempt
Now create a second verification without 'photo_id_key'
Test method 'get_initial_verification' still returns the correct  initial verification attempt which have 'photo_id_key' set
create the 'VerificationCheckpoint' checkpoint
Retrieving a checkpoint that doesn't yet exist will create it
Create the checkpoint
create the VerificationCheckpoint checkpoint
test creating the VerificationCheckpoint checkpoint with same course  id and checkpoint name
adding two check points.
make an attempt for the 'first_checkpoint'
make another attempt for the 'first_checkpoint'
make new attempt for the 'second_checkpoint'
remove the attempt from 'second_checkpoint'
adding verification status
test the status from database
add initial verification status for checkpoints
now add verification status for multiple checkpoint points
creating software secure attempt against checkpoint
add initial verification status for checkpoint
add verification status
create skipped attempt for different user
Initially, no deadlines are set
Create the deadlines
Warm the cache
Load the deadlines from the cache
Delete the deadlines
Verify that the deadlines are updated correctly
Enroll in a verified mode
testing service for skipped attempt.
No longer enrolled in a verified track
Should be marked as "skipped" (opted out)
A bad book id will be a 404.
A bad page id will cause a 404.
If the book id isn't an int, we'll get a 404.
If we have no books, asking for the first book will fail with a 404.
The chapter in the URL used to go right on the page.  It's no longer possible to use a non-integer chapter.
The page in the URL used to go right on the page.  It's no longer possible to use a non-integer page.
The page in the URL used to go right on the page.  It's no longer possible to use a non-integer page and a non-integer chapter.
If we have no books, asking for the first book will fail with a 404.
The chapter in the URL used to go right on the page.  It's no longer possible to use a non-integer chapter.
then remap all the chapter URLs as well, if they are provided.
define custom states used by InstructorTask
create the task_id here, and pass it into celery:
create the task, then save it:
In future, there should be a check here that the resulting JSON  will fit in the column.  In the meantime, just return an exception.
truncate any traceback that goes into the InstructorTask model:
Just setting the content encoding and type above should work  according to the docs, but when experimenting, this was necessary for  it to actually take.
exclude states that are "ready" (i.e. not "running", e.g. failure, success, revoked):
Create log entry now, so that future requests will know it's running.
Assume we don't always save the InstructorTask entry if we don't have to,  but that in most cases we will update the InstructorTask in-place with its  current progress.
construct a status message directly from the task result's result:  it needs to go back with the entry passed in.
on revocation, the result's result doesn't contain anything  but we cannot rely on the worker thread to set this status,  so we set it here.
save progress and state into the entry, even if it's not being saved:  when celery is run in "ALWAYS_EAGER" mode, progress needs to go back  with the entry passed in.
if the task is not already known to be done, then we need to query  the underlying task's result object:
create the key value by using MD5 hash:
create the key value by using MD5 hash:
check to see if task is already running, and reserve it otherwise:
return status for completed tasks and tasks in progress
special message for providing progress updates:  Translators: {action} is a past-tense verb that is localized separately. {attempted} and {succeeded} are counts.
Translators: {action} is a past-tense verb that is localized separately. {succeeded} and {attempted} are counts.
Translators: {action} is a past-tense verb that is localized separately. {succeeded} and {attempted} are counts.
provide a default:  Translators: {action} is a past-tense verb that is localized separately. {succeeded} and {attempted} are counts.
Translators: {skipped} is a count.  This message is appended to task progress status messages.
Translators: {total} is a count.  This message is appended to task progress status messages.
Update status in task result object itself:
-*- coding: utf-8 -*-
exclude states that are "ready" (i.e. not "running", e.g. failure, success, revoked):
check arguments:  let exceptions return up to the caller.
check arguments:  let exceptions return up to the caller.
check to see if task is already running, and reserve it otherwise
check problems for rescoring:  let exceptions return up to the caller.
check to see if task is already running, and reserve it otherwise
check arguments:  make sure that the usage_key is defined  (since that's currently typed in).  If the corresponding module descriptor doesn't exist,  an exception will be raised.  Let it pass up to the caller.
check arguments:  make sure entrance exam(section) exists for given usage_key
check arguments:  make sure that the usage_key is defined  (since that's currently typed in).  If the corresponding module descriptor doesn't exist,  an exception will be raised.  Let it pass up to the caller.
check arguments:  make sure entrance exam(section) exists for given usage_key
Remove Content milestones that user has completed
Assume that the course is defined, and that the user has already been verified to have  appropriate access to the course. But make sure that the email exists.  We also pull out the targets argument here, so that is displayed in  the InstructorTask status.
create the key value by using MD5 hash:
define different loggers for use within tasks and on client side
define value to use when no task_id is provided:  define values for update functions to use to return status to perform_module_state_update
The setting name used for events when "settings" (account settings, preferences, profile information) change.
if the InstructorTask object does not exist, then there's no point  trying to update it.
Get the InstructorTask to be updated. If this fails then let the exception return to Celery.  There's no point in catching it here.
Get inputs to use in this task from the entry
Now do the work
Release any queries that the connection has been hanging onto
Log and exit, returning task_progress info as task result
if problem_url is present make a usage key from it
find the problem descriptor:
if entrance_exam is present grab all problems in it
find the modules in question
get request-related tracking information from args passthrough, and supplement with task-specific  information:
reconstitute the problem's corresponding XModule:
get request-related tracking information from args passthrough, and supplement with task-specific  information:
This module isn't being used for front-end rendering  pass in a loaded course for override enabling
unpack the StudentModule:
Either permissions just changed, or someone is trying to be clever  and load something they shouldn't have access to.
This should also not happen, since it should be already checked in the caller,  but check here to be sure.
get request-related tracking information from args passthrough,  and supplement with task-specific information:
Use the data dict and html template to generate the output buffer
Loop over all our students and build our CSV lists in memory
Periodically update task status (this is a cache write)
An empty gradeset means we failed to grade a student.
Perform the actual upload
If there are any error rows (don't count the header), write them out as well
One last update before we close out...
First, sort out all the blocks into their correct assignments and all the  assignments into their correct types.  Put the assignments in order into the assignments list.
Compute result table and format it
Perform the upload
This struct encapsulates both the display names of each static item in the  header row as values as well as the django User field names of those items  as the keys.  It is structured in this way to keep the values related.
There was an error grading this student.  Generally there will be a non-empty err_msg, but that is not always the case.
compute the student features table and format it
Perform the upload
Periodically update task status (this is a cache write)
translate header into a localizable display string
Perform the actual upload
One last update before we close out...
Compute result table and format it
Perform the upload
get the course executive summary report information.
Perform the upload
Compute result table and format it
Perform the upload
Generate Certificates for all white listed students.
Whitelist students who did not get certificates already.
We want to skip 'filtering students' only when students are given and statuses to regenerate are not
Mark existing generated certificates as 'unavailable' before regenerating  We need to call this method after "students_require_certificate" otherwise "students_require_certificate"  would return no results.
Generate certificate for each student
Iterate through rows to get total assignments for task progress
cohorts_status is a mapping from cohort_name to metadata about  that cohort.  The metadata will include information about users  successfully added to the cohort, users not found, and a cached  reference to the corresponding cohort object to prevent  redundant cohort queries.
Try to use the 'email' field to identify the user.  If it's not present, use 'username'.
Raised when the user is already in the given cohort
Return Students that have Generated Certificates and the generated certificate status  lies in 'statuses_to_regenerate'  Fetch results otherwise subsequent operations on table cause wrong data fetch
compute those students whose certificates are already generated
Return all the enrolled student skipping the ones whose certificates have already been generated
Mark generated certificates as 'unavailable' and update download_url, download_uui, verify_uuid and  grade with empty string for each row
check status returned:
set up test user for performing test operations
get descriptor:
update the data in the problem definition  confirm that simply rendering the problem again does not result in a change  in the grade:
redefine the problem (as stored in Mongo) so that the definition of correct changes  confirm that simply rendering the problem again does not result in a change  in the grade (or the attempts):
rescore the problem for all students
all grades should change to being wrong (with no change in attempts)
student A will get 100%, student B will get 50% because  OPTION_1 is the correct option, and OPTION_2 is the  incorrect option
test resubmitting, by updating the existing record:
Validate that record was added to CertificateGenerationHistory
Validate that record was added to CertificateGenerationHistory
view task entry for task failure
Add a chapter to the course
add a sequence to the course to which the problems can be added
Expand the dict reader generator so we don't lose it's content
get status for a task that is still running:
check for missing task_output
Check number of items for each subtask
Check number of items for each subtask
check that entries were set correctly  run the task  check that entries were reset
check that entries were set correctly  run the task  check that entries were reset
In auto cohorting a group will be assigned to a user only when user visits a problem  In grading calculation we only add a group in csv if group is already assigned to  user rather than creating a group automatically at runtime
Create course with group configurations
Assign user_a to a group in the 'cohort'-schemed user  partition (by way of a cohort) to verify that the user  partition group does not show up in the "Experiment Group"  cell.
check button text
check button text
check button text
Expand the dict reader generator so we don't lose it's content
Add a sequence to the course to which the problems can be added
Create a vertical
Verify generated grades and expected grades match
Course is cohorted
Verify user enrollment
Verify generated grades and expected grades match
check button text
apply the coupon code to the item in the cart
This assertion simply confirms that the generation completed with no errors
This assertion simply confirms that the generation completed with no errors
create 10 students
mark 2 students to have certificates generated already
white-list 5 students
whitelist 3
generate certs for 2
the first 3 students (who were whitelisted) have passing  certificate statuses
The last 2 students still don't have certs
create 5 students
mark 2 students to have certificates generated already
white-list 4 students
the first 4 students have passing certificate statuses since  they either were whitelisted or had one before
The last student still doesn't have a cert
create 10 students
mark 2 students to have certificates generated already
mark 3 students to have certificates generated with status 'error'
mark 6th students to have certificates generated with status 'deleted'
white-list 7 students
Certificates should be regenerated for students having generated certificates with status  'downloadable' or 'error' which are total of 5 students in this test case
Default grade for students
create 10 students
mark 2 students to have certificates generated already
mark 3 students to have certificates generated with status 'error'
mark 6th students to have certificates generated with status 'deleted'
white-list 7 students
Regenerated certificates for students having generated certificates with status  'deleted' or 'generating'
grades will be '0.0' as students are either white-listed or ending in error  grades will be '-1' for students that were skipped
Default grade for students
create 10 students
mark 2 students to have certificates generated already
mark 3 students to have certificates generated with status 'error'
mark 2 students to have generated certificates with status 'unavailable'
mark 3 students to have generated certificates with status 'generating'
white-list all students
Regenerated certificates for students having generated certificates with status  'downloadable', 'error' or 'generating'
Verify from results from database  Certificates are being generated for 8 students that had statuses in 'downloadable', 'error' and 'generating'  2 students are skipped that had Certificate Status 'unavailable'
grades will be '0.0' as students are white-listed and have not completed any tasks  grades will be '-1' for students that have not been processed
Verify that students with status 'unavailable were skipped
create 10 students
mark 2 students to have certificates generated already
mark 3 students to have certificates generated with status 'error'
mark 6th students to have certificates generated with status 'deleted'
mark 7th students to have certificates generated with status 'norpassing'
white-list 7 students
Certificates should be regenerated for students having generated certificates with status  'downloadable' or 'error' which are total of 5 students in this test case
Number of times to retry if a subtask update encounters a lock on the InstructorTask.  (These are recursive retries, so don't make this number too large.)
yield remainder items for task, if any
and save the entry immediately, before any subtasks actually start work:
Calculate the number of tasks that will be created, and create a list of ids for each task.
Construct a generator that will return the recipients to use for each subtask.  Pass in the desired fields to fetch for each recipient.
Return the task progress as stored in the InstructorTask object.
According to Celery task cookbook, "Memcache delete is very slow, but we have  to use it to take advantage of using add() for atomic locking."
Update status:
Translators: This is a past-tense verb that is inserted into task progress messages as {action}.
Translators: This is a past-tense verb that is inserted into task progress messages as {action}.
Translators: This is a past-tense verb that is inserted into task progress messages as {action}.
Translators: This is a past-tense verb that is inserted into task progress messages as {action}.
Translators: This is a past-tense verb that is inserted into task progress messages as {action}.
Translators: This is a past-tense verb that is inserted into task progress messages as {action}.
Translators: This is a past-tense verb that is inserted into task progress messages as {action}.
Translators: This is a past-tense verb that is inserted into task progress messages as {action}.
Translators: This is a past-tense verb that is inserted into task progress messages as {action}.
Translators: This is a past-tense verb that is inserted into task progress messages as {action}.
Translators: This is a past-tense verb that is inserted into task progress messages as {action}.  An example of such a message is: "Progress: {action} {succeeded} of {attempted} so far"
Patching the ENABLE_DISCUSSION_SERVICE value affects the contents of urls.py,  so we need to call super.setUp() which reloads urls.py (because  of the UrlResetMixin)
create a course
Patch the comment client user save method so it does not try  to create a new cc user when creating a django user
Create the student
Enroll the student in the course
Log the student in
Mock the code that makes the HTTP requests to the cs_comment_service app  for the profiled user's active threads
Mock the code that makes the HTTP request to the cs_comment_service app  that gets the current user's info
Mock the code that makes the HTTP requests to the cs_comment_service app  for the profiled user's active threads
Mock the code that makes the HTTP request to the cs_comment_service app  that gets the current user's info
comments service adds these attributes when course_id param is present
strip_none is being used to perform the same transform that the  django view performs prior to writing thread data to the response
strip_none is being used to perform the same transform that the  django view performs prior to writing thread data to the response
Test uncached first, then cached now that the cache is warm.
Verify that the group name is correctly included in the HTML
Beta user does not have access to alpha_module.
If a thread returns context other than "course", the access check is not done, and the beta user  can see the alpha discussion module.
avoid causing a server error when the LMS chokes attempting  to find a group name for the group_id, when we're testing with  an invalid one.
Should never have a group_id if course_id was not included in the request.
In all these test cases, the requesting_user is the student (non-privileged user).  The profile returned on behalf of the student is for the profiled_user.
If the group_id is explicitly passed, it will be present in the request.
If the group_id is not explicitly passed, it will not be present because the requesting_user  has discussion moderator privileges.
pylint: disable=super-method-not-called
pylint: disable=super-method-not-called
pylint: disable=super-method-not-called
pylint: disable=super-method-not-called
pylint: disable=super-method-not-called
pylint: disable=super-method-not-called
If provided with a discussion id, filter by discussion id in the  comments_service.  Use the discussion id/commentable id to determine the context we are going to pass through to the backend.
If the user clicked a sort key, update their default sort key
patch for backward compatibility to comments service
print "start rendering.."
Verify that the student has access to this thread if belongs to a course discussion module
patch for backward compatibility with comments service
'content': content,
seed the forums permissions and roles
Patch the comment client user save method so it does not try  to create a new cc user when creating a django user
Enroll the student in the course
Enroll the moderator and give them the appropriate roles
seed the forums permissions and roles
Patching the ENABLE_DISCUSSION_SERVICE value affects the contents of urls.py,  so we need to call super.setUp() which reloads urls.py (because  of the UrlResetMixin)
Patch the comment client user save method so it does not try  to create a new cc user when creating a django user
Enroll the student in the course
Enroll the moderator and give them the appropriate roles
Add the student to the team so they can post to the commentable.
create_thread_helper verifies that extra data are passed through to the comments service
pylint: disable=super-method-not-called
pylint: disable=super-method-not-called
pylint: disable=super-method-not-called
pylint: disable=super-method-not-called
pylint: disable=super-method-not-called
pylint: disable=super-method-not-called
Create a team.
Dummy commentable ID not linked to a team
thread_author is who is marked as the author of the thread being updated.
pylint: disable=super-method-not-called
pylint: disable=super-method-not-called
unenroll self.student from the course.
There is a stated desire for an 'origin' property that will state  whether this thread was created via courseware or the forum.  However, the view does not contain that data, and including it will  likely require changes elsewhere.
Check for whether this commentable belongs to a team, and add the right context
Cohort the thread if required
patch for backward compatibility to comments service
course didn't exist, or requesting user does not have access to it.
400 is default status for JsonError
N.B. This will trigger a comments service query
We do not expect KeyError in production-- it usually indicates an improper test mock.
-*- coding: utf-8 -*-
Create a discussion module.
Assert that created discussion module is not an orphan.
Assert that there is only one discussion module in the course at the moment.
Assert that the discussion module is an orphan.
This test needs to use a course that has already started --  discussion topics only show up if the course has already started,  and the default start date for courses is Jan 1, 2030.
Courses get a default discussion topic on creation, so remove it
unlikely case, but make sure it works.
empty / default config
explicitly disabled cohorting
explicitly enabled cohorting
no topics
not cohorted
cohorted, but top level topics aren't
cohorted, including "Feedback" top-level topics aren't
Verify that team discussions are not cohorted, but other discussions are
This is a test of the test setup,  so it does not need to run as part of the unit test suite  You can re-enable it by commenting out the line below
Create the server
Start the server in a separate daemon thread
Send the request to the mock cs server
Receive the reply from the mock cs server
You should have received the response specified in the setup above
Retrieve the POST data into a dict.  It should have been sent in json format
Log the request  pylint: disable=logging-format-interpolation
Every good post has at least an API key  Log the response
Send a response back to the client
Respond with failure
Retrieve the PUT data into a dict.  It should have been sent in json format
Log the request  pylint: disable=logging-format-interpolation
Every good post has at least an API key  Log the response
Send a response back to the client
Respond with failure
First call superclass shutdown()
We also need to manually close the socket
Despite being from 2 different courses, TA_role_2 can still inherit  permissions from TA_role without error
get_role_ids returns a dictionary of only admin, moderator and community TAs.
Find the earliest start date for the entries in this category
If we've already seen this title, append an incrementing number to disambiguate  the category from other categores sharing the same title in the course discussion UI.
django-debug-toolbar monkeypatches the connection  cursor wrapper and adds extra information in each  item in connection.queries. The query time is stored  under the key "duration" rather than "time" and is  in milliseconds, not seconds.
Augment the specified thread info to include the group name if a group id is present.
Remove any cohort information that might remain if the course had previously been cohorted.
regular users always query with their own id.
Never pass a group_id to the comments service for a non-cohorted  commentable
this is the easy case :)
top level discussions have to be manually configured as cohorted  (default is not).  Same thing for inline discussions if the default is explicitly set to False in settings
inline discussions are cohorted by default
Tell Django to clean out all databases, not just default
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
This is a hack to force sqlite to add new rows after the earlier rows we  want to migrate.
Pg's bigserial is implicitly unsigned (doesn't allow negative numbers) and  goes 1-9.2x10^18
Import here instead of top of file since this module gets imported before  the course_modes app is loaded, resulting in a Django deprecation warning.
Only returns eligible certificates. This should be used in  preference to the default `objects` manager in most cases.
Normal object manager, which should only be used when ineligible  certificates (i.e. new audit certs) should be included in the  results. Django requires us to explicitly declare this.
Translators: This is a past-tense verb that is used for task action messages.
if task input is empty, it means certificates were generated for all learners  Translators: This string represents task was executed for all learners.
Import here instead of top of file since this module gets imported before  the course_modes app is loaded, resulting in a Django deprecation warning.
Short term fix to make sure old audit users with certs still see their certs  only do this if there if no honor mode
Import here instead of top of file since this module gets imported before  the course_modes app is loaded, resulting in a Django deprecation warning.
Statuses
Dummy full name for the generated certificate
Certificates HTML view end point to render web certs by user and course
Certificates HTML view end point to render web certs by certificate_uuid
this is here to support registering the signals in signals.py
anonymous user
set the expiration date in the past
prefetch all chapters/sequentials by saying depth=2
Add the certificate request to the queue
Add the certificate request to the queue
all states we have seen far all courses
print the heading for the report
Re-submit certificates for *all* courses
Re-submit certificates for particular courses
Retrieve the IDs of generated certificates with  error status in the set of courses we're considering.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
NOTE: the download URL is not currently being set for webview certificates.  In the future, we can update this to construct a URL to the webview certificate  for courses that have this feature enabled.
If the feature is disabled, then immediately return a False
Return the flag on the course object
fetch organization of the course
get Terms of Service and Honor Code page url
get Privacy Policy page url
get About page url
The caller can optionally pass a course in to avoid  re-fetching it from Mongo. If they have not provided one,  get it from the modulestore.
Needed for access control in grading.
For credit mode generate verified certificate
honor code and audit students
Log if the student is whitelisted
Check to see whether the student is on the the embargoed  country restricted list. If so, they should not receive a  certificate -- set their status to restricted and log it.
Finally, generate the certificate and send it off.
We send this extra parameter to differentiate  example certificates from other certificates.  This is not used by the certificates workers or XQueue.
The callback for example certificates is different than the callback  for other certificates.  Although both tasks use the same queue,  we can distinguish whether the certificate was an example cert based  on which end-point XQueue uses once the task completes.
Check the POST parameters, returning a 400 response if they're not valid.
Attempt to regenerate certificates
Check the POST parameters, returning a 400 response if they're not valid.
Check that the course exists
Attempt to generate certificate
pylint: disable=wildcard-import
Populate dynamic output values using the course/certificate data loaded above
Translators:  This text is bound to the HTML 'title' element of the page and appears in the browser title bar
Update the view context with the default ConfigurationModel settings
Translators:  'All rights reserved' is a legal term used in copyrighting to protect published content
Translators:  This text is bound to the HTML 'title' element of the page and appears  in the browser title bar when a requested certificate is not found or recognized
Translators: The &amp; characters represent an ampersand character and can be ignored
Translators: A 'Privacy Policy' is a legal document/statement describing a website's use of personal information
Translators: This line appears as a byline to a header image and describes the purpose of the page
Translators: Accomplishments describe the awards/certifications obtained by students on this platform
Translators:  This line appears on the page just before the generation date for the certificate
Translators:  The Certificate ID Number is an alphanumeric value unique to each individual certificate
Translators:  This text describes (at a high level) the mission and charter the edX platform and organization
Translators:  This text appears near the top of the certficate and describes the guarantee provided by edX
Translators:  This text represents the description of course
Translators: This line is displayed to a user who has completed a course and achieved a certification
Translators: This line leads the reader to understand more about the certificate that a student has been awarded
certificate is being viewed by learner or public
Badge Request Event Tracking Logic
Create the initial view context, bootstrapping with Django settings and passed-in values
Kick the user back to the "Invalid" screen if the feature is disabled
Load the course and user objects
Append/Override the existing view context values with any mode-specific ConfigurationModel values
Append organization info
Append course info
Append user info
Append social sharing info
Append/Override the existing view context values with certificate specific values
Append badge info
Append microsite overrides
Add certificate header/footer data to current context
Append/Override the existing view context values with any course-specific static values from Advanced Settings
Track certificate view events
FINALLY, render appropriate certificate
Check the parameters and rate limits  If these are invalid, return an error response.
Let the XQueue know that we handled the response
Verify that the certificate has status 'downloadable'
Since model-based configuration is cached, we need  to clear the cache before each test.
Enable the feature
Enable for the course
Disable for the course
Enable the feature
Enable for one course
Should be disabled for another course
Generate certificates for the course
Verify that the appropriate certs were added to the queue
Verify that the certificate status is "started"
Create verified and honor modes for the course
Generate certificates for the course
Verify that the appropriate certs were added to the queue
Generate certificates for the course
Generate certificates for the course
Make sure there are not unexpected keys in dict returned by 'get_certificate_footer_context'
ABOUT is present in MICROSITE_CONFIGURATION['test_microsite']["urls"] so web certificate will use that url
PRIVACY is present in MICROSITE_CONFIGURATION['test_microsite']["urls"] so web certificate will use that url
TOS_AND_HONOR is present in MICROSITE_CONFIGURATION['test_microsite']["urls"],  so web certificate will use that url
set pre-requisite course  get milestones collected by user before completing the pre-requisite course
Enroll as audit  Whitelist student
Verify that the task was sent to the queue with the correct callback URL
Ensure the certificate was not generated
Verify that the correct payload was sent to the XQueue
Verify the certificate status
Verify the error status of the certificate
Factories are self documenting  pylint: disable=missing-docstring
Since rate limit counts are cached, we need to clear  this before each test.
Exceed the rate limit for invalid requests  (simulate a DDOS with invalid keys)
The final status code should indicate that the rate  limit was exceeded.
Override with the command module you wish to test.
Enroll the user in the course
Create the certificate
Create a certificate with status 'error'
Re-submit all certificates with status 'error'
Expect that the certificate was re-submitted
Create a certificate with status 'error'  in three courses.
Re-submit certificates for two of the courses
Create certificates with an error status and some other status
Re-submit certificates for all courses
Only the certificate with status "error" should have been re-submitted
Verify that we make only one Mongo query  because the course is cached.
Expect that the certificate was NOT resubmitted  since the course doesn't actually exist.
Create the support staff user
Create a student
Create certificates for the student
Login as support staff
Assign the user to the role
Retrieve the page
Assign the user to the role
Make a POST request  Since we're not passing valid parameters, we'll get an error response  but at least we'll know we have access
Check that the user's certificate was updated  Since the student hasn't actually passed the course,  we'd expect that the certificate status will be "notpassing"
Missing username
Missing course key
Unenroll the user
Can no longer regenerate certificates for the user
Delete the user's certificate
Should be able to regenerate
A new certificate is created
Assign the user to the role
Make a POST request  Since we're not passing valid parameters, we'll get an error response  but at least we'll know we have access
Missing username
Missing course key
Unenroll the user
Can no longer regenerate certificates for the user
Delete the user's certificate
Should be able to generate
A new certificate is created
pylint: disable=invalid-name
Delete any existing statuses
Verify that the "latest" status is None
Now save asset with same file again, New file will be uploaded after deleting the old one with the same name.
Now replace the asset with another file
Validate certificate
Convert the cert to audit, with the specified eligibility
Validate certificate
make sure response html has only one organization logo container for edX
accessing certificate web view in preview mode without  staff or instructor access should show invalid certificate
Verify that Exception is raised when certificate is not in the preview mode
Enable the feature
pylint: disable=missing-docstring,invalid-name,maybe-no-member,attribute-defined-outside-init
Use mongo so that we can get a test with a SlashSeparatedCourseKey
Return the response so child classes do not have to repeat the request.
HTTP 401 should be returned if the user is not authenticated.
Access should be granted if the proper access token is supplied.
Access should be denied if the user is not course staff.
Data should be returned if the user is authorized.
Ensure course structure exists for the course
Course structure generation shouldn't take long. Generate the data and try again.
Ensure only course descriptors are returned.
Ensure only courses accessible by the user are returned.
Sort the results in a predictable manner.
If we don't have data stored, we will try to regenerate it, so  return a 503 and as them to retry in 2 minutes.
Determine the URL to redirect to following login/registration/third_party_auth
If we're already logged in, redirect to the dashboard
Retrieve the form descriptions from the user API
Allow external auth to intercept and handle the request
Prefer logged-in user's email
Increment the rate limit counter
As a reliable way of "skipping" the registration form, we just submit it automatically
Check for any error messages we may want to display:  msg may or may not be translated. Try translating [again] in case we are able to:
Since the user API is currently run in-process,  we simulate the server-server API call by constructing  our own request object.  We don't need to include much  information in the request except for the session  (to get past through CSRF validation)
Call the Django view function, simulating  the server-server API call
Return the content of the response
If the account on the third party provider is already connected with another edX account,  we display a message to the user.
Long email -- subtract the length of the @domain  except for one character (so we exceed the max length limit)
Create/activate a new account
Login
Request a password change while logged in, simulating  use of the password reset link from the account page
Check that an email was sent
Visit the activation link
Log the user out to clear session data
Verify that the new password can be used to log in
Verify that the old password cannot be used to log in
Verify that the new password continues to be valid
Log the user out
Log out the user created during test setup
Create a second user, but do not activate it
Send the view the email address tied to the inactive user
Expect that the activation email is still sent,  since the user may have lost the original activation email.
Log out the user created during test setup
Send the view an email address not tied to any user
Log out the user created during test setup, to prevent the view from  selecting the logged-in user's email address over the email provided  in the POST data
Make many consecutive bad requests in an attempt to trigger the rate limiter
Verify that we're redirected to the dashboard
The response should have a "Sign In" button with the URL  that preserves the querystring params
Verify that this parameter is also preserved
Do NOT simulate a running pipeline
For these tests, two third party auth providers are enabled by default:
Patch Milestones feature flag
create chapter
create vertical
create problem
create orphan
Setup gating milestones data
In case the transaction actually was not committed before the celery task runs,  run it again after 5 minutes. If the first completed successfully, this task will be a no-op.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Test when no configuration exists
Enable for a course
Disable for the course
Spy on number of calls to celery task.
Enable cohorting and create a verified cohort.  But do not enable the verified track cohorting feature.  No logging occurs if feature is disabled for course.
Enable cohorting and create a verified cohort.  Enable verified track cohorting feature
Enable cohorting, and create the verified cohort.  Create two random cohorts.  Enable verified track cohorting feature
Un-enroll from the course. The learner stays in the verified cohort, but is no longer active.
Change the name of the "default" cohort.
Note that this will enroll the user in the default cohort on initial enrollment.  That's good because it will force creation of the default cohort if necessary.
if not empty, this field contains a json serialized list of  the master course modules
avoid circular import problems
avoid circular import problems
avoid circular import problems
pylint: disable=protected-access
pylint: disable=protected-access
if user is staff or instructor then he can view ccx coach dashboard.
At this point we are done with verification that current user is ccx coach.
if ccx connector url is set in course settings then inform user that he can  only create ccx by using ccx connector url.
Make sure start/due are overridden for entire course
Enforce a static limit for the maximum amount of students that can be enrolled
Enroll the coach in the course
In case of section aka chapter we do not have due date.
in case the children are visible to staff only, skip them
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
prevent migration for deprecated course ids or invalid ids.
prevent migration for deprecated course ids or invalid ids.
the ccx detail view cannot call this function with a "None" value  so the following `error_code` should be never used, but putting it  to avoid a `NameError` exception in case this function will be used  elsewhere in the future
checking first if all the fields are present and they are not null
case if the user actually passed null as input
validating the rest of the input
prepare the course_modules to be stored in a json stringified field
Make sure start/due are overridden for entire course
Enforce a static limit for the maximum amount of students that can be enrolled
make the coach user a coach on the master course
get the master course key and master course object
Add the current page to the response.
This field can be derived from other fields in the response,  so it may make sense to have the JavaScript client calculate it  instead of including it in the response.
adding instructor to master course.
create a staff user  add staff role to the staff user
create an instructor user  add instructor role to the instructor user
create an coach user  add coach role to the coach user
there are no CCX courses
add sort order desc  the only thing I can check is that the display name is in alphabetically reversed order  in the same way when the field has been updated above, so with the id asc
check if the response has at least the same data of the request
check that only one email has been sent and it is to to the coach
create a staff user  add staff role to the staff user
create an instructor user  add instructor role to the instructor user
create an coach user  add coach role to the coach user
If ccx is not enable do not show ccx coach tab.
if user is staff or instructor then he can always see ccx coach tab.  check if user has coach access.
Create instructor account
One outer SAVEPOINT/RELEASE SAVEPOINT pair around everything caused by the  transaction.atomic decorator wrapping override_field_for_ccx.  One SELECT and one INSERT.  One inner SAVEPOINT/RELEASE SAVEPOINT pair around the INSERT caused by the  transaction.atomic down in Django's get_or_create()/_create_object_from_params().
One outer SAVEPOINT/RELEASE SAVEPOINT pair around everything caused by the  transaction.atomic decorator wrapping override_field_for_ccx.  One SELECT and one INSERT.  One inner SAVEPOINT/RELEASE SAVEPOINT pair around the INSERT caused by the  transaction.atomic down in Django's get_or_create()/_create_object_from_params().
I think Django already does this for you in their TestClient, except  we're bypassing that by using edxmako.  Probably edxmako should be  integrated better with Django's rendering and event system.
Login with the instructor account
adding staff to master course.
adding instructor to master course.
Get the ccx_key
check if the max amount of student that can be enrolled has been overridden
assert ccx creator has role=ccx_coach
assert that staff and instructors of master course has staff and instructor roles on ccx
we were redirected to our current location
a CcxMembership exists for this student
create ccx and limit the maximum amount of students that can be enrolled to 2
create some users
we were redirected to our current location
a CcxMembership does not exists for this student
we were redirected to our current location
some error messages are returned for one of the views only
Trying to wrap the whole thing in a bulk operation fails because it  doesn't find the parents. But we can at least wrap this part...
Create instructor account  create an instance of modulestore
Login with the instructor account
adding staff to master course.
adding instructor to master course.
Create instructor account
Create CCX
Are the grades downloaded as an attachment?
Create a CCX coach.
Create a CCX course and enroll the user in it.
build a fake key
Create instructor account
create an instance of modulestore
adding staff to master course.
adding instructor to master course.
assert that staff and instructors of master course has staff and instructor roles on ccx
adding instructor to master course.
adding instructor to master course.
adding instructor to master course.
assert that role of staff and instructors of master course removed from ccx.
Run again
Tell Django to clean out all databases, not just default
TEST_DATA must be overridden by subclasses
Switch to published-only mode to simulate the LMS  Clear all caches before measuring
Refill the metadata inheritance cache
We clear the request cache to simulate a new request in the LMS.
Reset the list of provider classes, so that our django settings changes  can actually take affect.
Trying to wrap the whole thing in a bulk operation fails because it  doesn't find the parents. But we can at least wrap this part...
Create instructor account  create an instance of modulestore
Setting override date [start or due]
Setting date from master course
Set parent date (vertical has same dates as subsections)
XXX: In the future, it would be nice to support more than one ccx per  coach per course.  This is a place where that might happen.
this call should be idempotent  Enroll the staff in the ccx
allow 'staff' access on ccx to staff of master course
this call should be idempotent  Enroll the instructor in the ccx
allow 'instructor' access on ccx to instructor of master course
revoke 'staff' access on ccx.
Unenroll the staff on ccx.
revoke 'instructor' access on ccx.
Unenroll the instructor on ccx.
-*- coding: utf-8 -*-
is_active is `None` if the user is not enrolled in the course
Calling UserPreference directly instead of get_user_preference because the user requesting the  information is not "user" and also may not have is_staff access.
for now, White Labels use 'shoppingcart' which is based on the  "honor" course_mode. Given the change to use "audit" as the default  course_mode in Open edX, we need to be backwards compatible with  how White Labels approach enrollment modes.
Since no User object exists for this student there is no "full_name" available.
load the state json  old_number_of_attempts = problem_state["attempts"]
save
add some helpers and microconfig subsitutions
see if we are running in a microsite and that there is an  activation email template definition available as configuration, if so, then render that
Remove leading and trailing whitespace from body
Email subject *must not* contain newlines
extended user profile fields are stored in the user_profile meta column
get the registration_code_redemption object if exists  get the paid_course registration item if exists
this happens when the registration code is not created via invoice or bulk purchase  scenario.
amount greater than 0 is invoice has bee paid  amount less than 0 is invoice has been refunded
proctored exam downloads...
Financial Report downloads..
Coupon Codes..
spoc gradebook
Cohort management
Certificates
Grade book: max students per page
calculate offsets for next and previous pages.
calculate current page number.
calculate total number of pages.
We are at first page, so there's no previous page.
We've reached the last page, so there's no next page.
Apply limit on queryset only if total number of students are greater then MAX_STUDENTS_PER_PAGE_GRADE_BOOK.
Checked above
Pass along email as an object with the information we desire
Get progress status message & success information
for white labels we use 'shopping cart' which uses CourseMode.DEFAULT_SHOPPINGCART_MODE_SLUG as  course mode for creating course enrollments.
Iterate each student in the uploaded csv file.
Email address already exists. assume it is the correct user  and just register the user in the course and send an enrollment email.
This email does not yet exist, so we need to create a new account  If username already exists in the database, then create_and_enroll_user  will raise an IntegrityError exception.
Create a new user
Enroll user to the course and add manual enrollment audit trail
First try to get a user object from the identifer
Flag this email as an error if invalid, but continue checking  the remaining in the list
catch and log any exceptions  so that one error doesn't cause a 500.
If no exception thrown, see if we should send an email  See if we should autoenroll the student  Check if student is already enrolled
Tabulate the action result of this email address
Check that user is active, because add_users  in common/djangoapps/student/roles.py fails  silently when we try to add an inactive user.
Are we dealing with an "old-style" problem location?
Allow for microsites to be able to define additional columns (e.g. )
Translators: 'Cohort' refers to a group of students within a course.
The task will assume the default file storage.
check if the generated code is in the Coupon Table
covert the course registration code number into integer
composes registration codes invoice email
append the finance email into the recipient_list
find all the registration codes in this course
parameter combinations
instructor authorization
Trust the submissions API to log the error
instructor authorization
Specifying for the history of a single task type
First get tasks list of bulk emails sent
Specifying for a single student's history on this problem
Specifying for single problem's history
If no problem or student, just get currently running tasks
Specifying for a single student's entrance exam history
Specifying for all student's entrance exam history
default roles require either (staff & forum admin) or (instructor)
EXCEPT FORUM_ROLE_ADMINISTRATOR requires (instructor)
filter out unsupported for roles
Create the CourseEmail object.  This is saved immediately, so that  any transaction that has been pending up to this point will also be  committed.
Submit the task, so that the correct InstructorTask object gets created (for monitoring purposes)
default roles require either (staff & forum admin) or (instructor)
EXCEPT FORUM_ROLE_ADMINISTRATOR requires (instructor)
It's possible the normal due date was deleted after an extension was granted:
Validate request data and return error response in case of invalid data
Certificate has not been generated yet, so just remove the certificate exception from white list
Generate Certificates for all white listed students
Invalid data, generate_for must be present for all certificate exceptions
verify that we have exactly two column in every row either email or username and notes but allow for  blank lines
Validate request data and return error response in case of invalid data
Re-Validate student certificate for the course course
Fetch CertificateInvalidation object
Deactivate certificate invalidation if it was fetched successfully.
We need to generate certificate only for a single student here
Temporarily show the "Analytics" section until we have a better way of linking to Insights
Gate access to course email by feature flag & by course-specific authorization
Gate access to Metrics tab by featue flag and staff authorization
Gate access to Ecommerce tab
Certificates panel  This is used to generate example certificates  and enable self-generated certificates for a course.
remove the redemption entry from the database.
Check that the new due date is valid:
We are deleting a due date extension. Check that it exists:
Create a course with mode 'audit'
Create a course with mode 'honor' and with price
test the log for email that's send to new created user.
test the log for email that's send to new created user.
test the log for email that's send to new created user.
Login Audit Course instructor
Verify enrollment modes to be 'audit'
Remove white label course price
Login Audit Course instructor
Verify enrollment modes to be 'honor'
Login white label course instructor
Verify enrollment modes to be CourseMode.DEFAULT_SHOPPINGCART_MODE_SLUG
Create invited, but not registered, user
test that the user is now enrolled
Check the outbox
test that the user is now enrolled
test that the user is now unenrolled
Check the outbox
test that the user is now unenrolled
Try with marketing site enabled and shib on
make this enrollment "verified"
now re-enroll the student through the instructor dash
upgrade enrollment
test the response data
Check the outbox
Check the outbox
Works around a caching bug which supposedly can't happen in prod. The instance here is not ==  the instance fetched from the email above which had its cache cleared
Check the outbox
Works around a caching bug which supposedly can't happen in prod. The instance here is not ==  the instance fetched from the email above which had its cache cleared
Seed forum roles for course.
Status code should be 200.
check button text
Now invalidate the same invoice number and expect an Bad request
now re_validate the invoice number
Now re_validate the same active invoice number and expect an Bad request
add the coupon code for the course
Coupon Redeem Count only visible for Financial Admins.
apply the coupon code to the item in the cart
now make the payment of your cart items  visit the instructor dashboard page and  check that the coupon redeem count should be 1
create a new user/student and enroll  in the course using a registration code  and then validates the generated detailed enrollment report
make sure problem attempts have been reset.
make sure the module has been deleted  module_id=self.module_to_reset.module_id,
Add instructor to invalid ee course
make sure problem attempts have been reset.
make sure the module has been deleted
check response
post again with same student
This time response message should be different
This should be given the value of 'unknown' if the task output  can't be properly parsed
Emails list should have one email
Email content should be what's expected
Emails list should be empty
firstly generating downloadable certificates with 'honor' mode
firstly generating downloadable certificates with 'honor' mode
Now generating downloadable certificates with 'verified' mode
total certificate count should be 2 for 'verified' mode.
retrieve the second certificate from the list
firstly generating downloadable certificates with 'honor' mode
Spent(used) Registration Codes
check for the last mail.outbox, The FINANCE_EMAIL has been appended at the  very end, when generating registration codes
get user invoice copy preference.
get user invoice copy preference.
Spent(used) Registration Codes
now check that the registration code should be marked as invalid in the db.
now the student course enrollment should be false.
now the student course enrollment should be false.
now check that the registration code should be marked as valid in the db.
Student is unknown, so the platform language should be used
Coupons should show up for White Label sites with priced honor modes.
removing the course finance_admin role of login user
Total amount html should render in e-commerce page, total amount will be 0
removing the course finance_admin role of login user
Course A updated total amount should be visible in e-commerce page if the user is finance admin
Value Error course price should be a numeric value
validation check passes and course price is successfully added
Get the response value, ensure the Coupon section is not included.  Coupons should show up for White Label sites with priced honor modes.
parent of the BaseEnrollmentReportProvider is EnrollmentReportProvider
Create instructor account
The course is Mongo-backed but the flag is disabled (should not work)  Assert that the URL for the email view is not in the response
Authorize the course to use email
Assert that instructor email is enabled for this course  Assert that the URL for the email view is in the response
Flag is disabled, but course is authorized  Authorize the course to use email
URL for instructor dash  URL for email view
Create instructor account
URL for instructor dash  URL for email view
enrollment objects
initialize & check before
do action
check after
Create a student module for the user
Delete student state using the instructor dash
Verify that the student's scores have been reset in the submissions API
For a CCX, what do we expect to get for the URLs?  Also make sure `auto_enroll` is properly passed through.
For a normal site, what do we expect to get for the URLs?  Also make sure `auto_enroll` is properly passed through.
For a site with a marketing front end, what do we expect to get for the URLs?  Also make sure `auto_enroll` is properly passed through.
Create instructor account
I think Django already does this for you in their TestClient, except  we're bypassing that by using edxmako.  Probably edxmako should be  integrated better with Django's rendering and event system.
Create instructor account
URL for instructor dash
no enrollment information should be visible
dashboard link hidden
Check that the number of professional enrollments is two
link to dashboard shown
link to dashboard shown
Create instructor account
prepare course structure
Create a course with the desired grading policy (from our class attribute)
Default >= 50% passes, so Users 5-10 should be passing for Homework 1 [6]  One use at the top of the page [1]
Users 1-5 attempted Homework 1 (and get Fs) [4]  Users 1-10 attempted any homework (and get Fs) [10]  Users 4-10 scored enough to not get rounded to 0 for the class (and get Fs) [7]  One use at top of the page [1]
All other grades are None [29 categories * 11 users - 27 non-empty grades = 292]  One use at the top of the page [1]
Users 9-10 have >= 90% on Homeworks [2]  Users 9-10 have >= 90% on the class [2]  One use at the top of the page [1]
User 8 has 80 <= Homeworks < 90 [1]  User 8 has 80 <= class < 90 [1]  One use at the top of the page [1]
User 7 has 70 <= Homeworks < 80 [1]  User 7 has 70 <= class < 80 [1]  One use at the top of the page [1]
User 6 has 60 <= Homeworks < 70 [1]  User 6 has 60 <= class < 70 [1]  One use at the top of the page [1]
Users 1-5 have 60% > grades > 0 on Homeworks [5]  Users 1-5 have 60% > grades > 0 on the class [5]  One use at top of the page [1]
User 0 has 0 on Homeworks [1]  User 0 has 0 on the class [1]  One use at the top of the page [1]
Need to clear the cache for model-based configuration
Enable the certificate generation feature
Instructors don't see the certificates section
Global staff can see the certificates section
Disable the feature flag
Now even global staff can't see the certificates section
Initially, no example certs are generated, so  the enable button should be disabled
Certs are disabled for the course, so the enable button should be shown
Enable certificates for the course
Now the "disable" button should be shown
When certs are disabled for a course, then don't allow them  to be enabled if certificate generation doesn't complete successfully
However, if certificates are already enabled, allow them  to be disabled even if an error has occurred
Enable certificate generation
Instructors do not have access
Global staff have access
Expect a redirect back to the instructor dashboard
Expect that certificate generation started  Cert generation will fail here because XQueue isn't configured,  but the status should at least not be None.
Expect a redirect back to the instructor dashboard
Expect that certificate generation is now enabled for the course
Create a generated Certificate of some user with status 'downloadable'
Assert 200 status code in response
Assert request is successful
Create a dummy course and GeneratedCertificate with the same status as the one we will use to access  'start_certificate_regeneration' but their error message should be displayed as GeneratedCertificate  belongs to a different course
Assert 400 status code in response
Assert Error Message
Assert 400 status code in response
Assert Error Message
Enable certificate generation
Assert successful request processing
Assert Certificate Exception Updated data
Assert 400 status code in response
Assert Request not successful
Assert Error Message
Assert 400 status code in response
Assert Request not successful
Assert 400 status code in response
Assert Request not successful
Assert Error Message
Assert Certificate Exception Updated data
add certificate exception for same user in a different course
Assert Certificate Exception Updated data
Assert 400 status code in response
Assert Request not successful
Assert Error Message
Assert successful request processing
Try to delete certificate exception without passing valid data  Assert error on request
Assert error on request
Enable certificate generation
Assert Success
Assert Request is successful  Assert Message
Assert Success
Assert Request is successful  Assert Message
Assert Failure
Assert Request is not successful  Assert Message
Global staff can see the certificates section
Global staff can see the certificates section
Assert successful request processing
Verify that CertificateInvalidation record has been created in the database i.e. no DoesNotExist error
Validate generated certificate was invalidated
Assert 400 status code in response
Assert 400 status code in response
Assert Error Message
Assert 400 status code in response
Assert 400 status code in response
Invalidate user certificate
Assert 400 status code in response
Invalidate user certificate
Assert 400 status code in response
Invalidate user certificate
Assert 204 status code in response
Verify that certificate invalidation successfully removed from database
Invalidate user certificate
Assert 400 status code in response
Assert Error Message
make sure the attempt is there
make sure the module has been deleted
Store the role
Clear existing courses to avoid conflicts
Create a new course
Log in as the an instructor or staff for the course  Make & register an instructor for the course
Make & register a staff member
Go to the data download section of the instructor dash
Click generate grade report button
Go to the data download section of the instructor dash
Go to the data download section of the instructor dash
Go to the data download section of the instructor dash
Find the grading configuration display
Wait for the data table to be populated
pylint: disable=no-member
module not enabled in the course
setting not enabled and the module is not enabled
module is enabled and the setting is not enabled
Mocks
Make sure no note with this ID ever exists for testing purposes
-*- coding: utf-8 -*-
Cap the number of notes that can be returned in one request
Wrapper class for HTTP response and data. All API actions are expected to return this.
Verify that the api should be accessible to this course
Locate the requested resource
not doing a strict boolean check on data becuase it could be an empty list
validate search parameters
set filters
Users have different sets of enrollments
The number of queries is one for the users plus one for each prefetch  in NotifierUsersViewSet (roles__permissions does one for each table).
See NotifierUserSerializer for notes about related tables
now coerce username to utf-8 encoded str, since we test with non-ascii unicdoe above and  the unittest framework has hard time coercing to unicode.  decrypt also can't take a unicode input, so coerce its input to str
Token not long enough to contain initialization vector
Token length not a multiple of AES block length
Invalid padding (ends in 0 byte)  Encrypted value: "testuser" + "\x00" * 8
Invalid padding (ends in byte > 16)  Encrypted value: "testusertestuser"
Invalid padding (entire string is padding)  Encrypted value: "\x10" * 16
Nonexistent user  Encrypted value: "nonexistentuser\x01"
start without a pref key
Calling UserPreference directly because this method is called from a couple of places,  and it is not clear that user is always the user initiating the request.
Compensate for the fact that some threads in the comments service do  not have the pinned field set
for multiple fields in a list
Avoid revealing the identity of an anonymous non-staff question  author who has endorsed a comment in the thread
Django Rest Framework v3 no longer includes None values  in the representation.  To maintain the previous behavior,  we do this manually instead.
params are validated at a higher level, so the only possible request  error is if the thread doesn't exist
The comments service returns the last page of results if the requested  page is beyond the last page, but we want be consistent with DRF's general  behavior and return a PageNotFoundError in that case
if a thread is closed; no new comments could be made to it
Shared fields
Test page past the last one
N.B. The mismatch between the number of children and the listed total  number of responses is unrealistic but convenient for this test
Only page
First page of many
Middle page of many
Last page of many
Page past the end
Matches paths like 'programs/123/' and 'programs/123/foo/', but not 'programs/123/foo/bar/'.
mock programs and credentials apis
mock programs and credentials apis
The user is selecting what he/she wants to purchase.
The user has been sent to the external payment processor.  At this point, the order should NOT be modified.  If the user returns to the payment flow, he/she will start a new order.
The user has successfully purchased the items in the order.
The user's order has been refunded.
The user's order went through, but the order was erroneously left  in 'cart'.
The user's order went through, but the order was erroneously left  in 'paying'.
maps order statuses to their defunct states
we need a tuple to represent the primary key of various OrderItem subclasses
a JSON dump of the CC processor response, for completeness
check to see if the cart has at least some item in it
if the caller is explicitly asking to check for particular types
remove any redemption entry associated with the item
Only the business order is HTML formatted. A single seat order confirmation is plain text.
save these changes on the order, then we can tell when we are in an  inconsistent state  this should return all of the objects with the correct types of the  subclasses
Generate the CSV file that contains all of the RegistrationCodes that have already been  generated when the purchase has transacted
Catch all exceptions here, since the Django view implicitly  wraps this in a transaction.  If the order completes successfully,  we don't want to roll back just because we couldn't send  the confirmation email.
Capturing all exceptions thrown while tracking analytics events. We do not want  an operation to fail because of an analytics event, so we will capture these  errors in the logs.
if an order is already retired, no-op:
general purpose field, not user-visible.  Used for reporting
This field has been deprecated.  The total amount can now be calculated as the sum  of each invoice item associated with the invoice.  For backwards compatibility, this field is maintained  and written to during invoice creation.
This field has been deprecated in order to support  invoices for items that are not course-related.  Although this field is still maintained for backwards  compatibility, you should use CourseRegistrationCodeInvoiceItem  to look up the course ID for purchased redeem codes.
A payment/refund is in process, but money has not yet been transferred
A payment/refund has completed successfully  This should be set ONLY once money has been successfully exchanged.
A payment/refund was promised, but was cancelled before  money had been transferred.  An example would be  cancelling a refund check before the recipient has  a chance to deposit it.
JSON-serialized representation of the current state  of the invoice, including its line items and  transactions (payments/refunds).
For backwards compatibility, we maintain the FK to "invoice"  In the future, we will remove this in favor of the FK  to "invoice_item" (which can be used to look up the invoice).
theoretically there could be more than one (e.g. someone self-unenrolls  then re-enrolls with a different regcode)  return the first one. In all normal use cases of registration codes  the user will only have one
user could have specified a mode that's not set, in that case return the DEFAULT_MODE
enroll in course and link to the enrollment_id
user could have specified a mode that's not set, in that case return the DEFAULT_SHOPPINGCART_MODE
we need to import here because of a circular dependency  we should ultimately refactor code to have save_registration_code in this models.py  file, but there's also a shared dependency on a random string generator which  is in another PR (for another feature)
pylint: disable=no-member
pylint: disable=no-member
Only refund verified cert unenrollments that are within bounds of the expiration date
Need this to be unicode in case the reminder strings  have been translated and contain non-ASCII unicode
Types of donations
The type of donation
If a donation is made for a specific course, then store the course ID here.  If the donation is made to the organization as a whole,  set this field to CourseKeyField.Empty
This will validate the currency but won't actually add the item to the order.
Create a line item description, including the name of the course  if this is a per-course donation.  This will raise an exception if the course can't be found.
The donation is for the organization as a whole, not a specific course
user is logged in and  do we have the feature turned on  does the user actually have a cart (optimized query to prevent creation of a cart when not needed)  user's cart has PaidCourseRegistrations or CourseRegCodeItem
Draw Order/Invoice No.
Draw Date
Amount header
Amount column (header + data items)
Quantity, List Price, Discount header
Description header
Quantity data items
Innergrid around the data rows.
The entire Table won't fit in the available space and requires splitting.  Draw the part that can fit, start a new page  and repeat the process with the rest of the table.
Table will fit without the need for splitting.
only print TaxID if we are generating an Invoice
if space left on page is smaller than the rendered height, render the table on the next page.
Billing Address Header styling
Billing Address Body styling
Disclaimer Body styling
TERMS AND CONDITIONS body styling
tick the rate limiter counter
Restrict the user from enrolling based on country access rules
Restrict the user from enrolling based on country access rules
remove the course from the cart if it was added there.
Any amount is okay as long as it's greater than 0  Since we've already quantized the amount to 0.01  and rounded down, we can check if it's less than 0.01
Add the donation to the user's cart
Course ID may be None if this is a donation to the entire organization
Start the purchase.  This will "lock" the purchase so the user can't change  the amount after we send the information to the payment processor.  If the user tries to make another donation, it will be added  to a new cart.
Construct the response params (JSON-encoded)
Add extra to make it easier to track transactions
The HTTP end-point for the payment processor.
Parameters the client should send to the payment processor
See if the order contained any certificate items  If so, the user is coming from the payment/verification flow.
Add a query string param for the order ID  This allows the view to query for the receipt information later.
Otherwise, send the user to the receipt page
We want to have the ability to override the default receipt page when  there is only one item in the order.
Error case: there was a badly formatted user-input date string
set up test carts
SUCCESS CASE first, rest are some sort of oddity
Moved reading of charged_amount here from the valid_params loop above because  only 'ACCEPT' messages have a 'ccAuthReply_amount' parameter
see if we have an override in the microsites
fallthrough case, which basically never happens
Import the processor implementation, using `CC_PROCESSOR_NAME`  as the name of the Python module in `shoppingcart.processors`
Translators: this text appears when an unfamiliar error code occurs during payment,  for which we don't know a user-friendly message to display in advance.
if we have the order and the id, log it
First see if the user cancelled the transaction  if so, then not all parameters will be passed back so we can't yet verify signatures
if the user decline the transaction  if so, then auth_amount will not be passed back so we can't yet verify signatures
CyberSource allows us to send additional data in "merchant defined data" fields
Retrieve the configuration settings for the active credit card processor
Check whether we're in a microsite that overrides our configuration  If so, find the microsite-specific configuration in the 'microsites'  sub-key of the normal processor configuration.
if the above verify_signature fails it will throw an exception, so basically we're just  testing for the absence of that exception.  the trivial assert below does that
if the above verify_signature fails it will throw an exception, so basically we're just  testing for the absence of that exception.  the trivial assert below does that
test base case
tests for missing key
tests for keys with value that can't be converted to proper type
tests for an order number that doesn't match up
tests for a reply amount of the wrong type
tests for a reply amount of the wrong type
tests for a not accepted order
finally, tests an accepted order
Check the callback URL override
Parameters determined by the Django (test) settings
Some fields will change depending on when the test runs,  so we just check that they're set to a non-empty string
Check the signature
We patch the purchased callback because  we're using the OrderItem base class, which throws an exception  when item doest not have a course id associated
We patch the purchased callback because  (a) we're using the OrderItem base class, which doesn't implement this method, and  (b) we want to verify that the method gets called on success.
Simulate a callback from CyberSource indicating that payment was successful
Expect that the item's purchased callback was invoked
Expect that the order has been marked as purchased
Simulate a callback from CyberSource indicating that the payment was rejected
Expect that we get an error message
Simulate a callback from CyberSource indicating that the payment was rejected
Expect that we get an error message
Use an invalid order ID
Expect an error
Change the payment amount (no longer matches the database order record)
Change the payment amount to a non-decimal
Expect an error
Change the payment amount to a non-decimal
Expect an error
Use a credit card number with no digits provided
Expect that the order has placeholders for the missing credit card digits
Remove a required parameter
Recalculate the signature with no signed fields so we can get past  signature validation.
Expect an error
Verify that this executes without a unicode error
if decision is in FAILED_DECISIONS list then remove  auth_amount from  signed_field_names list.
Parameters that change based on the test
Calculate the signature
Simulate a callback from CyberSource indicating that the payment was declined
Expect that we get an error message
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Fifth Order with course not attributed to any microsite but with a Donation
also add a donation not associated with a course to make sure the None case works OK
User 1 & 2 will be verified
User 6 is honor
check that we have the right number
Using excel mode csv, which automatically ends lines with \r\n, so need to convert to \n
since there's not many purchases, just run through the generator to make sure we've got the right number
Using excel mode csv, which automatically ends lines with \r\n, so need to convert to \n
delete the matching annotation
Saving another testing course mode
And for the XSS course
And the verified course
page not found error because order_type is not business
check for the default currency in the context
check for the override currency settings in the context
unit price should be updated for that course
after getting 10 percent discount
unit price should be updated for that course
unit price should be updated for that course
check button text
Ensure the course has a verified mode
check button text
Once upgraded, should be "verified"
Delete the discounted item, corresponding coupon redemption should  be removed for that particular discounted item
Delete the discounted item, corresponding coupon redemption should be removed for that particular discounted item
check for the default currency in the context
check for the override currency settings in the context
Should have gotten a successful response
One courses in user shopping cart
Should have gotten a successful response
Parse the response as JSON and check the contents
Total amount of a particular course that is purchased by different users
One courses in user shopping cart
check button text
check button text
make sure the enrollment_ids were stored in the PaidCourseRegistration items  refetch them first since they are updated  item1 has been deleted from the the cart.   User has been enrolled for the item1
check for the default currency settings in the context
check for the override currency settings in the context
mail is sent to these emails recipient_email, company_contact_email, order.user.email
fetch the newly generated registration codes
now redeem one of registration code from the previous order
now view the receipt page again to see if any registration codes  has been expired or not
now check for all the registration codes in the receipt  and one of code should be used at this point
add verified mode
Purchase a verified certificate
setting the attempting upgrade session value.
Create other carts first  This ensures that the order ID and order item IDs do not match
Purchase a verified certificate
update the testing_course enrollment dates
update the testing_course enrollment dates
update the testing_course enrollment dates
then the rate limiter should kick in and give a HttpForbidden response
now reset the time to 5 mins from now in future in order to unblock
then the rate limiter should kick in and give a HttpForbidden response
now reset the time to 5 mins from now in future in order to unblock
Registration Code Generation only available to Sales Admins.
get the first registration from the newly created registration codes
check button text
Create a valid registration code
The registration code should NOT be redeemed
The user should NOT be enrolled
Enable donations
Donate to our course
Verify the receipt page
Logged in -- should be a 404
Logged out -- should still be a 404
Purchase a single donation item  Optionally specify a particular course for the donation
Use the fake payment implementation to simulate the parameters  we would receive from the payment processor.
Use the response parameters to simulate a successful payment
PDF_RECEIPT_TERMS_AND_CONDITIONS not displayed in the receipt pdf
Reset the view state
Generate shoppingcart signatures
Simulate a POST request from the payment workflow  page to the fake payment page.
Expect that the response was successful
Expect that we were served the payment page  (not the error page)
Generate shoppingcart signatures
Tamper with the signature
Simulate a POST request from the payment workflow  page to the fake payment page.
Expect that we got an error
Generate shoppingcart signatures
Get the POST params that the view would send back to us
Check that the client accepts these
Generate shoppingcart signatures
Configure the view to declined payments
Check that the decision is "DECLINE"
Configure the view to fail payments
Check that the decision is "REJECT"
Configure the view to accept payments
Check that the decision is "ACCEPT"
Add mock tracker for event testing.
If we retrieve the cart for the user, we should get a different order
purchase the cart more than once
Simulate an error when sending the confirmation  email.  This should NOT raise an exception.  If it does, then the implicit view-level  transaction could cause a roll-back, effectively  reversing order fulfillment.
Verify that the purchase completed successfully
Verify that the user is enrolled as "verified"
check that the registration codes are generated against the order
If the expiration date has not yet passed on a verified mode, the user can be refunded
If there's an error sending an email to billing, we need to log this error
If the expiration date has passed, the user cannot get a refund
If there is no paid certificate, the refund callback should return nothing
No course ID provided, so this is a donation to the entire organization
Create a test course
Pay for a donation
Verify that the donation is in the cart
Purchase the item
Verify that the donation is marked as purchased
Delete the transactions
We use the same hashing function as the software under test,  because it mainly uses standard libraries, and I want  to avoid duplicating that code.
We store the payment status to respond with in a class  variable.  In a multi-process Django app, this wouldn't work,  since processes don't share memory.  Since Lettuce  runs one Django server process, this works for acceptance testing.
Configure all views to respond with the new status
Retrieve the list of signed fields
Calculate the public signature
Indicate whether the payment was successful
Add the list of signed fields
Calculate the public signature
URL to send the POST request to
POST params embedded in the HTML success form
POST params embedded in the HTML decline form
Create a PDF parser object associated with the file object.  Create a PDF document object that stores the document structure.  Supply the password for initialization.
receive the LTPage object for this page  layout is an LTPage object which may contain   child objects like LTTextBox, LTFigure, LTImage, etc.
text
We have to escape ';', because that is our  escape sequence identifier (otherwise, the escaping)  couldn't distinguish between us adding ';_' to the string  and ';_' appearing naturally in the string
Be sure this is really a handler.  We're checking the .__class__ instead of the block itself to avoid  auto-proxying from Descriptor -> Module, in case descriptors want  to ask for handler URLs without a student context.
Is the following necessary? ProxyAttribute causes an UndefinedContext error  if trying this without the module system.     raise ValueError("{!r} is not a handler name".format(handler_name))
If suffix is an empty string, remove the trailing '/'
If there is a query string, append it
If third-party, return fully-qualified url
test for when we haven't set the tag yet
Try to set tag in wrong scope
Try to get tag in wrong scope
Make sure that we don't repeatedly nest LmsFieldData instances
-*- coding: utf-8 -*-
Please do not remove, this is a workaround for Django 1.8.  more information can be found here: https://openedx.atlassian.net/browse/PLAT-902
Translators: "TOC" stands for "Table of Contents"
special case - means somewhere up the hierarchy, merged access rules have eliminated  all group_ids from this partition, so there's no possible intersection.  otherwise, if the parent defines group access rules for this partition,  intersect with the local ones.
add the group access rules for this partition to the merged set of rules.
Specified here so we can see what the value set at the course-level is.
Skip the validation check if the partition has been disabled
When called in the context of a microsite, return an empty result if the org  passed by the caller does not match the designated microsite org.
We only make it to this point if one of org or microsite_org is defined.  If both org and microsite_org were defined, the code would have fallen into the  first branch of the conditional above, wherein an equality check is performed.
When called in the context of a microsite, filtering can stop here.
See if we have filtered course listings in this domain
Filter out any courses belonging to a microsite, to avoid leaking these.
keep specialized logic for Edge until we can migrate over Edge to fully use  microsite definitions
we do not expect this case to be reached in cases where   marketing and edge are enabled
we do not expect this case to be reached in cases where   marketing is enabled or the courses are not browsable
Use the content type to decide what representation to serve
Show the OpenEdX logo in the footer
Include JS and CSS dependencies  This is useful for testing the end-point directly.
Override the language if necessary
-*- coding: utf-8 -*-
Translators: 'EdX', 'edX', and 'Open edX' are trademarks of 'edX Inc.'.  Please do not translate any of these trademarks and company names.
Translators: 'Open edX' is a brand, please keep this untranslated.  See http://openedx.org for more information.
In production, the static files URL will be an absolute  URL pointing to a CDN.  If this happens, we can just  return the URL.
For local development, the returned URL will be relative,  so we need to make it absolute.
If a microsite URL override exists, return it.  Otherwise return the marketing URL.
get marketing link, if marketing is disabled then platform url will be used instead.
if the MicrositeConfiguration has a value for the logo_image_url  let's use that
otherwise, use the legacy means to configure this
check to see that the default setting is to ALLOW iframing
check to see that the override value is honored
HTTP Host changed to edge.
Response should be instance of HttpResponseRedirect.  Location should be "/login".
make sure both courses are visible in the catalog
assert that the course discovery UI is not present
check the /courses view
assert that the course discovery UI is not present
make sure we have the special css class on the section
assert that the course discovery UI is not present
check the /courses view
check the /courses view
check the /courses view as well
Logo
Copyright
Load the footer with the specified language
Verify that the translation occurred
OpenEdX
EdX.org
OpenEdX
EdX.org
OpenEdX
EdX.org
url ends with "/"
url doesn't have "/" at the end
url with path that starts with "/"
url with path without "/"
url is not configured
if api url is None then constructed url should also be None  constructed url should startswith notes view url instead of api view url
constructed url should not contain extra params
constructed url should only has these params if present in api url
extract query params from constructed url
verify that constructed url has only correct params and params have correct values
disable course.edxnotes
reenable course.edxnotes
OAuth2 Client name for edxnotes
pylint: disable=method-hidden
Add a course run if necessary.
course is a locator w/o branch and version  so for uniformity we replace it with one that has them
Relying on default of returning first child
Open and parse the configuration file when the module is initialized
'course_id' is a deprecated field, please use 'id' instead.
Note: This makes a call to the modulestore, unlike the other  fields from CourseSerializer, which get their data  from the CourseOverview object in SQL.
AnonymousUser has no username, so we test for requesting_user's own  username before prohibiting an empty target_username.
This endpoint requires the usage_key for the starting block.
This endpoint is an alternative to the above, but requires course_id as a parameter.
convert the requested course_key to the course's root block's usage_key
add default requested_fields
Add additional requested_fields that are specified as separate  parameters, if they were requested.
Verify that access to all blocks is requested  (and not unintentionally requested).
return None for user
Verify user exists.
create ordered list of transformers, adding BlocksAPITransformer at end.
transform
serialize
return serialized data
collect basic xblock fields
collect basic xblock fields
add self to parent's descendants
This section is an exam.  It should be excluded unless the  user is not a verified student or has declined taking the exam.
'student_view_data'  'student_view_multi_device'
set the block_field_name to None so the entire data for the transformer is serialized
Provide the staff visibility info stored when VisibilityTransformer ran previously
collect basic xblock fields
collect basic xblock fields
collect data from containing transformers
collect phase
transform phase
collect phase
transform phase
pylint: disable=protected-access
collect phase
transform phase
verify count of chapters
verify count of problems
verify other block types are not counted
Build course.
Enroll user in course.
add requested fields
add higher order fields
verify the requested_fields in cleaned_data includes all fields
create a user, enrolled in the toy course
video blocks should have student_view_data
html blocks should have student_view_multi_device set to True
Create a staff user to be able to test visible_to_staff_only
verify root
verify blocks
Create a second course to be filtered out of queries.
Create a second course to be filtered out of queries.
Create a second course to be filtered out of queries.
No filtering.
With filtering.
Create a second course to be filtered out of queries.
'course_id' is a deprecated field, please use 'id' instead.
1 mongo call is made to get the course About overview text.
update the expected_data to include the 'overview' data.
Patch the xml libs
Disable PyContract contract checking when running as a webserver
Trigger a forced initialization of our modulestores since this can take a  while to complete and we want this done before HTTP requests are accepted.
This application object is used by the development server  as well as any WSGI server configured to use this file.
The next page is the dashboard; make sure it loads
These are the parameters that would be included if the user  were trying to enroll in a course.
Wait for the form to change before returning
Submit it
Submit it
Click the password reset link on the login page
Wait for the password reset form to load
Fill in the form
Submit it
Now also verify that focus has moved to this title (for screen readers):
Frontend will automatically switch to Search results tab when search  is running, so the view also needs to be changed.
Find the index of the section in the chapter
Retrieve the scores for the section
CSS indices are 1-indexed, so add one to the list index
The section titles also contain "n of m possible points" on the second line  We have to remove this to find the right title
Some links are blank, so remove them
CSS indices are 1-indexed, so add one to the list index
This is CSS selector means:  Get the scores for the chapter at `chapter_index` and the section at `section_index`  Example text of the retrieved elements: "0/1"
Convert text scores to tuples of (points, max_points)
Filter elements by course name, only returning the relevant course item
Filter elements by course name, only returning the relevant course item
There should only be one course listing corresponding to the provided course name.
Click the upgrade button
Get the link hrefs for all courses
Search for the first link that matches the course id
The only identifier for individual tabs is the link href  so we find the tab with `tab_name` in its text.
Use the private version of _is_on_tab to skip the page check
Get the URL of the instance under test
The URL used for user auth in testing
Check that the next_step_button is enabled before returning control to the caller
Out of the possible poll answers, we want  to select the one that matches POLL_ANSWER and click it.
The first time cohort management is selected, an ajax call is made.
Get rid of the last 4 elements: 'acceptance', 'pages', 'lms', and 'instructor_dashboard.py'  to point to the 'test' folder, a shared point in the path's tree.
Append the folders in the asset's path
Return the joined path of the required asset.
The page may be in either the traditional management state, or an 'add new cohort' state.  Confirm the CSS class is visible because the CSS class can exist on the page even in different states.
Both the edit and create forms have an element with id="cohort-name". Verify that the create form  has been rendered.
Manual assignment type will be selected by default for a new cohort  if we are not setting the assignment type explicitly
Expect the confirmation message substring. (The full message will differ depending on 1 or >1 students added)
Fill the email addresses after the email selector is visible.
Verify enrollment button is present before clicking
This will be present if exam is proctored  This will be present if exam is timed
The next page is the dashboard; make sure it loads
Check the first contribution option, then click the enroll button
Fill the ccx_name.
Verify create ccx button is present before clicking.
Overridden by subclasses to provide the relative path within the course  Paths should not include the leading forward slash.
pylint: disable=attribute-defined-outside-init
Make sure that the transcript button is there
toggle captions visibility state if needed
Verify that captions state is toggled/changed
Make sure that the captions are visible
Click triggers an ajax event
If we are going to click pause button, Ensure that player is not in buffering state
Width of the video container in css equal 75% of window if transcript enabled
Wait for browser to resize completely  Currently there is no other way to wait instead of explicit wait
Wait for browser to resize completely  Currently there is no other way to wait instead of explicit wait
Restore initial window size
check if we have a transcript with correct format
mouse over to transcript button
Sometimes language is not clicked correctly. So, if the current language code  differs form the expected, we try to change it again.
Make sure that all ajax requests that affects the display of captions are finished.  For example, request to get new translation etc.
For troubleshooting purposes show what the current state is.  The debug statements will only be displayed in the event of a failure.
The full time has the form "0:32 / 3:14" elapsed/duration
Dict to store the result
Get the section titles for each chapter
Add one to convert list index (starts at 0) to CSS index (starts at 1)
For test stability, disable JQuery animations (opening / closing menus)
Get the section by index
Click the section to ensure it's open (no harm in clicking twice if it's already open)  Add one to convert from list index to CSS index
Convert list indices (start at zero) to CSS indices (start at 1)
Click the subsection and ensure that the page finishes reloading
Get the index of the item in the sequence
Click on the sequence item at the correct index  Convert the list index (starts at 0) to a CSS index (starts at 1)  Click triggers an ajax event
Retrieve the subsection title for the section  Add one to the list index to get the CSS index, which starts at one
Regular expression to remove HTML span tags from a string
The modal is on the page at large, and not a subelement of the badge div.
This will eventually hold the details about the user account
Create query string parameters if provided
Get the URL of the instance under test
The URL used for user auth in testing
The footer element itself is non-generic, so check above it
Only make the call to size once (instead of once for the height and once for the width)  because otherwise you will trigger a extra query on a remote element.
Disable all animations for faster testing with more reliable synchronization  Click on the element in the browser
Some buttons trigger ajax posts  (e.g. .add-missing-groups-button as configured in split_test_author_view.js)  so after you click anything wait for the ajax call to finish
First make sure that an element with the view-container class is present on the page,  and then wait to make sure that the xblock has finished initializing.
Wait for the xblock javascript to finish initializing
Set content in the CodeMirror editor.
If the pdf upload section has not yet been toggled on, click on the upload pdf button
Clicking on the course will trigger an ajax event
Helpers
Look for the license text that will be displayed by default,  if no button is yet explicitly selected
Get rid of the last 4 elements: 'acceptance', 'pages', 'lms', and 'instructor_dashboard.py'  to point to the 'test' folder, a shared point in the path's tree.
Append the folders in the asset's path
Return the joined path of the required asset.
wait for upload button
wait for popup
upload image
wait for popup closed
Get the URL of the instance under test
This is a page section and can not be accessed directly
This is a page section and can not be accessed directly
Should grab common point between this page module and the data folder.
Click on the save button.
There are prefixes like 'Tools' and '>', but the text itself is not in a span.
pylint: disable=no-member
Should grab common point between this page module and the data folder.
Makes no sense to include this if the tasks haven't run.
CourseOutlineItem is also used as a mixin for CourseOutlinePage, which doesn't have a locator  Check for the existence of a locator so that errors when navigating to the course outline page don't show up  as errors in the repr method instead.
pylint: disable=no-member
pylint: disable=no-member
pylint: disable=no-member
pylint: disable=no-member
Now remove any non-direct descendants.
The None radio button
The Timed exam radio button
The Proctored exam radio button
The Practice exam radio button
The Prerequisite checkbox is visible
The Prerequisite checkbox is checked
The Prerequisites dropdown is visible
The Prerequisites dropdown is visible
Clicking on course with run will trigger an ajax event
Clear the current value, set the new one, then  Tab to move to the next field (so change event is triggered).
Ensure that we make it to another page
Sometimes get stale reference if I hold on to the array of elements
Switch to browser window that shows HTML Unit in LMS  The last handle represents the latest windows opened
Click the delete button  Click the confirmation dialog button
Wait until all xblocks rendered.
Now remove any non-direct descendants.
Overridden by subclasses to provide the relative path within the course  Does not need to include the leading forward or trailing slash
Click on the Advanced icon.
Make sure that the menu of advanced components is visible before clicking (the HTML is always on the  page, but will have display none until the large-advanced-icon is clicked).
Now click on the component to add it.
Adding some components, e.g. the Discussion component, will make an ajax call  but we should be OK because the click_css method is written to handle that.
"Common Problem Types" are shown by default.  For advanced problem types you must first select the "Advanced" tab.
Click on the HTML icon.
Make sure that the menu of HTML components is visible before clicking
Now click on the component to add it.
Adding some components will make an ajax call but we should be OK because  the click_css method is written to handle that.
Click in the input to give it the focus  Select all, then input the value  Return the input_element for chaining
Labels used to identify the fields on the edit modal:
And wait to make sure the ajax post has finished.
basic
We should wait 300 ms for event handler invocation + 200ms for safety.
Create video
Create query string parameters if provided
Pip 1.5 will try to install this package from outside  the directory containing setup.py, so we need to use an absolute path.
Install a course with sections/problems, tabs, updates, and handouts
Logout previously logged in user to be able to see Login page.
Get the URL of the Studio instance under test
Get the URL of the LMS instance under test
Get the URL of the XQueue stub used in the test
Get the URL of the Ora stub used in the test
Get the URL of the comments service stub used in the test
Get the URL of the EdxNotes service stub used in the test
Get the URL of the Programs service stub used in the test
Use auto-auth to retrieve the session for a logged in user
Info about the auto-auth user used to create the course/library.
Use auto-auth to retrieve the session for a logged in user
Create the new XBlock
Configure the XBlock
Create the new XBlock
Description of course updates to add to the course  `date` is a str (e.g. "January 29, 2014)  `content` is also a str (e.g. "Test course")
Set a default start date to the past, but use Studio's  default for the end date (meaning we don't set it here)
If the course already exists, this will respond  with a 200 and an error message, which we ignore.
This will occur if the course identifier is not unique
First, get the current values
Update the old details with our overrides
POST the updated details to Studio
Update the course's handouts HTML
POST advanced settings to Studio
Configure the stub to respond to submissions to our queue
Disable publishing for library XBlocks:
The discussion code assumes that user_id is a string. This ensures that it always will be.
go to the membership page on the instructor dashboard
create course with single cohort and two content groups (user_partition of type "cohort")
go to the membership page on the instructor dashboard
click on the inline save button.
verifies that changes saved successfully.
save button disabled again.
enable always inline discussion topics.
enable some inline discussion topic radio button.  I see that save button is enabled  I see that inline discussion topics are enabled
select some inline discussion topics radio button.
check the discussion topic.
Save button enabled.
verifies that changes saved successfully.
enable some inline discussion topics.
category should not be selected.
check the discussion topic.
verify that category is selected.
enable some inline discussion topics.
category should not be selected.
check the discussion topic.
verify that category is selected.
un-check the discussion topic.
category should not be selected.
enable some inline discussion topics.
category should not be selected.
verifies that changes saved successfully.
verify changes after reload.
go to the membership page on the instructor dashboard
Disable cohorts and verify that the post now shows as visible to everyone.
pylint: disable=unused-argument
Actual test method(s) defined in CohortedDiscussionTestMixin.
Actual test method(s) defined in NonCohortedDiscussionTestMixin.
Actual test method(s) defined in CohortedDiscussionTestMixin.
Actual test method(s) defined in NonCohortedDiscussionTestMixin.
verify threads are rendered on the page
From the thread_page_1 open & verify next thread
Verify that the focus is changed
Check if 'thread-wrapper' is focused after expanding thread
click all the way up through each page
click all the way back down
Create a course to register for.
Create a student who will be in "Cohort A"
Create a student who will be in "Cohort B"
Create a student who will end up in the default cohort group
Start logged in as the staff user.
After adding the cohort, it should automatically be selected
create test file in which index for this test will live
create a unit in course outline
Do the search again, this time we expect results from courses A & B, but not C
Some state is constructed by the parent setUp() routine
Load page objects for use by the tests
Navigate to the index page and get testing!
Useful to capture the current datetime for our tests
Ensure the introduction video element is not shown
Sadly, this sleep is necessary in order to ensure that  sorting by last_activity_at works correctly when running  in Jenkins.
We are doing these operations on this top-level page object to avoid reloading the page.
Get the base URL (the URL without any trailing fragment)
pylint: disable=no-member
Verify the new team was added to the topic list
Verify that if one switches to "My Team" without reloading the page, the newly created team is shown.
Verify that if one switches to "My Team" without reloading the page, the newly joined team is shown.
Verify that if one switches to "My Team" without reloading the page, the old team no longer shows.
Login as staff
Make the first subsection a prerequisite
Login as staff
Gate the second subsection based on the score achieved in the first subsection
Install a course with library content xblock
Missing problem type test
Some parameters are provided by the parent setUp() routine, such as the following:  self.course_id, self.course_info, self.unique_id
Load page objects for use by the tests
Navigate the authenticated, enrolled user to the dashboard page and get testing!
now datetime for usage in tests
reload the page for changes to course date changes to appear in dashboard
Test that proper course date with 'ended' message is displayed if a course has already ended
reload the page for changes to course date changes to appear in dashboard
Test that proper course date with 'started' message is displayed if a course is in running state
reload the page for changes to course date changes to appear in dashboard
Test that proper course date with 'starts' message is displayed if a course is about to start in future,  and course does not start within 5 days
reload the page for changes to course date changes to appear in dashboard
Test that proper course date with 'starts' message is displayed if a course is about to start in future,  and course starts within 5 days
create test file in which index for this test will live
Install a course with a hierarchy and problems
Auto-auth register for the course.
The hint button rotates through multiple hints
Rotate the hint and check the problem hint
Assert that new ccx is created and we are on ccx dashboard/enrollment tab.
Ensure that the superclass sets up
This redirects to an invalid URI.
Enter a submission, which will trigger a pre-defined response from the XQueue stub.
Configure the XQueue stub's response for the text we will submit
Wait 5 seconds for xqueue stub server grader response sent back to lms.
Install a course with sections/problems, tabs, updates, and handouts
Auto-auth register for the course.  Do this as global staff so that you will see the Staff View
After adding the cohort, it should automatically be selected
Masquerade as student in alpha cohort:
Masquerade as student in beta cohort:
pylint: disable=attribute-defined-outside-init
NOTE the first email change was never confirmed, so old has not changed.
Email is not saved until user confirms, so no events should have been  emitted.
Like email, since the user has not confirmed their password change,  the field has not yet changed, so no events will have been emitted.
Note that when we clear the year_of_birth here we're firing an event.
Navigate to the password reset page
Expect that reset password form is visible on the page
Navigate to the password reset page
Navigate to the password reset form and try to submit it
Expect that we're shown a success message
Create a course to enroll in
Create a user account
Navigate to the login page and try to log in
Expect that we reach the dashboard and we're auto-enrolled in the course
Navigate to the login page
User account does not exist
Verify that an error is displayed
Navigate to the password reset form and try to submit it
Expect that we're shown a success message
Navigate to the password reset form
User account does not exist
Expect that we're shown a failure message
Create a user account
Navigate to the login page  Baseline screen-shots are different for chrome and firefox.
Try to log in using "Dummy" provider
Now login with username and password:
Expect that we reach the dashboard and we're auto-enrolled in the course
Now logout and check that we can log back in instantly (because the account is linked):
Create a user account and link it to third party auth with the dummy provider:
When not logged in, try to load a course URL that includes the provider hint ?tpa_hint=...
We should now be redirected to the course page
switch to "Linked Accounts" tab
make sure we are on "Linked Accounts" tab after the account settings  page is reloaded
This must be done after linking the account, or we'll get cross-test side effects  switch to "Linked Accounts" tab
Create the user (automatically logs us in)
Log out
Create a course to enroll in
Navigate to the registration page
Expect that we reach the dashboard and we're auto-enrolled in the course
Navigate to the registration page
Navigate to the register page  Baseline screen-shots are different for chrome and firefox.
Try to authenticate using the "Dummy" provider
Set country, accept the terms, and submit the form:
Expect that we reach the dashboard and we're auto-enrolled in the course
Now logout and check that we can log back in instantly (because the account is linked):
Now unlink the account (To test the account settings view and also to prevent cross-test side effects)  switch to "Linked Accounts" tab
Create a course
Add an honor mode to the course
Add a verified mode to the course
Create a user and log them in
Navigate to the track selection page
Enter the payment and verification flow by choosing to enroll as verified
Proceed to the fake payment page
Submit payment
Proceed to verification
Take face photo and proceed to the ID photo step
Take ID photo and proceed to the review photos step
Submit photos and proceed to the enrollment confirmation step
Navigate to the dashboard
Expect that we're enrolled as verified in the course
Create a user and log them in
Navigate to the track selection page
Enter the payment and verification flow by choosing to enroll as verified
Proceed to the fake payment page
Submit payment
Navigate to the dashboard
Expect that we're enrolled as verified in the course
Create a user, log them in, and enroll them in the honor mode
Navigate to the dashboard
Expect that we're enrolled as honor in the course
Click the upsell button on the dashboard
Select the first contribution option appearing on the page
Proceed to the fake payment page
Submit payment
Navigate to the dashboard
Expect that we're enrolled as verified in the course
self.course_info['number'] must be shorter since we are accessing the wiki. See TNL-1751
Auto-auth register for the course
Access course wiki page
self.course_info['number'] must be shorter since we are accessing the wiki. See TNL-1751
Install a course with sections/problems, tabs, updates, and handouts
Auto-auth register for the course
Navigate to the course info page from the progress page
Expect just one update
Expect a link to the demo handout pdf
Navigate to the progress page from the info page
We haven't answered any problems yet, so assume scores are zero  Only problems should have scores; so there should be 2 scores.
From the course info page, navigate to the static tab
From the course info page, navigate to the static tab
Verify that Mathjax has rendered
From the course info page, navigate to the wiki tab
Navigate to the course page from the info page
Check that the course navigation appears correctly
Navigate to a particular section
Check the sequence items
Install a course with TextBooks
Auto-auth register for the course
Verify each PDF textbook tab by visiting, it will fail if correct tab is not loaded.
Auto-auth register for the course
Auto-auth register for the course
visit dashboard page and make sure there is not pre-requisite course message
Logout and login as a staff.
visit course settings page and set pre-requisite course
Logout and login as a student.
Install a course with sections and problems.
Auto-auth register for the course
Navigate to the problem page
Does the page have computation results?
Fill in the answer correctly.
Fill in the answer incorrectly.
Auto-auth register for the course
visit course page and make sure there is not entrance exam chapter.
Logout and login as a staff.
Logout and login as a student.
visit course info page and make sure there is an "Entrance Exam" section.
Add an honor mode to the course
Add a verified mode to the course
changed back to English language.
Initialize the page objects
Add a verified mode to the course
Auto-auth register for the course.
the track selection page cannot be visited. see the other tests to see if any prereq is there.  Navigate to the track selection page
Enter the payment and verification flow by choosing to enroll as verified
Proceed to the fake payment page
Submit payment
Visit the course outline page in studio
open the exam settings to make it a proctored exam.
select advanced settings tab
login as a verified student and visit the courseware.
Start the proctored exam.
Visit the course outline page in studio
open the exam settings to make it a proctored exam.
select advanced settings tab
login as a verified student and visit the courseware.
Start the timed exam.
Stop the timed exam.
Given that an exam has been configured to be a timed exam.
When I log in as an instructor,
And visit the Allowance Section of Instructor Dashboard's Special Exams tab
Then I can add Allowance to that exam for a student
When I click the Add Allowance button
Then popup should be visible
When I fill and submit the allowance form
Then, the added record should be visible
Given that an exam has been configured to be a proctored exam.
When I log in as an instructor,
And visit the Student Proctored Exam Attempts Section of Instructor Dashboard's Special Exams tab
Then I can see the search text field
And I can see one attempt by a student.
And I can remove the attempt by clicking the "x" at the end of the row.
Create the user (automatically logs us in)
go to the student admin page on the instructor dashboard
then we have alert confirming action
Verify that added exceptions are also synced with backend  Revisit Page
wait for the certificate exception section to render
validate certificate exception synced with server is visible in certificate exceptions list
Remove Certificate Exception
Verify that added exceptions are also synced with backend  Revisit Page
wait for the certificate exception section to render
validate certificate exception synced with server is visible in certificate exceptions list
Add a student to Certificate exception list
Add duplicate student to Certificate exception list
Click 'Add Exception' button without filling username/email field
Click 'Add Exception' button with invalid username/email field
Click 'Add Exception' button with invalid username/email field
Add a student to Certificate exception list
Click 'Generate Exception Certificates' button
Revisit Page & verify that added exceptions are also synced with backend
Wait for the certificate exception section to render
Validate certificate exception synced with server is visible in certificate exceptions list
Create course fixture once each test run
set same course number as we have in fixture json
we have created a user with this id in fixture, and created a generated certificate for it.
Enroll above test user in the course
Validate success message
Verify that added invalidations are also synced with backend  Revisit Page
wait for the certificate invalidations section to render
validate certificate invalidation is visible in certificate invalidation list
Verify that added invalidations are also synced with backend  Revisit Page
wait for the certificate invalidations section to render
click "Remove from Invalidation Table" button next to certificate invalidation
validate certificate invalidation is removed from the list
Click "Invalidate Certificate" with empty student username/email field
Click "Invalidate Certificate" with invalid student username/email
Click 'Invalidate Certificate' button with not enrolled student
Load certificate web view page for use by the tests
set same course number as we have in fixture json
Verify that their is no padding around the box containing certificate info.
Navigate to Test Subsection in Test Section Section
Navigate to Test Problem 1
Select correct value for from select menu
Select correct radio button for the answer
Submit the answer
Navigate to the 'Test Subsection 2' of 'Test Section 2'
Navigate to Test Problem 2
Fill in the answer of the problem
Submit the answer
create test file in which index for this test will live
Create a student who will end up in the default cohort group
After adding the cohort, it should automatically be selected
Install a course with sections/problems, tabs, updates, and handouts
Auto-auth register for the course.
Visit problem page as a student.
Logout and login as a staff user.
Visit course outline page in studio.
Set release date for subsection in future.
Logout and login as a student.
Visit courseware as a student.  Problem name should be "Test Problem 2".
Add a verified mode to the course
Auto-auth register for the course.
the track selection page cannot be visited. see the other tests to see if any prereq is there.  Navigate to the track selection page
Enter the payment and verification flow by choosing to enroll as verified
Proceed to the fake payment page
Submit payment
Install a course with sections/problems, tabs, updates, and handouts
start in first section
next takes us to next tab in sequential
go to last sequential position
next takes us to next sequential
next takes us to next chapter
previous takes us to previous chapter
previous takes us to last tab in previous sequential
previous takes us to previous tab in sequential
test UI events emitted by navigation
test UI events emitted by navigating via the course outline
Set the scope to the sequence navigation
Install a course with section, tabs and multiple choice problems.
Auto-auth register for the course.
Go to sequential position 1 and assert that we are on problem 1.
Update problem 1's content state by clicking check button.
Save problem 1's content state as we're about to switch units in the sequence.
Go to sequential position 2 and assert that we are on problem 2.
Come back to our original unit in the sequence and assert that the content hasn't changed.
Go to sequential position 1 and assert that we are on problem 1.
Update problem 1's content state by clicking save button.
Save problem 1's content state as we're about to switch units in the sequence.
Go to sequential position 2 and assert that we are on problem 2.
Come back to our original unit in the sequence and assert that the content hasn't changed.
Go to sequential position 1 and assert that we are on problem 1.
Save problem 1's content state as we're about to switch units in the sequence.
Go to sequential position 2 and assert that we are on problem 2.
Come back to our original unit in the sequence and assert that the content hasn't changed.
Change the privacy if requested by loading the page and  changing the drop down
Change the privacy setting if it is not the desired one already
Verify the current setting is as expected
Load the page
Set the privacy for the new user
Set the user's year of birth
Log the user out
Reload the page and verify that the profile is now public
Reload the page and verify that the profile is now private
Run the search  No error message appears
Tag group "cool"
Tag group "review"
Notes with no tags
visiting the page results in an ajax request to fetch the notes
visiting the page results in an ajax request to fetch the notes
visiting the page results in an ajax request to fetch the notes
visiting the page results in an ajax request to fetch the notes
Because all the notes (with tags) have the same tags, they will end up ordered alphabetically.
test pagination with valid page number
test pagination with invalid page number
test pagination with valid page number
test pagination with invalid page number
create test file in which index for this test will live
create a unit in course outline
Create content in studio without publishing.
Do a search, there should be no results shown.
Publish in studio to trigger indexing.
Do the search again, this time we expect results.
Create content in studio without publishing.
Do a search, there should be no results shown.
Publish in studio to trigger indexing, and edit chapter name afterwards.
Do a ReIndex from studio to ensure that our stuff is updated before the next stage of the test
Search after publish, there should still be no results shown.
Do a ReIndex from studio to ensure that our stuff is updated before the next stage of the test
Do the search again, this time we expect results.
Generate the problem XML using capa.tests.response_xml_factory
Make sure we're looking at the right problem
Answer the problem correctly
Answer the problem incorrectly
Set the scope to the problem container
Run the accessibility audit.
Correct answer is any two integers that sum to 10
If we want an incorrect answer, then change  the second addend so they no longer sum to 10
Auto-auth register for the course.
create index file
Fill in the conditional page poll  The conditional does not update on its own, so we need to reload the page.
Auto-auth register for the course.
Logout and login as staff
Visit course outline page in studio.
Logout and login as a student.
Visit courseware as a student.
Verify bookmarked breadcrumbs.
Install a course with two annotations and two annotations problems.
Auto-auth register for the course.
This will avoid scrolling related problems on different browsers and instead directly jump on the problem
For transcripts, you need to check an actual video, so we will  just specify our default video and see if that one is available.
if value is not an option choice then it should return false
Make sure specified option is actually selected
allow the filters to use "assert" to filter out events
Auto-auth register for the course.
Validate the event payload
A weak assertion for the timestamp as well
Validate the event payload
A weak assertion for the timestamp as well
go to video
go to video
wait until video stop playing
go to video
wait until video stop playing
go to video
wait until video stop playing
If there is only one language then there will be no subtitle/captions menu
reset youtube stub server
Video tests require at least one vertical with a single video.
Verify that video has rendered in "Youtube" mode
Verify that we see "好 各位同学" text in the transcript
Hide captions and make sure they're hidden and cookie is unset
Verify that we see "Welcome to edX." text in the captions
click video button "fullscreen"
check if video aligned correctly without enabled transcript
go to video
check if we can download transcript in "srt" format that has text "好 各位同学"
go to video
check if "Welcome to edX." text in the captions
check if we can download transcript in "srt" format that has text "Welcome to edX."
select language with code "zh"
check if we see "好 各位同学" text in the captions
check if we can download transcript in "srt" format that has text "好 各位同学"
go to video
make sure captions are opened
click video button "fullscreen"
check if video aligned correctly with enabled transcript
click video button "transcript"
check if video aligned correctly without enabled transcript
configure youtube server
configure youtube server
configure youtube server
The video should only be loaded once
configure youtube server
The video should only be loaded once
configure youtube server
check if caption button is visible
open the section with videos (open vertical containing video "A")
check if we can download transcript in "srt" format that has text "00:00:00,260"
select the transcript format "txt"
check if we can download transcript in "txt" format that has text "Welcome to edX."
open vertical containing video "B"
check if we can download transcript in "txt" format that has text "Equal transcripts"
open vertical containing video "C"
menu "download_transcript" doesn't exist
go to video
go to video
we start the video, then pause it to activate the transcript
go to video
go to second sequential position  import ipdb; ipdb.set_trace()
go back to first sequential position  we are again playing tab 1 videos to ensure that switching didn't broke some video functionality.  import ipdb; ipdb.set_trace()
select the "2.0" speed on video "A"
select the "0.50" speed on video "B"
open video "C"
go to the vertical containing video "A"
Video "A" should still play at speed 2.0 because it was explicitly set to that.
reload the page
go to the vertical containing video "A"
check if video "A" should start playing at speed "2.0"
select the "1.0" speed on video "A"
go to the vertical containing "B"
Video "B" should still play at speed .5 because it was explicitly set to that.
go to the vertical containing video "C"
The change of speed for Video "A" should  impact Video "C" because it still has  not been explicitly set to a speed.
go to video
no autoplay here, maybe video is too small, so pause is not switched
go to second sequential position
go back to first sequential position  we are again playing tab 1 videos to ensure that switching didn't broke some video functionality.
Verify that the video has rendered in "Youtube" mode
Verify that the video has autoplay mode disabled
Verify that error message is shown
Verify that error message has correct text
Verify that spinner is not shown
go to video
check if we see "好 各位同学" text in the captions
check if we can download transcript in "srt" format that has text "好 各位同学"
go to video
check if "Welcome to edX." text in the captions
check if we can download transcript in "srt" format that has text "Welcome to edX."
select language with code "zh"
check if we see "好 各位同学" text in the captions
Then I can download transcript in "srt" format that has text "好 各位同学"
go to video
make sure captions are opened
click video button "fullscreen"
check if video aligned correctly with enabled transcript
go to video
make sure captions are opened
check if we see "Welcome to edX." text in the captions
go to video
make sure captions are opened
check if we see "好 各位同学" text in the captions
go to video
limit the scope of the audit to the video player only.
This will be initialized later
Total video xblock components count should be equals to 2  Why 2? One video component is created by default for each test. Please see  test_studio_video_module.py:CMSVideoTest._create_course_unit  And we are creating second video component here.
Visit Course Outline page
Visit Unit page
The 0th entry is the unit page itself.
The 0th entry is the unit page itself.
The 0th entry is the unit page itself.
This will create a video by doing a single click and then ensure that video is created
change id of first default video
again open unit page and check that video controls show for both videos
verify that the error message isn't shown by default
we're loading a shorter transcript to ensure both skip links are available
limit the scope of the audit to the video player only.
Verify that each page is available
Add a verified mode to the course
Set the certificate properties
Save the certificate
Edit the certificate
Delete the certificate we just created
Reload the page and confirm there are no certificates
Edit the signatory in certificate
Make sure certificate is created
Make sure certificate is created
Make sure certificate is created
set up course number override in Advanced Settings Page
Add a video component to Group 1  Duplicate the first item in Group A
Drag newly added video component to top.  Drag duplicated component to top.
Group A itself has a delete icon now, so item_1 is index 1 instead of 0.
If anything other than 'All Students and Staff', is selected,  'Specific Content Groups' should be selected as well.
Make initial edit(s) and save
Re-open the modal and inspect its selected inputs
Will initially be in staff view, locked component should be visible.  Switch to student view and verify not visible
Will initially be in staff view, components always visible.  Switch to student view and verify visible.
Unfortunately no blocks in the core platform implement display_name_with_default  in an interesting way for this test, so we are just testing for consistency and not  the actual value.
The next page is the library edit view; make sure it loads:
Then go back to the home page and make sure the new library is listed there:
precondition check - the library block should be configured before we remove the library setting
Formerly flaky: see TE-745
Removed this assert until a summary message is added back to the author view (SOL-192)
Removed this assert until a summary message is added back to the author view (SOL-192)
precondition check - assert library has children matching filter criteria
Library should contain single Dropdown problem, so now there should be no errors again
precondition check - assert block is configured fine
Create a new block, causing a new library version:
Reset:
Get the time when the import has started.  import_page timestamp is in (MM/DD/YYYY at HH:mm) so replacing (second, microsecond) to  keep the comparison consistent
Get the time when the import has finished.  import_page timestamp is in (MM/DD/YYYY at HH:mm) so replacing (second, microsecond) to  keep the comparison consistent
Successful creation of course takes user to course outline page
Go back to dashboard and verify newly created course exists there
Successful creation of course takes user to course outline page
Go back to dashboard and verify newly created course exists there
Reload the page and expand all subsections to see that the change was persisted.
with collapsed outline
with first sequential expanded
expand first subsection
Define the dimensions that map to the UnitState constructor
Add a fixture for every state in the product of features
Verify that Release date visible by default  Verify that Due date and Policy hidden by default
Set new values
Verify that Release date visible by default  Verify that Due date and Policy are not present
Verify fields
Verify initial value
Set new value
Verify that Due date and Policy are not present
Set new values
Create a deprecated component with display_name to be empty and make sure  the deprecation warning is displayed with
Edit the second content group
Delete content group
Waiting for the page load and verify that we've landed on course outline page
Before every test, make sure to visit the page first
Feed an integer value for String field.  .set method saves automatically after setting a value
Test Modal
Save original values and feed wrong inputs
Test Modal
Save original values and feed wrong inputs
Let modal popup
Click Undo Changes button
Check that changes are undone
Check that the validation modal went away.
Check presence of modal
List of wrong settings item & what is presented in the modal should be the same
The course_license text will include a bunch of screen reader text to explain  the selected options
There are several existing color contrast errors on this page,  we will ignore this error in the test until we fix them.
limit the scope of the audit to the special exams tab on the modal dialog
from nose.tools import set_trace; set_trace()
Ensure jquery is loaded before running a jQuery  This text appears towards the end of the work that jQuery is performing on the page
upload image
upload image
upload image
First xblock is the container for the page, subtract 1.
Verify inactive xblocks appear after active xblocks
Wait for the xblock to be fully initialized so that the add button is rendered
Click the add button and verify that the groups were added on the page
Reload the page to make sure the groups were persisted.
The inactive group is the 2nd group, but it is the first one  with a visible delete button, so use index 0
To make sure that id is present on the page and it is not an empty.  We do not check the value of the id, because it's generated randomly and we cannot  predict this value
Expand the configuration
Collapse the configuration
Go to the Group Configuration Page
I publish and view in LMS and it is rendered correctly
Save the configuration
Remove group with name "New Group Name"  Rename Group A  Save the configuration
Add split test to vertical and assign newly created group configuration to it
`Group C` -> `Second Group`  Add new group
Remove Group A  Save the configuration
Click the add button and verify that the groups were added on the page
Create new group configuration
Cancel the configuration
Cancel the configuration
Try to save  Verify that configuration is still in editing mode  Verify error message
Create new group configuration  Leave empty required field
Save the configuration
Go to the Group Configuration Page and click on outline anchor
Waiting for the page load and verify that we've landed on course outline page
Go to the Group Configuration Page and click unit anchor
Waiting for the page load and verify that we've landed on the unit page
Delete first group configuration via detail view
Delete first group configuration via edit view
Appropriate Group Configuration is expanded.
Create group configuration and associated experiment
Display details view  Check that error icon and message are not present
Add a group
Display details view  Check that warning icon and message are not present
Remove a group
render in LMS correctly
render in LMS to see how inactive vertical is rendered
I go to split test and delete inactive vertical
render in LMS again
Create a new block:
Delete the first block:
Check that the save worked:
Create a second user for use in these tests:
There are several existing color contrast errors on this page,  we will ignore this error in the test until we fix them.
Ensure that the superclass sets up
The 0th entry is the unit page itself.
Reload the page to see that the change was persisted.
Before every test, make sure to visit the page first
Refresh the page again and confirm the prerequisite course selection is properly reflected
Refresh the page again to confirm the None selection is properly reflected
Re-pick the prerequisite course and confirm no errors are thrown (covers a discovered bug)
Refresh the page again to confirm the prerequisite course selection is properly reflected
getting the course outline page.
title with text 'Entrance Exam' should be present on page.
Delete the currently created entrance exam.
button with text 'New Unit' should be present.
button with text 'New Subsection' should not be present.
Set the course start date to tomorrow in order to allow setting pacing
Ensure that the superclass sets up
Make '_' a no-op so we can scrape strings. Using lambda instead of   `django.utils.translation.ugettext_noop` because Django cannot be imported in this file
don't display irrelevant gunicorn sync error
NOTE: we are importing this method so that any module that imports us has access to get_current_request
This is a tuple for holding scores, either from problems or sections.  Section either indicates the name of the problem or the name of the section
A special message type indicating that the xblock is not yet configured. This message may be rendered  in a different way within Studio.
HACK: This shouldn't be hard-coded to two types  OBSOLETE: This obsoletes 'type'
Stats event sent to DataDog in order to determine if old XML parsing can be deprecated.
This is the view that will be rendered to display the XBlock in the LMS.  It will also be used to render the block in "preview" mode in Studio, unless  the XBlock also implements author_view.
An optional view of the XBlock similar to student_view, but with possible inline  editing capabilities. This view differs from studio_view in that it should be as similar to student_view  as possible. When previewing XBlocks within Studio, Studio will prefer author_view to student_view.
The view used to render an editor in Studio. The editor rendering can be completely different  from the LMS student_view, and it is only shown when the author selects "Edit".
Views that present a "preview" view of an xblock (as opposed to an editing view).
cdodge: We've moved the xmodule.coffee script from an outside directory into the xmodule area of common  this means we need to make sure that all xmodules include this dependency which had been previously implicitly  fulfilled in a different area of code
Added xmodule.js separately to enforce 000 prefix for this only.
it'd be nice to have a useful default but it screws up other things; so,  use display_name_with_default for those
This indicates whether the xmodule is a problem-type.  It should respond to max_score() and grade(). It can be graded or ungraded  (like a practice problem).
Whether this module can be displayed in read-only mode.  It is safe to set this to True if  all user state is handled through the FieldData API.
if caller wants kvs, caller's assuming it's up to date; so, decache it
Be backwards compatible with callers using usage_key_filter
Skip rebinding if we're already bound a user, and it's this user.
If we are switching users mid-request, save the data from the old user.
Update scope_ids to point to the new user.
Clear out any cached instantiated children.
Clear out any cached field data scoped to the old user.
not the most elegant way of doing this, but if we're removing  a field from the module's field_data_cache, we should also  remove it from its _dirty_fields
Set the new xmodule_runtime and field_data (which are user-specific)
We are not allowing editing of xblock tag and name fields at this time (for any component).
Only use the fields from this class, not mixins
Set the descriptor first so that we can proxy to it
Take advantage of the children cache that the descriptor might have
VS[compat].  Backwards compatibility code that can go away after  importing 2012 courses.  A set of metadata key conversions that we want to make
update_version is the version which last updated this xblock v prev being the penultimate updater  leaving off original_version since it complicates creation w/o any obv value yet and is computable  by following previous until None  definition_locator is only used by mongostores which separate definitions from blocks
It'd be great to not reserialize and deserialize the xml
xmodule_instance is set by the XModule.__init__. If we had an error after that,  we need to clean it out so that we can set up the ErrorModule instead
NOTE: we generally don't want content errors logged as errors  work around
This is used by XModules to write out separate files during xml export
Currently, Modulestore is responsible for instantiating DescriptorSystems  This means that LMS/CMS don't have a way to define a subclass of DescriptorSystem  that implements the correct local_resource_url. So, for now, instead, we will reference a  global function that the application can override.
A stub publish method that doesn't emit any events from XModuleDescriptors.
getting the service from parent module. making sure of block service declarations.  Passing the block to service if it is callable e.g. ModuleI18nService. It is the responsibility of calling  service to handle the passing argument.
remove xblock-family from elements
get xblock-family from node  now process them & remove them from the xml payload
Remove value set transiently by XBlock
getting the service from parent module. making sure of block service declarations.  Passing the block to service if it is callable e.g. ModuleI18nService. It is the responsibility of calling  service to handle the passing argument.
First we try a lookup in the module system...
filter removes possible Nones in texts and tails
get numerators + denominators
This escaping is incomplete.  However, rather than switching this to use  markupsafe.escape() and fixing issues, better to put that energy toward  migrating away from this method altogether.
Translators: TBD stands for 'To Be Determined' and is used when a course  does not yet have an announced start date.
Make courses that have an announcement date have a lower  score than courses than don't, older courses should have a  higher score.
Youtube case:
HTML5 case
Only do redirect for English
If this video lives in library, the code below is not relevant and will error.
Try to return static URL redirection as last resort  if no translation is required
Make '_' a no-op so we can scrape strings. Using lambda instead of   `django.utils.translation.ugettext_noop` because Django cannot be imported in this file
To make sure that js files are called in proper order we use numerical  index. We do that to avoid issues that occurs in tests.
OrderedDict for easy testing of rendered context in tests
Determine if there is an alternative source for this video  based on user locale.  This exists to support cases where  we leverage a geography specific CDN, like China.
If we have an edx_video_id, we prefer its values over what we store  internally for download links (source, html5_sources) and the youtube  stream.
set the youtube url
If the user comes from China use China CDN for html5 videos.  'CN' is China ISO 3166-1 country code.  Video caching is disabled for Studio. User_location is always None in Studio.  CountryMiddleware disabled for Studio.
If there was no edx_video_id, or if there was no download specified  for it, we fall back on whatever we find in the VideoDescriptor
This won't work when we move to data that  isn't on the filesystem
This is the server's guess at whether youtube is available for  this user, based on what was recorded the last time we saw the  user, and defaulting to True.
For backwards compatibility -- if we've got XML data, parse it out and set the metadata fields
If `source` field value exist in the `html5_sources` field values,  then delete `source` field value and use value from `html5_sources` field.
Force download_video field to default value if it's not explicitly set for backward compatibility.
for backward compatibility.  If course was existed and was not re-imported by the moment of adding `download_track` field,  we should enable `download_track` if following is true:
We're loading a descriptor, so student_id is meaningless  We also don't have separate notions of definition and usage ids yet,  so we use the location for both
handle license specifically
First try a lookup in VAL. If we have a YouTube entry there, it overrides the  one passed in.
Handle the fact that youtube IDs got double-quoted for a period of time.  Note: we pass in "VideoFields.youtube_id_1_0" so we deserialize as a String--  it doesn't matter what the actual speed is for the purposes of deserializing.
Convert between key types for certain attributes --  necessary for backwards compatibility.  example: 'start_time': cls._example_convert_start_time
We export values with json.dumps (well, except for Strings, but  for about a month we did it for Strings also).
For backwards compatibility: Add `source` if XML doesn't have `download_video`  attribute.
For backwards compatibility: if XML doesn't have `download_track` attribute,  it means that it is an old format. So, if `track` has some value,  `download_track` needs to have value `True`.
Allow ValCannotCreateError to escape
load license if it exists
Check to see if there are transcripts in other languages besides default transcript
If the "only_on_web" field is set on this video, do not return the rest of the video's data  in this json view, since this video is to be accessed only through its web view."
Check in VAL data first if edx_video_id exists
get and cache bulk VAL data for course
Get the encoded videos if data from VAL is found
Fall back to other video URLs in the video module if not found in VAL
Include youtube link if there is no encoding for mobile- ie only a fallback URL or no encodings at all  We are including a fallback URL for older versions of the mobile app that don't handle Youtube urls
Make '_' a no-op so we can scrape strings. Using lambda instead of   `django.utils.translation.ugettext_noop` because Django cannot be imported in this file
`source` is deprecated field and should not be used in future.  `download_video` is used instead.
Data format: {'de': 'german_translation', 'uk': 'ukrainian_translation'}
2 convert /static/filename.srt  to filename.srt in self.transcripts.
2.
3.
remove key from transcripts because proper srt file does not exist in assets.
Used utf-8-sig encoding type instead of utf-8 to remove BOM(Byte Order Mark), e.g. U+FEFF
If we're not verifying the assets, we just trust our field values
clean up /static/ prefix from bumper transcripts
if no bumper sources, nothing will be showed
Contruction of the rewrite url is intentionally very flexible of input.  For example, https://www.edx.org/ + /foo.html will be rewritten to  https://www.edx.org/foo.html.
Mimic the behavior of removed get_video_from_cdn in this regard and  return None causing the caller to use the original URL.
If credentials were provided, authenticate the user.
Make '_' a no-op so we can scrape strings. Using lambda instead of   `django.utils.translation.ugettext_noop` because Django cannot be imported in this file
Class property that specifies the type of the tab.  It is generally a constant value for a  subclass, shared by all instances of the subclass.
The title of the tab, which should be internationalized using  ugettext_noop since the user won't be available in this context.
Class property that specifies whether the tab can be hidden for a particular course
Class property that specifies whether the tab is hidden for a particular course
The relative priority of this view that affects the ordering (lower numbers shown first)
Class property that specifies whether the tab can be moved within a course's list of tabs
Class property that specifies whether the tab is a collection of other tabs
True if this tab is dynamically added to the list of tabs
True if this tab is a default for the course (when enabled)
True if this tab can be included more than once for a course.
If there is a single view associated with this tab, this is the name of it
'other' is a dict-type tab and did not validate
allow tabs without names; if a name is required, its presence was checked in the validator.
only compare the persisted/serialized members: 'type' and 'name'
Presence of syllabus tab is indicated by a course attribute
If the course has a discussion link specified, use that even if we feature  flag discussions off. Disabling that is mostly a server safety feature  at this point, and we don't need to worry about external sites.
the discussion_link setting overrides everything else, even if there is a discussion tab in the course tabs
find one of the discussion tab types in the course tabs
If rendering inline that add each item in the collection,  else just show the tab itself as long as it is not empty.
Make '_' a no-op so we can scrape strings. Using lambda instead of   `django.utils.translation.ugettext_noop` because Django cannot be imported in this file
it'd be nice to have a useful default but it screws up other things; so,  use display_name_with_default for those
When we switch this to an XBlock, we can merge this with student_view,  but for now the XModule mixin requires that this method be defined.  pylint: disable=no-member
also look for .html versions instead of .xml
Add some specific HTML rendering context when editing HTML modules where we pass  the root /c4x/ url for assets. This allows client-side substitutions to occur.
log.debug("candidates = {0}".format(candidates))
add more info and re-raise
Write html to file, return an empty tag
write out the relative name
statuses
VS[compat]  backwards compatibility with old nested customtag structure
cdodge: look up the template as a module
in case we want to add to this class, a version will be handy  for deserializing old versions.  (This will be serialized in courses)
The Stevedore extension point namespace for user partition scheme plugins.
The collection of user partition scheme extensions.
The default scheme to be used when upgrading version 1 partitions.
If no scheme was provided, set it to the default ('random')
Version changes should be backwards compatible in case the code  gets rolled back.  If we see a version number greater than the current  version, we should try to read it rather than raising an exception.
Be sure to clean up the global scheme_extensions after the test.
Create a test partition
Make sure the names are set on the schemes (which happens normally in code, but may not happen in tests).
Derive a "user_id" from the username, just so we don't have to add an  extra param to this method. Just has to be unique per user.
assign the first group to be returned
get a group assigned to the user
switch to the second group and verify that it is returned for the user
Two StaticPartitionService objects that share the same cache:
A StaticPartitionService with its own local cache
A StaticPartitionService that never uses caching.
Set the group we expect users to be placed into
Make sure our partition services all return the right thing, but skip  ps_shared_cache_2 so we can see if its cache got updated anyway.
Now select a new target group
Both of the shared cache entries should return the old value, even  ps_shared_cache_2, which was never asked for the value the first time  Likewise, our separately cached piece should return the original answer
Our uncached service should be accurate.
And a newly created service should see the right thing
assign first group and verify that it is returned for the user
switch to the second group and verify that it is returned for the user
Make '_' a no-op so we can scrape strings. Using lambda instead of   `django.utils.translation.ugettext_noop` because Django cannot be imported in this file
All available user partitions (with value and display name). This is updated each time  editable_metadata_fields is called.  Default value used for user_partition_id
Specified here so we can see what the value set at the course-level is.
group_id is an int  child is a serialized UsageId (aka Location).  This child  location needs to actually match one of the children of this  Block.  (expected invariant that we'll need to test, and handle  authoring tools that mess this up)
Peak confusion is great.  Now that we set child_descriptor,  get_children() should return a list with one element--the  xmodule for the child
Sort active and inactive contents by group name.
raise error instead?  In fact, could complain on descriptor load...
the editing interface can be the same as for sequences -- just a container
Any existing value of user_partition_id will be in "old_content" instead of "old_metadata"  because it is Scope.content.
Don't need to call update_item in the modulestore because the caller of this method will do it.  If children referenced in group_id_to_child have been deleted, remove them from the map.
Update the list of partitions based on the currently available user_partitions.
Explicitly add user_partition_id, which does not automatically get picked up because it is Scope.content.  Note that this means it will be saved by the Studio editor as "metadata", but the field will  still update correctly.
Compute the inactive children in the order they were added to the split test
user.id - to be fixed by Publishing team
Make '_' a no-op so we can scrape strings. Using lambda instead of   `django.utils.translation.ugettext_noop` because Django cannot be imported in this file
The last page should be the last element in the table of contents,  but it may be nested. So recurse all the way down the last element
If we can't get to S3 (e.g. on a train with no internet), don't break  the rest of the courseware.
Ensure that courses imported from XML keep their image
Ensure that courses imported from XML keep their image
Ensure that courses imported from XML keep their image
Translators: This field is the container for course-specific certifcate configuration values  Translators: These overrides allow for an alternative configuration of the certificate web view
Specific certificate information managed via Studio (should eventually fold other cert settings into this)  Translators: This field is the container for course-specific certifcate configuration values  Translators: These overrides allow for an alternative configuration of the certificate web view
NOTE (THK): This is a last-minute addition for Fall 2012 launch to dynamically    disable the syllabus content for courses that do not provide a syllabus
Override any global settings with the course settings
Default to a blank policy dict
if we successfully read the file, stop looking at backups
bleh, have to parse the XML here to just pull out the url_name attribute  I don't think it's stored anywhere in the instance.
Try to load grading policy
now set the current instance. set_grading_policy() will apply some inheritance rules
Load the wiki tag if it exists
load license if it exists
handle license specifically. Default the course to have a license  of "All Rights Reserved", if a license is not explicitly set.
force the caching of the xblock value so that it can detect the change  pylint: disable=pointless-statement
NOTE WELL: this change will not update the processed graders. If we need that, this needs to call grader_from_conf
XBlock fields don't update after mutation
If this descriptor has been bound to a student, return the corresponding  XModule. If not, just use the descriptor itself
The xmoduledescriptors included here are only the ones that have scores.
HACK: This shouldn't be hard-coded to two types  OBSOLETE: This obsoletes 'type'
pylint: disable=no-member
Make '_' a no-op so we can scrape strings. Using lambda instead of   `django.utils.translation.ugettext_noop` because Django cannot be imported in this file
Make '_' a no-op so we can scrape strings. Using lambda instead of   `django.utils.translation.ugettext_noop` because Django cannot be imported in this file
Determine which of our children we will show:  Remove any selected blocks that are no longer valid:
If max_count has been decreased, we may have to drop some previously selected blocks:
Do we have enough blocks now?
reason "invalid" means deleted from library or a different library is now being used.
Save our selections to the user state, to ensure consistency:
The following JS is used to make the "Update now" button work on the unit page and the container view:
The only supported mode is currently 'random'.  Add the mode field to non_editable_metadata_fields so that it doesn't  render in the edit form.
May be None when creating bok choy test fixtures
Children have been handled.
Make '_' a no-op so we can scrape strings. Using lambda instead of   `django.utils.translation.ugettext_noop` because Django cannot be imported in this file
Name of poll to use in links to this poll
List of answers, in the form {'id': 'some id', 'text': 'the answer text'}
FIXME: fix this, when xblock will support mutable types.  Now we use this hack.
FIXME: fix this, when xblock will support mutable types.  Now we use this hack.
FIXME: hack for resolving caching `default={}` during definition  poll_answers field
FIXME: fix this, when xblock support mutable types.  Now we use this hack.
Check for presense of required tags in xml.
Make '_' a no-op so we can scrape strings. Using lambda instead of   `django.utils.translation.ugettext_noop` because Django cannot be imported in this file
The discussion XML format uses `id` and `for` attributes,  but these would overload other module attributes, so we prefix them  for actual use in the code
We may choose to enable sort_keys in the future, but while Kevin is investigating....
Make '_' a no-op so we can scrape strings  Using lambda instead of `django.utils.translation.ugettext_noop` because Django cannot be imported in this file
add any of descriptor's explicitly set fields to the inheriting list  inherited_settings values are json repr
xml backed courses are read-only, but they do have some computed fields
When blacklists are this, all children should be excluded
dict(version_guid, dict(BlockKey, module))
If no course index has been set, then no branches have changed
If there was no index in the database to start with, then all branches  are dirty by definition
handle split specific things and defer to super otherwise
handle version_guid based retrieval locally
handle ignore case and general use
Ensure that any edits to the index don't pollute the initial_index
If the content is dirty, then update the database
The structure hasn't been loaded from the db yet, so load it
cast string to ObjectId if necessary
The definition hasn't been loaded from the db yet, so load it
cast string to ObjectId if necessary
Only query for the definitions that aren't already cached.
If we have an active bulk write, and it's already been edited, then just use that structure
If we're in a bulk write, update the structure used there, and mark it as dirty
add any being built but not yet persisted or in the process of being updated
if we've specified a filter by org,  make sure we've honored that filter when  integrating in-transit records
drop the assets
This method supports lazy loading, where the descendent definitions aren't loaded  until they're actually needed.  Non-lazy loading: Load all descendants by id.  Turn definitions into a map.
convert_fields gets done later in the runtime's xblock_from_json
use the course id
This may be a bit too touchy but it's hard to infer intent
collect ids and then query for those
get the blocks for each course index (s/b the root)
The supplied CourseKey is of the wrong type, so it can't possibly be stored in this modulestore.
The supplied CourseKey is of the wrong type, so it can't possibly be stored in this modulestore.
The supplied CourseKey is of the wrong type, so it can't possibly be stored in this modulestore.
The supplied UsageKey is of the wrong type, so it can't possibly be stored in this modulestore.
this error only occurs if the course does not exist
The supplied UsageKey is of the wrong type, so it can't possibly be stored in this modulestore.
The supplied courselike key is of the wrong type, so it can't possibly be stored in this modulestore.
don't expect caller to know that children are in fields
No need of these caches unless include_orphans is set to False
Found, xblock has the path to the root
The supplied locator is of the wrong type, so it can't possibly be stored in this modulestore.
Check and verify the found parent_ids are not orphans; Remove parent which has no valid path  to the course root
find alphabetically least
The supplied CourseKey is of the wrong type, so it can't possibly be stored in this modulestore.
The supplied CourseKey is of the wrong type, so it can't possibly be stored in this modulestore.
The supplied CourseKey is of the wrong type, so it can't possibly be stored in this modulestore.
The supplied locator is of the wrong type, so it can't possibly be stored in this modulestore.
The supplied CourseKey is of the wrong type, so it can't possibly be stored in this modulestore.
if this looks in cache rather than fresh fetches, then it will probably not detect  actual change b/c the descriptor and cache probably point to the same objects
find course_index entry if applicable and structures entry
copy the structure and modify the new one
reconstruct the new_item from the cache
don't version the structure as create_item handled that already.
add new block as child and update parent's version
if the parent hadn't been previously changed in this bulk transaction, indicate that it's  part of the bulk transaction
db update
don't need to update the index b/c create_item did it for this version
don't copy assets until we create the course in case something's awry
build from inside out: definition, structure, index entry  if building a wholly new structure  create new definition and structure
check metadata
if updated, rev the structure
source_version records which revision a block was copied from. In this method, we're updating  the block, so it's no longer a direct copy, and we can remove the source_version reference.
fetch and return the new item--fetching is unnecessary but a good qc step
If no parent, then nothing to inherit.
decache pending children field settings
update the index entry if appropriate
fetch and return the new item--fetching is unnecessary but a good qc step
get the destination's index, and source and destination structures.
brand new course
update the db
Update the edit info:
Update the edit_info:
Return usage locators for all the new children:
Now clone block_key to new_block_key:  Note that new_block_info now points to the same definition ID entry as source_block_info did  Inherit the Scope.settings values from 'fields' to 'defaults'
And add new_block_key to the list of new_parent_block_key's new children:
Update the children of new_parent_block_key
The supplied UsageKey is of the wrong type, so it can't possibly be stored in this modulestore.
remove the source_version reference
update index if appropriate and structures
update the index entry if appropriate
Make sure we want to delete all of the child's parents  before slating it for deletion
this is the only real delete in the system. should it do something else?
the currently passed down values take precedence over any previously cached ones  NOTE: this should show the values which all fields would have if inherited: i.e.,  not set to the locally defined value but to value set by nearest ancestor who sets it
update the inheriting w/ what should pass to children
here's where we need logic for looking up in other structures when we allow cross pointers  but it's also getting this during course creation if creating top down w/ children set or  migration where the old mongo published had pointers to privates
Assets should be pre-sorted, so add them efficiently without sorting.  extend() will raise a ValueError if the passed-in list is not sorted.
update index if appropriate and structures
update the index entry if appropriate
Determine course key to use in bulk operation. Use the first asset assuming that  all assets will be for the same course.
update index if appropriate and structures
update the index entry if appropriate
Form an AssetMetadata.
Generate a Mongo doc from the metadata and update the course asset info.
update index if appropriate and structures
update the index entry if appropriate
update the index entry if appropriate
if this was taken from cache, then its fields are already converted
explicitly_set_fields_by_scope converts to json; so, avoiding it  the existing partition_fields_by_scope works on a dict not an xblock
perhaps replace by fixing the views or Field Reference*.from_json to return a Key
Extend the block's new edit_info with any extra edit_info fields from the source (e.g. original_usage):
If the block we are copying from was itself a copy, then just  reference the original source, rather than the copy.
any other value is hopefully only cloning or doing something which doesn't want this value add
version_agnostic b/c of above assumption in docstring
Publish both the child and the parent, if the child is a direct-only category
Libraries don't yet have draft/publish support:
check if the draft has changed since the published was created
check the children in the draft
create a new versioned draft structure
remove the block and its descendants from the new structure
This is a no-op in Split since a draft version of the data always remains
There is no published version xblock container, e.g. Library
hardcode course root block id
do the import
pylint: disable=protected-access
Always log cache misses, because they are unexpected
1 = Fastest (slightly larger results)
Stuctures are immutable, so we set a timeout of "never"
Set a write concern of 1, which makes writes complete successfully to the primary  only before returning. Also makes pymongo report write errors.
Always log cache misses, because they are unexpected
last_update not only tells us when this course was last updated but also helps  prevent collisions
set course_id attribute to avoid problems with subsystems that expect  it here. (grading, for example)
usage_key is either a UsageKey or just the block_key. if a usage_key,
trust the passed in key to know the caller's expectations of which fields are filled in.  particularly useful for strip_keys so may go away when we're version aware
look in cache
deeper than initial descendant fetch or doesn't exist
most recent retrieval is most likely the right one for next caller (see comment above fn)
If no usage id is provided, generate an in-memory id
If no definition id is provide, generate an in-memory id
Construct the Block Usage Locator:
for the situation if block_data has no asides attribute  (in case it was taken from memcache)
decache any pending field settings
If this is an in-memory block, store it in this system
pylint: disable=protected-access
a LocalId indicates that this block hasn't been persisted yet, and is instead stored  in-memory in the local_modules dictionary.
id is a BlockUsageLocator, def_id is the definition's guid
deepcopy so that manipulations of fields does not pollute the source
a decorator function for field values (to be called when a field is accessed)
load the definition to see if it has the aside_fields
load the field, if needed
return the "decorated" field value
handle any special cases
set the field
handle any special cases
delete the field value
handle any special cases
it's not clear whether inherited values should return True. Right now they don't  if someone changes it so that they do, then change any tests of field.name in xx._field_data
If not, try inheriting from a parent, then use the XBlock type's normal default value:
Things w/ these categories should never be marked as version=DRAFT
cache the branch setting on a local thread to support a multi-threaded environment
first check the thread-local cache  return the default value
We remove the branch, because publishing always means copying from draft to published
create the course: set fields to explicitly_set for each scope, id_root = new_course_locator, master_branch = 'production'
create a new version for the drafts
clean up orphans in published version: in old mongo, parents pointed to the union of their published and draft  children which meant some pointers were to non-existent locations in 'direct'
this only occurs if the parent was also awaiting adoption: skip this one, go to next  find index for module: new_parent may be missing quite a few of old_parent's children
sibling may move cursor
accumulate tuples of draft_modules and their parents in  this list:
if module has no parent, set its parent_url to `None`
ensure module has "xml_attributes" attr
Don't try to export orphaned items  and their descendents
change all of the references inside the course to use the xml expected key type w/o version & branch
Make any needed adjustments to the root node.
Process extra items-- drafts, assets, etc
Any last pass adjustments
export the static tabs
export the custom tags
export the course updates
export the 'about' data (e.g. overview, etc.)
Use url_name for split mongo because course_run is not used when loading policies.
export the grading policy
export the static assets
Create the Library.xml file, which acts as the index of all library contents.
don't change the children field but do recurse over the children
export content fields other then metadata and data in json format in current directory
Save the data in a multi-level dict - { phase1: { amount1: {ms1->ms2: duration, ...}, ...}, ...}.
Output comparison of each phase to a different table.
Add the table title and the table.
Name of the asset metadata XML schema definition file.
Characters used in name generation below.
Now - validate the XML against the XSD.
The dependency below needs to be installed manually from the development.txt file, which doesn't  get installed during unit tests!
Number of assets saved in the modulestore per test run.
Use only this course in asset metadata performance testing.
A list of courses to test - only one.
pylint: disable=invalid-name
Path where generated asset file is saved.
Path where asset XML schema definition file is located.
Use this attribute to skip this test on regular unittest CI runs.
First, make the fake asset metadata.
Use this attr to skip this test on regular unittest CI runs.
First, make the fake asset metadata.
More correct would be using the AssetManager.find() - but since the test  has created its own test modulestore, the AssetManager can't be used.
Use this attribute to skip this test on regular unittest CI runs.
First, make the fake asset metadata.
Ensure the asset collection exists.
List of names of computed fields on xmodules that are of type usage keys.  This list can be used to determine which fields need to be stripped of  extraneous usage key data when entering/exiting modulestores.
both DRAFT and PUBLISHED versions are queried, with preference to DRAFT versions
only DRAFT versions are queried and no PUBLISHED versions
only PUBLISHED versions are queried and no DRAFT versions
all revisions are queried
user ID to use for all management commands
user ID to use for primitive commands
user ID to use for tests that do not have a django user available
user ID for automatic update by the system
the relevant type of bulk_ops_record for the mixin (overriding classes should override  this variable)
Increment the number of active bulk operations (bulk operations  on the same course can be nested)
If this is the highest level bulk operation, then initialize it
If no bulk op is active, return
Send the pre-publish signal within the context of the bulk operation.  Writes performed by signal handlers will be persisted when the bulk  operation ends.
If this wasn't the outermost context, then don't close out the  bulk operation.
The bulk op has ended. However, the signal tasks below still need to use the  built-up bulk op information (if the signals trigger tasks in the same thread).  So re-nest until the signals are sent.
Signals are sent. Now unnest and clear the bulk op for good.
We remove the branch, because publishing always means copying from draft to published
For details, see caching_descriptor_system.py get_subtree_edited_by/on.
Guid for the structure which previously changed this XBlock.  (Will be the previous value of 'update_version'.)
Guid for the structure where this XBlock got its current field values.  May point to a structure not in this structure's history (e.g., to a draft  branch from which this version was published).
Datetime when this XBlock's fields last changed.  User ID which changed this XBlock last.
If this block has been copied from a library using copy_from_template,  these fields point to the original block in the library, for analytics.
Has the definition been loaded?
Contains the Scope.settings and 'children' field values.  'children' are stored as a list of (block_type, block_id) pairs.
XBlock type ID.
DB id of the record containing the content of this XBlock.
Scope.settings default values copied from a template block (used e.g. when  blocks are copied from a library to a course)
Additional field data that stored in connected XBlockAsides
EditInfo object containing all versioning/editing data.
Add new metadata sorted into the list.
Replace existing metadata.
Assets should be pre-sorted, so add them efficiently without sorting.  extend() will raise a ValueError if the passed-in list is not sorted.
Add assets of all types to the sorted list.
Add assets of a single type to the sorted list.
No limit on the results.
Flip the indices and iterate backwards.
Lazily create a sorted list if not already created.
pylint: disable=logging-format-interpolation
If an XBlock is passed-in, just match its fields.
BlockData is an object - compare its attributes in dict form.
note isn't handling any other things in the dict other than in
note isn't handling any other things in the dict other than nin
pylint: disable=invalid-name
temporary parms to enable backward compatibility. remove once all envs migrated  allow lower level init args to pass harmlessly
default is to say yes by not raising an exception
clone a default 'about' overview module as well
copy the assets
delete the assets
Backwards compatibility for prod systems that refererence  xmodule.modulestore.mongo.DraftMongoModuleStore
return the published version if ModuleStoreEnum.RevisionOption.published_only is requested
if the item is direct-only, there can only be a published version
return the draft version (without any fallback to PUBLISHED) if DRAFT-ONLY is requested
could use a single query wildcarding revision and sorting by revision. would need to  use prefix form of to_deprecated_son  first check for a draft version  otherwise, fall back to the published version
Note: does not need to inform the bulk mechanism since after the course is deleted,  it can't calculate inheritance anyway. Nothing is there to be dirty.  delete the assets
delete all of the db records for the course
check to see if the source course is actually there
clone the assets
repoint children
create a query to find all items in the course that have the given location listed as a child
find all the items that satisfy the query
filters out items that are not already in draft_items
return the new draft item (does another fetch)  get_item will wrap_draft so don't call it here (otherwise, it would override the is_draft attribute)
verify input conditions: can only convert to draft branch; so, verify that's the setting
ensure we are not creating a DRAFT of an item that is direct-only
delete the old PUBLISHED version if requested
convert the subtree using the original item as the root
ignore any descendants which are already draft
ignore the exception only if allow_not_found is True and  the item that wasn't found is the one that was passed in  we make this extra location check so we do not hide errors when converting any children to draft
single parent can have 2 versions: draft and published  get draft parents only while deleting draft module
recompute (and update) the metadata inheritance tree which is cached
handle child does not exist w/o killing publish
publish the children first
ignore noop attempt to publish something that can't be or isn't currently draft
try to find the originally PUBLISHED version, if it exists
update the published (not draft) item (ignoring that item is "draft"). The published  may not exist; (if original_published is None); so, allow_not_found
verify input conditions
ensure we are not creating a DRAFT of an item that is direct-only
first get non-draft in a round-trip
now we have to go through all drafts and replace the non-draft  with the draft. This is because the semantics of the DraftStore is to  always return the draft - if available
does non-draft exist in the collection  if so, replace it
convert the dict - which is used for look ups - back into a list
sort order that returns DRAFT items first
sort order that returns PUBLISHED items first
at module level, cache one instance of OSFS per filesystem root.
cdodge: other Systems have a course_id attribute defined. To keep things consistent, let's  define an attribute here as well, even though it's None
load the module and apply the inherited metadata
try looking it up just-in-time (but not if we're working with a detached block).
parent container pointers don't differentiate between draft and non-draft  so when we do the lookup, we should do so with a non-draft location
Convert the serialized fields values in self.cached_metadata  to python values
decache any computed pending field settings
"old" mongo does support asides yet
don't allow wildcards on revision, since public is set as None, so  its ambiguous between None as a real value versus None=wildcard
ensure it starts clean
If no name is specified for the asset metadata collection, this name is used.
Set a write concern of 1, which makes writes complete successfully to the primary  only before returning. Also makes pymongo report write errors.
Collection which stores asset metadata.
drop the assets
just get the inheritable metadata since that is all we need for the computation  this minimizes both data pushed over the wire
call out to the DB
it's ok to keep these as deprecated strings b/c the overall cache is indexed by course_key and this  is a dictionary relative to that course
now go through the results and order them by the location url  manually pick it apart b/c the db has tag and we want as_published revision regardless
now traverse the tree and compute down the inherited metadata
if not in subsystem, or we are on force refresh, then we have to compute
now write out computed tree to caching subsystem (e.g. memcached), if available
below is done for side effects when runtime is None
Load all children by id. See  http://www.mongodb.org/display/DOCS/Advanced+QueriesAdvancedQueries-%24or  for or-query syntax
If depth is None, then we just recurse until we hit all the descendents
if we are loading a course object, if we're not prefetching children (depth != 0) then don't  bother with the metadata inheritance
create any other necessary things as a side effect
@Cale, should this use LocalId like we do in split?
We're loading a descriptor, so student_id is meaningless  We also don't have separate notions of definition and usage ids yet,  so we use the location for both.
decache any pending field settings from init
attach to parent if given
See http://www.mongodb.org/display/DOCS/Updating for  atomic update syntax
update the edit info of the instantiated xblock
recompute (and update) the metadata inheritance tree which is cached  fire signal that we've written to DB
get bulk_record once rather than for each iteration
create a query with tag, org, course, and the children field set to the given location
if only looking for the PUBLISHED parent, set the revision in the query to None
query the collection, sorting by DRAFT first  no parents were found
no actual parent found
should never have multiple PUBLISHED parents
return the single PUBLISHED parent
there could be 2 different parents if    (1) the draft item was moved or    (2) the parent itself has 2 versions: DRAFT and PUBLISHED   if there are multiple parents with version PUBLISHED then choose from non-orphan parents
since we sorted by SORT_REVISION_FAVOR_DRAFT, the 0'th parent is the one we want
don't disclose revision outside modulestore
It would be nice to change this method to return UsageKeys instead of the deprecated string.
the course's run == its name. It's the only xblock for which that's necessarily true.
This record is in the old course assets format.  Ensure that no data exists before updating the format.  Update the format to a dict.
Pass back wrapped 'assets' dict with the '_id' key added to it for document update purposes.
Build an update set with potentially multiple embedded fields.
Update the document.
Update the document.
Form an AssetMetadata.
Generate a Mongo doc from the metadata and update the course asset info.
Update the document.
Using the course_id, find the course asset metadata document.  A single document exists per course to store the course asset metadata.  When deleting asset metadata, if a course's asset metadata is not present, no big deal.
Because we often scan for all category='course' regardless of the value of the other fields:
Because lms calls get_parent_locations frequently (for path generation):
To allow prioritizing draft vs published material
Some overrides that still need to be implemented by subclasses
pylint: disable=unused-import
OS X "companion files". See  http://www.diigo.com/annotated/0c936fda5da4aa1159c189cea227e174  Not a 'hidden file', then re-raise exception
strip away leading path from the name
Check extracted contentType in list of all valid mimetypes
first let's save a thumbnail so we can get back a thumbnail location
then commit the content
store the remapping information which will be needed  to subsitute in the module data
If we're going to remap the ID, then we can only do that with  a single target
first pass to find everything in /static/
Construct the asset key.
Now add all asset metadata to the modulestore.
Quick scan to get course module as we need some info from there.  Also we need to make sure that the course module is committed  first into the store
for old-style xblock where this was actually linked to kvs
tolerate same child occurring under 2 parents such as in  ContentStoreTest.test_image_import
This bulk operation wraps all the operations to populate the published branch.  Retrieve the course itself.
Import all static pieces.
Import asset metadata stored in XML.
Import all children
STEP 1: find and import course module
Note that dest_course_id will be in the format for the default modulestore.
store.has_course will return the course_key in the format for the modulestore in which it was found.  This may be different from dest_course_id, so correct to the format found.
The branch setting of published_only forces an overwrite of all draft modules  during the course import.
Importing the drafts potentially triggered a new structure version.  If so, the HEAD version_guid of the passed-in courselike will be out-of-date.  Fetch the course to return the most recent course version.
remove any export/import only xml_attributes  which are used to wire together draft imports
we want to convert all 'non-portable' links in the module_data  (if it is a string) to portable strings (e.g. /static/)
create a new 'System' object which will manage the importing
IMPORTANT: Be sure to update the module location in the NEW namespace  Update the module's location to DRAFT revision  We need to call this method (instead of updating the location directly)  to ensure that pure XBlock field data is updated correctly.
make sure our parent has us in its list of children  this is to make sure private only modules show up  in the list of children since they would have been  filtered out from the non-draft store export.
IMPORTANT: Be sure to update the parent in the NEW namespace
Sort drafts by `index_in_children_list` attribute.
everything is allowed
handle deprecated old attr
translate obsolete attr
now cache it on module where it's expected
get all modules of parent_category
check all data source path information
Adding the course_id as passed in for later reference rather than  having to recombine the org/course/url_name
tags that really need unique names--they store (or should store) state.
We're about to re-hash, in case something changed, so get rid of the tag_ and hash  append the hash of the content--the first 12 bytes should be plenty.
Fallback if there was nothing we could use:  Don't log a warning--we don't need this in the log.  Do  put it in the error tracker--content folks need to see it.
Normally, we don't want lots of exception traces in our logs from common  content problems.  But if you're debugging the xml loading code itself,  uncomment the next line.  exc_info=True
parent is alphabetically least
After setting up the descriptor, save any changes that we have  made to attributes on the descriptor to the underlying KeyValueStore.
id_generator is ignored, because each ImportSystem is already local to  a course, and has it's own id_generator already in place
All field data will be stored in an inheriting field data.
Special-case code here, since we don't have a location for the  course before it loads.  So, make a tracker to track load-time errors, then put in the right  place after the course loads and we have its location
Didn't load course.  Instead, save the errors elsewhere.
Parent XML should be something like 'library.xml' or 'course.xml'
VS[compat]: remove once courses use the policy dirs.
VS[compat] : 'name' is deprecated, but support it for now...
If we fail to load the course, then skip the rest of the loading steps
NOTE: The descriptors end up loading somewhat bottom up, which  breaks metadata inheritance via get_children().  Instead  (actually, in addition to, for now), we do a final inheritance pass  after we have the course descriptor.
now import all pieces of course_info which is expected to be stored  in <content_dir>/info or <content_dir>/info/<url_name>
now import all static tabs which are expected to be stored in  in <content_dir>/tabs or <content_dir>/tabs/<url_name>
Have to use SlashSeparatedCourseKey here because it makes sure the same format is  always used, preventing duplicate keys.
then look in a override folder based on the course run
ignore this exception  only new exported courses which use content fields other than 'metadata' and 'data'  will have this file '{dirname}.{field_name}.json'
We're loading a descriptor, so student_id is meaningless  We also don't have separate notions of definition and usage ids yet,  so we use the location for both
VS[compat]:  Hack because we need to pull in the 'display_name' for static tabs (because we need to edit them)  from the course policy
Support for passing a list as the name qualifier
here just to quell the abstractmethod. someone could write the impl if needed
return ModuleStoreEnum.Type.xml
if set, invalidate '_unwrapped_field_data' so it will be reset  the next time it will be called  pylint: disable=protected-access
here just to quell the abstractmethod. someone could write the impl if needed
This configuration must be executed BEFORE any additional Django imports. Otherwise, the imports may fail due to  Django not being configured properly. This mostly applies to tests.
We may not always have the request_cache module available
We also may not always have the current request user (crum) module available
A singleton instance of the Mixed Modulestore
Fall back to the default Django translator if the XBlock translator is not found.
get mapping information which is defined in configurations
compare hostname against the regex expressions set of mappings which will tell us which branch to use
leaving this in code structured in closure-friendly format b/c we might eventually cache this (again)  using request_cache
To keep track of where we came from, the work queue has  tuples (location, path-so-far).  To avoid lots of  copying, the path-so-far is stored as a lisp-style  list--nested hd::tl tuples, and flattened at the end.
get_parent_location raises ItemNotFoundError if location isn't found
print 'Processing loc={0}, path={1}'.format(next_usage, path)  Found it!  Orphaned item.
otherwise, add parent locations at the end
pull out the location names  Figure out the position
Load the course, but don't make error modules.  This will succeed,  but will record the errors.
Look up the errors during load. There should be none.
now set toy course to share the wiki with simple course
XML store allows published_only branch setting
XML store does NOT allow draft_preferred branch setting  verify that the above context manager raises a ValueError
ensure it's still a child of the other parent even tho it doesn't claim the other parent as its parent  children rather than get_children b/c the instance returned by get_children != shared_item
These tests won't work with courses, since they're creating blocks inside courses
Verify course summaries
Verify that all course summary objects have the required attributes.
Verify fetched accessible courses list is a list of CourseSummery instances
N.B. This block is being left as an orphan in old-mongo. This test will  fail when that is fixed. At that time, this condition should just be removed,  as SplitMongo and OldMongo will have the same semantics.
pass a copy of the old setting since the migration modifies the given setting
check whether the configuration is encapsulated within Mixed.
check whether the stores are in an ordered list
exclude split when comparing old and new, since split was added as part of the migration
compare each store configured in mixed
make sure there is no migration done on an already updated config
define attrs which get set in initdb to quell pylint
create chapter
try an unknown mapping, it should be the 'default' store
unset mappings
try negative cases
verify that an error is raised when the revision is not valid
try negative cases
verify that an error is raised when the revision is not valid
verify that an error is raised when the revision is not valid
Check that orphans are not found
Add an orphan to test course
Check that now an orphan is found
Check now `get_items` retrieves an extra item added above which is an orphan.
Check now `get_items` with `include_orphans` kwarg does not retrieves an orphan block.
Create dummy direct only xblocks
Check that neither xblock has changes
Create a dummy component to test against
Not yet published, so changes are present
Publish and verify that there are no unpublished changes
Change the component, then check that there now are changes
Publish and verify again
Create a dummy component to test against
Not yet published, so changes are present
Publish and verify that there are no unpublished changes
Publish and verify again
Create a dummy component to test against
Not yet published, so changes are present
Publish and verify that there are no unpublished changes
Discard changes and verify that there are no changes
Change the component, then check that there now are changes
Verify that changes are present
publish vertical changes
Discard changes and verify that there are no changes
Delete the component and verify that the unit has changes
publish sequential changes
delete vertical and check sequential has no changes
Publish the vertical units
Verify that there are no unpublished changes
Change the child
Publish the unit with changes
Verify that there are no unpublished changes
Verify that there are no unpublished changes
Verify that ancestors have changes
Publish one child
Verify that ancestors still have changes
Publish the other child
Verify that ancestors now have no changes
Test that the ancestors don't have changes
Create a new child and attach it to parent
Verify that the ancestors now have changes
Verify that ancestors now have no changes
Verify that there are no changes
Change the child
Verify that both parent and child have changes
Check the parent for changes should return True and not throw an exception
verify it's gone  verify it's gone from published too
verify that an error is raised when the revision is not valid
create a static tab of the course
now check that the course has same number of children
Now load with get_library and make sure it works:
Clear the mappings so we can test get_library code path without mapping set:
publish the course
make drafts of verticals
move child problem_x1a_1 to vertical_y1a
publish the course
make draft of vertical
delete child problem_y1a_1
Note: The following could be an unexpected result, but we want to avoid an extra database call
create parented children
add another parent (unit) "vertical_x1b" for problem "problem_x1a_1"
convert first parent (unit) "vertical_x1a" of problem "problem_x1a_1" to draft
now problem "problem_x1a_1" has 3 parents [vertical_x1a (draft),  vertical_x1a (published), vertical_x1b (published)]  check that "get_parent_location" method of draft branch returns first  published parent "vertical_x1a" without raising "AssertionError" for  problem location revision
each iteration has different find count, pop this iter's find count
Orphaned items should not be found.
delete leaf problem (will make parent vertical a draft)
Change display name of problem and update just it (so parent remains published)
It does not discard the child vertical, even though that child is a draft (with no published version)
create parented children
detached items (not considered as orphans)
create parented children
Test Mongo wiki
unpublish
make sure draft version still exists
Private -> Public
Public -> Private
Private -> Public
Public -> Draft with NO changes
Verify that all nodes were last edited in the past by create_user
Change the component, then check that there now are changes
but child didn't change
Change the child
Verify that child was last edited between after_create and after_edit by edit_user
Verify that ancestors edit info is unchanged, but their subtree edit info matches child
Verify that others have unchanged edit info
Create a dummy component to test against
Store the current edit time and verify that user created the component
Change the component
Verify the ordering of edit times and that dummy_user made the edit
Create a dummy component to test against
Store the current time, then publish
Verify the time order and that publish_user caused publication
test create_course to make sure we are autopublishing
test update_item of direct-only category to make sure we are autopublishing
test create_child of NOT direct-only category to make sure we aren't autopublishing
test create_item of NOT direct-only category to make sure we aren't autopublishing
test update_item of NOT direct-only category to make sure we aren't autopublishing
verify initial state - initially, we should have a wiki for the Mongo course
set Mongo course to share the wiki with simple course
now mongo_course should not be retrievable with old wiki_slug
check the display_name of the problem
there should be only 1 problem with the expected_display_name
verify Draft problem
PUBLISH the problem
verify Published problem
verify Draft-preferred
EDIT name
verify Draft problem has new name
verify Published problem still has old name  there should be no published problems with the new name
PUBLISH the problem
verify Published problem has new name  there should be no published problems with the old name
initialize the mixed modulestore
initialize the mixed modulestore
initialize the mixed modulestore
initialize the mixed modulestore
pylint: disable=protected-access
Course creation and publication should fire the signal
Course creation and publication should fire the signal
Course creation and publication should fire the signal
Test a draftable block type, which needs to be explicitly published, and nest it within the  normal structure - this is important because some implementors change the parent when adding a  non-published child; if parent is in DIRECT_ONLY_CATEGORIES then this should not fire the event
'units' and 'blocks' are draftable types
Course creation and publication should fire the signal
Course creation and publication should fire the signal
'units' and 'blocks' are draftable types
Create a course
Delete the course
Verify that the signal was emitted
No orphans in course
No published oprhans after delete, except  in old mongo, which still creates orphans
No orphans in course
No published orphans after delete, except  in old mongo, which still creates them
Verify that the imported block still is a draft, i.e. has changes.
Retrieve the published block and make sure it's published.
Get the published xblock from the imported course.  Verify that it still is published, i.e. has no changes.
Retrieve the published block and make sure it's published.
Get the published xblock from the imported course.  Verify that the published block still has a draft block, i.e. has changes.
Verify that the changes in the draft vertical still exist.
create sequential
create vertical - don't publish it!
Get the published xblock from the imported course.  Verify that the published block still has a draft block, i.e. has changes.
create sequential
Export the course - then import the course export.
Verify that the changes in the draft unit still exist.
create sequential
Export the course - then import the course export.
Verify that the published changes exist in the published unit.
create a new block and ensure its aside magically appears with the right fields
now update the values
update the values the second time
export course to xml
and restore the new one from the exported xml
export course to xml
and restore the new one from the exported xml
check that aside for the new chapter was exported/imported properly
the first aside item
the second aside item
create new item with two asides
initialize the mixed modulestore
after clone get connected aside and check that it was cloned correctly
remove item
create item again
check that aside has default values
Private -> Public
Public -> Private
We have to give a model for Factory.  However, the class that we create is actually determined by the category  specified in the factory
Pass the metadata just as field=value pairs
This error is raised if the caller hasn't provided either parent or parent_location  In this case, we'll just return the default parent_location
This code was based off that in cms/djangoapps/contentstore/views.py
replace the display name with an optional parameter passed in from the caller
pylint: disable=missing-docstring
verify the counter actually worked by ensuring we have counted greater than (or equal to) the minimum calls
now verify the number of actual calls is less than (or equal to) the expected maximum
Snippet of what would be in the django settings envs file
split requires the course to be created separately from creating items
Inherit the vertical and the problem from the library into the course:
Check that when capa modules are copied, their "markdown" fields (Scope.settings) are removed.  (See note in split.py:copy_from_template())
Override the display_name and weight:
Test that "Any previously existing children of `dest_usage`  that haven't been replaced/updated by this copy_from_template operation will be deleted."
Reload source_course since we need its branch and version to use copy_from_template:
Check that the auto-publish blocks have been published:  We can't use has_changes because it includes descendants
add direct children
Don't save assets 5 and 6.
Find existing asset metadata.
Find existing asset metadata.
Find asset metadata from non-existent course.
pylint: disable=bad-continuation
pylint: disable=bad-continuation
Save 'em.
pylint: disable=bad-continuation
Save 'em.
don't create django dependency; so, duplicates common.py in envs
drop the modulestore to force re init
Should have gotten 3 draft courses.
should have gotten 1 draft courses
should have gotten 2 draft courses
although this is already covered in other tests, let's  also not pass in org= parameter to make sure we get back  3 courses
check dates and graders--forces loading of descriptor
check dates and graders--forces loading of descriptor
first and second problem may show as same usage_id; so, need to ensure their histories are right
use the default cache, since the `course_structure_cache`  is a dummy cache during testing
make sure we clear the cache before every test...  ... and after
make a new course:
force get_cache to return the default cache so we can test  its caching behavior
when cache is warmed, we should have one fewer mongo call
now make sure that you get the same structure
if the cache isn't configured, we expect to have to make  another mongo call here if we want the same course structure
now make sure that you get the same structure
Since the test is using the dummy cache, it's not actually caching  anything
now make sure that you get the same structure
positive tests of various forms
not a course obj
check dates and graders--forces loading of descriptor
try to look up other branches
check that course version changed and course's previous is the other one
ensure trying to continue the old one gives exception
reorder children
now begin the test
delete a subtree  check subtree
Clean up the data so we don't break other tests which apparently expect a particular state
create 3 courses before bulk operation
now get_courses
unset on parent, retrieve child, verify unset
pylint: disable=star-args
pylint: disable=unused-argument, missing-docstring
used to create course subtrees in ModuleStoreTestCase.create_test_course  adds to self properties w/ the given block_id which hold the UsageKey for easy retrieval.  fields is a dictionary of keys and values. sub_tree is a collection of BlockInfo
pylint: disable=star-args
pylint: disable=unused-argument
Set the XBlock's location
Explicitly set the content and settings fields
Check the XBlock's location
Check the values of the fields.  The content and settings fields should be preserved
Expect that these fields are marked explicitly set
Set the XBlock's location
Do NOT set any values, so the fields should use the defaults
Check the values of the fields.  The content and settings fields should be the default values
The fields should NOT appear in the explicitly set fields
Set the XBlock's location
Inherited fields should NOT be explicitly set
Set the XBlock's location
Update location
Check the XBlock's location
Expect these fields pass "is_set_on" test
Explicitly list the courses to load (don't want the big one)
connect to the db
also test a course with no importing of static content
also import a course under a different course_id (especially ORG)
Destroy the test db.
now toy_course should not be retrievable with old wiki_slug
This will raise if the course image is missing
Retrieve the block and verify its fields
Clean up the data so we don't break other tests which apparently expect a particular state
Confirm that no specified asset collection name means empty asset metadata.
Confirm that invalid course key raises ItemNotFoundError
pylint: disable=W0613
don't use these 2 class vars as they restore behavior once the tests are done
since MongoModuleStore and MongoContentStore are basically assumed to be together, create this class  as well
ensure deleting a non-existent file is a noop
ensure it didn't remove any from other course
check that we return the expected urls
Writing a definition when no bulk operation is active should just  call through to the db_connection.
Writing a course index when no bulk operation is active should just call  through to the db_connection
Calling _end_bulk_operation without a corresponding _begin...  is a noop
If no definitions to get, then get_definitions() should *not* have been called.
An extra import write occurs in the first Split import due to the mismatch between  the course id and the wiki_slug in the test XML course. The course must be updated  with the correct wiki_slug during import.
First, import a course.
Read the fields on each block in order to ensure each block and its definition is loaded.
should find the course with exact locator
replace value for one of the keys  add a character at the end  add a character in the beginning
Set up a temp directory for storing filesystem content created during import
Delete the created directory on the filesystem
Set up a temp directory for storing filesystem content created during import
Delete the created directory on the filesystem
Make the modulestore creation function just return the already-created modulestores
Generate a fake list of stores to give the already generated stores appropriate names
Mongo modulestore beneath mixed.  Returns the entire collection with *all* courses' asset metadata.
Split modulestore beneath mixed.  Split stores all asset metadata in the structure collection.
VersioningModulestoreBuilder(),   FUTUREDO: LMS-11227
There are 12 created items and 7 parent updates  create course: finds: 1 to verify uniqueness, 1 to find parents  sends: 1 to create course, 1 to create overview
verify status  however, children are still draft, but I'm not sure that's by design
delete the draft version of the discussion
Create a list of all verticals for convenience.
Create a list of all html units for convenience.
For convenience, maintain a list of (block_type, block_id) pairs for all verticals/units.
Course block database is keyed on (block_type, block_id) pairs.  It's built during the course creation below and contains all the parent/child  data needed to check the OLX.
Form the checked attributes based on the block type.
Draft items are expected to have certain XML attributes.
If children exist, construct regular expressions to check them.  Grab the type of the first child as the type of all the children.  Construct regex out of all the child_ids that are included.
Construct the contentstore for storing the first import  Construct the modulestore for storing the first import (using the previously created contentstore)  Create the course.
Export the course.
MODULESTORE_DIFFERENCE:  In old Mongo, you can successfully publish an item whose parent  isn't published.
MODULESTORE_DIFFERENCE:  In Split, you cannot publish an item whose parents are unpublished.  Split will raise an exception when the item's parent(s) aren't found  in the published branch.
Ensure that both groups of verticals and children are drafts in the exported OLX.
Publish both vertical03 and vertical 04.
Ensure that the published verticals and children are indeed published in the exported OLX.  Ensure that the untouched vertical and children are still untouched.
The unit is a draft.  Since there's no published version, attempting an unpublish throws an exception.
The vertical is a draft.  Since there's no published version, attempting an unpublish throws an exception.
MODULESTORE_DIFFERENCE: This first line is different between old Mongo and Split for verticals.  Old Mongo deletes the draft vertical even when published_only is specified.
MODULESTORE_DIFFERENCE: This first line is different between old Mongo and Split for verticals.  Split does not delete the draft vertical when a published_only revision is specified.
Sequentials are auto-published.
Chapters are auto-published.
At first, no vertical is published.  Now, without publishing anything first, revert the same vertical to published.  Since no published version exists, an exception is raised.
At first, no vertical is published.  Then publish a vertical.  The vertical will be published.  Now, revert the same vertical to published.  Basically a no-op - there was no draft version to revert.
At first, no vertical is published.  Then publish a vertical.  The vertical will be published.
The vertical now has a draft -and- published version.  Now, revert the same vertical to published.  The draft version is now gone.
allow for additional options that can be keyed on a name, e.g. 'trashcan'
Do not create the modulestore if it does not exist.
All store requests now go through mixed  Use this modulestore if you specifically want to test mongo and not a mocked modulestore.
All store requests now go through mixed  Use this modulestore if you specifically want to test split-mongo and not a mocked modulestore.
Tell Django to clean out all databases, not just default
Now yield to allow the test class to run its setUpClass() setup code.  Now call the base class, which calls back into the test class's setUpTestData().
OverrideFieldData.provider_classes is always reset to `None` so  that they're recalculated for every test
Tell Django to clean out all databases, not just default
When testing CCX, we should make sure that  OverrideFieldData.provider_classes is always reset to `None` so  that they're recalculated for every test
Create the user so we can log them in.
Staff has access to view all courses
copy the old configurations into the new settings
Convert from dict, if needed
convert old-style (unordered) dict to (an ordered) list
move the found store to the top of the list
it'd be useful to add init args such as support_deprecated, force_deprecated
don't call super as base.BaseField.to_mongo calls to_python() for some odd reason
remove version and branch, by default
call the decorated function
strip the return value
replace all named pointers to the store into actual pointers
return the default store
Check if course is indeed unique. Save it in result if unique
filter out ones which were fetched from earlier stores but locations may not be ==  course is indeed unique. save it in result
filter out ones which were fetched from earlier stores but locations may not be ==  library is indeed unique. save it in result
If there is a mapping that match this org/course/run, use that
Otherwise, return the key created by the default store
Courses in the same modulestore can be handled by the modulestore itself.
first make sure an existing course doesn't already exist in the mapping
create the course
add new course to the mapping
first make sure an existing course/lib doesn't already exist in the mapping
create the library
add new library to the mapping
for a temporary period of time, we may want to hardcode dest_modulestore as split if there's a split  to have only course re-runs go to split. This code, however, uses the config'd priority
the super handles assets and any other necessities
drop database if the store supports it (read-only stores do not)
could be done in parallel threads if needed
return the thread-local cache, if found
else return the default store
read in and convert to XML
insert  new XML into tree in place of include
Log error  tell the tracker
Prepend _ so that sass just includes the files into a single file
assume all XML files are persisted as utf-8.
Support older serialized version.
Extension to append to filename paths
VS[compat].  Backwards compatibility code that can go away after  importing 2012 courses.  A set of metadata key conversions that we want to make
VS[compat] -- remove the below attrs once everything is in the CMS  Used for storing xml attributes between import and export, for roundtrips
Add info about where we are, but keep the traceback
Add the attributes from the pointer node
VS[compat].  Remove after all key translations done
don't load these
Store unknown attributes coming from policy.json  in such a way that they will export to xml unchanged
Note: removes metadata.
VS[compat] -- make Ike's github preview links work in both old and  new file layouts  new style -- contents actually at filepath
Set/override any metadata specified by policy
We're loading a descriptor, so student_id is meaningless
Set the tag on both nodes so we get the file path right.
Special case for course pointers:  add org and course attributes on the pointer tag
Shim from from_xml to the parse_xml defined in XmlParserMixin.  This only exists to satisfy subclasses that both:     a) define from_xml themselves     b) call super(..).from_xml(..)
Shim from export_to_xml to the add_xml_to_node defined in XmlParserMixin.  This only exists to satisfy subclasses that both:     a) define export_to_xml themselves     b) call super(..).export_to_xml(..)
Type for assets uploaded by a course author in Studio.
Asset section XML tag for asset metadata as XML.
Individual asset XML tag for asset metadata as XML.
Top-level directory name in exported course XML which holds asset metadata.
Filename of all asset metadata exported as XML.
created_by, created_by_email, and created_on should only be set here.
An AssetLocator is constructed separately from these parts.
Boolean.
None.
ISO datetime.
Integer representing user id.
Dictionary.
Get the value.
If this line does *not* raise, the XML is valid.
Map  key: <tag attribute in xml>  value: <name of module attribute>
if problem is full points
We don't throw an exception here because it is possible for  the descriptor of a required module to have a property but  for the resulting module to be a (flavor of) ErrorModule.  So just log and return false.
Calculate html ids of dependencies
HACK: This shouldn't be hard-coded to two types  OBSOLETE: This obsoletes 'type'
Overwrite the original sources attribute with the value from sources_list, as  Locations may have been changed to Locators.
We don't want to force a dependency on datadog, so make the import conditional
Make '_' a no-op so we can scrape strings. Using lambda instead of   `django.utils.translation.ugettext_noop` because Django cannot be imported in this file
Generate this many different variants of problems with rerandomize=per_student  Never produce more than this many different seeds, no matter what.
get the first few digits of the hash, convert to an int, then mod.
it'd be nice to have a useful default but it screws up other things; so,  use display_name_with_default for those
Need the problem location in openendedresponse to send out.  Adding  it to the system here seems like the least clunky way to get it  there.
see comment on randomization_bin
So that sandboxed code execution can be cached, but still have an interesting  number of possibilities, cap the number of different random seeds.
Progress objects expect total > 0
scale score and total by weight/total:
The logic flow is a little odd so that _('xxx') strings can be found for  translation while also running _() just once for each string.
Apply customizations if present
Apply customizations if present
If the problem is closed (past due / too many attempts)  then we do NOT show the "check" button  Also, do not show the "check" button if we're waiting  for the user to reset a randomized problem
If the problem is closed (and not a survey question with max_attempts==0),  then do NOT show the reset button.
Button only shows up for randomized problems if the question has been submitted  Do NOT show the button if the problem is correct
If the user has forced the save button to display,  then show it as long as the problem is not closed  (past due / too many attempts)
If the problem is closed (and not a survey question with max_attempts==0),  then do NOT show the save button  If we're waiting for the user to reset a randomized problem  then do NOT show the save button
Presumably, student submission has corrupted LoncapaProblem HTML.    First, pull down all student answers
Next, generate a fresh LoncapaProblem
Translators: Following this message, there will be a bulleted list of items.
Couldn't do it. Give up.
Translators: e.g. "Hint 1 of 3" meaning we are showing the first of three hints.
We report the index of this hint, the client works out what index to use to get the next hint
The convention is to pass the name of the check button if we want  to show a check button, and False otherwise This works because  non-empty strings evaluate to True.  We use the same convention  for the "checking" state text.
If demand hints are available, emit hint button and div.
Some of these tags span multiple lines  Note: could probably speed this up by calling sub() once with a big regex  vs. simply calling sub() many times as we have here.
used by conditional module
This is after the 'never' check because admins can see the answer  unless the problem explicitly prevents it
NOTE: this is slightly different from 'attempted' -- resetting the problems  makes lcp.done False, but leaves attempts unchanged.
pass along the xqueue message to the problem
save any state changes that may occur
If key has no underscores, then partition  will return (key, '', '')  We detect this and raise an error
If the submission wasn't deserializable, raise an error.
If the name already exists, then we don't want  to override it.  Raise an error instead
Can override current time
Problem queued. Students must wait a specified waittime before they are allowed to submit  IDEA: consider stealing code from below: pretty-print of seconds, cueing of time remaining
Save the user's state before failing
If the user is a staff member, include  the full exception, including traceback,  in the response
Otherwise, display just an error message,  without a stack trace  Translators: {msg} will be replaced with a problem's error message.
Save the user's state before failing
success = correct if ALL questions in this problem are correct
render problem into HTML
Do the unmask translates on a copy of event_info,  avoiding problems where an event_info is unmasked twice.
Look for answers/id
Add 'permutation' to event_info for permuted responses.
Add permutation record tuple: (one of:'shuffle'/'answerpool', [as-displayed list])
NOTE: The above process requires deep inspection of capa structures that may break for some  uncommon problem types.  Ensure that it does not prevent answer submission in those  cases.  Any occurrences of errors in this block should be investigated and resolved.
Translators: 'rescoring' refers to the act of re-submitting a student's solution so it can get a new score.
get old score, for comparison:
rescoring should have no effect on attempts, so don't  need to increment here, or mark done.  Just save.
success = correct if ALL questions in this problem are correct
NOTE: We are logging both full grading and queued-grading submissions. In the latter,        'success' will always be incorrect
Translators: 'closed' means the problem's due date has passed. You may no longer attempt to solve the problem.
Translators: A student must "make an attempt" to solve the problem on the page before they can reset it.
Reset random number generator seed.
Generate a new problem with either the previous seed or a new seed
Pull in the new problem seed
Make '_' a no-op so we can scrape strings. Using lambda instead of   `django.utils.translation.ugettext_noop` because Django cannot be imported in this file
parsing custom parameters to dict
LTI specs: 'custom_' should be prepended before each custom parameter, as pointed in link above.
Parameters required for grading:
Appending custom parameter for signing.
This is needed for body encoding:
Parse headers to pass to template as part of context:
oauthlib encodes signature with  'Content-Type': 'application/x-www-form-urlencoded'  so '='' becomes '%3D'.  We send form via browser, so browser will encode it again,  So we need to decode signature back:
Add LTI parameters to OAuth parameters for sending in form.
Raise exception if score is not float or not in range 0.0-1.0 regarding spec.
NOTE: calling self.get_children() doesn't work until we've picked a choice
Oops.  Children changed. Reset.
Now get_children() should return a list with one element
raise error instead?  In fact, could complain on descriptor load...
the editing interface can be the same as for sequences -- just a container
staff get to see all the details
staff get to see all the details
this string is not marked for translation because we don't have  access to the user context, and this will only be seen by staff
Save the error to display later--overrides other problems
We need to know the library's version so ensure it's set in library.location.library_key.version_guid
Apply simple filtering based on CAPA problem types:
should this move to cms since it's really only for module crud?
The capa format specifies that what we call max_attempts in the code  is the attribute `attempts`. This will do that conversion
pylint: disable=no-member
we should verify against get_outcome_service_url not  request url proxy and load balancer along the way may  change url presented to the method
-*- coding: utf-8 -*-
return anything except None to test LMS
test to make sure that role is checked in LMS
Make sure that answer for incorrect request is error json.
Make sure that ajax request works correctly.
Even though the minimum number is 3, this should grade correctly when 7 assignments are found
Test that graders can also be used instead of lists of dictionaries
construct module
Make sure the runtime knows that the block's children vary per-user:
Check how many children each user will see:  Check that get_content_titles() doesn't return titles for hidden/unused children
When source_library_id is blank, the validation summary should say this block needs to be configured:
When source_library_id references a non-existent library, we should get an error:
When source_library_id is set but the block needs to be updated, the summary should say so:
Now if we update the block, all validation should pass:
Set max_count to higher value than exists in library  In the normal studio editing process, editor_saved() calls refresh_children at this point
Add some capa problems so we can check problem type validation messages
Existing problem type should pass validation
... unless requested more blocks than exists in library
Missing problem type should always fail validation
Reload lc_block and set it up for a student:
Get the keys of each of our blocks, as they appear in the course:
Trigger a publish event:
Clear the cache (only needed because we skip saving/re-loading the block) pylint: disable=protected-access
Clear the cache (only needed because we skip saving/re-loading the block) pylint: disable=protected-access
deleted so that info can no longer be retrieved
load it
export it
Now make sure the exported xml is a sequential
pylint: disable=protected-access
Now export and check things
Does the course still have unicorns?
the course and org tags should be _only_ in the pointer
did we successfully strip the url_name from the definition contents?
Run the checks on the course node instead.
Check that the child does not inherit a value for due
Check that the child hasn't started yet
Test inherited metadata. Due does not appear here (because explicitly set on child).
pylint: disable=protected-access
Also check that the grading policy loaded
Also check that keys from policy are run through the  appropriate attribute maps -- 'graded' should be True, not 'true'
Not using get_courses because we need the modulestore object too afterward
Name should be 'video_{hash}'
No config -> False
empty config -> False
false config -> False
and finally...
Mock is_condition_satisfied
Tracking strange content repeating bug  Should appear once
Test with a Mongo course and '=' as padding.  Test with a Split course and '~' as padding.
Test course with no display name.  Test course with a display name that contains characters that need escaping.
Test course with no display name.  Test course with a display name that contains characters that need escaping.
Even though we don't care about testing mock_strftime_localized,  we still need to test it with a bad format string in order to  satisfy the coverage checker.
Location of common test DATA directory  '../../../../edx-platform/common/test/data/'
Disable XBlockAsides in most tests
Unlike XBlock Runtimes or DescriptorSystems,  each XModule is provided with a new ModuleSystem.  Construct one for the new XModule.
Use __ to not pollute the namespace of subclasses with what could be a fairly generic name.
Only wrap the first layer of assert functions by stashing away the manager  before executing the assertion.
Reconstruct the stack in which the error was thrown (so that the traceback)  isn't cut off at `assertion(*args, **kwargs)`.
Count the number of stack frames before you get to a  unittest context (walking up the stack from here).  This is the same criterion used by unittest to decide if a  stack frame is relevant to exception printing.
Run the assertion, and capture any raised assertionErrors
Handle the assertRaises family of functions by returning  a context manager that surrounds the assertRaises  with our assertion capturing context manager.
Formatting the message slows down tests of large courses significantly, so only do it if it would be used
compare fields
Children are handled specially
edited_on is updated upon import.
Policy files are json, and thus the values aren't passed through 'deserialize_field'  Therefor, the string 'null' is passed unchanged to the Float field, which will trigger  a ValueError
Extract all argument names used to construct XmlImportData objects,  so that the factory doesn't treat them as XML attributes
Make sure that the xml_module doesn't try and open a file to find the contents  of this node.
Test that the string inherited fields are passed through 'deserialize_field',  which converts the string "null" to the python value None
Use super(BulkAssertionTest) to make sure we get un-adulturated assertions
construct module
If user_tag has a missing value, we should still get back a valid child url
Patch the definition_to_xml for the html children.  The HtmlDescriptor definition_to_xml tries to write to the filesystem  before returning an xml object. Patch this to just return the xml.
Mock out the process_xml  Expect it to return a child descriptor for the SplitTestDescriptor when called.
Write out the xml.
user_partition_id will always appear in editable_metadata_settings, regardless  of the selected value.
user_partitions is empty, only the "Not Selected" item will appear.
Try again with a selected partition and verify that there is no option for "No Selection"
Finally try again with an invalid selected partition and verify that "No Selection" is an option
Verify that a split test has no active children if it has no specified user partition.
Verify that a split_test referring to a non-existent user partition has no active children
converting to int here because I keep putting "0" and "1" in the tests  since everything else is a string.
in the capa grace period format, not in time delta format
default, no due date, showanswer 'closed', so problem is open, and show_answer  not visible.
can see after attempts used up, even with due date in the future
can see after due date
can't see because attempts left
Can't see because grace period hasn't expired
can see because answer is correct, even with due date in the future
can see after due date, even when answer isn't correct
can also see after due date when answer _is_ correct
Can't see because grace period hasn't expired and answer isn't correct
can't see after attempts used up, even with due date in the future
can see after due date
can't see because attempts left
Can't see because grace period hasn't expired, even though have no more  attempts.
can see after attempts used up, even with due date in the future
can see after due date
can't see because attempts left and wrong
_can_ see because attempts left and right
Can see even though grace period hasn't expired, because have no more  attempts.
Attempts < Max attempts --> NOT closed
Attempts < Max attempts --> NOT closed
Attempts = Max attempts --> closed
Attempts > Max attempts --> closed
Max attempts = 0 --> closed
Past due --> closed
If we use [] at the end of a key name, we should always  get a list, even if there's just one value
If we have no underscores in the name, then the key is invalid
Check the problem
Expect that the problem is marked correct
Expect that we get the (mocked) HTML
Expect that the number of attempts is incremented by 1
Simulate marking the input incorrect
Check the problem
Expect that the problem is marked correct
Expect that the number of attempts is incremented by 1
Expect that number of attempts NOT incremented
Randomize turned on
Simulate that the problem is completed
Expect that we cannot submit
Expect that number of attempts NOT incremented
Randomize turned off
Expect that we can submit successfully
Expect that number of attempts IS incremented
Expect an AJAX alert message in 'success'
Expect that the number of attempts is NOT incremented
Create a request dictionary for check_problem.
Try each exception that capa_module should handle
Create the module
Ensure that the user is NOT staff
Simulate answering a problem that raises the exception
Expect an AJAX alert message in 'success'
Expect that the number of attempts is NOT incremented
Create the module
Ensure that the user is NOT staff
Ensure that DEBUG is on
Simulate answering a problem that raises the exception
Expect an AJAX alert message in 'success'
Create the module
Override the problem score to have a total of zero.
Check the problem
Try each exception that capa_module should handle
Create the module
Ensure that the user is NOT staff
Simulate answering a problem that raises the exception
Expect an AJAX alert message in 'success'
Expect that the number of attempts is NOT incremented
Try each exception that capa module should handle
Create the module
Ensure that the user IS staff
Simulate answering a problem that raises an exception
Expect an AJAX alert message in 'success'
We DO include traceback information for staff users
Expect that the number of attempts is NOT incremented
Stub out HTML rendering
Reset the problem
Expect that the request was successful
Expect that the problem HTML is retrieved
Expect that the problem was reset
pre studio default
Simulate that the problem is closed
Try to reset the problem
Expect that the problem was NOT reset
Simulate that the problem is NOT done
Try to reset the problem
Expect that the problem was NOT reset
Simulate that all answers are marked correct, no matter  what the input is, by patching LoncapaResponse.evaluate_answers()
Expect that the problem is marked correct
Expect that we get no HTML
Expect that the number of attempts is not incremented
make sure it also works when attempts have been reset,  so add this to the test:
Simulate that all answers are marked incorrect, no matter  what the input is, by patching LoncapaResponse.evaluate_answers()
Expect that the problem is marked incorrect
Expect that the number of attempts is not incremented
Simulate that the problem is NOT done
Try to rescore the problem, and get exception
Try to rescore the problem, and get exception
Create the module
Simulate answering a problem that raises the exception
Expect an AJAX alert message in 'success'
Expect that the number of attempts is NOT incremented
Save the problem
Expect that answers are saved to the problem
Expect that the result is success
Simulate that the problem is closed
Try to save the problem
Expect that the result is failure
Capa XModule treats 'always' and 'true' equivalently
Try to save
Expect that we cannot save
Capa XModule treats 'false' and 'per_student' equivalently
Try to save
Expect that we succeed
If last attempt, button name changes to "Final Check"  Just in case, we also check what happens if we have  more attempts than allowed.
Otherwise, button name is "Check"
If no limit on attempts, then always show "Check"
If we're after the deadline, do NOT show check button
If user is out of attempts, do NOT show the check button
If survey question (max_attempts = 0), do NOT show the check button
If user submitted a problem but hasn't reset,  do NOT show the check button  Note:  we can only reset when rerandomize="always" or "true"
Otherwise, DO show the check button
If the user has submitted the problem  and we do NOT have a reset button, then we can show the check button  Setting rerandomize to "never" or "false" ensures that the reset button  is not shown
If we're after the deadline, do NOT show the reset button
If the user is out of attempts, do NOT show the reset button
pre studio default value, DO show the reset button
If survey question for capa (max_attempts = 0),  DO show the reset button
If the question is not correct  DO show the reset button
If the question is correct and randomization is never  DO not show the reset button
If the question is correct and randomization is always  Show the reset button
Don't show reset button if randomization is turned on and the question is not done
Show reset button if randomization is turned on and the problem is done
If we're after the deadline, do NOT show the save button
If the user is out of attempts, do NOT show the save button
If user submitted a problem but hasn't reset, do NOT show the save button
If the user has unlimited attempts and we are not randomizing,  then do NOT show a save button  because they can keep using "Check"
pre-studio default, DO show the save button
If we're not randomizing and we have limited attempts,  then we can save
If survey question for capa (max_attempts = 0),  DO show the save button
If we're after the deadline, do NOT show the save button  even though we're forcing a save
If the user is out of attempts, do NOT show the save button
Otherwise, if we force the save button,  then show it even if we would ordinarily  require a reset first
Mock the system rendering function
Patch the capa problem's HTML rendering
Render the problem HTML
Also render the problem encapsulated in a <div>
Expect that we get the rendered template back
Check the rendering context
Assert that the encapsulated html contains the original html
HTML generation is mocked out to be meaningless here, so instead we check  the context dict passed into HTML generation.
Re-mock the module_id to a fixed string, so we can check the logging
check to make sure that the input_state and the keys have the same values
Save the original problem so we can compare it later
Simulate throwing an exception when the capa problem  is asked to render itself as HTML
Stub out the get_test_system rendering function
Turn off DEBUG
Try to render the module with DEBUG turned off
Check the rendering context
Expect that the module has created a new dummy problem with the error
Simulate throwing an exception when the capa problem  is asked to render itself as HTML
Stub out the get_test_system rendering function
Make sure DEBUG is on
Try to render the module with DEBUG turned on
Check the rendering context
Get the seed  By this point, the module should have persisted the seed
If we're not rerandomizing, the seed is always set  to the same value (1)
Check the problem
Expect that the seed is the same
Save the problem
Expect that the seed is the same
Reset the problem
Return the seed
Get the seed  By this point, the module should have persisted the seed
We do NOT want the seed to reset if rerandomize  is set to 'never' -- it should still be 1  The seed also stays the same if we're randomizing  'per_student': the same student should see the same problem
Otherwise, we expect the seed to change  to another valid seed
Since there's a small chance we might get the  same seed again, give it 5 chances  to generate a different seed
Reset the problem  By default, the problem is instantiated as unsubmitted
Return the seed
Get the seed  By this point, the module should have persisted the seed
Assert that we are limiting the number of possible seeds.  Get a bunch of seeds, they should all be in 0-999.
Whitespace screws up comparisons
There are potentially 2 track logs: answers and hint. [-1]=answers.
Mock the XQueueInterface.
converting to int here because I keep putting "0" and "1" in the tests  since everything else is a string.
Could set the internal state formally, but here we just jam in the score.
Successfully submitted and answered  Also, the number of attempts should increment by 1
Prior to TNL-4115, an exception would be raised when trying to parse invalid dates in this method
These shouldn't
check complex numbers just for the heck of it :)
only true if working on it
But None should be encoded as 0
Check != while we're at it
'download_track': True,
'download_track': True,
'download_track': True,
Export should succeed without VAL data if video does not exist
Check that download_video field is also set to default (False) in xml for backward compatibility
YouTube JavaScript API
URL to get YouTube metadata
allow for additional options that can be keyed on a name, e.g. 'trashcan'
No end date set, returns empty string.
No end date set, returns empty string.
Make sure we can detect when no teams exist.
add topics
remove them again
Add one HTML block to the library:
Call RandomizeModule which will select an element from the list of available items
export to the same directory--that way things like the custom_tags/ folder  will still be there.
HACK: filenames change when changing file formats  during imports from old-style courses.  Ignore them.
These modules are not editable in studio yet
pylint: disable=no-member
pylint: disable=no-member
Test that when an xmodule is generated from descriptor_cls  with mixed xmodule and xblock children, the test property holds
Test that when an xmodule is generated from descriptor_cls  with only xblock children, the test property holds
-*- coding: utf-8 -*-
return anything except None to test LMS
test to make sure that role is checked in LMS
We just want the above call to complete without exceptions, and to have called verify_oauth_body_sign
(bad inputs, error message expected)
@context missing
return anything except None to test LMS
test to make sure that role is checked in LMS
Blocks with nothing set with return the fields' defaults.
A child with get a value inherited from the parent.
Fields not in the inherited_names list won't be inherited.
Tests that the xblock fields (currently tags and name) get filtered out.  Also tests that xml_attributes is filtered out of XmlDescriptor.
Start of helper methods
False can be parsed as a int (converts to 0)  True can be parsed as a int (converts to 1)  2.78 can be converted to int, so the string will be deserialized
False can be parsed as a float (converts to 0)  True can be parsed as a float (converts to 1)
'false' cannot be converted to float, so input value is returned
json.loads converts the value to Python bool
json.loads fails, string value is returned.
2.78 can be converted to a bool, so the string will be deserialized
test that from_json produces no exceptions
Rerandomize isn't a basic attribute of Sequence
Rerandomize is added to the constructed sequence via the InheritanceMixin
Rerandomize is a known value coming from policy, and shouldn't appear  in xml_attributes
attempts isn't a basic attribute of Sequence
attempts isn't added to the constructed sequence, because  it's not in the InheritanceMixin
attempts is an unknown attribute, so we should include it  in xml_attributes so that it gets written out (despite the misleading  name)
`attribute` isn't a basic attribute of Sequence
`attribute` is added by InheritanceMixin
InheritanceMixin will be used when processing the XML
`attribute` is added to the constructed sequence, because  it's in the InheritanceMixin
`attribute` is a known attribute, so we shouldn't include it  in xml_attributes
We had a bug where a thumbnail location of None was getting transformed into a Location tuple, with  all elements being None. It is important that the location be just None for rendering.
return dict:
edx - HarvardX  cond_test - ER22x
Test ajax url is just usage-id / handler_name
Now change state of the capa problem to make it completed  Save our modifications to the underlying KeyValueStore so they can be persisted
pylint: disable=abstract-method
HACK: This shouldn't be hard-coded to two types  OBSOLETE: This obsoletes 'type'
Make '_' a no-op so we can scrape strings. Using lambda instead of   `django.utils.translation.ugettext_noop` because Django cannot be imported in this file
If position is specified in system, then use that instead.
We do this up here because proctored exam functionality could bypass  rendering after this section.
Is this sequential part of a timed or proctored exam?
Do we have an alternate rendering  from the edx_proctoring subsystem?
Get all descendant XBlock types and counts
Basic count of the number of Units (a.k.a. VerticalBlocks) we have in  this learning sequence
Count of all modules (leaf nodes) in this sequence (e.g. videos,  problems, etc.) The units (verticals) themselves are not counted.
None = no overridden view rendering
inject the user's credit requirements and fulfillments
See if the edx-proctoring subsystem wants to present  a special view to the student rather  than the actual sequence content  This will return None if there is no  overridden view to display given the  current state of the user
Make '_' a no-op so we can scrape strings. Using lambda instead of   `django.utils.translation.ugettext_noop` because Django cannot be imported in this file
fall-through handles all error cases
Fall through to returning grade and comment
According to http://www.imsglobal.org/lti/ltiv2p0/ltiIMGv2p0.html_Toc361225514  PUTting a JSON object with no "resultScore" field is equivalent to a DELETE.
Fall-through record the score and the comment in the module
if present, 'resultScore' must be a number between 0 and 1 inclusive
struct_times are always utc
strftime doesn't work for pre-1900 dates, so use  isoformat instead  isoformat adds +00:00 rather than Z
Timedeltas are immutable, see http://docs.python.org/2/library/datetime.htmlavailable-types
Timedeltas are immutable, see http://docs.python.org/2/library/datetime.htmlavailable-types
We've seen serialized versions of float in this field
Make '_' a no-op so we can scrape strings. Using lambda instead of   `django.utils.translation.ugettext_noop` because Django cannot be imported in this file
Student words from client.  FIXME: we must use raw JSON, not a post data (multipart/form-data)
FIXME: fix this, when xblock will support mutable types.  Now we use this hack.  speed issues
Save in all_words.
Update top_words.
Save all_words in database.
optional information about where this file was imported from. This is needed to support import/export  cycles
create a dummy asset location with a fake but unique name. strip off the name, and return it
Clean up the path, removing any static prefix and any leading slash.
If we couldn't parse the path, just let compute_location figure it out.  It's most likely a path like /image.png or something.
Break down the input path.
Convert our path to an asset key if it isn't one already.
See if this is an allowed file extension to serve.  Some files aren't served through the  CDN in order to avoid same-origin policy/CORS-related issues.
use a naming convention to associate originals with the thumbnail
I've seen some exceptions from the PIL library when trying to save palletted  PNG files to JPEG. Per the google-universe, they suggest converting to RGB first.
store this thumbnail as any other piece of content
log and continue as thumbnails are generally considered as optional
GridFS will throw an exception if the Database is wrapped in a MongoProxy. So don't wrap it.  The appropriate methods below are marked as autoretry_read - those methods will handle  the AutoReconnect errors.
getattr b/c caching may mean some pickled instances don't have attr
Deletes of non-existent files are considered successful
Escape invalid char from filename.
thumbnail is not technically correct but will be functionally correct as the code  only looks at the name which is not course relative.  getattr b/c caching may mean some pickled instances don't have attr
codifying the original order which pymongo used for the dicts coming out of location_to_dict  stability of order is more important than sanity of order as any changes to order make things  unfindable
NOTE, there's no need to state that run doesn't exist in the negative case b/c access via  SON requires equivalence (same keys and values in exact same order)
NOTE, there's no need to state that run doesn't exist in the negative case b/c access via  SON requires equivalence (same keys and values in exact same order)
first delete all of the thumbnails
then delete all of the assets
ok, save the content into the courseware
see if there is a thumbnail as well, if so move that as well
Make '_' a no-op so we can scrape strings. Using lambda instead of   `django.utils.translation.ugettext_noop` because Django cannot be imported in this file
Make '_' a no-op so we can scrape strings. Using lambda instead of   `django.utils.translation.ugettext_noop` because Django cannot be imported in this file
msg += "<p/> dot test " + to_latex(dot(sympy.Symbol('x'),sympy.Symbol('y')))
msg = msg.replace('<p>','<p><span class="inline-error">').replace('</p>','</span></p>')
options
if expected answer is a number, try parsing provided answer as a number also
exactly the same?
Used to return more keys: 'ex': fexpect, 'got': fsym
substitute back into latex form for scripts  literally something of the form  'scriptN' becomes '\\mathcal{N}'  note: can't use something akin to the _print_hat method above because we  sometimes get 'script(N)__B' or more complicated terms
make all lowercase real?
match things like the last example--  the second item in msub is an mrow with the first  character equal to \u200b
match things like the middle example-  the third item in msubsup is an mrow with the first  character equal to \u200b
pre-process the presentation mathml before sending it to snuggletex to convert to content mathml
convert to cmathml
parser tree for Content MathML
Expect that the exact same symbolic string is marked correct
Expect that equivalent symbolic strings are marked correct
for readability later
wrap
process the expression
success?
wrap
process the expression
success?
wrap
process the expression
success?
wrap
process the expression
success?
Subtract 1 second to help comparisons with file-modify time succeed,  since os.path.getmtime() is not millisecond-accurate
Find first number in the list
`reduce` will go from left to right; reverse the list.
Having reversed it, raise `b` to the power of `a`.
No need to go further.
Parse the tree.
Get our variables together.
...and check them
Create a recursion to evaluate the tree.
0.33 or 7 or .34 or 16.  pyparsing allows spaces between tokens--`Combine` prevents that.
SI suffixes and percent.
Predefine recursive variables.
Handle variables passed in. They must start with letters/underscores  and may contain numbers afterward.
Same thing for functions.
Do the following in the correct order to preserve order of operation.
Then treat it as a terminal node.
Find the value of the entire tree.
Generate parens and overwrite `self.latex`.
add capital greek letters
add hbar for QM
add infinity
Then 'a_b' must become 'a_{b}'
Put it together.  Return the function within the closure.
Switch to denominator mode.
Switch back to numerator mode.  First, render the current fraction and add it to the latex.
Reset back to beginning state
Add the fraction/numerator that we ended on.  We ended on a numerator--act like normal multiplication.
No need to go further
Parse tree
Get our variables together.
Create a recursion to evaluate the tree.
the following functions do not have 0 in their domain
Test sqrt
Test abs
Test a simple equation
Recall 'T' is a default constant, with value 298.15
Use 'x' as the first term (instead of, say, '1'), so it can't be  interpreted as a negative number.
If there is no exception thrown, this is a problem
Make complex value:  Example:  Create like 'p_l[p][first]' from {'first': {'p': 'p_l'}
checks if self or other is not empty list (empty lists  = false)
probably xml content mistake - wrong rules names
same as upper -  if we found element from 'user' list,  that not in 'correct' list - we return False.
Convert string `user_answer` to object.
Convert nested `user_answer` to flat format.
Step 1: Discount things which are not numbers
Special case: 0 is an okay resistor
Step 2: Move into the range [100, 1000)
Step 3: Discount things which are not integers, and cast to int
Step 4: Check if we're a valid EIA value
We can handle 1% components correctly; 2.2k is EIA24, but not EIA48.
Do all checks and complain before changing any state.
registering the same class multiple times seems silly, but ok
Ok, should be good to change state now.
Returning the cls means we can use this as a decorator.
extra things displayed after "show answers" is pressed
these get captured as student responses
These should be removed from HTML output, including all subelements
Set seed according to the following priority:        1. Contained in problem's state        2. Passed into capa_problem via constructor
Convert startouttext and endouttext to proper <text></text>
parse problem XML file into an element tree
handle any <include file="foo"> tags
construct script processor context (eg for customresponse problems)
dictionary of InputType objects associated with this problem    input_id string -> InputType object
Each LoncapaResponse will update its specific entries in cmap    cmap is passed by reference
check against each inputtype  if the input type has an ungraded function, pass in the values
if answers include File objects, convert them to filenames.
old CorrectMap
dict of (id, correct_answer)
Note that the modifications has been done, avoiding problems if called twice.
Grab the first choicegroup (there should only be one within each <multiplechoiceresponse> tag)
Find the student answer key that matches our <choicegroup> id
Do not displace the solution under these circumstances
If could not find the solution element, then skip the remaining steps below
Change our correct-choice explanation from a "solution explanation" to within  the set of targeted feedback, which means the explanation will render on the page  without the student clicking "Show Answer" or seeing a checkmark next to the correct choice
Add our solution instead to the targetedfeedbackset and change its tag name
open using LoncapaSystem OSFS filestore
read in and convert to XML
Separate paths by :, like the system path.
find additional comma-separated modules search path
An asset named python_lib.zip can be imported by Python code.
Store code source in context, along with the Python path needed to run it correctly.
Comment and ProcessingInstruction nodes are not Elements,  and we're ok leaving those behind.  BTW: etree gives us no good way to distinguish these things  other than to examine .tag to see if it's a string. :(
leave javascript intact.
save the input type so that we can make ajax calls on it if we need to
let each Response render itself
otherwise, render children recursively, and copy over attributes
copy attributes over if not innocufying
create and save ID for this response
instantiate capa Response  save in list in self
We're going to trap stdout/stderr from the problems (yes, some print)
These are actual answers we get from the responsetypes
all_answers is real_answers + blanks for other answer_ids for which the  responsetypes can't provide us pre-canned answers (customresponse)
Make '_' a no-op so we can scrape strings. Using lambda instead of   `django.utils.translation.ugettext_noop` because Django cannot be imported in this file
Overridable field that specifies whether this capa response type has support for  for rendering on devices of different sizes and shapes.  By default, we set this to False, allowing subclasses to override as appropriate.
ordered list of answer_id values for this response  for convenience
Does this problem have partial credit?  If so, what kind? Get it as a list of strings.
render ourself as a <span> + our content
problem author can make this span display:inline
Add a <div> for the message at the end of the response
self.runtime.track_function('get_demand_hint', event_info)  This this "feedback hint" event
Establish the outer style
Ready to go
We need the CorrectMap code for hint functions. No, this is not great.
make the hint appear after the last answer box in this  response
If no other hint form matches, try extended hints.
First try wrapping the text in a <div> and parsing  it as an XHTML tree
Set the css class of the message <div>
Sets up generator, grader, display, and their dependencies.
If a choice does not have an id, assign 'A' 'B', .. used by CompoundHint
No partial credit? Get grade right now.
Translators: 'partial_credit' and the items in the 'graders' object  are attribute names or values and should not be translated.
Only one type of credit at a time.
Make sure we're using an approved style.
Run the appropriate grader.
Compound hints are a special thing just for checkboxgroup, trying  them first before the regular extended hints.
Selector words are space separated and not case-sensitive
call secondary setup for MultipleChoice questions, to set name  attributes
define correct choices (after calling secondary setup)
Warning: mostly student_answer is a string, but sometimes it is a list of strings.
No partial credit? Grade it right away.
Translators: 'partial_credit' and the items in the 'graders' object  are attribute names or values and should not be translated.
Only one type of credit at a time.
Make sure we're using an approved style.
Run the appropriate grader.
With masking disabled, this computation remains interesting to see  the displayed order, even though there is no unmasking.
Translators: 'answer-pool' is an attribute name and should not be translated.
Note in the response that answerpool is done.  Both to avoid double-processing, and to feed the logs.
Remove all choices in the choices_list (we will add some back in later)
Sample from the answer pool to get the subset choices and solution id
Add back in randomly selected choices
Limit the number of incorrect choices to what we actually have
Select the one correct choice
Put together the result, pushing most of the work onto rng.shuffle()
convert val into unicode because student answer always be a unicode string  even it is a list, dict etc.
Find the tolerance
What multiple of the tolerance is worth partial credit?
Translators: This is an error message for a math problem. If the instructor provided a  boundary (end limit) for a variable that is a complex number (a + bi), this message displays.
Translators: This is an error message for a math problem. If the instructor did not  provide a boundary (end limit) for a variable, this message displays.
backward compatibility, can be removed in future, it is up to @Lyla Fisher.  end of backward compatibility
Note the atypical case of using self.id instead of self.answer_id
We follow the check_string convention/exception, adding ^ and $
if given answer is empty.
backward compatibility, should be removed in future.  end of backward compatibility
Translators: Separator used in StringResponse to display multiple answers.  Example: "Answer: Answer_1 or Answer_2 or Answer_3".
Standard amount for partial credit if not otherwise specified:
if <customresponse> has an "expect" (or "answer") attribute then save  that
the <answer>...</answer> stanza should be local to the current <customresponse>.  So try looking there first.  print "xml = ",etree.tostring(xml,pretty_print=True)
if we have a "cfn" attribute then look for the function specified by cfn, in  the problem context ie the comparison function is defined in the  <script>...</script> stanza instead
global variable in context which holds the Presentation MathML from dynamic math input  ordered list of dynamath responses
NOTE: correct = 'unknown' could be dangerous. Inputtypes such as textline are  not expecting 'unknown's
put these in the context of the check function evaluator  note that this doesn't help the "cfn" version - only the exec version  my ID
expected answer (if given as attribute)
ordered list of student answers from entry boxes in our subtree
ordered list of ID's of all entry boxes in our subtree
ordered list of all javascript inputs in our subtree
dict of student's responses, with keys being entry box IDs
the list to be filled in by the check function
the list of messages to be filled in by the check function
a message that applies to the entire response  instead of a particular input
any options to be passed to the cfn
Pass DEBUG to the check function.
Run the check function
If there is only one input, apply the message to that input  Otherwise, apply the message to the whole problem
Otherwise, we do not recognize the dictionary  Raise an exception
If *msg* is an empty string, then the code below  will return "</html>".  To avoid this, we first check  that *msg* is a non-empty string.
When we parse *msg* using etree, there needs to be a root  element, so we wrap the *msg* text in <html> tags
Replace < characters
Use etree to prettify the HTML
Remove the <html> tags we introduced earlier, so we're  left with just the prettified message markup
Strip leading and trailing whitespace
If we start with an empty string, then return an empty string
Log the error if we are debugging
Notify student with a student input error
Symbolic response always uses symmath_check()  If the XML did not specify this, then set it now  Otherwise, we get an error from the superclass
Let CustomResponse do its setup
Since we have limited max_inputfields to 1,  we can assume that there is only one submission
Translators: 'SymbolicResponse' is a problem type and should not be translated.
VS[compat]:  Check if XML uses the ExternalResponse format or the generic  CodeResponse format
Note that submission can be a file
Prepare xqueue request
State associated with the queueing request
Queueing mechanism flags:    1) Backend: Non-null CorrectMap['queuestate'] indicates that       the problem has been queued    2) Frontend: correctness='incomplete' eventually trickles down       through inputtypes.textbox and .filesubmission to inform the       browser to poll the LMS
Translators: 'grader' refers to the edX automatic code grader.
FIXME - hardcoded URL
response is XML; parse it
Find the tolerance
Case insensitive
Case sensitive
Default
pylint: disable=broad-except
Untested; never used
Translators: 'SchematicResponse' is a problem type and should not be translated.
use answers provided in input elements
answer is correct if (x,y) is within the specified  rectangle
Check the binary choices first.  Only return correct if the student got both the binary  and numtolerance_inputs are correct
Initialize the two dictionaries that are returned
`selected_choices` is a list of binary choices which were "checked/selected"  when the student submitted the problem.  Keys in a_dict ending with 'bc' refer to binary choices.
Convert the name of a numtolerance_input into the name of the binary  choice that it is contained within, and append it to the list if  the numtolerance_input's parent binary_choice is contained in  `selected_choices`.
If `self.corrrect_inputs` does not contain an entry for  `answer_name`, this means that answer_name is a decoy  input's value, and validation of its numericality is the  only thing of interest from the later call to  `compare_with_tolerance`.
Ignore the results of the comparisons which were just for  Numerical Validation.  If any input is not correct, set the return value to False
pylint: disable=invalid-all-object  pylint: enable=invalid-all-object
LMS Interface to external queueing system (xqueue)
Attempt to send to queue
Utility functions used in CAPA responsetypes
If an input is infinite, we can end up with `abs(student_complex-instructor_complex)` and  `tolerance` both equal to infinity. Then, below we would have  `inf <= inf` which is a fail. Instead, compare directly.
because student_complex and instructor_complex are not necessarily  complex here, we enforce it here:
v1 and v2 are, in general, complex numbers:  there are some notes about backward compatibility issue: see responsetypes.get_staff_ans()).
Files are stored as a list, even if one file
start with empty dict
See the documentation for 'set_dict' for the use of kwargs
empty current dict
if not correct and no points have been assigned, return 0
Subclasses override this to specify the file name of the template  to be loaded from capa/templates.  The template name should include the .html extension:  for example: choicegroup.html
add dummy STATIC_URL to template context
Should mark the entire problem correct
Should NOT mark individual options
Should NOT mark individual options
Should NOT mark individual options
Should NOT mark the whole problem
Should NOT mark the whole problem
Should NOT mark the entire problem correct/incorrect
Should NOT mark individual options
Expect to see the message
Expect that we do NOT see the message yet
Expect that we get a <div> with correct class
Expect that we get a <span> with class="status"  (used to by CSS to draw the green check / red x)
Expect that we get a <div> with correct class
If return_to_annotation set, then show the link
Otherwise, do not show the links
Expect that the correct option is selected
Test cases of `(input_status, expected_css_class)` tuples
Because the HTML is unescaped, we should be able to  descend to the <b> tag
Because the HTML is unescaped, we should be able to  descend to the <b> tag
HTML from `tail` should NOT be escaped.  We should be able to traverse it as part of the XML tree
Create options 0-4, and select option 2
Should have a dummy default
Should have the correct option selected
Expect a <div> with the status
Expect a <p> with the status
Assert that the JSON-encoded string was inserted without  escaping the HTML.  We should be able to traverse the XML tree.
Should mark the entire problem correct
Should NOT mark individual options
Should NOT mark individual options
Should NOT mark individual options
Should NOT mark the whole problem
Should NOT mark the whole problem
The following comes into existence by virtue of being called  capa_module.runtime.track_function
just a handy shortcut
check that escaping single quotes with leading backslash (\') properly works  note: actual input by user will be hasn\'t but json parses it as hasn\\'t
'label': '',
check that exception is raised during parsing for html.
Check that compensating for the dot size works properly.
'label': '',
'label': '',
status is in the mapping
Check that calling it multiple times yields the same thing
New problem with same XML -- try the correct choice.
Check that calling it multiple times yields the same thing
The student choses one with no feedback, but alwaysShowCorrectChoiceExplanation  is in force, so we should see the correct solution feedback.
The student chooses one with no feedback set, so we check that there's no feedback.
Q1 has feedback1 and Q2 has nothing
If something is wrong, show it to us.
Ensure that we get the expected number of points  Using assertAlmostEqual to avoid floating point issues
Invalid choices should be marked incorrect (we have no choice 3)
Invalid choices should be marked incorrect
Define a rectangle with corners (10,10) and (20,20)
Define two rectangles
Define a triangular region with corners (0,0), (5,10), and (0, 10)
Define multiple regions that the user can select
Should not allow multiple inputs, since we specify  only one "expect" value
Simulate what the Snuggletex server would respond
Options not in the list should be marked incorrect
Test that option response properly escapes quotes inside options strings
Sample variables x and y in the range [-10, 10]
The expected solution is numerically equivalent to x+2y
Expect an equivalent formula to be marked correct  2x - x + y + y = x + 2y
Expect an incorrect formula to be marked incorrect  x + y != x + 2y
Sample variables x and y in the range [-10, 10]
The expected solution is numerically equivalent to x+2y
Calculate the answer using a script
Sample x in the range [-10,10]
The expected solution is numerically equivalent to 2*x
Expect that the inputs are graded correctly
Exact string should be correct  Other strings and the lowercase version of the string are incorrect
Exact string should be correct
test with case_sensitive not specified
Test single answer
should also be case_sensitive if case sensitivity is not specified
Exact string should be correct
Other strings and the lowercase version of the string are incorrect
Test multiple answers
Other strings and the lowercase version of the string are incorrect
right way to search for \
Test single answer
Both versions of the string should be allowed, regardless  of capitalization
Other strings are not allowed
Test multiple answers
Exact string should be correct
Other strings and the lowercase version of the string are incorrect
We should NOT get a hint for Michigan (the correct answer)
We should NOT get a hint for any other string
We should NOT get a hint for Michigan (the correct answer)
We should NOT get a hint for any other string
CodeResponse requires internal CorrectMap state. Build it now in the unqueued state
Incorrect queuekey, state should not be updated
CodeResponse requires internal CorrectMap state. Build it now in the unqueued state
Queue state only tracks up to second
No choice 3 exists --> mark incorrect
No choice 3 exists --> mark incorrect
First: Every Decision Counts grading style
Second: Halves grading style
Third: Halves grading style with more options
Ensure that we get the expected number of points  Using assertAlmostEqual to avoid floating point issues  First: Every Decision Counts grading style
Second: Halves grading style
Third: Halves grading style with more options
Compile coffee files into javascript used by the response
Test that we get graded correctly
If the system says to disallow unsafe code execution, then making  this problem will raise an exception.
no complex number in range tolerance staff answer
test invalid range tolerance answer
test empty boundaries
Check results
Check that the message for the particular input was received
Check that the overall message (for the whole response) was received
Make sure the seed from the problem gets fed into the script execution.
Correct answer
Partially Credit answer
Incorrect answer
CustomResponse also adds 'expect' to the problem context; check that directly first:
Also make sure the problem was graded correctly:
Correct answer -- expect both inputs marked correct
One answer incorrect -- expect both inputs marked partially correct
Both answers incorrect -- expect both inputs marked incorrect
Expect that we receive the overall message (for the whole response)
Correct answer
Partially Correct answer
Incorrect answer
Grade the inputs (one input incorrect)
Grade the inputs (one input partially correct)
Grade the inputs (everything correct)
Message is interpreted as an "overall message"
Expect that an exception gets raised when we check the answer
Construct a script that will raise an exception
Expect that an exception gets raised when we check the answer
Expect that an exception gets raised when we check the answer
Create the problem
Expect that we can grade an answer without  getting an exception
Create the problem
Expect that we can grade an answer without  getting an exception
euqal to correct order after sorting at get_score
To test that the context is set up correctly,  we create a script that sets *correct* to true  if and only if we find the *submission* (list)
The actual dictionary would contain schematic information  sent from the JavaScript simulation
Expect that the problem is graded as true  (That is, our script verifies that the context  is what we expect)
Construct a script that will raise an exception
Expect that an exception gets raised when we check the answer
Choice is whether this choice is correct  Answers contains a list of answers to textinpts for the choice
Radio/Checkbox inputs in choicetext problems follow  a naming convention that gives them names ending with "bc"  Build the names for the numtolerance_inputs and add their answers  to `answer_dict`.
In `answer_id` `index` represents the ordinality of the  choice and `ind` represents the ordinality of the  numtolerance_input inside the parent choice.
Test that error is raised for input in selected correct choice.
Test that error is raised for input in selected incorrect choice.
Dictionary from name of test_scenario to (problem_name, correctness)  Correctness is used to test whether the problem was graded properly
Dictionary problem_name: problem
Load the test problem's name and desired correctness  Load the problem
Make sure the actual grade matches the expected grade
Check about masking
attributes *not* present
NOTE: not testing get_html yet because I don't understand why it's doing what it's doing.
just a handy shortcut
Our test_capa_system "renders" templates to a div with the repr of the context.
The root is <problem>
Add a script if there is one
The problem has a child <p> with question text
Add the response(s)
Set partial credit
Add input elements
Names of group elements
Create the <choicegroup>, <checkboxgroup>, or <radiogroup> element
Add a name identifying the choice, if one exists  For simplicity, we use the same string as both the  name attribute and the text of the element
Add point values for partially-correct choices.
The line below throws a false positive pylint violation, so it's excepted.
Create the response element
Create the <schematicresponse> element
Insert the <answer> script if one is provided
Since we are providing an <answer> tag,  we should override the default behavior  of including a <solution> tag as well
Create the <coderesponse> element
Create the <codeparam> element.
Set the initial display text
Set the answer display text
Set the grader payload string
Create the input within the response
Since we create this in create_response_element(),  return None here
Create the <formularesponse> element
Set the sample information
Set the tolerance
Set the answer
Include hints, if specified
For each hint, create a <formulahint> element
We could sample a different range, but for simplicity,  we use the same sample string for the hints  that we used previously.
Both display_src and display_class given,  or neither given
Create the <javascriptresponse> element
Create the <optioninput> element
Set the "correct" attribute
Create the <stringresponse> element
Set the answer attribute
Retrieve **kwargs
Symmath check expects a string of options
Construct the <symbolicresponse> element
Ensure that the first element of choices is an ordered  collection. It will start as a list, a tuple, or not a Container.
If the current `choice` contains any("answer": number)  elements, turn those into numtolerance_inputs  `answers` will be a list or tuple of answers or a single  answer, representing the answers for numtolerance_inputs  inside of this specific choice.
Make sure that `answers` is an ordered collection for  convenience.
Default type is 'radiotextgroup'
Give each choice text equal to it's position(0,1,2...)
Add all of the inputs as children of this choice
Create the problem
Create a test file to include
Create the problem
Render the HTML
Expect that the include file was embedded in the problem
Create the problem
Render the HTML
Expect that the <startouttext /> and <endouttext />  were converted to <span></span> tags
Create the problem
Render the HTML
Expect that the anonymous_student_id was converted to "student"
Create the problem
Render the HTML
Expect that the script element has been removed from the rendered HTML
Create the problem
Render the HTML
expect the javascript is still present in the rendered html
Mock out the template renderer
Create the problem and render the HTML
Expect problem has been turned into a <div>
Expect question text is in a <p> child
Expect that the response has been turned into a <span>
Expect that the response <span>  that contains a <div> for the textline
Expect a child <div> for the solution  with the rendered template
Generate some XML for a CustomResponse
Create the problem and render the html
Grade the problem
Render the html
Expect that the <div> contains our message (as part of the XML tree)
Create the problem and render the HTML
Expect that the variable $test has been replaced with its value
Create the problem
Render the HTML
Intentionally testing an item that's not in cmap.
Default is an empty string string
Set a message that applies to the whole question
Retrieve the message
Setting the message to None --> empty string
Create a second cmap, then update it to have the same properties  as the first cmap
Assert that it has all the same properties
Should get an exception if we try to update() a CorrectMap  with a non-CorrectMap value
We'll need the code from lazymod.py for use in safe_exec, so read it now.
Create the complete code we'll run.
Decide which code executor to use.
Run the code!  Results are side effects in globals_dict.
Put the result back in the cache.  This is complicated by the fact that  the globals dict might not be entirely serializable.
If an exception happened, raise it now.
Future division: 1/2 is 0.5.
Math is always available.
Without a seed, the results are unpredictable
With a seed, the results are predictable
Can't test for forbiddenness if CodeJail isn't configured for python.
Actual cache implementations have limits on key length
Actual cache implementations have limits on key length
Fiddle with the cache, then try it again.
Caching used to die on memcache with more than 250 bytes of code.  Check that it doesn't any more.
Used to be that running code that raised an exception didn't cache  the result.  Check that now it does.
The exception should be in the cache now.
Change the value stored in the cache, the result should change.
Check that using non-ASCII unicode does not raise an encoding error.  Try several non-ASCII unicode characters.
Check that our dicts are equal, but with different key order.
Save all the names of all the imported modules.
Get a list of modules that didn't exist when we were created  and delete them all so another import will run code for real again.
Each test will remove modules that it imported.
wsgiref is a module with submodules that is not already imported.  Any similar module would do. This test demonstrates that the module  is not already im
We must include any text that was following our original <clarification>...</clarification> XML node.:
status: css class
want to allow default to be None, but also allow required objects
not required, so return default
put hint above msg if it should be displayed
Pre-parse and process all the declared requirements.
Call subclass "constructor" -- means they don't have to worry about calling  super().__init__, and are isolated from changes to the input  constructor interface.
Something went wrong: add xml to message, but keep the traceback
If `html` contains attrs with no values, like `controls` in <audio controls src='smth'/>,  XML parser will raise exception, so wee fallback to html5parser, which will set empty "" values for such attrs.
convert single quotes inside option values to html encoded string
parse the set of possible options  Allow options to be separated by whitespace as well as commas
remove quotes  convert escaped single quotes (html encoded string) back to single quotes
make list of (option_id, option_description), with description=id
Translators: '<choice>' and '<compoundhint>' are tag names and should not be translated.
Need to provide a value that JSON can parse if there is no  student-supplied value yet.
attribute is set to false.
Check if problem has been queued  Flag indicating that the problem has been queued, 'msg' is length of  queue
Another (older) name--at some point we may want to make it use a  non-codemirror editor.
For CodeMirror  Template expects tabsize to be an int it can do math with
if no student input yet, then use the default input given by the  problem
If neither can parse queue_msg, it contains invalid xml.
only send data if xqueue exists
pull relevant info out of get
save the input state if successful
Note: we subtract 15 to compensate for the size of the dot on the screen.  (is a 30x30 image--lms/static/images/green-pointer.png).
this is unexpected, so log
this is unexpected, so log
add labels to images?:
image drag and drop onto
custom background color for labels:
Need to provide a value that JSON can parse if there is no  student-supplied value yet.
Make `value` an empty dictionary, if it currently has an empty  value. This is necessary because the template expects a  dictionary.
Translators: a "tag" is an XML element, such as "<b>" in HTML
Initialize our dict for the next content
Add any tail text("is the mean" in the example)
Add the tuple for the current choice to the list of choices
open render.html to look at rendered equations
factors don't match
order still doesn't matter
Phases tests
round point to closes 0.05 value
if suffix is explicitly 1, like ^1-  strip 1, leave only sign: ^-
And recurse
If an integer, return that integer  If a fraction, return the fraction
this won't be reached unless we add more arrow types, but keep it to avoid explosions when  that happens.
only one side
Also for lists of multimolecules without factors and phases  sorting seems to work fine.
parsed final trees
check if expressions are correct without factors
factors are not proportional
return ratio
order matters -- need to try <-> first
left sides don't match
right sides don't match
factors don't match (molecule counts to add up)
want an exact match.
Don't want external users to have to deal with parsing exceptions.  Just return False.
This should be imported after lxml.etree so that it overrides the following attributes.
This part is for ability to get xblock instance in xblock_noauth handlers, where user is unauthenticated.
Another thread has already created this entry, so  continue
cache key format e.g user.<user_id>.profile.country = 'SG'
Location is no longer used, but is held here for backwards compatibility  for users imported from our first class.
There are legal implications regarding how we can contact users and what information we can make public  based on their age, so we must take the most conservative estimate.
Remove profile images for users who require parental consent
Cache "old" field values on the model instance so that they can be  retrieved in the post_save callback when we emit an event with new and  old field values.
pylint: disable=protected-access
pylint: disable=protected-access
Setting course_id to '' makes it not affect the generated hash,  and thus produce the old per-student anonymous id
first element should be the last time we reset password
no history, then let's take the date the user joined
just limit the result set to the number of different  password we need
did we go over the limit in attempts  yes, then store when this account is locked out until
To avoid circular imports.
If is_active is False, then the student is not considered to be enrolled  in the course (is_enrolled() will return False)
Represents the modes that are possible. We'll update this later with a  list of possible values.
Maintain a history of requirement status updates for auditing purposes
cache key format e.g enrollment.<username>.<course_key>.mode = 'honor'
Private variable for storing course_overview to minimize calls to the database.  When the property .course_overview is accessed for the first time, this variable will be set.
If we *did* just create a new enrollment, set some defaults
if is_active is None, then the call to update_enrollment didn't specify  any value, so just leave is_active as it is
if mode is None, the call to update_enrollment didn't specify a new  mode, so leave as-is
Only emit mode change events when the user's enrollment  mode has changed from its previous setting
User is allowed to enroll if they've reached this point.
If the student has already been given a certificate they should not be refunded
If it is after the refundable cutoff date they should not be refunded.
Deprecated. Please use the `course_overview` property instead.
blank org is for global group based roles such as course creator (may be deprecated)  blank course_id implies org wide role
Don't try--it won't work, and it will fill the logs with lots of errors
Deprecated
if skip_entrance_exam is True, then student can skip entrance exam  for the course
Ensure that at most one value exists for a given user/name.
Studio permissions:  In addition to the above, one is always allowed to "demote" oneself to a lower role within a course, or remove oneself
if not, then check inferred permissions
can always remove self (at this layer)
superuser
Note that this lives in LMS, so this dependency should be refactored.
This appears to be an unused context parameter, at least for the master templates...
TO DISPLAY A YOUTUBE WELCOME VIDEO  1) Change False to True
2) Add your video's YouTube ID (11 chars, eg "123456789xX"), or specify via microsite config  Note: This value should be moved into a configuration setting and plumbed-through to the  context via the microsite configuration workflow, versus living here
allow for microsite override of the courses list
Insert additional context for use in the template
Sort the data by the reverification_end_date
If the course is missing or broken, log an error and skip it.
If we are in a Microsite, then filter out anything that is not  attributed (by ORG) to that Microsite.
Conversely, if we are not in a Microsite, then filter out any enrollments  with courses attributed (by ORG) to Microsites.
Else, include the enrollment.
If enabled, show the LinkedIn "add to profile" button  Clicking this button sends the user to LinkedIn where they  can add the certificate information to their profile.
Determine the URL to redirect to following login:
Determine the URL to redirect to following login:
for microsites, we want to filter and only show enrollments for courses within  the microsites 'ORG'
Let's filter out any courses in an "org" that has been declared to be  in a Microsite
remove our current Microsite from the "filter out" list, if applicable
Build our (course, enrollment) list for the user, but ignore any courses that no  longer exist (because the course IDs have changed). Still, we don't delete those  enrollments, because it could have been a data push snafu.
sort the enrollment pairs by the enrollment date
Check to see if the student has recently enrolled in a course.  If so, display a notification message confirming the enrollment.
Global staff can see what courses errored on their dashboard  Show any courses that errored on load
Get any programs associated with courses being displayed.  This is passed along in the template context to allow rendering of  program-related information on the dashboard.
Construct a dictionary of course mode information  used to render the course list.  We re-use the course modes dict  we loaded earlier to avoid hitting the database.
only show email settings for Mongo course and when bulk email is turned on
Verification Attempts  Used to generate the "you must reverify for course x" banner
Gets data for midcourse reverifications, if any are necessary or have failed
If there are *any* denied reverifications that have not been toggled off,  we'll display the banner
Populate the Order History for the side-bar.
get list of courses having pre-requisites yet to be completed
If the enrollment has no created date, we are explicitly excluding the course  from the list of recent enrollments.
Feature flag off
Get the user
Ensure the user is authenticated
Ensure we received a course_id
Record the user's email opt-in preference
Check whether the user is blocked from enrolling in this course  This can occur if the user's IP is on a global blacklist  or if the user is enrolling in a country in which the course  is not available.
Otherwise, there is only one mode available (the default)
Need different levels of logging
This is actually the common case, logging in user without external linked login
if the user doesn't exist, we want to set the username to an invalid  username so that authentication is guaranteed to fail and we can take  advantage of the ratelimited backend
this occurs when there are too many attempts from the same IP address
tick the failed login counters if the user exists in the database
successful login, clear failed login attempts counters, if applicable
Ensure that the external marketing site can  detect that the user is logged in.
add this account creation to password history  NOTE, this will be a NOP unless the feature has been turned on in configuration
Copy params so we can modify it; we can't just do dict(params) because if  params is request.POST, that results in a dict containing lists of values
allow for microsites to define their own set of required/optional/hidden fields
Perform operations within a transaction that are critical to account creation  first, create the account
Perform operations that are non-critical parts of account creation
If the user is registering via 3rd party auth, track which provider they use
Track the user's registration
composes activation email  Email subject *must not* contain newlines
Immediately after a user creates an account, we log them in. They are only  logged in until they close the browser. They can't log in again until they click  the activation link from the email.
get the enrolled by user and reason from the ManualEnrollmentAudit table.  then create a new ManualEnrollmentAudit table entry for the same email  different transition state.
Resume the third-party-auth pipeline if necessary.
Generate a unique name to use if none provided
mode has to be one of 'honor'/'professional'/'verified'/'audit'/'no-id-professional'/'credit'
Set the user's global staff bit
Activate the user
ensure parental consent threshold is met
Enroll the user in a course
Apply the roles
Log in as the user
Enroll student in any pending courses he/she may have if auto_enroll flag is set
Add some rate limiting here by re-using the RateLimitMixin as a helper class
When password change is complete, a "edx.user.settings.changed" event will be emitted.  But because changing the password is multi-step, we also emit an event here so that we can  track where the request was initiated.
bad user? tick the rate limiter counter
cribbed from django.contrib.auth.views.password_reset_confirm
tie in password strength enforcement as an optional level of  security protection
we also want to pass settings.PLATFORM_NAME in as extra_context
Support old password reset URLs that used base36 encoded user IDs.  https://github.com/django/django/commit/1184d077893ff1bc947e45b00a4d565f3df81776diff-c571286052438b2e3190f8db8331a92bR231
remember what the old password hash is before we call down
get the updated user
did the password hash change, if so record it in the PasswordHistory
if activation_key is not passing as an argument, generate a random key
Send it to the old email...
And send it to the new email...
Exclude deprecated fields
We must first un-register the User model since it may also be registered by the auth app.
A list of registered access roles.
don't check is_authenticated nor is_active on purpose
pylint: disable=protected-access  Cache a list of tuples identifying the particular roles that a user has  Stored as tuples, rather than django models, to make it cheaper to construct objects for comparison
Org roles don't query by CourseKey, so use CourseKeyField.Empty for that query
pylint: disable=protected-access
python manage.py assigngroups skip_capacitor:0.3,capacitor:0.7 log.txt "Do we show capacitor in linearity tutorial?"
Transfer students from the old demoX class to a new one.
Transfer students from old course to new, with original certificate items.
Transfer students from the old demoX class into two new classes.
Find the old enrollment.
Un Enroll from source course but don't mess  with the enrollment in the destination course.
Un-enroll from the new course if the user had un-enrolled  form the old course.
parse out the course into a coursekey  if it's not a new-style course key, parse it from an old-style  course key
Generate the output filename from the course ID.  Change slashes to dashes first, and then append .csv extension.
Figure out which students are enrolled in the course
parse out the course into a coursekey  if it's not a new-style course key, parse it from an old-style  course key
The passed email address doesn't match this username's email address.  Assume a problem and fail.
resolve the specified groups
warn, but move on.
Needed for sqlite backend (i.e. in tests) because  name.max_length won't be enforced by the db.  See also http://www.sqlite.org/faq.htmlq9
give a more helpful error
give a more helpful error  this will raise a LookupError if it fails.
check idempotency
check idempotency
check idempotency
check removing a permission
check removing all permissions
check idempotency
Verify users are not in honor mode yet
Verify correct number of users are now in honor mode
Verify users are not in honor mode yet
Verify correct number of users are now in honor mode
Create and purchase a verified cert for the original course.
New Course 1
Run the actual management command
When there is no expiration date on a verified mode, the user can always get a refund
Enumeration of per-course verification statuses  we display on the student dashboard.
Retrieve all verifications for the user, sorted in descending  order by submission datetime
Check whether the user has an active or pending verification attempt  To avoid another database hit, we re-use the queryset we have already retrieved.
Retrieve verification deadlines for the enrolled courses
If the user hasn't enrolled as verified, then the course  won't display state related to its verification status.
Retrieve the verification deadline associated with the course.  This could be None if the course doesn't have a deadline.
Picking the max verification datetime on each iteration only with approved status
By default, don't show any status related to verification
Check whether the user was approved or is awaiting approval
Otherwise, the student missed the deadline, so show  them as "honor" (the kind of certificate they will receive).
Set the status for the course only if we're displaying some kind of message  Otherwise, leave the course out of the dictionary.
Query string parameters that can be passed to the "finish_auth" view to manage  things like auto-enrollment.
Before we redirect to next/dashboard, we need to handle auto-enrollment:
Note: if we are resuming a third party auth pipeline, then the next URL will already  be saved in the session as part of the pipeline state. That URL will take priority  over this one.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
if feature is disabled user can keep reusing same password
also create a user who doesn't have any history
Status code should be 400.
Status code should be 400.
Explicitly import the cache from ConfigurationModel so we can reset it after each test
test when the display is unavailable or notpassing, we get the correct results out
Simulate a successful verification attempt
Simulate a successful verification attempt
Audit mode does not have a banner.  Assert no banner element.
check button text
Now re-validating the invoice
Without linked-in config don't show Add Certificate to LinkedIn button
If user has a certificate with valid linked-in config then Add Certificate to LinkedIn button  should be visible. and it has URL value with valid parameters.
Create a course and log in the user.  Creating a new course will trigger a publish event and the course will be cached
"Explore courses" is shown in the side panel
But other links are hidden in the navigation
Enrolling them again should be harmless
Unenrolling them again should also be harmless
The enrollment record should still exist, just be inactive
Testing enrollment of newly unsaved user (i.e. no database entry)
Unenroll does nothing
Implicit save() happens on new User object when enrolling, so this  should still work
This won't throw an exception, even though the user is not found
Now unenroll them by email
Harmless second unenroll
Unenroll on non-existent user shouldn't throw an error
Creating an enrollment doesn't actually enroll a student  (calling CourseEnrollment.enroll() would have)
Until you explicitly activate it
Activating something that's already active does nothing
Now deactive
Deactivating something that's already inactive does nothing
A deactivated enrollment should be activated if enroll() is called  for that user/course_id combination
same enrollment mode does not emit an event
now try to enroll that student
count total courses appearing on student dashboard
for verified enrollment view the program detail button will have  the class 'base-btn'  for other modes view the program detail button will have have the  class border-btn
ensure that our course id was included in the API call regardless of start/end dates  count total courses appearing on student dashboard
count total courses appearing on student dashboard
verify that only normal courses (non-programs courses) appear on  the student dashboard.
Call add_users a second time, then remove just once.
get dashboard
get dashboard
Invalidate (e.g., delete) the corresponding CourseOverview, forcing get_course to be called.
Make user staff. This will cause CourseCreatorRole().has_user to return True.
check that a user who has not been added to the group still returns false
remove first user from the group and verify that CourseCreatorRole().has_user now returns false
Add user to creator group.
DISABLE_COURSE_CREATION overrides (user is not marked as staff).
Mark as staff. Now CourseCreatorRole().has_user returns true.
Remove user from creator group. CourseCreatorRole().has_user still returns true because is_staff=True
Explicitly import the cache from ConfigurationModel so we can reset it after each test
Assert that can_refund overrides this and allows refund
Assert that can_refund overrides this and allows refund
adding new role from django admin page
adding new role from django admin page
adding new role from django admin page
adding new role from django admin page
In the year that your turn a certain age you will also have been a  year younger than that in that same year.  We calculate age based off of  the youngest you could be that year.
New Course
get courses through iterating all courses
Check if response is escaped
Enable the enrollment success message
Enable donations
Create the course mode(s)
Check that the donate button is or is not displayed
Enable the enrollment success message and donations
Create a white-label course mode  (honor mode with a price set)
Create student account
Assert that the URL for the email view is in the response
Assert that the URL for the email view is not in the response
Assert that instructor email is not enabled for this course  Assert that the URL for the email view is not in the response  if this course isn't authorized
Create student account
The flag is enabled, and since REQUIRE_COURSE_EMAIL_AUTH is False, all courses should  be authorized to use email. But the course is not Mongo-backed (should not work)
Email disabled, shouldn't see link.
Invoke UrlResetMixin
Expect that the course appears on the dashboard  without any verification messaging
Enroll the student in a verified mode, but don't  create any verified course mode.  This won't happen unless someone deletes a course mode,  but if so, make sure we handle it gracefully.
Since the student has not submitted a photo verification,  the student should see a "need to verify" message
Start the photo verification process, but do not submit  Since we haven't submitted the verification, we should still  see the "need to verify" message
Upload images, but don't submit to the verification service  We should still need to verify
The student has submitted a photo verification
Now the student should see a "verification submitted" message
The student has an approved verification
Expect that the successfully verified message is shown
Check that the "verification good until" date is displayed
Expiration date in the past
The student does NOT have an approved verification  so the status should show that the student missed the deadline.
Expiration date in the past
The student didn't have an approved verification at the deadline,  so we should show that the student missed the deadline.
Expiration date in the past
The student didn't have an approved verification at the deadline,  so we should show that the student missed the deadline.
Expiration date in the future
Create a verification with the specified status
Since this is not a status we handle, don't display any  messaging relating to verification
Expiration date in the future
Create a verification with the specified status
Since this is not a status we handle, don't display any  messaging relating to verification
Expiration date in the future
Create a verification attempt that:  1) Is current (submitted in the last year)  2) Will expire by the deadline for the course
This attempt will expire tomorrow, before the course deadline
Expect that the "verify now" message is hidden  (since the user isn't allowed to submit another attempt while  a verification is active).
Expiration date in the past
The deadline has passed, and we've asked the student  to reverify (through the support team).
Expect that the user's displayed enrollment mode is verified.
Expect that the successfully verified message is shown
Check that the "verification good until" date is displayed
Adding another verification with different course.  Its created_at is greater than course deadline.
The student has an approved verification
Sanity check: verify that the course is on the page
Verify that the correct banner is rendered on the dashboard
Verify that the correct banner color is rendered
Verify that the correct copy is rendered on the dashboard  Different states might have different messaging  so in some cases we check several possibilities  and fail if none of these are found.
Combine all possible messages into a single list
Verify that none of the messages are displayed
create clients
set stock url to test disabled accounts' access to site
This simulates any db access in the templates.
Thorough tests for safe_get_host are elsewhere; here we just want a quick URL sanity check
New emails for the users
Create a another user 'user2' & make request for change email
Send requests & ensure no error was thrown
Thorough tests for safe_get_host are elsewhere; here we just want a quick URL sanity check
Set Up Registration
Ensure that the user starts inactive
Until you explicitly activate it
Ensure that the user starts inactive
Until you explicitly activate it
Default (no course modes in the database)  Expect that we're redirected to the dashboard  and automatically enrolled
Audit / Verified  We should always go to the "choose your course" page.  We should also be enrolled as the default mode.
Audit / Verified / Honor  We should always go to the "choose your course" page.  We should also be enrolled as the honor mode.  Since honor and audit are currently offered together this precedence must  be maintained.
Create the course modes (if any) required for this test case
Reverse the expected next URL, if one is provided  (otherwise, use an empty string, which the JavaScript client  interprets as a redirect to the dashboard)
Enroll in the course and verify the URL we get sent to
If we're not expecting to be enrolled, verify that this is the case
Enroll the student in the course
Attempt to unenroll the student
Expect that we're no longer enrolled
Create the course modes (if any) required for this test case
Enroll in the course
Verify that the profile API has been called as expected
Verify that we weren't enrolled
Verify that we were enrolled
Log out, so we're no longer authenticated
Try to enroll, expecting a forbidden response
Try unenroll without first enrolling in the course
create staff on course.
create instructor on course.
Verify that even a child does not require parental consent
Verify that an image cannot be set for a user with no year of birth set
verify that a user's profile image is removed when they switch to requiring parental controls
Verify that we remove the temporary `_changed_fields` property from  the model after we're done emitting events.
Patching the settings.FEATURES['AUTOMATIC_AUTH_FOR_TESTING']  value affects the contents of urls.py,  so we need to call super.setUp() which reloads urls.py (because  of the UrlResetMixin)
Check that the user has a profile
By default, the user should not be global staff
Create a user and enroll in a course
Check that a course enrollment was created for the user
Create a user and enroll in a course
Make the same call again, re-enrolling the student in the same course
Check that only one course enrollment was created for the user
Create a user and enroll in a course
Check that a course enrollment was created for the user
Check that the redirect was to the course info/outline page
Create user and redirect to 'home' (cms) or 'dashboard' (lms)
Check that the redirect was to either /dashboard or /home
Create user and redirect to specified url
Check that session and CSRF are set in the response
Patching the settings.FEATURES['AUTOMATIC_AUTH_FOR_TESTING']  value affects the contents of urls.py,  so we need to call super.setUp() which reloads urls.py (because  of the UrlResetMixin)
Create a course and configure it as a credit course
Configure a credit provider
Configure a single credit requirement (minimum passing grade)
Enroll the user in the course as "verified"
The user is not yet eligible for credit, so no additional information should be displayed on the dashboard.
Simulate that the user has completed the only requirement in the course  so the user is eligible for credit.
The user should have the option to purchase credit
Move the eligibility deadline so it's within 30 days
Simulate that the user has purchased credit, but has not  yet initiated a request to the credit provider
Simulate that the user has purchased credit and initiated a request,  but we haven't yet heard back from the credit provider.
Expect that the user's status is "pending"
Simulate that the user has purchased credit and initiated a request,  and had that request approved by the credit provider
Expect that the user's status is "approved"
Simulate that the user has purchased credit and initiated a request,  and had that request rejected by the credit provider
Expect that the user's status is "approved"
Simulate an error condition: the user has a credit enrollment  but no enrollment attribute indicating which provider the user  purchased credit from.
Expect an error message
Simulate that the user has completed the only requirement in the course  so the user is eligible for credit.
The user should have the option to purchase credit
Create one user and save it to the database
Create a registration for the user
Create a profile for the user
Create the test client
Store the login url
De-activate the user
De-activate the user
Verify the format of the "user info" cookie set on login
Check that the version is set
Check that the username and email are set
Check that the URLs are absolute
Check that the marketing site cookies have been set
Log out
Check that the marketing site cookies have been deleted  (cookies are deleted by setting an expiration date in 1970)
When logged in cookie names are loaded from JSON files, they may  have type `unicode` instead of `str`, which can cause errors  when calling Django cookie manipulation functions.
Reload the user from the database
second login should log out the first
this test can be run with either lms or studio settings  since studio does not have a dashboard url, we should  look for another url that is login_required, in that case
client1 will be logged out
Assert that no profile is created.
Reload the user from the database
Assert that profile is created.
second login should log out the first
this test can be run with either lms or studio settings  since studio does not have a dashboard url, we should  look for another url that is login_required, in that case
client1 will be logged out
Reload the user from the database
second login should log out the first
check that send_mail is called
Missing
Empty, too short
Too long
Invalid
Missing
Empty, too short
Too long
Invalid
Missing
Empty, too short
Matching username
Missing
Empty, too short
Missing
Empty, invalid
True
Missing  Need to change username/email because user was created above
Missing
Empty, invalid
True
Missing
Empty
Too short
This relies on third party auth being enabled in the test  settings with the feature flag `ENABLE_THIRD_PARTY_AUTH`
Provide a course ID to the login page, simulating what happens  when a user tries to enroll in a course without being logged in
Expect that the course ID is added to the third party auth entry  point, so that the pipeline will enroll the student and  redirect the student to the track selection page.
Verify that the third party auth URLs include the redirect URL  The third party auth pipeline will redirect to this page  once the user successfully authenticates.
Get the login page
Verify that the parameters are sent on to the next page correctly
Get the login page
Verify that the parameters are sent on to the next page correctly
until we set up the configuration, the LinkedIn action  button should not be visible
now we should see it
now we should not see it because we are in a microsite
Convert relative URL paths to absolute URIs
Default cache timeout
Always use the cached "real" instance if available
Lookup cached instance
Already patched
Try and construct a User instance from data stored in the cache
Raise an exception to fall through to the except clause below.
Fallback to constructing the User from the database.
Try and construct instance from dictionary
Ensure instance knows that it already exists in the database,  otherwise we will fail any uniqueness checks when saving the  instance.
Specify database so that instance is setup correctly. We don't  namespace cached objects by their origin database, however.
Error when deserialising - remove from the cache; we will  fallback and return the underlying instance
Harmless to save, but saves space in the dictionary - we already know  the primary key when we lookup
Avoid problems with serializing FileFields  by only serializing the file name
pylint: disable=protected-access
although deprecated keys allowed run=None, new keys don't if there is no version.
If we can't find a 'general' CACHE defined in settings.py, we simply fall back  to returning the default cache. This will happen with dev machines.
Don't use the cache.
Skip the throttle check entirely if we've disabled rate limiting.  Otherwise, perform the checks (as usual)
No-op if the class isn't a Django Rest Framework view.
If we ARE explicitly disabling rate limiting,  modify the class to always allow requests.  Note that this overrides both rate limiting applied  for the particular view, as well as global rate limits  configured in Django settings.
Resolve a URL so that the new urlconf gets loaded
Reload only the root urls.py
Reload urls from my_app
Reload urls from my_app and another_app
pylint: disable=protected-access
Clean for whitespace and control characters, which  cause memcache to raise an exception
Attempt to combine the prefix, version, and key
If the total length is too long for memcache, hash it
Return the result
Translators: the translation for "LONG_DATE_FORMAT" must be a format  string for formatting dates in a long form.  For example, the  American English form is "%A, %B %d %Y".  See http://strftime.org for details.
Translators: the translation for "DATE_TIME_FORMAT" must be a format  string for formatting dates with times.  For example, the American  English form is "%b %d, %Y at %H:%M".  See http://strftime.org for details.
This only happens if the string ends with a %, which is not legal.
All the other format codes: just let built-in strftime take  care of them.
Now that we are done defining constants, we have to restore the real pgettext  so that the functions in this module will have the right definition.
add requirement course milestone
add fulfillment course milestone
add milestones if pre-requisite course is selected
If there are required courses, add them to the result dict.
we have not seeded milestone relationship types
Get all of the outstanding milestones for this course, for this user
In debug mode let django process the 500 errors and display debug info for the developer
Display custom 500 page if either    1. test_func is None (meaning nothing to test)    2. or test_func(request) returns True
Do not show custom 500 error when test fails
As of 2012-05-08, Zendesk is using a CA that is not  installed on our servers
Tag all issues with LMS to distinguish channel in Zendesk; requested by student support team
Per edX support, we would like to be able to route white label feedback items  via tagging
Support uses Zendesk groups to track tickets. In case we  haven't been able to correctly group this ticket, log its ID  so it can be found later.
Do not proceed without parameters: Compatibility check with existing tests  that do not supply these parameters
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
TransactionManagementError used below actually *does* derive from the standard "Exception" class.  pylint: disable=nonstandard-exception
Tests in TestCase subclasses are wrapped in an atomic block to speed up database restoration.  So we must disabled this manager.  https://github.com/django/django/blob/1.8.5/django/core/handlers/base.pyL129-L132
This will set the transaction isolation level to READ COMMITTED for the next transaction.
Commit transaction  An error during rollback means that something  went wrong with the connection. Drop it.
Roll back transaction  An error during rollback means that something  went wrong with the connection. Drop it.
Outermost block exit when autocommit was enabled.
Decorator: @commit_on_success(...) or context manager: with commit_on_success(...): ...
Otherwise, this shouldn't be nested in any atomic block.
This will set the transaction isolation level to READ COMMITTED for the next transaction.
Decorator: @outer_atomic(...) or context manager: with outer_atomic(...): ...
If a file already exists with the supplied name, file_storage will make the filename unique.
The setting name used for events when "settings" (account settings, preferences, profile information) change.
Object is new, so fields haven't technically changed.  We'll return  an empty dict as a default value.
Country is not JSON serializable.  Return the country code.
Remove the now inaccurate _changed_fields attribute.
Compute the maximum value length so that two copies can fit into the maximum event size  in addition to all the other fields recorded.
do not rebind the module if it's already bound to a user.
If ALLOWED_HOSTS is set properly, and the host is valid, we just return the user-provided host
If ALLOWED_HOSTS is set properly but the host is invalid, we should get a SuspiciousOperation
Test whitespace, control characters, and some non-ASCII UTF-16
Numeric key
Numeric prefix
Numeric version
Choose lengths close to memcached's cutoff (250)
Generate a key of that length
Make the key safe
The key should now be valid
Long key
Long prefix
Long version
Generate a key with that character
Make the key safe
The key should now be valid
Generate a prefix with that character
Make the key safe
The key should now be valid
Generate a version with that character
Make the key safe
The key should now be valid
Check the length
Check that there are no spaces or control characters
Verify the file was deleted.
Verify the file still exists
strftime doesn't like Unicode, so do the work in UTF8.
fetch non existing org
Enable rate limiting using model-based config
By default, should enforce rate limiting  Since our fake throttle always rejects requests,  we should expect the request to be rejected.
Disable rate limiting using model-based config
With rate-limiting disabled, the request  should get through.  The `check_throttles()` call  should return without raising an exception.
There should be absolutely no interaction with Zendesk
There should be absolutely no interaction with Zendesk
We'll make assets named this be importable by Python code in the sandbox.
accommodates course api urls, excluding any course api routes that do not fall under v*/courses, such as v1/blocks.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Try and load the asset.
Set the basics for this request. Make sure that the course key for this  asset has a run, which old-style courses do not.  Otherwise, this will  explode when the key is serialized to be sent to NR.
Figure out if this is a CDN using us as the origin.
Check if this content is locked or not.
Check that user has access to the content.
Figure out if the client sent us a conditional request, and let them know  if this asset has changed since then.
If the header field is syntactically invalid it should be ignored.
Only accept ranges in bytes
According to Http/1.1 spec content for multiple ranges should be sent as a multipart message.  http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.htmlsec14.16  But we send back the full content.
If Range header is absent or syntactically invalid return a full content response.
"Accept-Ranges: bytes" tells the user that only "bytes" ranges are allowed
This is a CDN request.
See if we can load this item from cache.  Not in cache, so just try and load it from the asset manager.
Parse the byte ranges.  Case 0:
Merge settings list with one in the admin config;
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Initialize the deprecated modules settings with empty list
pylint: disable=missing-docstring
Monkey-patch some social auth models' Meta class to squelch Django19 warnings.  pylint: disable=protected-access
Use this key to store a reference to the unpatched copy
Patch constants as a set instead of a list.
Do we have this feature enabled?  what time is it now?
Get the last time user made a request to server, which is stored in session data
have we stored a 'last visited' in session? NOTE: first time access after login  this key will not be present in the session data  compute the delta since last time user came to the server
did we exceed the timeout limit?  yes? Then log the user out
django_url is assigned late in the process of loading lettuce,  so we import this as a module, and then read django_url from  it to get the correct value
Unlock XBlock factories, because we're randomizing the collection  name above to prevent collisions
There is an issue with ChromeDriver2 r195627 on Ubuntu  in which we sometimes get an invalid browser session.  This is a work-around to ensure that we get a valid session.
If we were unable to get a valid session within the limit of attempts,  then we cannot run the tests.
For transcripts, you need to check an actual video, so we will  just specify our default video and see if that one is available.
A hackish way to skip a test in lettuce as there is no proper way to skip a test conditionally
No need to check all the URLs
Choose the list of handlers based on the HTTP method
Check the path (without querystring params) against our list of handlers  If we don't have a handler for this URL and/or HTTP method,  respond with a 404.
Get notes from range
Respond to request with correct lti endpoint
Send request ignoring verifirecation of SSL certificate
Show roles only for LTI launch.
Currently LTI module doublequotes the lis_result_sourcedid parameter.  Unquote response two times.
This is needed for body encoding:
Log to stdout, including debug messages
Default number of seconds to delay the response to simulate network latency.
Delay the response to simulate network latency
Construct the response content
django_comment_client calls GET comment before doing a DELETE, so that's what this is here to support.
add some configuration data
reset server configuration
ensure that server config dict is empty after successful reset
Without user
Without user
Without any pagination parameters
With pagination parameters
Delete all notes
Patch the timer async calls
Patch POST requests
Check the response we receive  (Should be the default grading response)
Configure the default response for submissions to any queue
Check the response we receive  (Should be the default grading response)
Configure the XQueue stub response to any submission to the test queue
Check that we receive the response we configured
Configure the XQueue stub with two responses that  match the same submission
Expect that we do NOT receive a response  and that an error message is logged
Expect that the response is success
Return back the header, so we can authenticate the response we receive
Check the response posted back to us  This is the default response
Check that the POST request was made with the correct params
JSON-encode each parameter
Check that the expected values were set in the configuration
Send unicode without json-encoding it
Expect success when we provide the required param
Expect failure when we do not proivde the param
Expect failure when we provide an empty param
Expect success when we provide the required param
Expect failure when we do not proivde the param
Expect failure when we provide an empty param
Check for required values
If nothing is missing, execute the function as usual
The POST dict will contain a list of values for each key.  None of our parameters are lists, however, so we map [val] --> val  If the list contains multiple entries, we pick the first one
By default, `parse_qs` returns a list of values for each param  For convenience, we replace lists of 1 element with just the element
Decode the params as UTF-8
No parameters sent to configure, so return success by default
Subclasses override this to provide the handler class to use.  Should be a subclass of `StubHttpRequestHandler`
Create a dict to store configuration values set by the client
Start the server in a separate thread
Log the port we're using to help identify port conflict errors
First call superclass shutdown()
We also need to manually close the socket
Respond only to grading requests
If the message doesn't have a header or body,  then it's malformed.  Respond with failure
If we could not decode the body or header,  respond with failure
Send an immediate response of success  The grade request is formed correctly
Wait a bit before POSTing back to the callback url with the  grade result configured by the server  Otherwise, the problem will not realize it's  queued and it will keep waiting for a response indefinitely
If we get a request that's not to the grading submission  URL, return an error
Send the response indicating success/failure
First check if we have a configured response that matches the submission body
Multiple matches, so abort and log an error
Fall back to the default grade response configured for this queue,  then to the default response.
Wrap the message in <div> tags to ensure that it is valid XML
If not configured, do not need to send anything
Retrieve the grader payload, which should be a JSON-encoded dict.  We pass the payload directly to the service we are notifying, without  inspecting the contents.
django_url is assigned late in the process of loading lettuce,  so we import this as a module, and then read django_url from  it to get the correct value
Settings - Schedule & Details
Settings - Advanced Settings
Content - Outline  Note that calling your org, course number, or display name, 'course' will mess this up
Pages
we ran this on the wrong page. Wait a bit, and try again, when the  browser has loaded the next page.
we ran this on the wrong page. Wait a bit, and try again, when the  browser has loaded the next page.
We got a require.js error  Sometimes requireJS will throw an error with requireType=require  This doesn't seem to cause problems on the page, so we ignore it
stick jquery at the front
Wait a bit, and try again, when the browser has reloaded the page.
If we're expecting a non-empty string, give the page  a chance to fill in text fields.
If we're expecting a non-empty string, give the page  a chance to fill in text fields.
If we're expecting a non-empty string, give the page  a chance to fill in values
Wait for the css selector to appear
Wait for the css selector to appear
Ensure that jquery is loaded
Disable jQuery animations
If the user already exists, don't try to create it again
Save the user info in the world scenario_dict for use in the tests
Note: this flag makes the user global staff - that is, an edX employee - not a course staff.  See courseware.tests.factories for StaffFactory and InstructorFactory.
Activate user  Enroll them in the course
Put in alphabetical order
We will munge 'rel-ter' to be 'rel', so the 'rel-ter'  user will actually receive the released language 'rel'  (Otherwise, the user will actually end up getting the server default)
Since we have only released "rel-ter", the requested code "rel" will  fuzzy match to "rel-ter", in addition to "rel-ter" exact matching "rel-ter"
Release es-419
If I release 'es', 'es-AR' should get 'es', not English
Release 'es-419, es, es-es'
Preview lang should always override selection.
this is the UserPreference key for the currently-active dark language, if any
-*- coding: utf-8 -*-
Converted from the original South migration 0002_enable_on_install.py
-*- coding: utf-8 -*-
If django 1.7 or higher is used, the right-side can be updated with new-style codes.  The following are the new-style language codes for chinese language
delete the session language key (if one is set)
Reset user's dark lang preference to null  Get & set user's preferred language
Get the user's preview lang - this is either going to be set from a query  param `?preview-lang=xx`, or we may have one already set as a dark lang preference.  Get the request user's dark lang preference
User doesn't have a dark lang preference, so just return
Set the session key to the requested preview lang
Make sure that we set the requested preview lang as the dark lang preference for the  user, so that the lang_pref middleware doesn't clobber away the dark lang preview.
the course that this mode is attached to
the reference to this mode that can be used by Enrollments to generate  similar behavior for the same slug across courses
The 'pretty' name that can be translated and displayed
the currency these prices are in, using lower case ISO currency codes
The system prefers to set this automatically based on default settings. But  if the field is set manually we want a way to indicate that so we don't  overwrite the manual setting of the field.
DEPRECATED: the `expiration_date` field has been replaced by `expiration_datetime`
DEPRECATED: the suggested prices for this mode  We used to allow users to choose from a set of prices, but we now allow only  a single price.  This field has been deprecated by `min_price`
optional description override  WARNING: will not be localized
Optional bulk order SKU for integration with the ecommerce service
Modes that allow a student to pursue a verified certificate
Modes that allow a student to pursue a non-verified certificate
Modes that allow a student to earn credit with a university partner
Modes that are allowed to upsell
Courses purchased through the shoppingcart  should be "honor". Since we've changed the DEFAULT_MODE_SLUG from  "honor" to "audit", we still need to have the shoppingcart  use "honor"
Only set explicit flag if we are setting an actual date.
Assign default modes if nothing available in the database
Filter out expired course modes if include_expired is not set
we prefer professional over verify
Professional and no-id-professional mode courses are always behind a paywall
White-label uses course mode honor with a price  to indicate that the course is behind a paywall.
Check that a free mode is available.
the course that this mode is attached to
the reference to this mode that can be used by Enrollments to generate  similar behavior for the same slug across courses
The 'pretty' name that can be translated and displayed
minimum price in USD that we would like to charge for this mode of the course
the suggested prices for this mode
the currency these prices are in, using lower case ISO currency codes
turn this mode off after the given expiration date
pylint seems to dislike as_view() calls because it's a `classonlymethod` instead of `classmethod`, so we disable the warning
Check whether the user has access to this course  based on country access rules.
If a user has already paid, redirect them to the dashboard.
The user will have already been enrolled in the audit mode at this  point, so we just redirect them to the dashboard, thereby avoiding  hitting the database a second time attempting to enroll them.
Validate the amount passed in and force it into two digits
Check for minimum pricing
Try pulling querystring parameters out of the request
Attempt to create the new mode for the given course
Return a success message and a 200 response
need to keep legacy modes around for awhile
django admin saving the date with default timezone to avoid time conversion from form to db  changes its tzinfo to UTC
Verification deadlines are allowed only for verified modes
Verification deadline must be after the upgrade deadline,  if an upgrade deadline is set.  There are cases in which we might want to set a verification deadline,  but not an upgrade deadline (for example, a professional education course that requires verification).
Since the verification deadline is stored in a separate model,  we need to handle saving this ourselves.  Note that verification deadline can be `None` here if  the deadline is being disabled.
Display a more user-friendly name for the custom expiration datetime field  in the Django admin list view.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Create a new course mode from django admin page
Verify that the expiration datetime is the same as what we set  (hasn't changed because of a timezone translation).
Configure a verification deadline for the course
Configure a course mode with both an upgrade and verification deadline  and load the form to edit it.
Configure a verification deadline for the course
Create the course mode Django admin form
Update the verification deadline form data  We need to set the date and time fields separately, since they're  displayed as separate widgets in the form.
Check that the deadline was updated
Configure a verification deadline for the course
Create the course mode Django admin form
Use the form to disable the verification deadline
Check that the deadline was disabled
Only the verified mode should have a verification deadline set.  Any other course mode should raise a validation error if a deadline is set.
Factories are self documenting  pylint: disable=missing-docstring
Create the course modes
Enroll the user in the test course
Configure whether we're upgrading or not
Check whether we were correctly redirected
Create the course modes
Enroll the user in the test course
Create the course modes
User visits the track selection page directly without ever enrolling
Create the course modes
Enroll the user in the test course to emulate  automatic enrollment
Verify that the prices render correctly
Create the course modes
Check whether credit upsell is shown on the page  This should *only* be shown when a credit mode is available
The only course mode is professional ed
Go to the "choose your track" page
Since the only available track is professional ed, expect that  we're redirected immediately to the start of the payment flow.
Now enroll in the course
Expect that this time we're redirected to the dashboard (since we're already registered)
Choose the mode (POST request)
Create the course modes
Choose the mode (POST request)
Expect that the contribution amount is stored in the user's session
Create the course modes
Enroll the user in the default mode (honor) to emulate  automatic enrollment
Explicitly select the honor mode (POST request)
Verify that the user's enrollment remains unchanged
Create the supported course modes
Choose an unsupported mode (POST request)
Hit the mode creation endpoint with no querystring params, to create an honor mode
Create an honor mode
Create a verified mode
Create the course modes
Load the track selection page
Verify that the header navigation links are hidden for the edx.org version
URL-encoded version of 1/1/15, 12:00 AM
Construct the URL for the track selection page
shouldn't be able to find a corresponding course
no modes, should get 0
verify that the professional mode is preferred
Has no payment options.
Now we do have a payment option.
Remove the verified option.
Finally, give the honor mode payment options
Has payment options.
Create the modes and min prices
Verify that we can or cannot auto enroll
Verify that the proper auto enroll mode is returned
Unexpired, no expiration date
Unexpired, expiration date in future
Expired
Check that we get a default mode for when no course mode is available
check that tuple has professional mode with None
check that mode slug is verified or not
Create the course modes
Check the selectable modes, which should exclude credit
When we get all unexpired modes, we should see credit as well
The following assumes that the rows with the most recent date also have the highest IDs
The number of seconds
Translators: this label indicates the name of the user who made this change:
Disable caching while testing the API
When a view call fails due to a permissions error, it raises an exception.  An uncaught exception breaks the DB transaction for any following DB operations  unless it's wrapped in a atomic() decorator or context manager.
Return the currently active configuration
Set the requesting user as the one who is updating the configuration
Don't allow deletion of configuration
Make all fields read-only when editing an object
Show only the most recent row for each key.
Don't add the message if course_message is blank.
We don't have a course-specific message, so pass.
Clear the cache between test runs.
When we don't have any data set.
-*- coding: utf-8 -*-
The current() value for GlobalStatusMessage is cached.
Check that the user specified is either the same user, or this is a server-to-server request.  Return a 404 instead of a 403 (Unauthorized). If one user is looking up  other users, do not let them deduce the existence of an enrollment.
Lookup the user, instead of using request.user, since request.user may not match the username POSTed.
Will reactivate inactive enrollments.
Enroll a user test@example.com into the demo course
If the user is already enrolled in the course, do nothing.
Second run does not impact the first run (i.e., the  user is still enrolled, no exception was raised, etc)
The cache backend could raise an exception (for example, memcache keys that contain spaces)
Catch any unexpected errors during caching.
If the client has requested an enrollment deactivation, we want to include expired modes  in the set of available modes. This allows us to unenroll users from expired modes.
We retrieve the settings in-line here (rather than using the  top-level constant), so that @override_settings will work  in the test suite.
Find deleted courses and filter them out of the results
Corresponding information to help resolve the error.
Default (no course modes in the database)  Expect automatically being enrolled as "honor".
Audit / Verified / Honor  We should always go to the "choose your course" page.  We should also be enrolled as "honor" by default.
Check for professional ed happy path.
Add a fake course enrollment information to the fake data API  Enroll in the course and verify that we raise CourseModeNotFoundError
Add a fake course enrollment information to the fake data API  Enroll in the course and verify the URL we get sent to
Default (no course modes in the database)  Expect that users are automatically enrolled as "honor".
Audit / Verified / Honor  We should always go to the "choose your course" page.  We should also be enrolled as "honor" by default.
Check for professional ed happy path.
Add a fake course enrollment information to the fake data API
No enrollments
Enroll in the course and verify the URL we get sent to
Add fake course enrollment information to the fake data API
Hit the fake data API.
Reset the fake data API, should rely on the cache.
The data matches
Default (no course modes in the database)  Expect that users are automatically enrolled as "honor".
Audit / Verified / Honor  We should always go to the "choose your course" page.  We should also be enrolled as "honor" by default.
Create the course modes (if any) required for this test case
Confirm the returned enrollment and the data match up.
Enroll the user in the course
Determine that the returned enrollment is inactive.
Expect that we're no longer enrolled
No course modes, no course enrollments.
Audit / Verified / Honor course modes, with three course enrollments.
No course modes, no course enrollments.
Audit / Verified / Honor course modes, with three course enrollments.
Create all the courses
Create the original enrollment.
Compare the created enrollments with the results  from the get enrollments request.
Default (no course modes in the database)  Expect that users are automatically enrolled as "honor".
Audit / Verified / Honor  We should always go to the "choose your course" page.  We should also be enrolled as "honor" by default.
Try to get an enrollment before it exists.
Default (no course modes in the database)  Expect that users are automatically enrolled as "honor".
Audit / Verified / Honor  We should always go to the "choose your course" page.  We should also be enrolled as "honor" by default.
Verify that an audit message was logged.
If multiple enrollment calls are made in the scope of a  single test, we want to validate that audit messages are  logged for each call.
Pass emit_signals when creating the course so it would be cached  as a CourseOverview.
Default (no course modes in the database)  Expect that users are automatically enrolled as the default
Audit / Verified  We should always go to the "choose your course" page.  We should also be enrolled as the default.
Create the course modes (if any) required for this test case
Create an enrollment
Create the prod ed mode.
Enroll in the course, this will fail if the mode is not explicitly professional.
Log out, so we're no longer authenticated
Try to enroll, this should fail.
Log out the default user, Bob.
Create a user account
Log in with the unactivated account
Deactivate the user. Has to be done after login to get the user into the  request and properly logged in.
Enrollment should succeed, even though we haven't authenticated.
Verify that the server still has access to this endpoint.
Load a CourseOverview. This initial load should result in a cache  miss; the modulestore is queried and course metadata is cached.
Check enrollment list course details
Create a professional ed course mode.
Create an enrollment
Create a honor mode for a course.
Create a verified mode for a course.
Ensure that both course modes are returned
Ensure that only one course mode is returned and that it is honor
Create an honor and verified mode for a course. This allows an update.
Create an enrollment
Create an enrollment
Create an enrollment
Create an honor and verified mode for a course. This allows an update.
Create a 'verified' enrollment
Configure a set of modes for the course.
Create an enrollment with the selected mode.
Verify that a non-Boolean enrollment status is treated as invalid.
Verify that the enrollment has been deactivated, and that the mode is unchanged.
Verify that enrollment deactivation is idempotent.
Verify that omitting the mode returns 400 for course configurations  in which the default mode doesn't exist.
Create verified enrollment.
Deactivate enrollment.
Create a default and a verified mode for a course. This allows an update.
Create an enrollment
simulate the server-server api call under test
call should have succeeded
Load a CourseOverview. This initial load should result in a cache  miss; the modulestore is queried and course metadata is cached.
Expect an error response
Expect that the redirect URL is included in the response
Verify that we were not enrolled
Use the helper to setup the embargo and simulate a request from a blocked IP address.
Clear the cache to remove the effects of previous embargo tests
Update the user's profile, linking the user to the embargoed country.
Setup the embargo
Verify that users without black-listed country codes *can* be enrolled
Verify that we were enrolled
Expect that the request gets through successfully,  passing the CSRF checks (including the referer check).
get best association
not necesary, keys will timeout
not necesary, keys will timeout
Default to a `None` response, indicating that external auth  is not handling the request.
SSL login doesn't require a view, so redirect  branding and allow that to process the login if it  is enabled and the header is in the request.
If CAS is enabled, redirect auth handling to there
Redirect to branding to process their certificate if SSL is enabled  and registration is disabled.
save this for use by student.views.create_account
default conjoin name, no spaces, flattened to ascii b/c django can't handle unicode usernames, sadly  but this only affects username, not fullname
detect if full name is blank and ask for it from user
validate provided mail and if it's not valid ask the user
try the direct apache2 SSL key
Just to make sure we're calling this only at MIT:
no certificate information - go onward to main index
couldn't find the course, will just return vanilla signin page
now the dispatching conditionals.  Only shib for now
Default fallthrough to normal signin page
couldn't find the course, will just return vanilla registration page
now the dispatching conditionals.  Only shib for now  shib-login takes care of both registration and login flows
Default fallthrough to normal registration page
construct sreg response
not using OpenID attribute exchange extension
construct ax response
get and add extensions
create http response from OpenID response
add OpenID headers to response
not using trusted roots
don't allow empty trust roots
ensure trust root parses cleanly (one wildcard, of form *.foo.com, etc.)
don't allow empty return tos
ensure return to is within trust root
check that the root matches the ones we trust
make and validate endpoint
initialize store and server
don't allow invalid and non-trusted trust roots
checkid_immediate not supported, require user interaction
user failed login on previous attempt
OpenID response
don't allow invalid trust roots
authentication succeeded, so fetch user information  that was requested  remove error from session since login succeeded
redirect user to return_to location
Note too that this is hardcoded, and not really responding to  the extensions that were registered in the first place.
the request succeeded:
display login page
add custom XRDS header necessary for discovery process
custom XRDS header necessary for discovery process
custom XRDS header necessary for discovery process
-*- coding: utf-8 -*-
assert that we are logged in
Now that we are logged in, make sure we don't see the registration page
Test that they do signin if they don't have a cert
Call decorated mock function to make sure it passes  the call through without hitting the external_auth functions and  thereby creating an external auth map object.
Test logged in user gets called
Make sure that even though we logged out, we have logged back in
For the sake of python convention we'll make all of these variable names ALL_CAPS  These values would all returned from request.META, so they need to be str, not unicode
no audit logging calls
no audit logging calls
identity k/v pairs will show up in request.META
First we pop the registration form  Then we have the user answer the registration form  These are unicode because request.POST returns unicode
use RequestFactory instead of TestClient here because we want access to request.user
check that the created user has the right email, either taken from shib or user input
temporarily set the branch to draft-preferred so we can update the course
Tests the two case for courses, limited and not
method = 'POST'  undo the URL encoding of the POST arguments
method = 'GET'
the provider URL must be converted to an absolute URL in order to be  used as an openid provider.
now we can begin the login process by invoking a local openid client,  with a pointer to the (also-local) openid provider:
the provider URL must be converted to an absolute URL in order to be  used as an openid provider.
override the default args with any given arguments
try logging in 30 times, the default limit in the number of failed  log in attempts before the rate gets limited
verify that we are not returning the default 403  clear the ratelimit cache so that we don't fail other logins
call url again, this time with username and password
call url again, this time with username and password
login to the client so that we can persist session information
login once to get the right session information  We trigger situation where user is not active at final phase of  OpenId login.
call url again, this time with username and password
the provider URL must be converted to an absolute URL in order to be  used as an openid provider.
Wait until we get the result
Access the service status page, which starts a delayed  asynchronous task
HTTP response should be successful
Expect to get a JSON-serialized dict with  task and time information
Was it successful?
We should get a "pong" message back
We don't know the other dict values exactly,  but we can assert that they take the right form
Register signal handlers  pylint: disable=unused-import
By default use the statsd agent
Not all arguments are documented.  Look at the source code for details.
The settings SITE_NAME may contain a port number, so we need to  parse the full URL.
Construct the fake request.  This can be used to construct absolute  URIs to other paths.
The course to embargo
Whether or not to embargo
pylint: disable=no-member
The countries to embargo
checking is_restricted_course method also here to make sure course exists in the list otherwise in case of  no course found it will throw the key not found error on 'disable_access_check'
First check the cache to see if we already have  a URL for this (course_key, access_point) tuple
If there's a cache miss, we'll need to retrieve the message  configuration from the database
First check whether this is a restricted course.  The list of restricted courses is cached, so this does  not require a database query.
If the country code is not in the list of all countries,  we don't want to automatically exclude the user.  This can happen, for example, when GeoIP falls back  to using a continent code because it cannot determine  the specific country.
Retrieve all rules in one database query, performing the "join" with the Country table
If there are no whitelist countries, default to all countries
Consolidate the rules into a single list of countries  that have access to the course.
This restriction ensures that a country is on  either the whitelist or the blacklist, but  not both (for a particular course).
If a restricted course changed, we need to update the list  of which courses are restricted as well as any rules  associated with the course.
If the restricted course and its rules are being deleted,  the restricted course may not exist at this point.  However, the cache should have been invalidated  when the restricted course was deleted.
Invalidate the cache of countries for the course.
Hook up the cache invalidation receivers to the appropriate  post_save and post_delete signals.
Backwards compatibility with themes created for  earlier implementations of the embargo app.
The access point determines which set of messages to use.  This allows us to show different messages to students who  are enrolling in a course than we show to students  who are enrolled and accessing courseware.
Is this an valid ip address?
Clear the cache to ensure that previous tests don't interfere  with this test.
Remove all existing rules for the course
Create the country object  Ordinarily, we'd create models for every country,  but that would slow down the test suite.
Create a model for the restricted course
Ensure that there is a blacklist rule for the country
Simulate that the user is coming from the blacklisted country
Yield the redirect url so the tests don't need to know  the embargo messaging URL structure.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
No-op if the country access feature is not enabled
Always give global and course staff access, regardless of embargo settings.
Retrieve the country code from the IP address  and check it against the allowed countries list for a course
Retrieve the country code from the user's profile  and check it against the allowed countries list for a course.
A user-facing description of the message
The mako template used to render the message
Backwards compatibility with themes  created for earlier implementations of the embargo app.
Clear the cache to prevent interference between tests
Configure the access rules
Configure the user's profile country
Appear to make a request from an IP in a particular country  Call the API.  Note that the IP address we pass in doesn't  matter, since we're injecting a mock for geo-location
Verify that the access rules were applied correctly
The user is set to None, because the user has not been authenticated.
The user is set to None, because the user has not been authenticated.
No restricted course model for this course key,  so all access checks should be skipped.
The second check should require no database queries
Test the scenario that will go through every check  (restricted course, but pass all the checks)
Verify that we can check the user's access without error
Test the scenario that will go through every check  (restricted course, but pass all the checks)  This is the worst case, so it will hit all of the  caching code.
Add a country to the blacklist
Appear to make a request from an IP in the blocked country
Expect that the user is blocked, because the user isn't staff
Add the user to the role
Now the user should have access
Retrieve the URL to the blocked message page
The first time we retrieve the message, we'll need  to hit the database.
The second time, we should be using cached values
No restrictions for the course
Use a default path
Retrieve the URL once, populating the cache with the list  of restricted courses.
Delete the restricted course entry
Clear the message URL cache
Try again.  Even though the cache results are stale,  we should still get a valid URL.
Explicitly import the cache from ConfigurationModel so we can reset it after each test
Invalid format for the course key
Validation shouldn't work
Explicitly clear ConfigurationModel's cache so tests have a clear cache  and don't interfere with each other
No custom override specified for the "default" message
Test that course is not authorized by default
Authorize
Now, course should be embargoed
Azerbaijan and France should not be blocked  Gah block USA and Antartica
Block
Change embargo - block Isle of Man too
Warm the cache
it should come from cache
it should come from cache
deleting an object will delete cache also.and hit db on  get the is_restricted course
it should come from cache
Warm the cache
Deleting an object will invalidate the cache
Delete the first rule
Delete the second rule
Create a rule
Delete the course (and, implicitly, all the rules)
Change the message key
Expect a history entry with the changed keys
Check that the record is for the correct course
Load the history entry and verify the message keys
For each rule, check that there is an entry  in the history record.
Check that there are no duplicate entries
Clear the cache to avoid interference between tests
Add the course to the list of restricted courses  but don't create any access rules
Expect that we can access courseware
Ensure that IP blocking works for anonymous users
Set up the IP rules
Check that access is enforced
Blacklist an IP address
Whitelist an IP address
Expect that we were still able to access the page,  even though we would have been blocked by country  access rules.
Make the user staff so that it has permissions to access the views.
Blacklist an IP address
Test with a fully-restricted course
Don't block the embargo message pages; otherwise we'd  end up in an infinite redirect loop.
Don't block the Django admin pages.  Otherwise, we might  accidentally lock ourselves out of Django admin  during testing.
Do not block access to course metadata. This information is needed for  sever-to-server calls.
If embargoing is turned off, make this middleware do nothing
Never block certain patterns by IP address
If the IP is blacklisted, reject.  This applies to any request, not just courseware URLs.
If the IP is whitelisted, then allow access,  skipping later checks.
Otherwise, perform the country access checks.  This applies only to courseware URLs.
Created is the time this action was initiated
Updated is the last time this entry was modified
Course that is being acted upon
Action that is being taken on the course
Current state of the action.
MANAGERS
WARNING - when you edit this value, you're also modifying the max_length  of the `message` column (see below)
Whether or not the status should be displayed to users
Message related to the status
FIELDS  Original course that is being rerun
Display name for destination course
MANAGERS  Override the abstract class' manager with a Rerun-specific manager that inherits from the base class' manager.
-*- coding: utf-8 -*-
initiate
set state to succeed
dismiss ui and verify
initiate
set state to fail
dismiss ui and verify
Sequence of Action models to be tested with ddt.
create course action states for all courses
some state changes may not be user-initiated so override the user field only when provided
update any additional fields in kwargs
Calculate the full URL, including any hashes added to the filename by the pipeline.  This will also include the base static URL (for example, "/static/") and the  ".js" extension.
To make the string comparision easy remove the whitespaces
Verify the default behavior
Verify that raw keyword causes raw URLs to be emitted
Verify that a single JS file is rendered with the pipeline enabled
Verify that multiple JS files are rendered with the pipeline disabled
invalid_client isn't really the right code, but this mirrors  https://github.com/edx/django-oauth2-provider/blob/edx/provider/oauth2/forms.pyL331
Ensure user does not re-enter the pipeline
pylint: disable=no-member
Initialize to minimal data
This is generally the same thing as the UID, expect when one backend is used for multiple providers
Details about the user sent back from the provider.
To be precise, it's set by AUTHENTICATION_BACKENDS - which aws.py sets from THIRD_PARTY_AUTH_BACKENDS
To allow instances to avoid storing secrets in the DB, the secret can also be set via Django:
Remove the prefix from the UID
To allow instances to avoid storing keys in the DB, the key pair can also be set via Django:
To allow instances to avoid storing keys in the DB, the private key can also be set via Django:
This provider is not visible to users
LTI login cannot be initiated by the tool provider
Remove the prefix from the UID
Whitelisted URL query parameters retrained in the pipeline session.  Params not in this whitelist will be silently dropped.
Inject exception middleware to make redirects fire.
Where to send the user if there's an error during social authentication  and we cannot send them to a more specific URL  (see middleware.ExceptionMiddleware).
Where to send the user once social authentication is successful.
Required so that we can use unmodified PSA OAuth2 backends:
We let the user specify their email address during signup.
Disable exceptions by default for prod so you get redirect behavior  instead of a Django error page. During development you may want to  enable this when you want to get stack traces rather than redirections.
Clean any partial pipeline data
Save validated LTI parameters (or None if invalid or not submitted)
Set a auth_entry here so we don't have to receive that as a custom parameter
We do this import internally to avoid initializing settings prematurely
'next' may be set to '/account/finish_auth/.../' if this user needs to be auto-enrolled  in a course. Otherwise, just redirect them to the dashboard, which displays a message  about activating their account.
At this point, we know 'name' is not set in a [OAuth2|LTI|SAML]ProviderConfig row.  It's probably a global Django setting like 'FIELDS_STORED_IN_SESSION':
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
The following are various possible values for the AUTH_ENTRY_KEY.
Entry modes into the authentication process by a remote API call (as opposed to a browser session).
User has authenticated with the third party provider but we don't know which edX  account corresponds to them yet, if any.
User has authenticated with the third party provider and now wants to finish  creating their edX account.
Pass the username, email, etc. via query params to the custom entry page:
Only return the user matched by email if their email has been activated.  Otherwise, an illegitimate user can create an account with another user's  email address and the legitimate user would now login to the illegitimate  account.
Users may want to view/edit the providers used for authentication before they've  activated their account, so we allow inactive users.
We are querying permissions for a user other than the current user.  Return a 403 (Unauthorized) without validating 'username', so that we  do not let users probe the existence of other user accounts.
provider existence checking
build our query filters  When using multi-IdP backend, we only retrieve the ones that are for current IdP.  test if the current provider has a slug  if yes, we add a filter for the slug on uid column
doesn't have access token or no provider_id specified
These users will be created and linked to third party accounts:
The "testshib:" prefix is stored in the UserSocialAuth.uid field but should  not be present in the 'remote_id', since that's an implementation detail:
Login as a super user
Get baseline provider count
Create a provider
Get the provider instance with active flag
Remove the icon_image from the POST data, to simulate unchanged icon_image
Change the name, to verify POST
Post the edit form: expecting redirect
Editing the existing provider creates a new provider instance
Ensure the icon_image was preserved on the new provider instance
This is ultimately probablistic since we could randomly select a good character 100000 consecutive times.
Also check the row ID. Note this 'id' changes whenever the configuration does:
Enable two providers - Google and LinkedIn:
Also check the row ID. Note this 'id' changes whenever the configuration does:
Define some XML namespaces:
Test two slightly different key pair export formats
To test an OAuth1 provider, we need to patch an additional method:
Check that the user was created correctly
Mock out HTTP requests that may be made to TestShib:
Configure the SAML library to use the same request ID for every request.  Doing this and freezing the time allows us to play back recorded request/response pairs
The SAML provider (TestShib) will authenticate the user, then get the browser to POST a response:
For the Dummy provider, the provider redirect URL is self.complete_url
Provider information:  Information about the user expected from the provider:
Now check that we can login again, whether or not we have yet verified the account:
Now check that we can login again:
Override setUp and set this:
Request malformed -- just one of email/password given.
Request well-formed and credentials good.
Request well-formed but credentials bad.
The combined login/registration page dynamically generates the login button,  but we can still check that the provider name is passed in the data attribute  for the container element.
pylint: disable=protected-access
The combined login/registration page dynamically generates the register button,  but we can still check that the provider name is passed in the data attribute  for the container element.
Instrument the pipeline to get to the dashboard with the full  expected state.
First we expect that we're in the unlinked state, and that there  really is no association in the backend.
We should be redirected back to the complete page, setting  the "logged in" cookie for the marketing site.
Set the cookie and try again
Fire off the auth pipeline to link.
Now we expect to be in the linked state, with a backend entry.
We're already logged in, so simulate that the cookie is set correctly
Instrument the pipeline to get to the dashboard with the full  expected state.
First we expect that we're in the linked state, with a backend entry.
Fire off the disconnect pipeline to unlink.
Now we expect to be in the unlinked state, with no backend entry.
pylint: disable=protected-access
Begin! Ensure that the login form contains expected controls before  the user starts the pipeline.
The pipeline starts by a user GETting /auth/login/<provider>.  Synthesize that request and check that it redirects to the correct  provider page.
Next, the provider makes a request against /auth/complete/<provider>  to resume the pipeline.  pylint: disable=protected-access
At this point we know the pipeline has resumed correctly. Next we  fire off the view that displays the login form and posts it via JS.
Next, we invoke the view that handles the POST, and expect it  redirects to /auth/complete. In the browser ajax handlers will  redirect the user to the dashboard; we invoke it manually here.
We should be redirected back to the complete page, setting  the "logged in" cookie for the marketing site.
Set the cookie and try again
First, create, the request and strategy that store pipeline state.  Mock out wire traffic.
Begin! Grab the registration page and check the login control on it.
The pipeline starts by a user GETting /auth/login/<provider>.  Synthesize that request and check that it redirects to the correct  provider page.
Next, the provider makes a request against /auth/complete/<provider>.  pylint: disable=protected-access
At this point we know the pipeline has resumed correctly. Next we  fire off the view that displays the registration form.
The user must not exist yet...
At this point the user object exists, but there is no associated  social auth.
We should be redirected back to the complete page, setting  the "logged in" cookie for the marketing site.
pylint: disable=protected-access
Create twice: once successfully, and once causing a collision.
Dict of string -> object. Information about the token granted to the  user. Override with test values in subclass; None to force a throw.
Dict of string -> object. Information about the user themself. Override  with test values in subclass; None to force a throw.
Now our custom login/registration page must resume the pipeline:
Explicitly set a server name that is compatible with all our providers:  (The SAML lib we use doesn't like the default 'testserver' as a domain)
Providers are only enabled via ConfigurationModels in the database
Guard against submitting a conf change that's convenient in dev but  bad in prod.
In facebook responses, the "id" field is used as the user's identifier
In google-oauth2 responses, the "email" field is used as the user's identifier
Fall back to django settings's SOCIAL_AUTH_LOGIN_ERROR_URL.
Safe because it's already been validated by  pipeline.parse_query_params. If that pipeline step ever moves later  in the pipeline stack, we'd need to validate this value because it  would be an injection point for attacker data.
Check if we have an auth entry key we can use instead
Just return the original path; don't kill everything.
Don't mess with things that end in '?raw'
if not, then assume it's courseware specific content and then look in the  Mongo-backed database
Otherwise, look the file up in staticfiles_storage, and append the data directory if needed
No namespace => no change to path
Namespace => content url
Create an unlocked image.
Create a locked image.
Create a thumbnail of the images.
Create an unlocked image in a subdirectory.
Create a locked image in a subdirectory.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
To enable the Geoinfo feature on a per-view basis, use:
If no referer is specified, we can't check if it's a cross-domain  request or not.
-*- coding: utf-8 -*-
Set the META `CROSS_DOMAIN_CSRF_COOKIE_USED` flag so  that `CsrfCrossDomainCookieMiddleware` knows to set  the cross-domain version of the CSRF cookie.
Decorate the request with Django's  `ensure_csrf_cookie` to ensure that the usual  CSRF cookie gets set.
Check whether this is a secure request from a domain on our whitelist.
this is the UserPreference key for the user's preferred language
Named tuples can be referenced using object-like variable  deferencing, making the use of tuples more readable by  eliminating the need to see the context of the tuple packing.
Intersect the list of valid language tuples with the list  of release language codes
nothing set in the session or the prefs
language set in the user preferences and not the session
Dark lang middleware should run after this middleware, so it can  set a session language as an override of the user's preference.
Setting the session language to the browser language, if it is supported.
remove any port number from the hostname
on an update case, get the original and archive it
for archiving
for archiving
look up based on the HTTP request domain name  this will need to be a full domain name match,  not a 'startswith' match
if no match, then try to find a 'default' key in Microsites
if we have a match, then set up the microsite thread local  data
cdodge: This approach will not leverage any caching, although I think only Studio calls  this
This should be cacheable (via memcache to keep consistent across a cluster)  I believe this is called on the dashboard and catalog pages, so it'd be good to optimize
we take the list of ORGs associated with this microsite from the database mapping  tables. NOTE, for now, we assume one ORG per microsite
we must have at least one ORG defined
just take the first one for now, we'll have to change the upstream logic to allow  for more than one ORG binding
cache is empty so pull template from DB and fill cache.
if no match on subdomain then see if there is a 'default' microsite defined  if so, then use that
Filter at the setting file
Get the orgs in the db
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
invalid backend path
invalid class or class name is a method
module does not have a class
load a valid class
remove microsite root directory paths first
remove microsite root directory paths first
if microsite config does not exist
if no microsite exists
if no config is set
if microsite config does not exist default config should be used
See if we are running in a Microsite *AND* we have a custom SESSION_COOKIE_DOMAIN defined  in configuration
define wrapper function for the standard set_cookie()
only override if we are setting the cookie name to be the one the Django Session Middleware uses  as defined in settings.SESSION_COOKIE_NAME
then call down into the normal Django set_cookie method
then point the HttpResponse.set_cookie to point to the wrapper and keep  the original around
handle empty string for models being created w/o fields populated
strip key before comparing
raise validation error if the use of this field says it can't be blank but it is
pylint: disable=unused-import, missing-docstring
Ignore empty values to turn-off default tracker backends
Other mongo connection arguments
By default disable write acknowledgments, reducing the time  blocking during an insert
Make timezone aware by default
The event will be lost in case of a connection error or any error  that occurs when trying to insert the event into Mongo.  pymongo will re-connect/re-authenticate automatically  during the next event.
Check if time is stored in UTC
-*- coding: utf-8 -*-
These fields are present elsewhere in the event at this point  This field is only used for Segment web analytics and does not concern researchers
documentation of fields here: https://segment.com/docs/integrations/google-analytics/  this should *only* be used on events destined for segment.com and eventually google analytics
Some duplicated fields are passed into event-tracking via the context by track.middleware.  Remove them from the event here since they are captured elsewhere.
supplement event information with additional information  about the task in which it is running.
The middleware emits an event, reset the mock to ignore it since we aren't testing that feature.
The middleware emits an event, reset the mock to ignore it since we aren't testing that feature.
The middleware normally emits an event, make sure it doesn't in this case.
We use the same expected payload for all of these types of events, but the load video event is the only  one that is not actually expected to contain a "current time" field. So we remove it from the expected  event here.
We use the same expected payload for all of these types of events, but the load video event is the  only one that is not actually expected to contain a "current time" field. So we remove it from the  expected event here.
The POST body will contain the JSON encoded event
We mostly care about the properties
Start with the context provided by Segment in the "client" field if it exists  We should tightly control which fields actually get included in the event emitted.
Build up the event context by parsing fields out of the event received from Segment
Ignore event names that are unsupported
copy the entire segment's context dict as a sub-field of our custom context dict
remove duplicate and unnecessary fields from our copy
Overlay any context provided in the properties
pylint: disable=protected-access
Reset backends
The bytes in the string on the right are utf8 encoded in the source file, so we decode them to construct  a valid unicode string.
Localize to UTC naive datetime objects
Convert to UTC datetime objects from other timezones
Reverse-sort the keys to find the longest matching prefix.
Convert edx.video.seeked to edx.video.position.changed because edx.video.seeked was not intended to actually  ever be emitted.
Not a typo. See:  http://en.wikipedia.org/wiki/HTTP_refererOrigin_of_the_term_referer
Note we are explicitly relying on python's internal caching of  compiled regular expressions here.
HTTP headers may contain Latin1 characters. Decoding using Latin1 encoding here  avoids encountering UnicodeDecodeError exceptions when these header strings are  output to tracking logs.
requestcontext should not be None.
requestcontext should be None.
link_map maps URLs from the marketing site to the old equivalent on  the Django site
special case for when we only want the root marketing URL  only link to the old pages when the marketing site isn't on
don't try to reverse disabled marketing links
see if there is an override template defined in the microsite
In various testing contexts, there might not be a current request context.
"Fix" CSRF token by evaluating the lazy object
fetch and render template
Also clear the internal caches. Ick.
Make a copy of the list of directories for each namespace.
Get rid of all the lookups.
Re-create the lookups from our saved list.
base_loader is an instance of a BaseLoader subclass
This is a mako template
Just having this makes the template load as an instance, instead of a class.
collapse context_instance to a single dictionary for mako
This used to happen when a RequestContext object was initialized but was  moved to a different part of the logic when template engines were introduced.  Since we are not using template engines we do this here.  https://github.com/django/django/commit/37505b6397058bcc3460f23d48a7de9641cd6ef0
We've enrolled the student, so make sure they have the Student role
use existing table that was originally created from django_comment_client app
pylint: disable=no-member
use existing table that was originally created from django_comment_client app
self.assertIn(student_role, another_student.roles.all())
Check a staff account because those used to get the Moderator role
-*- coding: utf-8 -*-
For now, Community TA == Moderator, except for the styling.
Use an in-memory database since this settings file is only used for updating assets
Use RequireJS optimized storage
Redirect to the test_root folder within the repo
Store the static files under test root so that they don't overwrite existing static assets
Disable uglify when tests are running (used by build.js).  1. Uglify is by far the slowest part of the build process  2. Having full source code makes debugging tests easier for developers
SERVICE_VARIANT specifies name of the variant used, which decides what JSON  configuration files are read during startup.
CONFIG_ROOT specifies the directory where the JSON configuration  files are expected to be found. If not specified, use the project  directory.
CONFIG_PREFIX specifies the prefix of the JSON configuration files,  based on the service variant. If no variant is use, don't use a  prefix.
Don't use a connection pool, since connections are dropped by ELB.
For the Result Store, use the django cache named 'celery'
When the broker is behind an ELB, use a heartbeat to refresh the  connection and to detect if it has been dropped.
Each worker should only fetch one message at a time
Things like server locations, ports, etc.
DEFAULT_COURSE_ABOUT_IMAGE_URL specifies the default image to show for courses that don't provide one
GITHUB_REPO_ROOT is the base directory  for course data
social sharing settings
Set the names of cookies shared with the marketing site  These have the same cookie domain as the session, which in production  usually includes subdomains.
Determines whether the CSRF token can be transported on  unencrypted channels. It is set to False here for backward compatibility,  but it is highly recommended that this is True for environments accessed  by end users.
Theme overrides
Push to LMS overrides
Translation overrides
Additional installed apps
Event Tracking
Secret things: passwords, access keys, etc.
Note that this is the Studio key for Segment. There is a separate key for the LMS.
Disabling querystring auth instructs Boto to exclude the querystring parameters (e.g. signature, access key) it  normally appends to every returned URL.
Datadog for events!
Video Caching. Pairing country codes with CDN URLs.  Example: {'CN': 'http://api.xuetangx.com/edx/video?s3_url='}
Use ElasticSearch for the search engine
OpenID Connect issuer ID. Normally the URL of the authentication endpoint.
Partner support link for CMS footer
Affiliate cookie tracking
Django REST framework configuration
Dummy secret key for dev/test
for consistency in user-experience, keep the value of the following 3 settings  in sync with the ones in lms/envs/common.py
email address for studio staff (eg to request course creation)
Segment - must explicitly turn it on for production
Enable URL that shows information about the status of various services
Don't autoplay videos for course authors
If set to True, new Studio users won't be able to author courses unless  edX has explicitly added them to the course creator group.
whether to use password policy enforcement or not
Turn off account locking if failed login attempts exceeds a limit
Allow editing of short description in course settings in cms
Hide any Personally Identifiable Information from application logs
Toggles the embargo functionality, which blocks users  based on their location.
Turn on/off Microsites feature
Allow creating courses with non-ascii characters in the course id
Prevent concurrent logins per user
Turn off Advanced Security by default
Turn off Video Upload Pipeline through Studio, by default
let students save and manage their annotations  for consistency in user-experience, keep the value of this feature flag  in sync with the one in lms/envs/common.py
Enable support for content libraries. Note that content libraries are  only supported in courses using split mongo.
Milestones application flag
Prerequisite courses feature flag
Toggle course entrance exams feature
Toggle platform-wide course licensing
Enable the courseware search functionality
Enable content libraries search functionality
Enable course reruns, which will always use the split modulestore
Certificates Web/HTML Views
Teams feature
Show video bumper in Studio
How many seconds to show the bumper again, default is 7 days:
Enable credit eligibility feature
Can the visibility of the discussion tab be configured on a per-course basis?
Special Exams, aka Timed and Proctored Exams
Show Language selector
Note: Ensure 'CUSTOM_COURSE_URLS' has a matching value in lms/envs/common.py
For geolocation ip database
Change 'debug' in your environment settings files - not here.
use the ratelimit backend to prevent brute force attacks
These are standard regexes for pulling out info like course_ids, usage_ids, etc.  They are used so that URLs with deprecated-format strings still work.
Forwards-compatibility with Django 1.7  It is highly recommended that you override this in any environment accessed by  end users
Ignore deprecation warnings (so we don't clutter Jenkins builds/production)
Instead of SessionMiddleware, we use a more secure version  'django.contrib.sessions.middleware.SessionMiddleware',
Instead of AuthenticationMiddleware, we use a cache-backed version  Enable SessionAuthenticationMiddleware in order to invalidate  user sessions after a password change.
This is used to set or update the user language preferences.
Allows us to dark-launch particular languages
Detects user-requested locale from 'accept-language' header in http request
needs to run after locale middleware (or anything that modifies the request context)
catches any uncaught RateLimitExceptions and returns a 403 instead of a 500
for expiring inactive sessions
use Django built in clickjacking protection
Clickjacking protection can be enabled by setting this to 'DENY'
Platform for Privacy Preferences header
Import after sys.path fixup
These are the Mixins that should be added to every XBlock.  This should be moved into an XBlock Runtime/Application object  once the responsibility of XBlock creation is moved out of modulestore - cpennington
Paths to wrapper methods which should be applied to every XBlock's FieldData.
Modulestore-level field override providers. These field override providers don't  require student context.
Path to a sandboxed Python executable.  None means don't bother.  User to run as in the sandbox.
Configurable limits.  How many CPU seconds can jailed code use?
Change DEBUG in your environment settings files, not here
Site info
Get git revision of the current file
Not a git repository
Static content
This is how you would use the textbook images locally  ("book", ENV_ROOT / "book_images"),
Messages
Don't use compression by default
Ignore tests
Symlinks used by js-test-tool
The baseUrl to pass to the r.js optimizer, relative to STATIC_ROOT.
The name of the require.js script used by your project, relative to REQUIRE_BASE_URL.
A dictionary of standalone modules to build with almond.js.
Whether to run django-require in debug mode.
A tuple of files to exclude from the compilation result of r.js.
YouTube JavaScript API
URL to get YouTube metadata
Common views
History tables
Database-backed configuration
Monitor the status of services
Testing
For CMS
Tracking
Monitoring
For asset pipelining
Theming
Site configuration for theming and behavioral modification
comment common
for course creator table
for managing course modes
Dark-launching languages
Student identity reverification
User preferences
Monitoring signals
Course action state
Credit courses
edX Proctoring
Bookmarks
programs support
Self-paced course configuration
django-oauth2-provider (deprecated)
django-oauth-toolkit
These are apps that aren't strictly needed by Studio, but are imported by  other apps that are.  Django 1.8 wants to have imported models supported  by installed apps.
Microsite configuration application
edx-milestones service
Static i18n support
Tagging
We're already logging events, and we don't want to capture user  names/passwords.  Heartbeat events are likely not interesting.
edx-ora2
edxval
Organizations App (http://github.com/edx/edx-organizations)
First attempt to only find the module rather than actually importing it,  to avoid circular references - only try to import if it can't be found  by find_module, which doesn't work with import hooks
Empty by default
FAQ url to direct users to if they upload  a file that exceeds the above size
Specify XBlocks that should be treated as advanced problems. Each entry is a  dict:        'component': the entry-point name of the XBlock.        'boilerplate_name': an optional YAML template to be used.  Specify as                None to omit.
Default to no Search Engine
Adding components in this list will disable the creation of new problems for  those advanced components in Studio. Existing problems will work fine  and one can edit them in Studio.  DEPRECATED. Please use /admin/xblock_django/xblockdisableconfig instead.
Initial delay used for retrying tasks.  Additional retries use longer delays.  Value is in seconds.
Maximum number of retries per task for errors that are not related  to throttling.
Maximum age in seconds of timestamps we will accept  when a credit provider notifies us that a student has been approved  or denied for credit.
OpenID Connect issuer ID. Normally the URL of the authentication endpoint.
5 minute expiration time for JWT id tokens issued for external API requests.
Partner support link for CMS footer
Affiliate cookie tracking
https://stackoverflow.com/questions/2890146/how-to-force-pyyaml-to-load-strings-as-unicode-objects
SERVICE_VARIANT specifies name of the variant used, which decides what YAML  configuration files are read during startup.
CONFIG_ROOT specifies the directory where the YAML configuration  files are expected to be found. If not specified, use the project  directory.
CONFIG_PREFIX specifies the prefix of the YAML configuration files,  based on the service variant. If no variant is use, don't use a  prefix.
Don't use a connection pool, since connections are dropped by ELB.
For the Result Store, use the django cache named 'celery'
When the broker is behind an ELB, use a heartbeat to refresh the  connection and to detect if it has been dropped.
Each worker should only fetch one message at a time
Delete keys from ENV_TOKENS so that when it's imported  into settings it doesn't override what was set above
collectstatic will fail if STATIC_URL is a unicode string
Additional installed apps
Disable transaction management because we are using a worker. Views  that request a task and wait for the result will deadlock otherwise.
import settings from LMS for consistent behavior with CMS  pylint: disable=unused-import
mongo connection settings
Nose Test Runner
Want static files in the same dir for running on jenkins.
For testing "push to lms"
Avoid having to run collectstatic before the unit test suite  If we don't add these settings, then Django templates that can't  find pipelined assets will raise a ValueError.  http://stackoverflow.com/questions/12816941/unit-testing-with-django-pipeline
allow for additional options that can be keyed on a name, e.g. 'trashcan'
Create tables directly from apps' models. This can be removed once we upgrade  to Django 1.9, which allows setting MIGRATION_MODULES to None in order to skip migrations.
hide ratelimit warnings while running tests
Ignore deprecation warnings (so we don't clutter Jenkins builds/production)  https://docs.python.org/2/library/warnings.htmlthe-warnings-filter  Change to "default" to see the first instance of each hit  or "error" to convert all into errors
These ports are carefully chosen so that if the browser needs to  access them, they will be available through the SauceLabs SSH tunnel
http://slacy.com/blog/2012/04/make-your-tests-faster-in-django-1-4/
No segment key
Toggles embargo on for testing
For consistency in user-experience, keep the value of this setting in sync with  the one in lms/envs/test.py
Enable a parental consent age limit for testing
Enable content libraries code for the tests
MILESTONES
ENTRANCE EXAMS
Courseware Search Index
teams feature
Dummy secret key for dev/test
API access management -- needed for simple-history to run.
Set the default Oauth2 Provider Model so that migrations can run in  verbose mode
Enable debug so that static assets are served by Django
Set REQUIRE_DEBUG to false so that it behaves like production
Fetch static files out of the pipeline's static root
Needed for the reset database management command
Enable debug so that static assets are served by Django
Use the auto_auth workflow for creating users and logging them in
Enable milestones app
Enable pre-requisite course
Enable student notes
Enable teams feature
Enable custom content licensing
Enable partner support link in Studio footer
Disable some block types to test block deprecation logic
Path at which to store the mock index
this secret key should be the same as lms/envs/bok_choy.py's
Lastly, see if the developer has any local overrides.
To see stacktraces for MongoDB queries, set this to True.  Stacktraces slow down page loads drastically (for pages with lots of queries).
Don't use S3 in devstack, fall back to filesystem
Disable noisy loggers
Skip packaging and optimization in development
By default don't use a worker, execute tasks as if they were local functions
To see stacktraces for MongoDB queries, set this to True.  Stacktraces slow down page loads drastically (for pages with lots of queries).
Needed to enable licensing on video modules
Whether to run django-require in debug mode.
See if the developer has any local overrides.
Lastly, run any migrations, if needed.
Dummy secret key for dev
You need to start the server in debug mode,  otherwise the browser will not render the pages correctly
Output Django logs to a file
set root logger level
allow for additional options that can be keyed on a name, e.g. 'trashcan'
Use the auto_auth workflow for creating users and logging them in
HACK  Setting this flag to false causes imports to not load correctly in the lettuce python files  We do not yet understand why this occurs. Setting this to true is a stopgap measure
Where to run: local, saucelabs, or grid
Lastly, see if the developer has any local overrides.
Generate a random UUID so that different runs of acceptance tests don't break each other
import settings from LMS for consistent behavior with CMS
'origin': 'git@github.com:MITx/6002x-fall-2012.git',
Make the keyedcache startup warnings go away
Dummy secret key for dev
By default don't use a worker, execute tasks as if they were local functions
To see stacktraces for MongoDB queries, set this to True.  Stacktraces slow down page loads drastically (for pages with lots of queries).
Enable URL that shows information about the status of various services
If there's an environment variable set, grab it to turn on Segment  Note that this is the Studio key. There is a separate key for the LMS.
Lastly, see if the developer has any local overrides.
Import everything from .aws so that our settings are based on those.
You never migrate a read_replica
set the default Django settings module for the 'celery' program.
Using a string here means the worker will not have to  pickle the object when using Windows.
There is a course creators admin table.
temporary landing page for a course
User API endpoints
Update session view
User creation and updating views
restful api
We need to explicitly include external Django apps that are not in LOCALE_PATHS.
enable automatic login
These views use a configuration model to determine whether or not to  display the Programs authoring app. If disabled, a 404 is returned.  Drops into the Programs authoring app, which handles its own routing.
Custom error pages  pylint: disable=invalid-name
display error page templates, for testing purposes
This will make sure the app is always imported when  Django starts so that shared_task will use this app.
Comprehensive theming needs to be set up before django startup,  because modifying django template paths after startup has no effect.
Workaround for setting THEME_NAME to an empty  string which is the default due to this ansible  bug: https://github.com/ansible/ansible/issues/4812
Calculate the location of the theme's files
Namespace the theme's static files to 'themes/<theme_name>' to  avoid collisions with default edX static files
Make sure that we don't repeatedly nest CmsFieldData instances
-*- coding: utf-8 -*-
Call get_preview_fragment directly.
Now ensure the acid_aside is not in the result
Ensure about video don't have asides
-*- coding: utf-8 -*-
Within this class, allow access to protected members of client classes.  This comes up when accessing kvs data and caches during kvs saves and modulestore writes.
parse removes the id; so, grab it before parse
Before a graceperiod has ever been created, it will be None (once it has been  created, it cannot be set back to None).
'minimum_grade_credit' cannot be set to None
force propagation to definition
Copy the filtered list to avoid permanently changing the class attribute.
Do not show giturl if feature is not enabled.
Do not show edxnotes if the feature is disabled.
Do not show video_upload_pipeline if the feature is disabled.
Do not show teams configuration if feature is disabled.
Do not show enable_ccx if feature is not enabled.
Don't filter on the tab attribute if filter_tabs is False.
Validate the values before actually setting them.
If did validate, go ahead and update the metadata
A signal that will be sent when users should be added or removed from the creator group
A signal that will be sent when admin should be notified of a pending user request
A signal that will be sent when user should be notified of change in course creator privileges
If user has been denied access, granted access, or previously granted access has been  revoked, send a notification message to the user.
If the user has gone into the 'pending' state, send a notification to interested admin.
User is defined to be unique, can assume a single entry.
Store who is making the request.
changed to unrequested or pending
-*- coding: utf-8 -*-
User is initially unrequested.
try logging in 30 times, the default limit in the number of failed  login attempts in one 5 minute period before the rate gets limited
Since we are using the default rate limit behavior, we are  expecting this to return a 403 error to indicate that there have  been too many attempts
Calling add again will be a no-op (even if state is different).
Calling add_user_with_status_granted impacts is_user_in_course_group_role.
Calling add again will be a no-op (even if state is different).
Will not "downgrade" to pending because that would require removing the  user from the authz course creator group (and that can only be done by an admin).
Users marked as is_staff will not be added to the course creator table.
Users marked as is_staff will not be added to the course creator table.
if feature is not enabled then do a quick exit
likewise if course does not have these features turned on  then quickly exit
get all sequences, since they can be marked as timed/proctored exams
filter out any potential dangling sequences
only create/update exam policy for the proctored exams
remove any associated review policy
then see which exams we have in edx-proctoring that are not in  our current list. That means the the user has disabled it
This means it was turned off in Studio, we need to mark  the exam as inactive (we don't delete!)
Force the lazy i18n values to turn into actual unicode objects
then call into the credit subsystem (in /openedx/djangoapps/credit)  to perform any 'on_publish' workflow
import here, because signal is registered at startup, but items in tasks are not yet able to be loaded
import here, because signal is registered at startup, but items in tasks are not yet able to be loaded
If the script did not complete the last time it was run,  the admin user will already exist.
Some users will be both staff and instructors. Those folks have been  added with status granted above, and add_user_with_status_unrequested  will not try to add them again if they already exist in the course creator database.
Rethrow GitExportError as CommandError for SystemExit
N.B. This code breaks many abstraction barriers. That's ok, because  it's a one-time cleanup command.  pylint: disable=protected-access
purposely avoids auth.add_user b/c it doesn't have a caller to authorize
Remove all redundant Mac OS metadata files
try getting the ElasticSearch engine
if reindexing is done during devstack setup step, don't prompt the user  in case of --setup or --all, get the list of course keys from all courses  that are stored in the modulestore
in case course keys are provided as arguments
can this query modulestore for the list of write accessible stores or does that violate command pattern?
make sure this module wasn't deleted
call delete orphans, specifying the published branch  of the course
grab the published branch of the course
assert that this orphan is present in both branches
delete this orphan from the draft branch without  auto-publishing this change to the published branch
now there should be no orphans in the draft branch, but  there should be one in published
pylint: disable=protected-access
Create a course using split modulestore
verify that course has changes.
get draft and publish branch versions
verify that draft and publish point to different versions
force publish course
verify that course has no changes
get new draft and publish branch versions
verify that the draft branch didn't change while the published branch did
verify that draft and publish point to same versions now
Create good course xml
Create course XML where TRUNCATED_COURSE.org == BASE_COURSE_ID.org  and BASE_COURSE_ID.startswith(TRUNCATED_COURSE.course)
Load up base course and verify it is available
Now load up the course with a similar course_id and verify it loads
Clear out the modulestore mappings, else when the next import command goes to create a destination  course_key, it will find the existing course and return the mongo course_key. To reproduce TNL-1362,  the destination course_key needs to be the one for split modulestore.
With the bug, this fails because the chapter's course_key is the split mongo form,  while the course's course_key is the old mongo form.
Send bad url to get course not exported
Send bad course_id to get course not exported
Setup good repo with bad course to test xml export
Test bad git remote after successful clone
get course again in order to update its children list
create a dangling usage key that we'll add to the course's children list
the course block should now point to two children, one of which  doesn't actually exist
make sure the dangling pointer was removed from  the course block's children
lack of error implies success
Temp directories (temp_dir_1: relative path, temp_dir_2: absolute path)
Clean temp directories
Test `export` management command with invalid course_id
Test `export` management command with correct course_id
check that both courses exported successfully
pylint: disable=protected-access
Run it this way:    ./manage.py cms --settings dev edit_course_tabs --course Stanford/CS99/2013_spring
Cute: translate to CommandError so the CLI error prints nicely.
-*- coding: utf-8 -*-
This should be in a class which inherits from XmlDescriptor
update db record  remove status key
delete update item from given index  soft delete course update item
update db record
return 0 if no index found
update db record
Push to all Android devices
Used utf-8-sig encoding type instead of utf-8 to remove BOM(Byte Order Mark), e.g. U+FEFF
Allow upload only if any video link is presented  Generate and save for 1.0 speed, will create subs_sub_attr.srt.sjson subtitles file in storage.
We are creating transcripts for every video source, if in future some of video sources would be deleted.  Updates item.sub with `video_name` on success.
Check for youtube transcripts presence
new value of item.sub field, that should be set in module.
youtube transcripts are of high priority than html5 by design
find rejected html5_id and remove appropriate subs from store
updates item.sub with new_name if it is successful.
subtitles file `item.sub` is not presented in the system. Nothing to copy or rename.
If `new_sub` is empty, it means that user explicitly does not want to use  transcripts for current video ids and we remove all transcripts from storage.
This is placed before has_course_author_access() to validate the location,  because has_course_author_access() raises  r if location is invalid.
use the item's course_key, because the usage_key might not have the run
NOTE: This list is disjoint from ADVANCED_COMPONENT_TYPES
Fetch the XBlock info for use by the container page. Note that it includes information  about the block's ancestors and siblings for use by the Unit Outline.
need to figure out where this item is in the list of children as the  preview will need this
The component_templates array is in the order of "advanced" (if present), followed  by the components in the order listed in COMPONENT_TYPES.
Libraries do not support discussions
Libraries do not support advanced components at this time.
usage_key's course_key may have an empty run property
Let the module handle the AJAX
unintentional update to handle any side effects of handle call  could potentially be updating actual course data or simply caching its values
Deny access if the entrance exam feature is disabled
Deny access if the user is valid, but they lack the proper object access privileges
Retrieve the entrance exam module for the specified course (returns 404 if none found)
if request contains empty value or none then save the default one.
Remove the entrance exam module for the specified course (returns 204 regardless of existence)
No other HTTP verbs/methods are supported at this time
Provide a default value for the minimum score percent if nothing specified
Confirm the course exists
Create the entrance exam section item.
Clean up any pre-existing entrance exam graders
Regex to capture Content-Range header ranges.
Do everything in a try-except block to make sure everything is properly cleaned up.
Use sessions to keep info about import progress
If the course has an entrance exam then remove it and its corresponding milestone.  current course state before import.
Get upload chunks byte ranges
no Content-Range header, so make one that will work
try-finally block for proper clean up after receiving last chunk.  This was the last chunk.
set failed stage number with negative sign in case of unsuccessful import
if we have a nested exception, then we'll show the more generic error message
an _accept URL parameter will be preferred over HTTP_ACCEPT in the header.
Only HTML or x-tgz request formats are supported (no JSON).
static tab needs its locator information to render itself as an xmodule
Tabs are identified by tab_id or locators.  The locators are used to identify static tabs since they are xmodules.  Although all tabs have tab_ids, newly created static tabs do not know  their tab_ids since the xmodule editor uses only locators to identify new objects.
original tab list in original order
the old_tab_list may contain additional tabs that were not rendered in the UI because of  global or course settings.  so add those to the end of the list.
persist the new order of the tabs
Tabs are identified by tab_id or locator
set the is_hidden attribute on the requested tab
Note for future implementations: if you delete a static_tab, then Chris Dodge  points out that there's other stuff to delete beyond this element.  This code happens to not delete static_tab so it doesn't come up.
Convert the field name to the Mongo name
note, due to the schema change we may not have a 'thumbnail_location'  in the result set
Does the course actually exist?!? Get anything from it to prove its  existence  no return it as a Bad Request response
compute a 'filename' which is similar to the location formatting, we're  using the 'filename' nomenclature since we're using a FileSystem paradigm  here. We're just imposing the Location string formatting expectations to  keep things a bit more consistent
first let's see if a thumbnail can be created
delete cached thumbnail even if one couldn't be created this time (else  the old thumbnail will continue to show)  now store thumbnail location only if we could create it
then commit the content
Make sure the item to delete actually exists.
ok, save the content into the trashcan
delete the original  remove from cache
Needed for Backbone delete/update.
points to the temporary course landing page with log in and sign up
points to the temporary edge page
All other xblocks with children have their own page
We should use the 'fields' kwarg for newer module settings/values (vs. metadata or data)
Entrance Exams: Chapter module positioning
remove first slash in asset path  otherwise it generates InvalidKeyError in case of split modulestore  If the asset was not found, it doesn't have to be deleted...
Include the data contract version  Ensure a signatories list is always returned
Some keys are not required, such as the title override...
Return a new Certificate object instance
The top-level course field is 'certificates', which contains various properties,  including the actual 'certificates' list that we're working with in this context
Now drop the certificate record
for certificate activation/deactivation, we are assuming one certificate in certificates collection.
we are assuming only one certificate in certificates collection.
Only global staff (PMs) are able to edit active certificate configuration
Only global staff (PMs) are able to delete active certificate configuration
pylint: disable=unused-variable
afaik, this is only used in lti
Default expiration, in seconds, of one-time URLs used for uploading videos.
Translators: This is the header for a CSV file column  containing URLs for video encodings for the named profile  (e.g. desktop, mobile high quality, mobile low quality)
For now, assume all studio users that have access to the course can upload videos.  In the future, we plan to add a new org-level role for video uploaders.
convert VAL's status to studio's Video Upload feature status.
Useful constants for defining predicates
wrap the generated fragment in the xmodule_editor div so that the javascript  can bind to it correctly
Determine the items to be shown as reorderable. Note that the view  'reorderable_container_child_preview' is only rendered for xblocks that  are being shown in a reorderable container, so the xblock is automatically  added to the list.
Set up the context to be passed to each XBlock's render method.
Update after the callback so any changes made in the callback will get persisted.
Perform all xblock changes within a (single-versioned) transaction
Don't allow updating an xblock and discarding changes in a single operation (unsupported by UI).  Returning the same sort of result that we do for other save operations. In the future,  we may want to return the full XBlockInfo.
set the children on the xblock
update the xblock and call any xblock callbacks
Make public after updating the xblock, in case the caller asked for both an update and a publish.  Used by Bok Choy tests and by republishing of staff locks.
Note that children aren't being returned until we have a use case.
Only these categories are supported at this time.
Change the blockID to be unique.
Children are not automatically copied over (and not all xblocks have a 'children' attribute).  Because DAGs are not fully supported, we need to actually duplicate each child as well.
specify branches when deleting orphans
Create a new one for certain categories only. Used for course info handouts.
Pre-cache has changes for the entire course because we'll need it for the ancestor info  Except library blocks which don't [yet] use draft/publish
Note that children aren't being returned until we have a use case.
this should not be calculated for Sections and Subsections on Unit page or for library blocks
Filter the graders data as needed
We need to load the course in order to retrieve user partition information.  For this reason, we load the course once and re-use it when recursively loading children.
defining the default value 'True' for delete, drag and add new child actions in xblock_actions for each xblock.
Update with gating info
Entrance exam subsection should be hidden. in_entrance_exam is  inherited metadata, all children will have it.
Note that a unit that has never been published will fall into this category,  as well as previously published units with draft content.
If year of start date is less than 1900 then reset the start date to DEFAULT_START_DATE  For old mongo courses, accessing the start attribute calls `to_json()`,  which raises a `ValueError` for years < 1900.
Treat DEFAULT_START_DATE as a magic number that means the release date has not been set
check that logged in user has permissions to this item
the page only lists staff and assumes they're a superset of instructors. Do a union to ensure.
Ordered list of roles: can always move self to the right, but need STUDIO_EDIT_ROLES to move any user left
All of the following code is for editing/promoting/deleting users.  Check that the user has STUDIO_EDIT_ROLES permission or is editing themselves:
User has STUDIO_EDIT_ROLES permission or  is currently a member of a higher role, and is thus demoting themself
Remove the user from this old role:
The user may be newly added to this course.  auto-enroll the user in the course so that "View Live" will work.
Let the module handle the AJAX
xmodules can check for this attribute during rendering to determine if  they are being rendered for preview (i.e. in Studio)
This wrapper wraps the module in the template specified above
This wrapper replaces urls in the output that start with /static  with the correct course-specific url for the static content
stick the license wrapper in front
Set up functions to modify the fragment produced by student_view  Get the raw DescriptorSystem, not the CombinedSystem
POST requests were coming in w/ these header values causing an error; so, repro error here
refetch using provided id
now put in an evil update
try json w/o required fields
test an update with text in the tail of the header
update w/ malformed html
set to valid html which would break an xml parser
now try to delete a non-existent update
now confirm that the bad news and the iframe make up single update
create a course via the view handler
check that response status is 200 not 400
check that push notifications are not sent
check posting on handouts
check that response status is 200 not 500
Create some more libraries
extra_user has not been assigned to the library so should not show up in the list:
Now extra_user should apear in the list:
Verify course URL
Verify video URL
Verify library URL
Verify child vertical type display name
had a problem where index showed course but has_access failed to retrieve it for non-staff
Add a library:
now test that url
register a non-staff member and try to delete the course branch
test access
Finally, validate the entire response for consistency
try when no notification exists
verify that we get an empty dict out
create a test notification
add an instructor to this course
create a test notification
delete nofications that are dismissed
Change 'display_coursenumber' field to None and update the course.
Assert that 'display_coursenumber' field has been changed successfully.
Perform GET request on course outline url with the course id.
Assert that response code is 200.
Assert that 'display_course_number' is being set to "" (as display_coursenumber was None).
Finally, validate the entire response for consistency
Verify that None is returned for a non-existent locator
A course with the default release date should display as "Unscheduled"
info['blocks'] should be empty here because there is nothing  published or un-published present
Delete the un-published vertical or problem so that CourseStructure updates its data
info['blocks'] should only contain the info about vertical2 which is published.  There shouldn't be any info present about un-published vertical1
A course with the default release date should display as "Unscheduled"
register a non-staff member and try to delete the course branch
A course with the default release date should display as "Unscheduled"
set mocked exception response
Start manual reindex and check error in response
results are indexed because they are published from ItemFactory
Start manual reindex
Check results remain the same
results are indexed because they are published from ItemFactory
set mocked exception response
Start manual reindex and check error in response
results are indexed because they are published from ItemFactory
set mocked exception response
Start manual reindex and check error in response
results are indexed because they are published from ItemFactory
set mocked exception response
Start manual reindex and check error in response
set mocked exception response
Start manual reindex and check error in response
register a non-staff member and try to delete the course branch
results are indexed because they are published from ItemFactory
Start manual reindex
Check results are the same following reindex
results are indexed because they are published from ItemFactory
set mocked exception response
Start manual reindex and check error in response
results are indexed because they are published from ItemFactory
set mocked exception response
Start manual reindex and check error in response
results are indexed because they are published from ItemFactory
set mocked exception response
Start manual reindex and check error in response
set mocked exception response
Start manual reindex and check error in response
Test the draft version of the container
Now publish the unit and validate again
Check for invalid 'usage_key_strings'
Check 200 response if 'usage_key_string' is correct
Reload the test course now that the exam module has been added
What we have now is a course milestone requirement and no valid fulfillment  paths for the specified user.  The LMS is going to have to ignore this situation,  because we can't confidently prevent it from occuring at some point in the future.  milestone_key_1 =
No return, so we'll just ensure no exception is thrown
call super class to setup course, etc.
Set the URL for tests
add a static tab to the course, for code coverage
JSON GET request not supported
invalid JSON POST request
get the original tab ids
make sure we have enough tabs to play around with
reorder the last two tabs
remove the middle tab  (the code needs to handle the case where tabs requested for re-ordering is a subset of the tabs in the course)
post the request
reload the course and verify the new tab order
reorder the first two tabs
post the request
find the tab
visibility should be different from new setting
reload the course and verify the new visibility setting
Check that discussion has shifted up
Note: Actual contentType for textbook.pdf in asset.json is 'text/pdf'
simulation of html page where base_url is up-to asset's main directory  and relative_path is dom element with its src  browser append relative_path with base_url
Verify valid page requests
pylint: disable=protected-access
pylint: disable=protected-access
Load the toy course.
Lock the asset
Unlock the asset
First, upload something.
Check that `import_status` returns the appropriate stage (i.e., the  stage at which import failed).
Create a non_staff user and add it to course staff only
Check that course display name have changed after import
Now check that non_staff user has his same role
Now check that non_staff user has his same role
test trying to open a tar outside of the normal data directory
the extract_dir needs to be passed as a relative dir to  import_library_from_xml
the extract_dir needs to be passed as a relative dir to  import_library_from_xml
the extract_dir needs to be passed as a relative dir to  import_library_from_xml
Import the exported library into a different content library.
Compare the two content libraries for equality.
we don't have resp.context right now,  due to bugs in our testing harness :(
should be the same, except for added ID
Save the data that we've just changed to the underlying  MongoKeyValueStore before we update the mongo datastore.
Crude check for presence of data in returned HTML
Top level missing files key
Entry missing file_name
Entry missing content_type
If extra calls are made, return a dummy
Ensure response is correct
Call get_preview_fragment directly.
Call get_preview_fragment directly.
No property name.
Verify that user_partitions is properly updated in the course.
Verify that user_partitions is still the same.
Verify that user_partitions is properly updated in the course.
Verify that user_partitions is properly updated in the course.
Verify that user_partitions is still the same.
Verify that user_partitions is still the same.
Get the actual content group information
Assert that actual content group information is same as expected one.
This used to cause an exception since the code assumed that  only one partition would be available.
pylint: disable=no-value-for-parameter
When no data is provided, expect creation prompt.
When data is provided, expect a program listing.
Enable Programs authoring interface
this comparison is a little long-handed because we need to compare user instances directly
ext_user is not currently a member of the course team, and so should  not show up on the page.
reload user from DB
reload user from DB
reload user from DB
reload user from DB
reload user from DB
reload user from DB
Verify that ext_user is not enrolled in the new course before being added as a staff member.
Enable subsection gating for the test course
create a chapter
Remove all transcripts for current module.
incorrect xml produces incorrect item category error
Verify that ufeff character is in filedata.
Test for raising `InvalidLocationError` exception.
Add a vertical
Retrieve it
XBlock messages are added by the Studio wrapper.  Make sure that "wrapper-xblock" does not appear by itself (without -message at end).
Verify that the header and article tags are still added
Add a problem beneath a child vertical
Get the preview HTML
Verify that the Studio element wrapper has been added
Add a wrapper with child beneath a child vertical
Add static tab
Now delete it. There was a bug that the delete was failing (static tabs do not exist in draft modulestore).
create a chapter
get the course and ensure it now points to this one
use default display name
non-existent boilerplate: creates a default
Add a new static tab with no explicit name
Check that its name is not None
pylint: disable=no-member
pylint: disable=no-member
Set the location, display name, and parent to be the same so we can make sure the rest of the  duplicate is equal.
pylint: disable=no-member
Create a parent chapter (for testing children of children).
create a sequential containing a problem and an html component
create problem and an html component
Create a second sequential just (testing children of children)
2 because duplicate of problem should be located before.
Test duplicating something into a location that is not the parent of the original item.  Duplicated item should appear at the end.
Uses default display_name of 'Text' from HTML component.
The sequence does not have a display_name set, so category is shown.
Now send a custom display name for the duplicate.
Create a parent chapter
create a sequential containing a problem and an html component
create problem and an html component
create a chapter
create 2 sequentials
Remove one child from the course.
Verify that the child is removed.
The sequential already has a child defined in the setUp (a problem).  Children must be on the sequential to reproduce the original bug,  as it is important that the parent (sequential) NOT be in the draft store.
move unit 1 from sequential1 to sequential2
verify children
verify children
verify children
When the problem is first created, it is only in draft (because of its category).
Publish the item
Both published and draft content should be different
The unit and its children should be private initially
Even though user_partition_id is Scope.content, it will get saved by the Studio editor as  metadata. The code in item.py will update the field correctly, even though it is not the  expected scope.
Verify the partition_id was saved.
Initially, no user_partition_id is set, and the split_test has no children.
Set the user_partition_id to 0.
Set to first group configuration.
Set to first group configuration.
Set again to first group configuration.
Set to first group configuration.
Set to an group configuration that doesn't exist.
Set to first group configuration.
group_id_to_child and children have not changed yet.
Call add_missing_groups again -- it should be a no-op.
component_handler calls modulestore.get_item to get the descriptor of the requested xBlock.  Here, we mock the return value of modulestore.get_item so it can be used to mock the handler  of the xBlock descriptor.
Have to use the right method to create the request to get the HTTP method that we want
Initialize the deprecated modules settings with empty list
calls should be same after adding two new children for split only.
in case of entrance exam subsection, header should be hidden.
sequential xblock info should not contains the key of 'is_header_visible'.
Finally, validate the entire response for consistency
Finally, validate the entire response for consistency
Finally, validate the entire response for consistency
Finally, validate the entire response for consistency
Finally, validate the entire response for consistency
exam proctoring should be enabled and time limited.
exam proctoring should be enabled and time limited.
In case the staff_only state was set, return the updated xblock.
Finally verify the state of the chapter
Check that chapter has scheduled state
Change course pacing to self paced
Check that in self paced course content has live state now
Verify drag handles always appear.
Verify that there are no add buttons for public blocks
must have name of the certificate
an empty json
in html response
pylint: disable=unused-argument
ensure that we have a course and an action state
we assume any delete requests dismiss actions from the UI
Can't dismiss a notification that doesn't exist in the first place
We remove all permissions for this course key at this time, since  no further access is required to a course that failed to be created.
The CourseRerunState is no longer needed by the UI; delete
Custom Courses for edX (CCX) is an edX feature for re-using course content.  CCXs cannot be edited in Studio (aka cms) and should not be shown in this dashboard.
If the course_access does not have a course_id, it's an org-based role, so we fall back
ignore deleted, errored or ccx courses
No need to worry about ErrorDescriptors - split's get_libraries() never returns them.
user has global access so no need to get courses from django groups
user have some old groups or there was some error getting courses from django groups  so fallback to iterating through all courses
force the start date for reruns and allow us to override start via the client
Set default language from settings and enable web certs
Creating the course raises DuplicateCourseError if an existing course with this org/name is found
Make sure user has instructor and staff access to the new course
Initialize permissions for user in the new course
verify user has access to the original course
create destination course key
verify org course and run don't already exist
Make sure user has instructor and staff access to the destination course  so the user can see the updated status for that course
Mark the action as initiated
Clear the fields that must be reset for the rerun
Rerun the course as a new celery task
Return course listing page
check that logged in user has permissions to this item (GET shouldn't require this level?)
exclude current course from the list of available courses
get and all credit eligibility requirements  pair together requirements with same 'namespace' values
if 'minimum_grade_credit' of a course is not set or 0 then  show warning message to course author.
encoder serializes dates, old locations, and instances
If the entrance exam box on the settings screen has been unchecked,  and the course has an entrance exam attached...
Perform the normal update workflow for the CourseDetails model
encoder serializes dates, old locations, and instances
update credit course requirements if 'minimum_grade_credit'  field value is changed
Additionally update any tabs that are provided by non-dynamic course views
Save the tabs into the course if they have been changed
validate data formats and update the course module.  Note: don't update mongo yet, but wait until after any tabs are changed
update the course tabs if required by any setting changes
now update mongo
Handle all errors that validation doesn't catch
stick a random digit in front
add a random ASCII character to the end
create a new group configuration for the course
django is rewriting one to the other
User not grandfathered in as an existing user, has not previously visited the dashboard page.  Add the user to the course creator admin table with status 'unrequested'.
request method is get, since only GET and POST are allowed by @require_http_methods(('GET', 'POST'))
Give the user admin ("Instructor") role for this library:
Redirect to course to login to process their certificate if SSL is enabled  and registration is disabled.
SSL login doesn't require a login view, so redirect  to course now that the user is authenticated via  the decorator.
If CAS is enabled, redirect auth handling to there
Note: the following content group configuration strings are not  translated since they are not visible to users.
Check both that the user is created, and inactive
and now we try to activate
Now make sure that the user is now actually activated
clear the cache so ratelimiting won't affect these tests
we have a constraint on unique usernames, so this should fail
we can have two users with the same password, so this should succeed
Not activated yet.  Login should fail.
Now login should work
account should not be locked out after just one attempt
do one more login when there is no bad login counter row at all in the database to  test the "ObjectNotFound" case
we want to test the rendering of the activation page when the user isn't logged in
check the the HTML has links to the right login page. Note that this is merely a content  check and thus could be fragile should the wording change on this page
These are pages that should just load when the user is logged in  (no data needed)
need an activated user
Create a new session
Not logged in.  Should redirect to login.
Logged in should work.
not logged in.  Should return a redirect.
make sure we can access courseware immediately
then wait a bit and see if we get timed out
re-request, and we should get a redirect to login page
test if user gives empty blackout date it should return true for forum_posts_allowed
Make user staff to access course listing
Change 'display_coursenumber' field and update the course.
Check if response is escaped
get courses through iterating all courses
get courses by reversing group name formats
check both course lists have same courses
Create a course and assign access roles to user.
Create a ccx course key and add assign access roles to user.
Test that CCX courses are filtered out.
Get all courses which user has access.
Verify that CCX course exists in access but filtered by `_accessible_courses_list_from_groups`.
get courses through iterating all courses
get courses by reversing group name formats
Assign & verify staff role to the user
Fetch accessible courses list & verify their count
Verify fetched accessible courses list is a list of CourseSummery instances
Now count the db queries for staff
get courses through iterating all courses
get courses by reversing group name formats
get courses through iterating all courses
Verify fetched accessible courses list is a list of CourseSummery instances and only one course  is returned
get courses by reversing group name formats
check course lists have same courses
now delete this course and re-add user to instructor group of this course
Get courses through iterating all courses
Get course summaries by iterating all courses
Get courses by reversing group name formats
Test that course list returns no course
create list of random course numbers which will be accessible to the user
time the get courses by iterating through all courses
time again the get courses by iterating through all courses
time the get courses by reversing django groups
time again the get courses by reversing django groups
test that the time taken by getting courses through reversing django groups is lower then the time  taken by traversing through all courses (if accessible courses are relatively small)
Now count the db queries
Two types of org-wide roles have edit permissions: staff and instructor.  We test both
Verify fetched accessible courses list is a list of CourseSummery instances and test expacted  course count is returned
simulate initiation of course actions
Build out local bare repo, and set course git url to it
make sure that any children with one orphan parent and one non-orphan  parent are not deleted
Get a course with orphan modules
Verify `OrphanVert` is an orphan
Verify `multi_parent_html` is child of both `Vertical1` and `OrphanVert`
Verify `OrhanChapter` is an orphan
Verify chapter1 is parent of vertical1.
Make `Vertical1` the parent of `HTML0`. So `HTML0` will have to parents (`Vertical0` & `Vertical1`)
Get parent location & verify its either of the two verticals. As both parents are non-orphan,  alphabetically least is returned
test that course 'display_name' same as imported course 'display_name'
make sure course.static_asset_path is correct
make sure we have NO assets in our contentstore
we try to refresh the inheritance tree for each update_item in the import
_get_cached_metadata_inheritance_tree should be called only once
Import first time
Re-import
Check transcripts_utils.GetTranscriptsFromYouTubeException not thrown
Disabled 11/14/13  This test is flakey because it performs an HTTP request on an external service  Re-enable when `requests.get` is patched using `mock.patch`
Check transcripts_utils.GetTranscriptsFromYouTubeException not thrown
Check transcripts_utils.TranscriptsGenerationException not thrown.  Also checks that uppercase file extensions are supported.
unspecified start - should inherit from container
Only published modules should be in the index
Publish the vertical as is, and any unpublished children should now be available
Publish the vertical to start with
Now publish it and we should find it  Publish the vertical as is, and everything should be available
index the course in search_index
Publish the vertical to start with
just a delete should not change anything
but after publishing, we should no longer find the html_unit
Publish the vertical to start with
even after publishing, we should not find the non-indexable item
Publish the vertical
Add a new sequential
index based on time, will include an index of the origin sequential  because it is in a common subtree but not of the original vertical  because the original sequential's subtree is too old
full index again
index full course
reload course to allow us to delete one single unit
delete the first chapter
index and check correctness
Catch any exception here to see when we fail
unspecified start - should inherit from container
Note that this test will only succeed if celery is working in inline mode
Note that this test will only succeed if celery is working in inline mode
libraries work only with split, so do library indexer
updating a library item causes immediate reindexing
deleting a library item causes immediate reindexing
Activate french, so that if the fr files haven't been loaded, they will be loaded now.
wrap the ugettext functions so that 'XYZ ' will prefix each translation
Check that the old ugettext has been put back into place
Create the use so we can log them in.
1. import and populate test toy course
NOTE: When the code above is uncommented this can be removed.
Create a course using split modulestore
Check if re-run was successful
Now, we want to make sure that .children has the total  of potential  children, and that get_child_descriptors() returns the actual children  chosen for a given student.  In order to be able to call get_child_descriptors(), we must first  call bind_for_student:
Check which child a student will see:
Refresh the children:  Now re-load the block and try yet again, in case refreshing the children changed anything:
Next, create a course:
Next, create a course:
Add a LibraryContent block to the course:
Next, create a course:
Add a LibraryContent block to the course:
Create a course:
Add a LibraryContent block to the course:
Create a course:
Add a LibraryContent block to the course:
Create a course:
Add a LibraryContent block to the course:
Now log out and ensure we are forbidden from creating a library:
At this point, one library exists, created by the currently-logged-in staff user.  Create another library as staff:  Login as non_staff_user:
Now manually intervene to give non_staff_user access to library2_key:
Create some libraries as the staff user:
Login as a non-staff:
Now manually intervene to give non_staff_user access to all "PacificX" libraries:
As staff user, add a block to self.library:
Login as a non_staff_user:
Give non_staff_user read-only permission:
As staff user, add a block to self.library:  And create a course:
Assign roles:
As staff user, add a block to self.library:  And create a course:
Assign roles:
Create a problem block in the library:
Refresh library now that we've added something.
Also create a course:
Reset:
Save, reload, and verify:
Refresh our reference to the library
Refresh our reference to the block
The library has changed...
Create a course in an incompatible modulestore.
Add a LibraryContent block to the course:
default for ENABLE_MKTG_SITE is False.
test preview
now test with the course' location
Create an unreleased draft version of the xblock
any xblock with visible_to_staff_only set to True should not be visible to students.
Orphan the orphaned xblock
Test with group_access set to Falsey values.
This is a no-op.
Update group access and expect that now one group is marked as selected.
Select a group that is not defined in the partition
Expect that the inactive scheme is excluded from the results
Expect that the partition with no groups is excluded from the results
Verify the course has imported successfully
Get & verify that course actually has two assets
Verify both assets have similar `displayname` after saving.
Test course export does not fail
Verify that asset have been overwritten during export.
Remove exported course
this one should be in a non-override folder
make sure we have some assets in our contentstore
make sure we have some thumbnails in our contentstore
verify that course info update module has same data content as in data file from which it is imported  check 'data' field content
now export the course to a tempdir and test that it contains files 'updates.html' and 'updates.items.json'  with same content as in course 'info' directory
verify that exported course has same data content as in course_info_update module
then check a intra courseware link
export out to a tempdir
check for static tabs
check for about content
check for grading_policy.json
compare what's on disk compared to what we have in our course
check for policy.json
remove old course
reimport over old course
import to different course id
reimport
verify content of the course
create a new video module and add it as a child to a vertical  this re-creates a bug whereby since the video template doesn't have  anything in 'data' field, the export was blowing up
export out to a tempdir
export out to a tempdir
Create a module, and ensure that its `data` field is empty
Export the course
Reimport and get the video back
It should now contain empty data
Export the course
Reimport and get the video back
create OpenAssessmentBlock:  convert it to draft
note that it has no `xml_attributes` attribute
export should still complete successfully
just pick one vertical
Test that malicious code does not appear in html
This could be made better, but for now let's just assert that we see the advanced modules mentioned in the page  response HTML
Verify that the course has only one asset and it has been added with an invalid asset name.
Verify that only single asset has been exported with the expected asset name.
Remove tempdir
Fetch & verify course assets to be equal to 2.
Verify both assets have similar 'displayname' after saving.
Verify that asset have been overwritten during export.
Remove tempdir
just pick one vertical
refetch to check metadata
publish module
refetch to check metadata
put back in draft and change metadata and see if it's now marked as 'own_metadata'
Save the data that we've just changed to the underlying  MongoKeyValueStore before we update the mongo datastore.
read back to make sure it reads as 'own-metadata'
republish
and re-read and verify 'own-metadata'
make sure no draft items have been returned
put into draft
make sure we can query that item and verify that it is a draft
now requery with depth
make sure just one draft item have been returned
check that there's actually content in the 'question' field
also try a custom response which will trigger the 'is this course in whitelist' logic
make sure the parent points to the child object which is to be deleted  need to refetch chapter b/c at the time it was assigned it had no children
make sure the parent no longer points to the child object which was deleted
now try to find it in store, but they should not be there any longer
now try to find it and the thumbnail in trashcan - should be in there
let's restore the asset
now try to find it in courseware store, and they should be back after restore
make sure there's something in the trashcan
empty the trashcan
make sure trashcan is empty
this test presumes old mongo and split_draft not full split
Now test that 404 response is returned when user tries to access  asset of some invalid course from split ModuleStore
delete the course
assert that there's absolutely no non-draft modules in the course  this should also include all draft items
assert that all content in the asset library is also deleted
get module info (json)
make sure we pre-fetched a known sequential which should be at depth=2
make sure we don't have a specific vertical which should be at depth=3
Verify that the creator is now registered in the course.
should raise an exception for checking permissions on deleted course
unseed the forums for the first course  should raise an exception for checking permissions on deleted course
permissions should still be there for the other course
test that a user gets his enrollment and its 'student' role as default on creating a course
check that user's enrollment for this course is not deleted  check that user has form role "Student" for this course even after deleting it
Add user in possible groups and check that user in instructor groups of this course
Now delete course and check that user not in instructor groups of this course
Update our cached user since its roles have changed
b/c the intent of the test with bad chars isn't to test auth but to test the handler, ignore
One test case involves trying to create the same course twice. Hence for that course,  the user will be enrolled. In the other cases, initially_enrolled will be False.
Helper function for getting HTML for a page in Studio and  checking that it does not error.
delete a component
delete a unit
delete a unit
delete a chapter
we should have a number of modules in there  we can't specify an exact number since it'll always be changing
first check PDF textbooks, to make sure the url paths got updated
make sure we found the item (e.g. it didn't error while loading)
create a discussion item
now fetch it from the modulestore to instantiate its descriptor
refetch it to be safe
and make sure the same discussion items have the same discussion ids
and make sure that the id isn't the old "$$GUID$$"
let's assert on the metadata_inheritance on an existing vertical
crate a new module and add it as a child to a vertical
flush the cache
check for grace period definition which should be defined at the course level
now let's define an override at the leaf node level
flush the cache and refetch
Use conditional_and_poll, as it's got an image already
Make sure the course image is set to the right place
Ensure that the imported course image is present -- this shouldn't raise an exception
create data to post
post the request
Verify that the creator is now enrolled in the course.
Verify both courses are in the course listing section
Verify that the VAL copies videos to the rerun
Verify that the creator is not enrolled in the course.
Verify that the existing course continues to be in the course listings
Verify that the failed course is NOT in the course listings
Verify that the course rerun action doesn't exist
Verify that the existing course continues to be in the course listing
Verify created course's wiki_slug.
Verify rerun course's wiki_slug.
Logout redirects.
refetch parent which should now point to child
This test is in the CMS module because the test configuration to use a draft  modulestore is dependent on django.
get the review policy object
the hide after due value only applies to timed exams
update the sequence
simulate a publish
reverify
republish course
look through exam table, the dangling exam  should be disabled
there shouldn't be any exams because we haven't enabled that  advanced setting flag
create and log in a staff user.
create a course via the view handler to create course
check that user has enrollment for this course
check that user has his default "Student" forum role for this course
check that user's enrollment for this course is not deleted
check that user has forum role for this course even after deleting it
check that user has enrollment and his default "Student" forum role for this course
delete this course and recreate this course with same user
check that user has his enrollment for this course
check that user has his default "Student" forum role for this course
check that user has enrollment and his default "Student" forum role for this course  delete this course and recreate this course with same user
now create same course with different name case ('uppercase')
check that user has his default "Student" forum role again for this course (with changed name case)
create a Private (draft only) vertical
lock an asset
verify draft vertical has a published version with published children
verify that it has a draft too
make sure that we don't have a sequential that is in draft mode
verify that we have the private vertical
verify that we have the public vertical
verify that we have the draft html
verify that we have the draft video
verify verticals are children of sequential
verify draft html is the child of the public vertical
verify draft video is the child of the public vertical
verify textbook exists
verify asset attributes of locked asset key
verify non-portable links are rewritten
compare published state
compare meta-data
assert pre_requisite_courses is initialized
fetch updated course to assert pre_requisite_courses has new values
entrance_exam_minimum_score_pct is not present in the request so default value should be saved.
Unlike other tests, need to actually perform a db fetch for this test since update_cutoffs_from_json   simply returns the cutoffs you send into it, rather than returning the db contents.
update_grace_period_from_json doesn't return anything, so query the db for its contents.
Get the descriptor and the section_grader_type and assert they are the default values
see if test makes sense
Check valid results from validate_and_update_from_json
try fresh fetch to ensure no update happened
First ensure that none of the tabs are visible
verify that the course wasn't saved into the modulestore
the login error may be absent or invisible. Check absence first,  because css_visible will throw an exception if the element is not present
files_string should be comma separated with no spaces.
Since our only test for deletion right now deletes  the only file that was uploaded, our success criteria  will be that there are no files.  In the future we can refactor if necessary.
resetting the file back to its original state
resetting the file back to its original state
Note that world.visit would trigger a 403 error instead of displaying "Unauthorized"  Instead, we can drop back into the selenium driver get command.
try to change the grade range -- this should throw an exception
check to be sure that nothing has changed
Set the new grace period
The default value is 00:00  so we need to wait for it to change
This view presents the given problem component in uppercase. Assert that the text matches  the component selected
Wait for the saving notification to pop up then disappear
The display name for the unit uses the same structure, must differentiate by level-element.
Verify order of pages
For some reason, the drag_and_drop method did not work in this case.
To make this go to port 8001, put  LETTUCE_SERVER_PORT = 8001  in your settings.py file.
Navigate to the studio dashboard
Navigate to the studio dashboard
hit TAB or provided key to trigger save content
The file upload dialog is a faux modal, a div that takes over the display
Clicking the Upload button triggers an AJAX POST.
The modal stays up with a "File uploaded succeeded" confirmation message, then goes away.  It should take under 2 seconds, so wait up to 10.  Note that is_css_not_present will return as soon as the element is gone.
admins get staff privileges, as well
Verifying that the display name can be a string containing a floating point value  (to confirm that we don't throw an error because it is of the wrong type).
Go to course outline
We should wait 300 ms for event handler invocation + 200ms for safety.
Store the current URL so we can return here
Upload subtitles for the video using the upload interface
Return to the video
update .sub filed with proper subs name (which mimics real Studio/XML behavior)  this is needed only for that videos which are created in acceptance tests.
We should wait 300 ms for event handler invocation + 200ms for safety.
For some reason ChromeDriver doesn't trigger an 'input' event after filling  the field with an empty value. That's why we trigger it manually via jQuery.
A few deprecated settings for testing toggling functionality.
Test only a few of the existing properties (there are around 34 of them)
Sometimes get stale reference if I hold on to the array of elements
Verify course start date (required) and time still there
Time should have stayed from before attempt to clear date.
hit Enter to apply the changes
We need to wait for JavaScript to fill in the field, so we use  css_has_value(), which first checks that the field is not blank
Unset times get set to 12 AM once the corresponding date has been set.
Format dropdown  Font dropdown
This is our custom "code style" button. It uses an image instead of a class.
Verify that CodeMirror editor is not hidden  Verify that TinyMCE Editor is not present
Click on plugin button
Click on plugin button
Wait for the plugin window to open.
Trigger the action
Click OK
Count how many of that module is on the page. Later we will  assert that one more was added.  We need to use world.browser.find_by_css instead of world.css_find  because it's ok if there are currently zero of them.
Disable the jquery animation for the transition to the menus.
Wait for the advanced tab items to be displayed
The tab shows buttons for the given category
Find the button whose text matches what you're looking for
There should be one and only one
Sometimes this click does not work if you go too fast.
Retry this in case the list is empty because you tried too fast.
Wait for the link to be clickable. If you go too fast it is not.
Select the 'settings' tab if there is one (it isn't displayed if it is the only option)
We have a known issue that modifications are still shown within the edit window after cancel (though)  they are not persisted. Refresh the browser to make sure the changes WERE persisted after Save.
We have a known issue that modifications are still shown within the edit window after cancel (though)  they are not persisted. Refresh the browser to make sure the changes were not persisted.
can't use auth.add_users here b/c it requires user to already have Instructor perms in this course
seed the forums
auto-enroll the course creator in the course so that "View Live" will work.
set default forum roles (assign 'Student' role)
in the django layer, we need to remove all the user permissions groups associated with this course
Root will be "https://www.edx.org". The complete URL will still not be exactly correct,  but redirects exist from www.edx.org to get to the Drupal course about page URL.
Strip off https:// (or http://) to be consistent with the formatting of LMS_BASE.
If there's no published version then the xblock is clearly not visible
If visible_to_staff_only is True, this xblock is not visible to students regardless of start date.
Check start date
No start date, so it's always visible
Stop searching at the section level
Orphaned xblocks set their own release date
Stop searching if this xblock has explicitly set its own staff lock
Stop searching at the section level
Orphaned xblocks set their own staff lock
Exclude disabled partitions, partitions with no groups defined  Also filter by scheme name if there's a filter defined.
First, add groups defined by the partition
Put together the entire partition dictionary
Pre-process the partitions to make it easier to display the UI
import here, at top level this import prevents the celery workers from starting up correctly
deserialize the payload
use the split modulestore as the store for the rerun course,  as the Mongo modulestore doesn't support multiple runs of the same course.
set initial permissions for the user to access the course.
update state: Succeeded
call edxval to attach videos to the rerun
do NOT delete the original course, only update the status
update state: Failed
cleanup any remnants of the course
it's possible there was an error even before the course module was created
remove the +00:00 from the end of the formats generated within the system
Wrap counter in dictionary - otherwise we seem to lose scope inside the embedded function `prepare_item_index`
items_index is a list of all the items index dictionaries.  it is used to collect all indexes and index them using bulk API,  instead of per item index API call.
if it's not indexable and it does not have children, then ignore
First perform any additional indexing from the structure object
Now index the content
broad exception so that index operation does not prevent the rest of the application from working
Source location options - either from the course or the about info
load data for all of the 'about' modules for this course into a dictionary
Broad exception handler so that a single bad property does not scupper the collection of others
Broad exception handler to protect around and report problems with indexing
Open and parse the configuration file when the module is initialized
Patch the xml libs before anything else.
Disable PyContract contract checking when running as a webserver
This application object is used by the development server  as well as any WSGI server configured to use this file.
Delete all data added by data migrations. Unit tests should setup their own data using factories.
Start with empty caches
Make sure that cache contents don't leak out after the isolation is ended
N.B. As of 2016-04-20, Django won't return any caches  from django.core.cache.caches.all() that haven't been  accessed using caches[name] previously, so we loop  over our list of overridden caches, instead.
The sites framework caches in a module-level dictionary.  Clear that.
pylint: disable=method-hidden
force garbarge collection
pylint: disable=protected-access
If any mixins have been applied, then use the unmixed class
The block is acting as an XModule
The block is acting as an XModuleDescriptor
Replace / with \/ so that "</script>" in the data won't break things.
Replace / with \/ so that "</script>" in the data won't break things.
Passing module_id this way prevents sql-injection.
build edit link to unit in CMS. Can't use reverse here as lms doesn't load cms's urls.py
return edit link in rendered HTML for display
if original, unmangled filename exists then use it (github  doesn't like symlinks)
Need to define all the variables that are about to be used
could enforce that update[0].tag == 'h2'
return list in reversed order (old format: [4,3,2,1]) for compatibility
There's no need to get_parents
If filter_func isn't provided, make it a no-op.
Use deque for the stack, which is O(1) for pop and append.  Use the _Node class to keep track of iterated children.
Keep track of which nodes have been visited.
Peek at the current node at the top of the stack.
Verify the node wasn't already visited and the node  satisfies the filter_func.  Since already visited or filtered out, remove from the  stack and continue with the next node.
See if there are any additional children for this node.
Since there are no children left, visit the node and  remove it from the stack.
If so, add the child to the top of the stack.
If filter_func isn't provided, make it a no-op.
Use deque for the stack, which is O(1) for pop and append.
While there are more nodes on the stack...
Take a node off the top of the stack.
If we're doing a topological traversal, then make sure all  the node's parents have been visited. If they haven't,  then skip the node for now; we'll encounter it again later  through another one of its parents.
If all of the parents have not yet been visited, continue.
If none of the parents have yielded, continue, unless  specified otherwise (via yield_descendants_of_unyielded).
If the current node has already been visited, continue.
For a topological sort, add all the children since  they would not have been visited.
For a pre-order sort, filter out already visited  children.
Add the node's unvisited children to the stack in reverse  order so they are traversed in their original order.
Yield the result of the node if the node satisfies the  filter_func.
Keep track of whether or not the node was yielded so we  know whether or not to yield its children.
Service users may not have user profiles.
Service users may not have user profiles.
Revert to INFO if an invalid string is passed in
default to a blank string so that if SERVICE_VARIANT is not  set we will not log to a sub directory
if image_key is empty, use the default image url from settings
Not intented for programmatic use, so we print the keys out
See if there's a startup module in each app.
If the module has a run method, run it.
use lynx to get plaintext
Stevedore extension point namespaces
Register a prefix that collectstatic will add to each path
First, try to update the existing instance  If no instance exists yet, create it.  This is backwards-compatible with the behavior of DRF v2.
Backwards compatibility with DRF v2 behavior, which would catch model-level  validation errors and return a 400
For PUT-as-create operation, we need to ensure that we have  relevant permissions, as if this was a POST request.  This  will either raise a PermissionDenied exception, or simply  return None.
PATCH requests where the object does not exist should still  return a 404 response.
map over the search results and get a list of database objects in the same order
Unauthenticated, CSRF validation not required  This is where regular `SessionAuthentication` checks that the user is active.  We have removed that check in this implementation.  But we added a check to prevent anonymous users since we require a logged-in account.
CSRF passed with authenticated user
AuthenticationFailed is a subclass of drf_exceptions.AuthenticationFailed,  but we don't want to post-process the exception detail for our own class.
Note: we're creating the extension manager lazily to ensure that the Python path  has been correctly set up. Trying to create this statically will fail, unfortunately.
Demonstrate the base issue we are trying to solve.
This is the a change we've made from the django-rest-framework-oauth version  of these tests.
If no Authorization header is provided that contains a bearer token,  authorization passes to the next registered authorization class, or  (in this case) to standard DRF fallback code, so no error_code is  provided (yet).
This case is handled directly by DRF so no error_code is provided (yet).
Avoid double-binding the field, otherwise we'll get  an error about the source kwarg being redundant.
This is used to namespace gating-specific milestones
We should only ever have one gating milestone per UsageKey  Log a warning here and pick the first one
Get the unfulfilled gating milestones for this course, for this user
create chapter
Allow the "event" field to be a string, currently this is the case for all browser events.
Allow unexpected fields to exist in the top level event dictionary.
Allow unexpected fields to exist in the "context" dictionary. This is where new fields that appear in multiple  events are most commonly added, so we frequently want to tolerate variation here.
Allow unexpected fields to exist in the "event" dictionary. Typically in unit tests we don't want to allow this  type of variance since there are typically only a small number of tests for a particular event type.
NOTE: "payload_extra_fields" is deliberately excluded from this list since we want to detect erroneously added  fields in the payload by default.
Some events store their payload in a JSON string instead of a dict. Comparing these strings can be problematic  since the keys may be in different orders, so we parse the string here if we were expecting a dict.
Verify the API was actually hit (not the cache)
Warm up the cache.
Hit the cache.
Verify that only two requests were made, not four.
Test to see if the token is an uuid1 hex value
Links are interpreted relative to the directory containing the link
check that we're not trying to import outside of the data_dir
pylint: disable=protected-access
Set the timeout value for the cache to 1 day as a fail-safe  in case the signal to invalidate the cache doesn't come through.
Deserialize and construct the block structure.
Check if the xblock was already visited (can happen in  DAGs).
Add the xBlock.
Add relations with its children and recurse.
A dictionary key value for storing a transformer's version number.
List of usage keys of this block's parents.  list [UsageKey]
List of usage keys of this block's children.  list [UsageKey]
The usage key of the root block for this structure.  UsageKey
Map of a block's usage key to its block relations. The  existence of a block in the structure is determined by its  presence in this map.  defaultdict {UsageKey: _BlockRelations}
Add the root block.
Create a new block relations map to store only those blocks  that are still linked
Build the structure from the leaves up by doing a post-order  traversal of the old structure, thereby encountering only  reachable blocks.  If the block is in the old structure,  Add it to the new pruned structure
Add a relationship to only those old children that  were also added to the new pruned structure.
Replace this structure's relations with the newly pruned one.
Map of xblock field name to the field's value for this block.  dict {string: any picklable type}
Map of transformer name to the transformer's data for this  block.  defaultdict {string: dict}
Map of a block's usage key to its collected data, including  its xBlock fields and block-specific transformer data.  defaultdict {UsageKey: _BlockData}
Map of a transformer's name to its non-block-specific data.  defaultdict {string: dict}
Remove block from its children.
Remove block from its parents.
Remove block.
Recreate the graph connections if descendants are to be kept.
Map of a block's usage key to its instantiated xBlock.  dict {UsageKey: XBlock}
Set of xBlock field names that have been requested for  collection.  set(string)
pylint: disable=protected-access
get_children
get_parents
__contains__
create block structure
add each block
request fields
verify fields have not been collected yet
collect fields
verify values of collected fields
update the graph connecting the old parents to the old children
update all descendants  if the child has another parent, continue  add descendant to missing blocks and empty its  children
None case
1 registered
2 registered
1 unregistered
1 registered and 1 unregistered
An in-memory map of cache keys to cache values.
Use the class' name for Mock transformers.
0     / \    1  2   / \  3   4
0       /      1     /    2   /  3
0     / \    1  2    \ / \     3  4    / \   5  6
create empty block structure
_add_relation
Verify presence
Verify children
JSON mapping of discussion ids to usage keys for the corresponding discussion modules
Usage key strings might not include the course run, so we add it back in with map_into_course
find the dictionary entry for the current node
Reload the data to ensure the init signal is fired to decompress the data.
Method requires string input
Import tasks here to avoid a circular import.
Delete the existing discussion id map cache to avoid inconsistencies
Note: The countdown=0 kwarg is set to to ensure the method below does not attempt to access the course  before the signal emitter has finished all operations. This is also necessary to ensure all tests pass.
-*- coding: utf-8 -*-
Backwards compatibility with the behavior of DRF v2.  When the grader dictionary was missing keys, DRF v2 would default to None;  DRF v3 unhelpfully raises an exception.
Backwards compatibility with the behavior of DRF v2  Include a NULL value for "parent" in the representation  (instead of excluding the key entirely)
Backwards compatibility with the behavior of DRF v2  Leave the children list as a list instead of serializing  it to a string.
If we don't have data stored, generate it and return an error.
For some reason, `listen_for_course_publish` is not called when we run  all (paver test_system -s cms) tests, If we run only run this file then tests run fine.
If we don't disconnect then tests are getting failed in test_crud.py
Add this blocks children to the stack so that we can traverse them as well.
Import here to avoid circular import.
Ideally we'd like to accept a CourseLocator; however, CourseLocator is not JSON-serializable (by default) so  Celery's delayed tasks fail to start. For this reason, callers should pass the course key as a Unicode string.
IMPORTANT: Bump this whenever you modify this model and/or add a migration.
Cache entry versioning.
Start/end dates
URLs
Certification data
Grading
Access parameters
Enrollment details
Catalog information
Throw away old versions of CourseOverview, as they might contain stale data.
Regenerate the thumbnail images if they're missing (either because  they were never generated, or because they were flushed out after  a change to CourseOverviewImageConfig.
Note: If a newly created course is not returned in this QueryList,  make sure the "publish" signal was emitted when the course was  created. For tests using CourseFactory, use emit_signals=True.
In rare cases, courses belonging to the same org may be accidentally assigned  an org code with a different casing (e.g., Harvardx as opposed to HarvardX).  Case-insensitive exact matching allows us to deal with this kind of dirty data.
creates circular import; hence explicitly referenced is_discussion_enabled
This is either the raw image that the course team uploaded, or the  settings.DEFAULT_COURSE_ABOUT_IMAGE_URL if they didn't specify one.
Default all sizes to return the raw image if there is no  CourseOverviewImageSet associated with this CourseOverview. This can  happen because we're disabled via CourseOverviewImageConfig.
The URL can't be empty.
If this is an absolute URL, just return it as is.  It could be a domain  that isn't ours, and thus CDNing it would actually break it.
If image thumbnails are not enabled, do nothing.
If a course object was provided, use that. Otherwise, pull it from  CourseOverview's course_id. This happens because sometimes we are  generated as part of the CourseOverview creation (course is available  and passed in), and sometimes the CourseOverview already exists.
Small thumbnail, for things like the student dashboard
Large thumbnail, for things like the about page
The course about fields are accessed through the CourseDetail  class for the course module, and stored as attributes on the  CourseOverview objects.
test tabs for both cached miss and cached hit courses
Note: We specify a value for 'run' here because, for some reason,  .create raises an InvalidKeyError if we don't (even though my  other test functions don't specify a run but work fine).
Create a course where mobile_available is True.
Set mobile_available to False and update the course.  This fires a course_published signal, which should be caught in signals.py, which should in turn  delete the corresponding CourseOverview from the cache.
Make sure that when we load the CourseOverview again, mobile_available is updated.
Verify that when the course is deleted, the corresponding CourseOverview is deleted as well.
Creating a new course will trigger a publish event and the course will be cached
The cache will be hit and mongo will not be queried
This mock makes it so when the module store tries to load course data,  an exception is thrown, which causes get_course to return an ErrorDescriptor,  which causes get_from_id to raise an IOError.
Because the course overview now has an old version number, it should  be thrown out after being loaded from the cache, which results in  a call to get_course.
mock the CourseOverview ORM to raise a DoesNotExist exception to force re-creation of the object
Verify the CourseOverview is loaded successfully both times,  including after an IntegrityError exception the 2nd time.
This will create a version 10 CourseOverview
Now we're going to muck with the values and manually save it as v09
Now we're going to ask for it again. Because 9 < 10, we expect  that this entry will be deleted() and that we'll get back a new  entry with version = 10 again.
Now we're going to muck with this and set it a version higher in  the database.
Because CourseOverview is encountering a version *higher* than it  knows how to write, it's not going to overwrite what's there.
Test case-insensitivity.
Because we're sending None and '', we expect to get the generic  fallback URL for course images.
Even though there was no source image to generate, we should still  have a CourseOverviewImageSet object associated with this overview.
Disable model generation using config models...
Since we're disabled, we should just return the raw source image back  for every resolution in image_urls.
Because we are disabled, no image set should have been generated.
This initial seeding should create an entry for the image_set.
Now just throw in some fake data to this image set, something that  couldn't possibly work.
Now disable the thumbnail feature
Fetch a new CourseOverview
Assert that the data still exists for debugging purposes
But because we've disabled it, asking for image_urls should give us  the raw source image for all resolutions, and not our broken images.
Now enable the CDN...
Now enable the CDN...
Strictly speaking, this would fail anyway because there's no data  backing sample_image.png, but we're going to make the side-effect  more dramatic. ;-)
This will generate a CourseOverview and verify that we get the  source image back for all resolutions.
Make sure we were called (i.e. we tried to create the thumbnail)
Now an image set does exist, even though it only has blank values for  the small and large urls.
The next time we create a CourseOverview, the images are explicitly  *not* regenerated.
Save a real image here...
If create_after_overview is True, disable thumbnail generation so  that the CourseOverview object is created and saved without an  image_set at first (it will be lazily created later).
Now generate the CourseOverview...
If create_after_overview is True, no image_set exists yet. Verify  that, then switch config back over to True and it should lazily  create the image_set on the next get_from_id() call.
Save the image to the contentstore...
Now generate the CourseOverview...
Naming convention for thumbnail
Actual thumbnail data
Set config to False so that we don't create the image yet
First create our CourseOverview
Now create an ImageSet by hand...
Now do it the normal way -- this will cause an IntegrityError to be  thrown and suppressed in create_for_course()
All the URLs that come back should be for the expected_url
import CourseAboutSearchIndexer inline due to cyclic import  Delete course entry from Course About Search_index
ensure that the newly created courses aren't in course overviews
CourseOverview will be populated with all courses in the modulestore
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Removed because we accidentally removed this column without first  removing the code that refers to this.  This can cause errors in production.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
add a sequence to the course to which the problems can be added
Create a vertical to contain our split test
combine all values if there were multiple specified individually
parse them into a set
pylint: disable=abstract-method
Register signal handlers
Import here instead of top of file since this module gets imported before  the programs app is loaded, resulting in a Django deprecation warning.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Under cms the following setting is not defined, leading to errors during tests.
If either programs or credentials config models are disabled for this  feature, it may indicate a condition where processing of such tasks  has been temporarily disabled.  Since this is a recoverable situation,  mark this task for retry instead of failing it altogether.
Don't retry for this case - just conclude the task.
Determine which program certificates the user has already been  awarded, if any.
Retry because a misconfiguration could be fixed
keep trying to award other certs, but retry the whole task to fix any missing entries
N.B. This logic assumes that this task is idempotent
Disable certification to prevent the task from being triggered when  setting up test data (i.e., certificates with a passing status), thereby  skewing mock call counts.
The alternate course is used here to verify that the status and run_mode  queries are being ANDed together correctly.
Verify the API was actually hit (not the cache).
Warm up the cache.
Hit the cache.
Verify only one request was made.
Hit the Programs API twice.
Verify that three requests have been made (one for student, two for staff).
Enrollment for the shared course ID created last (most recently).
No enrollments, no program engaged.
Bypass caching for staff users, who may be creating Programs and want  to see them displayed immediately.
enrollment.course_id is really a course key ಠ_ಠ
This end-point is available to anonymous users,  so do not require authentication.
Translators: This label appears above a field on the login form  meant to hold the user's email address.
Translators: This example email address is used as a placeholder in  a field on the login form meant to hold the user's email address.
Translators: These instructions appear on the login form, immediately  below a field meant to hold the user's email address.
Translators: This label appears above a field on the login form  meant to hold the user's password.
For the initial implementation, shim the existing login view  from the student Django app.
This end-point is available to anonymous users,  so do not require authentication.
Map field names to the instance method used to add the field to the form
Default fields are always required
Custom form fields can be added via the form set in settings.REGISTRATION_EXTENSION_FORM
Extra fields configured in Django settings  may be required, optional, or hidden
Translators: This label appears above a field on the registration form  meant to hold the user's email address.
Translators: This example email address is used as a placeholder in  a field on the registration form meant to hold the user's email address.
Translators: This label appears above a field on the registration form  meant to hold the user's full name.
Translators: This example name is used as a placeholder in  a field on the registration form meant to hold the user's name.
Translators: These instructions appear on the registration form, immediately  below a field meant to hold the user's full name.
Translators: This label appears above a field on the registration form  meant to hold the user's public username.
Translators: These instructions appear on the registration form, immediately  below a field meant to hold the user's public username.
Translators: This example username is used as a placeholder in  a field on the registration form meant to hold the user's username.
Translators: This label appears above a field on the registration form  meant to hold the user's password.
Translators: This label appears above a dropdown menu on the registration  form used to select the user's highest completed level of education.
Translators: This label appears above a dropdown menu on the registration  form used to select the user's gender.
Translators: This label appears above a dropdown menu on the registration  form used to select the user's year of birth.
Translators: This label appears above a field on the registration form  meant to hold the user's mailing address.
Translators: This phrase appears above a field on the registration form  meant to hold the user's reasons for registering with edX.
Translators: This label appears above a field on the registration form  which allows the user to input the city in which they live.
Translators: This label appears above a field on the registration form  which allows the user to input the State/Province/Region in which they live.
Translators: This label appears above a field on the registration form  which allows the user to input the Company
Translators: This label appears above a field on the registration form  which allows the user to input the Title
Translators: This label appears above a field on the registration form  which allows the user to input the First Name
Translators: This label appears above a field on the registration form  which allows the user to input the First Name
Translators: This label appears above a dropdown menu on the registration  form used to select the country in which the user lives.
Separate terms of service and honor code checkboxes
Combine terms of service and honor code checkboxes  Translators: This is a legal document users must agree to  in order to register a new account.
Translators: "Terms of Service" is a legal document users must agree to  in order to register a new account.
Translators: "Terms of Service" is a legal document users must agree to  in order to register a new account.
Translators: This is a legal document users must agree to  in order to register a new account.
Translators: "Terms of service" is a legal document users must agree to  in order to register a new account.
Translators: "Terms of service" is a legal document users must agree to  in order to register a new account.
Override username / email / full name
Hide the password field
This end-point is available to anonymous users,  so do not require authentication.
Translators: This label appears above a field on the password reset  form meant to hold the user's email address.
Translators: This example email address is used as a placeholder in  a field on the password reset form meant to hold the user's email address.
Translators: These instructions appear on the password reset form,  immediately below a field meant to hold the user's email address.
Only check for true. All other values are False.
The minimum and maximum length for the name ("full name") account field
The minimum and maximum length for the username account field
The minimum and maximum length for the email account field
The minimum and maximum length for the password account field
Indicates the user's preference that all users can view the shareable fields in their account information.
Indicates the user's preference that all their account information be private.
Don't pass the 'configuration' arg up to the superclass
Don't pass the 'custom_fields' arg up to the superclass
Currently no read-only field, but keep this so view code doesn't need to know.
Calling UserPreference directly because the requesting user may be different from existing_user  (and does not have to be is_staff).
when user does not have profile it raises exception, when exception  occur we can simply get default image.
Public access point for this function.
If user has requested to change email, we must call the multi-step process to handle this.  It is not handled by the serializer (which considers email to be read-only).
If user has requested to change name, store old name because we must update associated metadata  after the save process is complete.
Check for fields that are not editable. Marking them read-only causes them to be ignored, but we wish to 400.
Build up all field errors, whether read-only, validation, or email errors.
If we have encountered any validation errors, return them to the user.
We have not found a way using signals to get the language proficiency changes (grouped by user).  As a workaround, store old and new values here and emit them after save is complete.
if any exception is raised for user preference (i.e. account_privacy), the entire transaction for user account  patch is rolled back and the data is not saved
Validate the username, password, and email  This will raise an exception if any of these are not in a valid format.
Create the user account, setting them to "inactive" until they activate their account.
Create a registration to track the activation process  This implicitly saves the registration.
Create an empty user profile with default values
Return the activation key, which the caller should send to the user
This implicitly saves the registration
Binding data to a form requires that the data be passed as a dictionary  to the Form class constructor.
Validate that a user exists with the given email address.  Generate a single-use link for performing a password reset  and email it to the user.  No user with the provided email address exists.
Ensure that parental controls don't apply to this user
With default configuration settings, email is not shared with other (non-staff) users.
Send a read-only error, serializer error, and email validation error.
Verify that the name change happened, even though the attempt to send the email failed.
Verify that no email change request was initiated.
Create a new account, which should have empty account settings by default.
Retrieve the account settings
Expect a date joined field but remove it to simplify the following comparison
Long email -- subtract the length of the @domain  except for one character (so we exceed the max length limit)
Create the account, which is initially inactive
Activate the account and verify that it is now active
Username and password cannot be the same
Create and activate an account
Request a password change
Verify that one email message has been sent
Verify that the body of the message contains something that looks  like an activation link
Verify that no email messages have been sent
Create an account, but do not activate it
Verify that the activation email was still sent
this is used in one test to check the behavior of profile image url  generation with a relative url in the config.
pylint: disable=no-member
Update user account visibility setting.
Verify how the view parameter changes the fields that are returned.
Badges aren't on by default, so should not be present.
Now make sure that the user can get the same information, even if not active
Ensure the user has birth year set, and is over 13, so  account_privacy behaves normally
If there are no values that would fail validation, then empty string should be supported;  except for account_privacy, which cannot be an empty string.
Make sure that gender did not change.
Although throwing a 400 might be reasonable, the default DRF behavior with ModelSerializer  is to convert to None, which also seems acceptable (and is difficult to override).
Verify that the behavior is the same for sending None.
Verify the new name was also stored.
Since request is multi-step, the email won't change on GET immediately (though goals will update).
Verify that the shared view is still private
Send a PATCH request with updates to both profile information and email.  Throw an error from the method that is used to process the email change request  (this is the last thing done in the api method). Verify that the profile did not change.
Fields output in the CSV
Number of records to read at a time when making  multiple queries over a potentially large dataset.
Default datetime if the user has not set a preference
Retrieve all the courses for the org.  If we were given a specific list of courses to include,  filter out anything not in that list.
Add in organizations from the course keys, to ensure  we're including orgs with different capitalizations
If no courses are found, abort
Let the user know what's about to happen
Open the output file and generate the report.
Remind the user where the output file is
Log the number of rows we processed
Use the read replica if one has been configured
The user isn't enrolled in the course, so the output should be empty
By default, if no preference is set by the user is enrolled, opt in
Enroll in a course that's not in the org
Opt out of the other course
The first course is included in the results,  but the second course is excluded,  so the user should be opted in by default.
Enroll in two courses, both in the org
Opt into the first course, then opt out of the second course
Enroll in the course and set a preference
Unenroll from the course
Enrollments should still appear in the outpu
Enroll in several courses in the org
Set a preference for the aliased course
Unenroll from the aliased course
No course available for this particular org
Create several courses in the same org
Execute the command, but exclude the second course from the list
Generate the report
Expect that every enrollment shows up in the report
Lowercase some of the org names in the course IDs
Set preferences for both courses
Create a temporary directory for the output  Delete it when we're finished
Sanitize the arguments
Override the query interval to speed up the tests
Execute the command
Return the output as a list of dictionaries
Include an empty "default" option at the beginning of the list
If there are overrides for this field, apply them now.  Any field property can be overwritten (for example, the default value or placeholder)
Transform kwarg "field_type" to "type" (a reserved Python keyword)
Transform kwarg "default" to "defaultValue", since "default"  is a reserved word in JavaScript
Ensure that the POST querydict is mutable
The login and registration handlers in student view try to change  the user's enrollment status if these parameters are present.  Since we want the JavaScript client to communicate directly with  the enrollment API, we want to prevent the student views from  updating enrollments.
Call the original view to generate a response.  We can safely modify the status code or content  of the response, but to be safe we won't mess  with the headers.
Otherwise, it's a general authentication failure.  Ensure that the status code is a 403 and pass  along the message from the view.
If an error condition occurs, send a status 400  The student views tend to send status 200 even when an error occurs  If the JSON-serialized content has a value "success" set to False,  then we know an error occurred.
If the response is successful, then return the content  of the response directly rather than including it  in a JSON-serialized dictionary.
Return the response, preserving the original headers.  This is really important, since the student views set cookies  that are used elsewhere in the system (such as the marketing site).
-*- coding: utf-8 -*-
persist the value as a course tag
There was no preference with that key, raise a 404.
Check that a 27 year old can opt-in
Check that a 32-year old can opt-out
Check that someone 14 years old can opt-in
Check that someone 13 years old cannot opt-in (must have turned 13 before this year)
Check that someone 12 years old cannot opt-in
Create the course and account.
Test that the API still works if no age is specified.  Create the course and account.
Check that a 27 year old can opt-in, then out.
Check that a 32-year old can opt-out, then in.
Check that someone 13 years old can opt-in, then out.
Check that someone 12 years old cannot opt-in, then explicitly out.
Create the course and account.
Create some test preferences values.
Verify that a preference can be deleted
Verify that deleting a non-existent preference throws a 404
Staff can always see profiles.  This should never return Multiple, as we don't allow case name collisions on registration.
Scopes  (currently only allows per-course tags.  Can be expanded to support  global tags (e.g. using the existing UserPreferences table))
get a tag that doesn't exist
Create a test user
get a group assigned to the user
make sure we get the same group back out every time
We should not get any group because assign is False which will  protect us from automatically creating a group for user
We should get a group automatically assigned to user
get a group assigned to the user
get a group assigned to the user - should be group 0 or 1
Now, get a new group using the same call - should be 3 or 4
We should get the same group over multiple calls
Changing the name of the group shouldn't affect anything  get a group assigned to the user - should be group 0 or 1
Now, get a new group using the same call
Verify that the raised exception has the error message
Verify that the error logger is called  This will include the stack trace for the original exception  because it's called with log level "ERROR"
Expect that the enrollment action and course ID  were stripped out before reaching the wrapped view.
Expect that the analytics course ID was passed to the view
Factories are self documenting  pylint: disable=missing-docstring
Retrieve the login form
Create a test user
Login
Verify that we logged in successfully by accessing  a page that requires authentication.
Create a test user
Login and remember me
Verify that the session expiration was set correctly
Create a test user
Invalid password
Invalid email address
Create a test user
Missing password
Missing email
Missing both email and password
Retrieve the password reset form
'min_length': account_api.PASSWORD_MIN_LENGTH,  'max_length': account_api.PASSWORD_MAX_LENGTH
Password field should be hidden
Verify that we've been logged in  by trying to access a page that requires authentication
Verify the user's account
Verify that we've been logged in  by trying to access a page that requires authentication
Initially, the field values are all valid
Override the valid fields, making the input invalid
Attempt to create the account, expecting an error response
Send a request missing a field
Retrieve the registration form description
Verify that the form description matches what we'd expect
Search the form for this field
Retrieve the registration form description
Modify the tag and save it. Check if the modified timestamp is updated.
does a round trip
get preference for key that doesn't exist for user
Middleware should pass request through
The middleware should clean up the context when the request is done
Even if the tracker blows up, the middleware should still return the response
-*- coding: utf-8 -*-
Nothing can be themed if we don't have a theme location.
This applies @with_comprehensive_theme to the func.
Because we want to match the original loader_tags.py file as closely as  possible, we should disable pylint so it doesn't complain about the violations  that are already in that file  pylint: skip-file
strip the prefix
-*- coding: utf-8 -*-
add SiteConfiguration to database
Verify an entry to SiteConfigurationHistory was added.
Make sure an entry (and only one entry) is saved for SiteConfiguration
add SiteConfiguration to database
Verify an entry to SiteConfigurationHistory was added.
Make sure two entries (one for save and one for update) are saved for SiteConfiguration
add SiteConfiguration to database
Verify an entry to SiteConfigurationHistory was added.
Make sure entry is saved if there is no error
try to add a duplicate entry
Make sure no entry is saved if there an error
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Warm up the cache.
Hit the cache.
Verify only one request was made.
Hit the Credentials API twice.
Verify that three requests have been made (one for student, two for staff).
create credentials and program configuration
Mocking the API responses from programs and credentials
checking response from API is as expected
create credentials and program configuration
Mocking the API responses from programs and credentials
Checking result is as expected
create credentials and program configuration
Mocking the API responses from programs and credentials
Mocking the API responses from programs and credentials
Bypass caching for staff users, who may be generating credentials and  want to see them displayed immediately.
still need these for now b/c the client's screen shows these 3  fields
Default course license is "All Rights Reserved"
Ignore an attempt to delete an item that doesn't exist
NOTE: below auto writes to the db w/o verifying that any of  the fields actually changed to make faster, could compare  against db or could have client send over a list of which  fields changed.
Could just return jsondict w/o doing any db reads, but I put  the reads in as a means to confirm it persisted correctly
Signal that fires when a user is graded (in lms/courseware/grades.py)
update the course information on ccxcon using celery  import here, because signal is registered at startup, but items in tasks are not yet able to be loaded
-*- coding: utf-8 -*-
get the entire list of instructors  get anonymous ids for each of them  extract the course details
make the POST request
Trying to wrap the whole thing in a bulk operation fails because it  doesn't find the parents. But we can at least wrap this part...
no args used for the call
second call with different status code
no args used for the call
in case the maximum amount of retries has not been reached,  insert another task delayed exponentially up to 5 retries
Figure out what the XBlock class is from the block type, and  then open whatever resource has been requested.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
order credit requirements according to their appearance in courseware
Maintain a history of requirement status updates for auditing purposes
Deadline for when credit eligibility will expire.  Once eligibility expires, users will no longer be able to purchase  or request credit.  We save the deadline as a database field just in case  we need to override the deadline for particular students.
Check all requirements for the course to determine if the user  is eligible.  We need to check all the *requirements*  (not just the *statuses*) in case the user doesn't yet have  a status for a particular requirement.
Enforce the constraint that each user can have exactly one outstanding  request to a given provider.  Multiple requests use the same UUID.
Retrieve all in-course reverification blocks in the course
Update the verification definitions in the course descriptor  This will also clean out old verification partitions if checkpoints  have been deleted.
Exclude all previously used IDs, even for partitions that have been disabled  (e.g. if the course author deleted an in-course reverifification block but  there are courseware components that reference the disabled partition).
Preserve existing, non-verified partitions from the course  Mark partitions for deleted in-course reverification as disabled.
Don't consume `.json` style suffixes
Filter by provider ID
Get the provider, or return HTTP 404 if it doesn't exist
Validate the course key
Validate the username
Ensure the user is actually eligible to receive credit
This endpoint should be open to all external credit providers.
Ensure the input data is valid
This CSRF exemption only applies when authenticating without SessionAuthentication.  SessionAuthentication will enforce CSRF protection.
Convert the serialized course key into a CourseKey instance  so we can look up the object.
Import here, because signal is registered at startup, but items in tasks  are not yet able to be loaded
This needs to be imported here to avoid a circular dependency  that can cause syncdb to fail.
Student received a passing grade
Grade was good, but submission arrived too late
Student failed to receive minimum grade
Ensure we converted the timestamp to a datetime
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
This seems to need to be here otherwise we get  circular references when starting up the app
This seems to need to be here otherwise we get  circular references when starting up the app
not enrolled
This seems to need to be here otherwise we get  circular references when starting up the app
quick exit, if course is not credit enabled
need to get user_name from the user object
This seems to need to be here otherwise we get  circular references when starting up the app
quick exit, if course is not credit enabled
always log any deleted activity to the credit requirements  table. This will be to help debug any issues that might  arise in production
need to get user_name from the user object
Retrieve all information we need to determine the user's group  as a multi-get from the cache.
Try a multi-get from the cache
Retrieve whether the user is enrolled in a verified mode.
Retrieve whether the user has skipped any checkpoints in this course
Retrieve the user's verification status for each checkpoint in the course.
Check whether the user has completed this checkpoint  "Completion" here means *any* submission, regardless of its status  since we want to show the user the content if they've submitted  photos.
create the root email message  add 'alternative' part to root email message to encapsulate the plain and  HTML versions, so message agents can decide which they want to display.  render the credit notification templates
add alternative plain text message
attach logo image
add email addresses of sender and receiver
send the root email message
insert style tag in the html and run pyliner.
Translators: The join of two university names (e.g., Harvard and MIT).
Translators: The join of three or more university names. The first of these formatting strings  represents a comma-separated list of names (e.g., MIT, Harvard, Dartmouth).
Initiate a new request if one has not already been created
Retrieve user account and profile info
Retrieve the final grade from the eligibility table
NOTE (CCB): Limiting the grade to seven characters is a hack for ASU.
Getting the students's enrollment date
Getting the student's course completion date
Sign the parameters using a secret key we share with the credit provider.
Retrieve all credit requirements for the course  We retrieve all of them to avoid making a second query later when  we need to check whether all requirements have been satisfied.
Find the requirement we're trying to set
Update the requirement status
If we're marking this requirement as "satisfied", there's a chance that the user has met all eligibility  requirements, and should be notified. However, if the user was already eligible, do not send another notification.
Find the requirement we're trying to remove
Remove the requirement status
check if an already added requirement is modified
Update the requirements, removing an existing requirement
Expect that now only the grade requirement is returned
Configure a credit eligibility that expired yesterday
The user should NOT be eligible for credit
The eligibility should NOT show up in the user's list of eligibilities
Configure a credit eligibility for a disabled course
The user should NOT be eligible for credit
The eligibility should NOT show up in the user's list of eligibilities
Initially, the status should be None
Set the requirement to "satisfied" and check that it's actually set
Set the requirement to "failed" and check that it's actually set
make sure the 'order' on the 2nd requirement is set correctly (aka 1)
Configure a credit course with no requirements
A user satisfies a requirement. This could potentially  happen if there's a lag when the requirements are removed  after the course is published.
Since the requirement hasn't been published yet, it won't show  up in the list of requirements.
Configure a course with two credit requirements
Satisfy one of the requirements, but not the other
The user should not be eligible (because only one requirement is satisfied)
Satisfy the other requirement
Now the user should be eligible
Credit eligibility email should be sent
Now check that html email content has same logo image 'Content-ID'  as the attached logo image 'Content-ID'
test text email contents
Credit eligibility email should be sent  Now check that on sending eligibility notification again cached  logo image is used
Configure a credit course with no requirements
A user satisfies a requirement.  This could potentially  happen if there's a lag when the requirements are updated  after the course is published.
Since the requirement hasn't been published yet, it won't show  up in the list of requirements.
Credit eligibility email should be sent
Verify the email subject
By default, configure the database so that there is a single  credit requirement that the user has satisfied (minimum grade)
Disable the provider; it should be hidden from the list
now test that user gets empty dict for non existent credit provider
Initiate a credit request
Validate the UUID
Validate the timestamp
Initiate a request with automatic integration disabled
Initial status should be "pending"
Update the status
- 2 queries: Retrieve and update the request  - 1 query: Update the history table for the request.
Create the first request
Request UUID should be the same
Request should use the updated information
Create the first request
Provider updates the status
Attempting a second request raises an exception
Create the first request
Create a request for a second course
Check that the requests have the correct course number
User did not specify a mailing address
Request should include an empty mailing address field
Simulate users who registered accounts before the country field was introduced.  We need to manipulate the database directly because the country Django field  coerces None values to empty strings.
Request should include an empty country field
Simulate an error condition that should never happen:  a user is eligible for credit, but doesn't have a final  grade recorded in the eligibility requirement.
The request UUID must exist
Warm up the cache.
Hit the cache.
Verify only one request was made.
pylint:disable=missing-docstring,no-member
creating course, checkpoint location and user partition mock object.
creating user and enroll them.
Check that a user is in verified allow group if that user has skipped  any ICRV block.
no db queries this time.
Check that a user is in verified allow group if that user has approved status at  any ICRV block.  this will warm the cache.
no db queries this time.
Check that a user is in verified allow group if that user has denied at  any ICRV block.
this will warm the cache.
no db queries this time.
Check that a user is in honor mode.  any ICRV block.  this will warm the cache.
no db queries this time.
this will warm the cache.
no db queries this time.
This value must be set here, as setting it outside of a method results in issues with CMS/Studio tests.
Create a user and login, so that we can use session auth for the  tests that aren't specifically testing authentication or authorization.
Non-staff users should not have access to the API
Staff users should have access to the API
Retrieve a CSRF token
Ensure POSTs made with the token succeed.
Non-staff users should not have access to the API
Staff users should have access to the API
Verify the API returns the serialized CreditCourse
Verify the CreditCourse was actually created
Verify the API returns the serialized CreditCourse
Verify the API returns a list of serialized CreditCourse objects
Verify the serialized CreditCourse is returned
Verify the data was persisted
Enable provider integration
Add a single credit requirement (final grade)
Mark the user as having satisfied the requirement and eligible for credit.
Check that the user's request status is pending
Check request parameters
Enable provider integration
Cannot initiate a request because we cannot sign it
Authentication should NOT be required for this endpoint.
Simulate a callback from the credit provider with an invalid signature  Since the signature is invalid, we respond with a 403 Not Authorized.
Initially, the status should be "pending"
First call sets the status to approved
Second call succeeds as well; status is still approved
Create an additional credit provider
Initiate a credit request with the first provider
Attempt to update the request status for a different provider
Response should be a 404 to avoid leaking request UUID values to other providers.
Request status should still be 'pending'
Callback from the provider is not authorized, because the shared secret isn't configured.
Test a key that has type `unicode` but consists of ASCII characters  (This can happen, for example, when loading the key from a JSON configuration file)  When retrieving the shared secret, the type should be converted to `str`
Test a key that contains non-ASCII unicode characters  This should return `None` and log an error; the caller  is then responsible for logging the appropriate errors  so we can fix the misconfiguration.
Run the tests in split modulestore  While verification access will work in old-Mongo, it's not something  we're committed to supporting, since this feature is meant for use  in new courses.
Disconnect the signal receiver -- we'll invoke the update code ourselves
Check that the groups for the partition were created correctly
Delete the reverification block, then update the partitions
Add an additional ICRV block in another section
Delete the first ICRV block and update partitions
Delete the ICRV block, so the number of ICRV blocks is zero
2 calls: get the course (definitions + structures)  2 calls: look up ICRV blocks in the course (definitions + structures)
One ICRV block created in the setup method  Additional call to load the ICRV block
Total of two ICRV blocks (one created in setup method)  Additional call to load each ICRV block
Reload each component so we can see the changes
Sanity check -- initially user partitions should be empty
pylint: disable=no-member
make sure we don't have a proctoring requirement
make sure we don't have a proctoring requirement
practice proctored exams aren't requirements
make sure we don't have a proctoring requirement
There should be one ICRV requirement
Delete the parent section containing the ICRV block
Check that the ICRV block is no longer visible in the requirements
Create multiple ICRV blocks
Add two additional ICRV blocks that have no start date  and the same name.
Since the last two requirements have the same display name,  we need to also check that their internal names (locations) are the same.
Create multiple ICRV blocks
Enable the course for credit
Configure a credit provider for the course
Add a single credit requirement (final grade)
mark the grade as satisfied
mark the grade as satisfied
now the status should be "satisfied" when looking at the credit_requirement_status list
remove the requirement status.
now the status should be None when looking at the credit_requirement_status list
mark the grade as satisfied
remove the requirement status with the invalid user id
this should be a no-op
make sure it is not returned by default
this should be a no-op
mark the grade as satisfied
mark the grade as satisfied
Note: we need to check if found components have been orphaned  due to a bug in split modulestore (PLAT-799).  Once that bug  is resolved, we can skip the `_is_in_course_tree()` check entirely.
XBlocks that can be added as credit requirements
pylint: disable=not-callable
sort credit requirements list based on start date and put all the  requirements with no start date at the end of requirement list.
Note: Need to import here as there appears to be  a circular reference happening when launching Studio  process
Note: groups associated with particular runs of a course.  E.g. Fall 2012 and Spring  2013 versions of 6.00x will have separate groups.
This block will transactionally commit updates to CohortMembership and underlying course_user_groups.  Note the use of outer_atomic, which guarantees that operations are committed to the database on block exit.  If called from a view method, that method must be marked with @transaction.non_atomic_requests.
If the membership was newly created, all the validation and course_user_group logic was settled  with a call to self.save(force_insert=True), which gets handled above.
Needs to exist outside class definition in order to use 'sender=CohortMembership'
pylint: disable=invalid-name
ironic, isn't it?
this is the easy case :)
First check whether the course is cohorted (users shouldn't be in a cohort  in non-cohorted courses, but settings can change after course starts)
Otherwise assign the user a cohort.
Add the new and update the existing cohorts  Update the manual cohorts already present in CourseUserGroup
Migrate cohort settings for this course
Note: error message not translated because it is not exposed to the user (UI prevents this state).
Note: error message not translated because it is not exposed to the user (UI prevents this state).
Note: error message not translated because it is not exposed to the user (UI prevents this state).  If cohort_id is specified, update the existing cohort. Otherwise, create a new cohort.
Note: error message not translated because it is not exposed to the user (UI prevents this state).
If group_id was specified as None, unlink the cohort if it previously was associated with a group.
this is a string when we get it here
this will error if called with a non-int cohort_id.  That's ok--it  shouldn't happen for valid clients.
These strings aren't user-facing so don't translate them
this is a string when we get it here
this is a string when we get it here
this is a string when we get it here  add staff check to make sure it's safe if it's accidentally deployed.
We extract the data for the course wide discussions from the category map.
First, we're going to simulate some problem states that can arise during this window
When migrations were first run, the users were assigned to CohortMemberships correctly
run the post-CohortMembership command, dry-run
run the post-CohortMembership command, and commit it
verify that both databases agree about the (corrected) state of the memberships
In this case, allow the pre-existing entry to be "correct"
-*- coding: utf-8 -*-
pylint: disable=attribute-defined-outside-init  pylint: disable=no-member
course-wide discussion
inline discussion
verify the default cohort is not created when the course is not cohorted
create a cohorted course without any auto_cohorts
verify the default cohort is not yet created until a user is assigned
create enrolled users
mimic users accessing the discussion forum  Default Cohort will be created here
set auto_cohort_groups  these cohort config will have not effect on lms side as we are already done with migrations
We should expect the DoesNotExist exception because above cohort config have  no effect on lms side so as a result there will be no AutoGroup cohort present
Create a new cohort with random assignment
Create a new cohort with random assignment
Check that the name didn't change.
create inline & course-wide discussion to verify the different map.
pylint: disable=invalid-name
Not implemented for XMLModulestore, which is used by test_cohorts.
Not implemented for XMLModulestore, which is used by test_cohorts.
pylint: disable=no-member
Modify existing cohort
Add non-cohort group
Add users to cohort
Remove users from cohort
Clear users from cohort
Clear users from non-cohort group
Add cohorts to user
Remove cohorts from user
Clear cohorts from user
Clear non-cohort groups from user
Make sure we get a Http404 if there's no course
Make the course cohorted...
Add an auto_cohort_group to the course...
get_cohort should return None as no group is assigned to user
get_cohort should return a group for user
Add an auto_cohort_group to the course...
Add an auto_cohort_group to the course...
Now set the auto_cohort_group to something different  This will have no effect on lms side as we are already done with migrations
Make the auto_cohort_group list empty
Add an auto_cohort_group to the course  This will have no effect on lms side as we are already done with migrations
add manual cohorts to course 1
Note that the following get() will fail with MultipleObjectsReturned if race condition is not handled.
place student 0 into first cohort
move student from first cohort to second cohort
move the student out of the cohort
assign user to cohort (but cohort isn't linked to a partition group yet)  scheme should not yet find any link
link cohort to group 0  now the scheme should find a link
link cohort to group 1 (first unlink it from group 0)  scheme should pick up the link
unlink cohort from anywhere  scheme should now return nothing
don't assign the student to any cohort initially
get the default cohort, which is automatically created  during the `get_course_cohorts` API call if it doesn't yet exist
map that cohort to a group in our partition
The student will be lazily assigned to the default cohort  when CohortPartitionScheme.get_group_for_user makes its internal  call to cohorts.get_cohort.
link cohort to group 0  place student into cohort  check link is correct
to simulate a non-destructive configuration change on the course, create  a new partition with the same id and scheme but with groups renamed and  a group added
the link should still work
to simulate a destructive change on the course, create a new partition  with the same id, but different group ids.
to simulate another destructive change on the course, create a new  partition with a different id, but using the same groups.
When the staff user is masquerading as being in a None group  (within an existent UserPartition), we should treat that as  an explicit None, not defaulting to the user's cohort's  partition group.
student doesn't have a cohort
cohort isn't mapped to any partition group.
Default error message for user
Add `current_page` value, it's needed for pagination footer.
Add `start` value, it's needed for the pagination header.
Note: The countdown=0 kwarg is set to ensure the method below does not attempt to access the course  before the signal emitter has finished all operations. This is also necessary to ensure all tests pass.
Don't pass the 'fields' arg up to the superclass  Instantiate the superclass normally
Drop any fields that are not specified in the `fields` argument.
-*- coding: utf-8 -*-
User can create up to max_bookmarks_per_course bookmarks
Without Serialized.
As bookmarks are sorted by -created so we will compare in that order.
Invalid course id.
self.other_vertical_1 has two parents
The bookmark object already created should have been returned without modifications.
Find a leaf block.
Block does not exist
Block is an orphan
Get bookmark that does not exist.
Assert False for item that does not exist.
Add this blocks children to the stack so that we can traverse them as well.
Ideally we'd like to accept a CourseLocator; however, CourseLocator is not JSON-serializable (by default) so  Celery's delayed tasks fail to start. For this reason, callers should pass the course key as a Unicode string.
Catalogs live in course discovery, so we do not create any  tables in LMS. Instead we override the save method to not  touch the database, and use our API client to communicate  with discovery.
We want to fill in a few fields ourselves, so remove them  from the form so that the user doesn't see them.
Delete any existing applications if the user has decided to regenerate their credentials
If no username is provided, bounce back to this page.
Get rid of the colons at the end of the field labels.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Only add the 'value' attribute if a value is non-empty.
Assert that no POST was made to the catalog API
Assert that no PATCH was made to the Catalog API
pylint: disable=missing-docstring
Verify that initial save logs email errors properly  Verify object saved
Verify that updating request status logs email errors properly  Verify object saved
Service users may not have user profiles.
process the upload.
no matter what happens, delete the temporary file when we're done
image file validation.
generate profile pic and thumbnails and store them
update the user account to reflect that a profile image is available.
send client response.
update the user account to reflect that the images were removed.
remove physical files from storage.
send client response.
The if/else dance below is required, because PIL raises an exception if  you pass None as the value of the exif kwarg.
get the size of the image file and ensure it's square jpeg
subclasses should override this with the name of the view under test, as  per the urls.py configuration.
Reset the mock event tracker so that we're not considering the  initial profile creation events.
it's necessary to reload this model from the database since save()  would have been called on another instance.
Try another upload and make sure that a second event is emitted.
Ignore previous event
Ignore UserProfileFactory creation events.
Override Client.login method to update cookies with safe  cookies.
pre-verify steps 3, 4, 5
verify step 1: safe cookie data is parsed
verify step 2: cookie value is replaced with parsed session_id
verify step 3: session set in request
verify steps 4, 5: user_id stored for later verification
delete_cookies is called even if there are no cookies set
create and verify
serialize
parse and verify
compare
Should return the same digest twice.
The user ID is sometimes not set for  3rd party Auth and external Auth transactions  as some of the session requests are made as  Anonymous users.
For security reasons, we don't support requests with  older or invalid session cookie models.
The process_request pipeline has been short circuited so  return the response.
Mobile apps have custom handling of authentication failures. They  should *not* be redirected to the website's login page.
The user at response time is expected to be None when the user  is logging out. To prevent extra noise in the logs,  conditionally set the log level.
Create safe cookie data that binds the user with the session  in place of just storing the session_key in the cookie.
Update the cookie's value with the safe_cookie_data.
Nose runs setUpClass methods even if a class decorator says to skip  the class: https://github.com/nose-devs/nose/issues/946  So, skip the test class here if we are not in the LMS.
Check whether adding new resource is successful
Add resources, assume correct here, tested in test_add_resource
Test
Test
Test
Test
Test
Test
Test
Test
Test
Test
Test
Test
Test
Test
Upload file with wrong extension name or magic number
Upload file with correct extension name and magic number
... and log in as the appropriate user
Nose runs setUpClass methods even if a class decorator says to skip  the class: https://github.com/nose-devs/nose/issues/946  So, skip the test class here if we are not in the LMS.
We return a little bit of metadata helpful for debugging.  What is in this is not a defined part of the API contract.
This is a stop-gap until we can load OLX and/or OLX from  normal workbench scenarios
pylint: disable=no-member
We confirm we don't have errors rendering the student view
We confirm state sticks around
And confirm we render correctly
Nose runs setUpClass methods even if a class decorator says to skip  the class: https://github.com/nose-devs/nose/issues/946  So, skip the test class here if we are not in the LMS.
Generate a SECRET_KEY for this build
for use in openedx/core/djangoapps/profile_images/images.py
Django 1.7's setup is required before touching translated strings.
Add any paths that contain templates here, relative to this directory.
Add any paths that contain custom static files (such as style sheets) here,  relative to this directory. They are copied after the builtin static files,  so a file named "default.css" will overwrite the builtin "default.css".
django configuration  - careful here
Add any paths that contain templates here, relative to this directory.
Add any paths that contain custom static files (such as style sheets) here,  relative to this directory. They are copied after the builtin static files,  so a file named "default.css" will overwrite the builtin "default.css".
0,     os.path.abspath(         os.path.normpath(             os.path.dirname(__file__) + '/../../../..'         )    )
django configuration  - careful here
List of patterns, relative to source directory, that match files and  directories to ignore when looking for source files.
If your documentation needs a minimal Sphinx version, state it here.
Add any paths that contain templates here, relative to this directory.
The suffix of source filenames.
The encoding of source files.
The master toctree document.
General information about the project.
The version info for the project you're documenting, acts as replacement for  |version| and |release|, also used in various other places throughout the  built documents.  The short X.Y version.  The full version, including alpha/beta/rc tags.
There are two options for replacing |today|: either, you set today to some  non-false value, then it is used:  Else, today_fmt is used as the format for a strftime call.
List of patterns, relative to source directory, that match files and  directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all documents.
If true, '()' will be appended to :func: etc. cross-reference text.
If true, the current module name will be prepended to all description  unit titles (such as .. function::).
If true, sectionauthor and moduleauthor directives will be shown in the  output. They are ignored by default.
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
If true, keep warnings as "system message" paragraphs in the built documents.
The theme to use for HTML and HTML Help pages.  See the documentation for  a list of builtin themes.
Theme options are theme-specific and customize the look and feel of a theme  further.  For a list of options available for each theme, see the  documentation.
Add any paths that contain custom themes here, relative to this directory.
The name for this set of Sphinx documents.  If None, it defaults to  "<Studio> v<release> documentation".
A shorter title for the navigation bar.  Default is the same as html_title.
The name of an image file (relative to this directory) to place at the top  of the sidebar.
The name of an image file (within the static path) to use as favicon of the  docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32  pixels large.
Add any paths that contain custom static files (such as style sheets) here,  relative to this directory. They are copied after the builtin static files,  so a file named "default.css" will overwrite the builtin "default.css".
If not '', a 'Last updated on:' timestamp is inserted at every page bottom,  using the given strftime format.
If true, SmartyPants will be used to convert quotes and dashes to  typographically correct entities.
Custom sidebar templates, maps document names to template names.
Additional templates that should be rendered to pages, maps page names to  template names.
If false, no module index is generated.
If false, no index is generated.
If true, the index is split into individual pages for each letter.
If true, links to the reST sources are added to the pages.
If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
If true, an OpenSearch description file will be output, and all pages will  contain a <link> tag referring to it.  The value of this option must be the  base URL from which the finished HTML is served.
This is the file name suffix for HTML files (e.g. ".xhtml").
Output file base name for HTML help builder.
The paper size ('letterpaper' or 'a4paper').
The font size ('10pt', '11pt' or '12pt').
Additional stuff for the LaTeX preamble.
Grouping the document tree into LaTeX files. List of tuples  (source start file, target name, title, author, documentclass [howto/manual]).
The name of an image file (relative to this directory) to place at the top of  the title page.
For "manual" documents, if this is true, then toplevel headings are parts,  not chapters.
If true, show page references after internal links.
If true, show URL addresses after external links.
Documents to append as an appendix to all manuals.
If false, no module index is generated.
One entry per manual page. List of tuples  (source start file, name, description, authors, manual section).
If true, show URL addresses after external links.
Documents to append as an appendix to all manuals.
If false, no module index is generated.
How to display URL addresses: 'footnote', 'no', or 'inline'.
If true, do not generate a @detailmenu in the "Top" node's menu.
Bibliographic Dublin Core info.
The language of the text. It defaults to the language option  or en if the language is not set.
The scheme of the identifier. Typical schemes are ISBN or URL.
The unique identifier of the text. This can be a ISBN number  or the project homepage.
A unique identification for the text.
A tuple containing the cover image and cover page html template filenames.
A sequence of (type, uri, title) tuples for the guide element of content.opf.
HTML files that should be inserted before the pages created by sphinx.  The format is a list of tuples containing the path and title.
HTML files shat should be inserted after the pages created by sphinx.  The format is a list of tuples containing the path and title.
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
Allow duplicate toc entries.
Fix unsupported image types using the PIL.
Scale large images.
If 'no', URL addresses will not be shown.
If false, no index is generated.
Example configuration for intersphinx: refer to the Python standard library.
Maintain backwards compatibility with manage.py,  which calls "studio" "cms"
Ensure that we have a directory to put logs and reports
load data in db_fixtures
Create course in order to seed forum data underneath. This is  a workaround for a race condition. The first time a course is created;  role permissions are set up for forums.
this means it's already been done
Construct "multiprocess" nosetest substring
Clear any test data already in Mongo or MySQLand invalidate  the cache
load data in db_fixtures
load courses if self.imports_dir is set
Ensure the test servers are available
Default to running all tests if no specific test is specified
Skip any additional commands (such as nosetests) if running in  servers only mode
If imports_dir has been specified, assume the files are  already there -- no need to fetch them from github. This  allows someome to crawl a different course. They are responsible  for putting it, un-archived, in the directory.
Otherwise, obey `--skip-fetch` command and use the default  test course.  Note that the fetch will also be skipped when  using `--fast`.
Uses __enter__ and __exit__ for context  run the tests for this class, and for all subsuites
Since we are using SQLLite, we can reset the database by deleting it on disk.
Copy the cached database to the test root directory
Create the cache if it doesn't already exist
Handle "--failed" as a special case: we want to re-run only  the tests that failed within our Django apps  This sets the --failed flag for the nosetests command, so this  functionality is the same as described in the nose documentation
This makes it so we use nose's fail-fast feature in two cases.  Case 1: --fail_fast is passed as an arg in the paver command  Case 2: The environment variable TESTS_FAIL_FAST is set as True
Use one process per core for LMS tests, and no multiprocessing  otherwise.
We need to use $DIR/*, rather than just $DIR so that  django-nose will import them early in the test process,  thereby making sure that we load any django models that are  only defined in test files.
We delete the files but preserve the directory structure  so that coverage.py has a place to put the reports.
We delete the files but preserve the directory structure  so that coverage.py has a place to put the reports.
The mongo command will connect to the service,  failing with a non-zero exit code if it cannot connect.
Attempt to set a key in memcache. If we cannot do so because the  service is not available, then this will return False.
Root of the git repository (edx-platform)
Reports Directory
Python unittest dirs
For the time being, stubs are used by both the bok-choy and lettuce acceptance tests  For this reason, the stubs package is currently located in the Django app called "terrain"  where other lettuce configuration is stored.
Directory that videos are served from
Mongo databases that will be dropped before/after the tests run
Test Ids Directory
Directory for i18n test reports
Service variant (lms, cms, etc.) configured with an environment variable  We use this to determine which envs.json file to load.
Otherwise, load the file as JSON and return the resulting dict
pylint: disable=unexpected-keyword-arg
If the user is performing a dry run of a task, then just log  the command strings and return so that no destructive operations  are performed.
pylint: disable=broad-except
pylint: disable=unexpected-keyword-arg
Wait for process to actually finish
Subsuites to be added to the main suite
Main suite to be run
This may be that the coverage files were generated using -p,  try to combine them to the one file that we need.
Check if the .coverage data file is larger than the base file,  because coverage combine will always at least make the "empty" data  file even when there isn't any data to be combined.
Generate the coverage.py XML report  Generate the coverage.py HTML report
Find all coverage XML files (both Python and JavaScript)
Generate the diff coverage reports (HTML and console)
Developers can have private requirements, for local copies of github repos,  or favorite debugging tools, etc.
For files, hash the contents of the file
Compare the old hash to the new hash  If they do not match (either the cache hasn't been created, or the files have changed),  then execute the code within the block.
To add a package to the uninstall list, just add it to this list! No need  to touch any other part of this file.
Run pip to find the packages we need to get rid of.  Believe it or not,  edx-val is installed in a way that it is present twice, so we have a loop  to really really get rid of it.
Uninstall the pacakge
We tried three times and didn't manage to get rid of the pests.
Write our version.
Include all of the requirements files in the fingerprint.
Also fingerprint the directories where packages get installed:  ("/edx/app/edxapp/venvs/edxapp/lib/python2.7/site-packages")
In a virtualenv, "-e installs" get put in a src directory.
Also fingerprint this source file, so that if the logic for installations  changes, we will redo the installation.
Directory to put the pylint report in.  This makes the folder if it doesn't already exist.
Make sure the metrics subdirectory exists
Directory to put the pylint report in.  This makes the folder if it doesn't already exist.
Print number of violations to log
Also write the number of violations to a file
Fail number of violations is greater than the limit
An example string:  common/lib/xmodule/xmodule/tests/test_conditional.py:21: [C0111(missing-docstring), DummySystem] Missing docstring  More examples can be found in the unit tests for this method
If the string is parsed into four parts, then we've found a violation. Example of split parts:  test file, line number, violation name, violation details
Make sure the metrics subdirectory exists
Print number of violations to log
Also write the number of violations to a file
Fail if any violations are found
Ensure directory structure is in place: metrics dir, and an empty complexity report dir.
Record the metric
Fail if number of violations is greater than the limit
Record the metric  Print number of violations to log.
Print number of violations to log.
Record the metric  Output report to console.
Raise a build error if the file is not found
Example of the last line of a complexity report: "Average complexity: A (1.93953443446)"
Example of the last line of a jshint report (for example): "3482 errors"
An AttributeError will occur if the regex finds no matches.  A ValueError will occur if the returned regex cannot be cast as a float.
An AttributeError will occur if the regex finds no matches.  A ValueError will occur if the returned regex cannot be cast as a float.
Directory to put the diff reports in.  This makes the folder if it doesn't already exist.
Save the pass variable. It will be set to false later if failures are detected.
Run pep8 directly since we have 0 violations on master
Print number of violations to log
Also write the number of violations to a file
Set up for diff-quality pylint call   Set the string, if needed, to be used for the diff-quality --compare-branch switch.
Set the string, if needed, to be used for the diff-quality --fail-under switch.
run diff-quality for pylint.
run diff-quality for jshint.
If one of the quality runs fails, then paver exits with an error when it is finished
Directory to install static vendor files
To refresh the hash values of static xmodule content
Note: import sass only when it is needed and not at the top of the file.  This allows other paver commands to operate even without libsass being  installed. In particular, this allows the install_prereqs command to be  used to install the dependency.
Skip processing of the libraries if this is just a dry run
Ensure that the vendor directory exists
Copy each file to the vendor directory, overwriting any existing file.
Don't watch assets when performing a dry run
when running as a separate process, the main thread needs to loop  in order to allow for shutdown by contrl-c
Note: Bok Choy uses firefox if SELENIUM_BROWSER is not set. So we are using  firefox as the default here.
The default settings use DEBUG mode for running the server which means that  the optimized assets are ignored, so we skip collectstatic in that case  to save time.
First update assets for both LMS and Studio but don't collect static yet
Now collect static for each system separately with the appropriate settings.  Note that the default settings use DEBUG mode for running the server which  means that the optimized assets are ignored, so we skip collectstatic in that  case to save time.
Install an asset watcher to regenerate files that change
pylint: disable=line-too-long
Reset Environment back to original state
Check that we exited with a failure status code.
Check that we exited with a failure status code.
No System Exit is expected
No System Exit is expected
No exception should be raised
Mock the paver @needs decorator
Mock shell commands
Cleanup mocks
the cmd will dumbly compose whatever we pass in for the default_store
Mock shell commands
Cleanup mocks
Create a mock Paver environment
Don't run pre-reqs
Pep8 violations should be ignored.
Mock the paver @needs decorator
Temporary file infrastructure
Cleanup various mocks and tempfiles
Test that pep8, pylint, and jshint were called by counting the calls to  _get_pep8_violations (for pep8) and sh (for diff-quality pylint & jshint)
Test that pylint is NOT called by counting calls
Essentially mock diff-quality exiting with 1
Essentially mock diff-quality exiting with 1
Mock the paver @needs decorator
Cleanup mocks
Need to then compile the new dummy strings
Generate static i18n JS files.
Patch the xml libs before anything else.
can override with '--contracts' argument
This will trigger django-admin.py to print out its help
Dynamically calculate the version based on django.VERSION.
RE for option descriptions without a '--' prefix
code-block directives
code-block directives
Some filenames have '_', which is special in latex.
Prevent rawsource from appearing in output a second time.
Don't use border=1, which docutils does by default.
Make sure we get the version of this copy of Django
Add any Sphinx extension module names here, as strings. They can be extensions  coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
Spelling check needs an additional module that is not installed by default.  Add it only if spelling check is requested so docs can be generated without it.
Spelling language.
Location of word list.
The suffix of source filenames.
The master toctree document.
General substitutions.
The "development version" of Django
Location for .po/.mo translation files used when language is set
There are two options for replacing |today|: either, you set today to some  non-false value, then it is used:  today = ''  Else, today_fmt is used as the format for a strftime call.
List of patterns, relative to source directory, that match files and  directories to ignore when looking for source files.
If true, '()' will be appended to :func: etc. cross-reference text.
If true, the current module name will be prepended to all description  unit titles (such as .. function::).
If true, sectionauthor and moduleauthor directives will be shown in the  output. They are ignored by default.
The name of the Pygments (syntax highlighting) style to use.
The theme to use for HTML and HTML Help pages.  See the documentation for  a list of builtin themes.
Add any paths that contain custom themes here, relative to this directory.
If not '', a 'Last updated on:' timestamp is inserted at every page bottom,  using the given strftime format.
If true, SmartyPants will be used to convert quotes and dashes to  typographically correct entities.
HTML translator class for the builder
Additional templates that should be rendered to pages, maps page names to  template names.
Output file base name for HTML help builder.
Grouping the document tree into LaTeX files. List of tuples  (source start file, target name, title, author, document class [howto/manual]).  latex_documents = []
One entry per manual page. List of tuples  (source start file, name, description, authors, manual section).
List of tuples (startdocname, targetname, title, author, dir_entry,  description, category, toctree_only)
Bibliographic Dublin Core info.
The HTML theme for the epub output. Since the default themes are not optimized  for small screen space, using the same theme for HTML and epub output is  usually not wise. This defaults to 'epub', a theme designed to save visual  space.
A tuple containing the cover image and cover page html template filenames.
-- ticket options
Fail loudly when content has control chars (unsupported in XML 1.0)  See http://www.w3.org/International/questions/qa-controls
create a dummy class for Python 3.5+ where it's been removed
If there's already a max-age header but we're being asked to set a new  max-age, use the minimum of the two ages. In practice this happens when  a decorator and a piece of middleware both operate on a given view.
Allow overriding private caching and vice versa
We need to keep the cookies, see ticket 4994.
If-None-Match must be ignored if original result would be anything  other than a 2XX or 304 status. 304 status would result in no change.  http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.htmlsec14.26
If-Modified-Since must be ignored if the original result was not a 200.  http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.htmlsec14.25
first check if LocaleMiddleware or another middleware added  LANGUAGE_CODE to request, then fall back to the active language  which in turn can also fall back to settings.LANGUAGE_CODE
if there is no Vary header, we still need a cache key  for the request.build_absolute_uri()
First find the logging configuration function ...
... then invoke it with the logging settings
Since we add a nicely formatted traceback on our own, create a copy  of the log record without the exception data.
Put 2XX first, since it should be the common case
Any 5XX, or any other response
Configuration for urlize() function.
List of possible strings used for bullets in bulleted lists.
Escape every ASCII character with a value less than 32.
Tilde is part of RFC3986 Unreserved Characters  http://tools.ietf.org/html/rfc3986section-2.3  See also http://bugs.python.org/issue16285
Handle IDN before quoting.  invalid IPv6 URL (normally square brackets in hostname part).
Separately unquoting key/value, so as to not mix querystring separators  included in query values. See 22267.  urlencode will take care of quoting
Remove trail for unescaped if it was not consumed by unescape
Trail was consumed by unescape (as end-of-entity marker), move it to text
Continue trimming until middle remains unchanged.
Trim trailing punctuation.
lead: Current punctuation trimmed from the beginning of the word.  middle: Current state of the word.  trail: Current punctuation trimmed from the end of the word.  Deal with punctuation.
Do a linear scan to work out the special features of this pattern. The  idea is that we scan once here and collect all the information we need to  make future decisions.
A "while" loop is used here because later on we need to be able to peek  at the next character and possibly go around without consuming another  one at the top of the loop.
Replace "any character" with an arbitrary representative.
FIXME: One day we'll should do this, but not in 1.0.
All of these are ignorable. Walk to the end of the  group.
Non-capturing group
Anything else, other than a named group, is something  we cannot reverse.
Quantifiers affect the previous item in the result list.  We had to look ahead, but it wasn't need to compute the  quantifier, so use this character next time around the  main loop.
Anything else is a literal.
A case of using the disjunctive form. No results for you!
Consume the trailing '?', if necessary.
-*- encoding: utf-8 -*-
For backwards compatibility. (originally in Django, then added to six 1.9)
The input is the result of a gettext_lazy() call.
The input is the result of a gettext_lazy() call.
backwards compatibility for Python 2
I know about `os.sep` and `os.altsep` but I want to leave  some flexibility for hardcoding separators.
Do not return default here because __setitem__() may store  another value -- QueryDict.__setitem__() does. Look it up.
Do not return default_list here because setlist() may store  another value -- QueryDict.setlist() does. Look it up.
All list mutation functions complain.
Useful for very coarse version differentiation.
Jython always uses 32 bits.
It's possible to have sizeof(long) != sizeof(Py_ssize_t).
32-bit
64-bit
This is a bit ugly, but it avoids running this again by  removing this descriptor.
Subclasses should override this
in case of a reload
Add windows specific modules.
This requires a bit of explanation: the basic idea is to make a dummy  metaclass for one level of class instantiation that replaces itself with  the actual metaclass.
memoryview and buffer are not strictly equivalent, but should be fine for  django core usage (mainly BinaryField). However, Jython doesn't support  buffer (see http://bugs.jython.org/issue1521), so we have to be careful.
Support datetime objects older than 1900
Force ints to unicode
Force ints to unicode
datetime.now(tz=utc) is slower, as documented in django.utils.timezone.now
Spec: http://blogs.law.harvard.edu/tech/rss
Categories.
Spec: https://tools.ietf.org/html/rfc4287
Summary.
Categories.
Rights.
This isolates the decision of what the system default is, so calling code can  do "feedgenerator.DefaultFeed" instead of "feedgenerator.Rss201rev2Feed".
We only support timezone when formatting datetime objects,  not date objects (timezone information not appropriate),  or time objects (against established django policy).
Have to use tzinfo.tzname and not datetime.tzname  because datatime.tzname does not expect Unicode
pytz raises AmbiguousTimeError during the autumn DST change.  This happens mainly when __init__ receives a naive datetime  and sets self.timezone = get_default_timezone().
pytz raises AmbiguousTimeError during the autumn DST change.  This happens mainly when __init__ receives a naive datetime  and sets self.timezone = get_default_timezone().
`offset` is a datetime.timedelta. For negative values (to the west of  UTC) only days can be negative (days=-1) and seconds are always  positive. e.g. UTC-1 -> timedelta(days=-1, seconds=82800, microseconds=0)  Positive offsets have days=0
pytz raises AmbiguousTimeError during the autumn DST change.  This happens mainly when __init__ receives a naive datetime  and sets self.timezone = get_default_timezone().
format_cache is a mapping from (format_type, lang) to the format string.  By using the cache, it is possible to avoid running get_format_modules  repeatedly.
Return the general setting by default
Special case where we suspect a dot meant decimal separator (see 22171)
This relies on os.environ['TZ'] being set to settings.TIME_ZONE.
for pytz timezones
for regular tzinfo objects
If `value` is naive, astimezone() will raise a ValueError,  so we don't need to perform a redundant check.  This method is available for pytz time zones.
timeit shows that datetime.now(tz=utc) is 24% slower
This method is available for pytz time zones.
Check that we won't overwrite the timezone of an aware datetime.  This may be wrong around DST changes!
If `value` is naive, astimezone() will raise a ValueError,  so we don't need to perform a redundant check.  This method is available for pytz time zones.
make an integer out of the number
Standard connector type. Clients usually won't use this at all and  subclasses will usually override the value.
Attempt to import the app's module.
Reset the registry to the state before the last import  as this import will have to reoccur on the next request and  this could raise NotRegistered and AlreadyRegistered  exceptions (see 8245).
Decide whether to bubble up this error. If the app just  doesn't have the module in question, we can ignore the error  attempting to import it, otherwise we want it to bubble up.
package isn't a package.
None indicates a cached miss; see mark_miss() in Python/import.c.
Since the remainder of this function assumes that we're dealing with  a package (module with a __path__), so if it's not, then bail here.
Exhausted the search, so the module cannot be found.
Capitalizes the first letter of a string.
The truncation text didn't contain the %(truncated_text)s string  replacement argument so just append it to the text.  But don't append the truncation text if the current text already  ends in this.
Don't consider combining characters  as adding to the string length
Return the truncated string
Return the original string since no truncation was necessary
Count non-HTML chars/words and keep note of open tags
Checked through whole string
It's an actual non-HTML word or char  Check for tag
Don't worry about non tags or tags after our truncate point
Close any tags still open  Return string
Translators: This string is used as a separator between list elements
A directory
This code was mostly based on ipaddr-py  Copyright 2007 Google Inc. https://github.com/google/ipaddr-py  Licensed under the Apache License, Version 2.0 (the "License").
This algorithm can only handle fully exploded  IP strings
If needed, unpack the IPv4 and return straight away  - no need in running the rest of the algorithm
Remove leading zeroes
Determine best hextet to compress  Start of a sequence of zeros.  This is the longest sequence of zeros so far.
not an ipv4 mapping
already sanitized
We need to have at least one ':'.
We can only have one '::' shortener.
'::' should be encompassed by start, digits or end.
We can never have more than 7 ':' (1::2:3:4:5:6:7:8 is invalid)
If we have no concatenation, we need to have 8 fields with 7 ':'.  We might have an IPv4 mapped address.
We've already got a longhand ip_str.
If there is a ::, we need to expand it with zeroes  to get to 8 hextets - unless there is a dot in the last hextet,  meaning we're doing v4-mapping
Deal with leapyears by subtracing the number of leapdays
Break the definition into the role,  plus the list of specific instructions.  The role must be in upper case
All remaining instructions are options
The nocolor palette has all available roles.  Use that palette as the basis for determining  if the role is valid.
If there are no colors specified, return the empty palette.
Need to preserve any existing attributes of 'func', including the name.
Defer running of process_response until after the template  has been rendered:
etag_str has wrong format, treat it as an opaque string then
Chrome treats \ completely as / in paths but it could be part of some  basic auth credentials so we need to check both URLs.
Handle case of a control-name with no equal sign
The Trans class is no more needed, so remove it from the namespace.
String doesn't contain a placeholder for the number
Translations are cached in a dictionary for every language.  The active translations are stored by threadid to make them thread local.
The default translation is based on the settings file.
magic gettext number to separate context from message
A module-level cache is used for caching 'django' translations
default lang should have at least one translation file available.
No catalogs found for this language, set an empty catalog.
Take plural and _info from first catalog found (generally Django's).
If we don't have a real translation object, assume it's the default language.
str() is allowing a bytestring message to remain bytestring on Python 2
Returns an empty value of the corresponding type if an empty message  is given, instead of metadata, which is the default gettext behavior.
Translation not found  force unicode, because lazy version expects unicode
Translation not found
if fr-fr is not supported, try fr-ca.
Adding the u prefix allows gettext to recognize the Unicode string  (26093).
This library does not support strftime's "%s" or "%y" format strings.  Allowed if there's an even number of "%"s because they are escaped.
Also finds overlaps
For every non-leap year century, advance by  6 years to get into the 28-year repeat cycle
You can't trivially replace this with `functools.partial` because this binds  to classes and returns bound instances, whereas functools.partial (on  CPython) is a type and its instances don't bind.
All __promise__ return the same wrapper method, they  look up the correct implementation when called.
Builds a wrapper around some magic method  Automatically triggers the evaluation of a lazy value and  applies the given magic method of the result type.
object defines __str__(), so __prepare_class__() won't overload  a __str__() method from the proxied class.
Creates the proxy object, instead of the actual value.
Avoid infinite recursion when tracing __init__ (19456).
Note: if a subclass overrides __init__(), it will likely need to  override __copy__() and __deepcopy__() as well.
Assign to __dict__ to avoid infinite __setattr__ loops.
We have to explicitly override __getstate__ so that older versions of  pickle don't try to pickle the __dict__ (which in the case of a  SimpleLazyObject may contain a lambda). The value will end up being  ignored by our __reduce__ and custom unpickler.
If uninitialized, copy the wrapper. Use type(self), not  self.__class__, because the latter is proxied.
If initialized, return a copy of the wrapped object.
We have to use type(self), not self.__class__, because the  latter is proxied.
Introspection support
Need to pretend to be the wrapped class, for the sake of objects that  care about this (especially in equality tests)
If uninitialized, copy the wrapper. Use SimpleLazyObject, not  self.__class__, because the latter is proxied.
If initialized, return a copy of the wrapped object.
We have to use SimpleLazyObject, not self.__class__, because the  latter is proxied.
This import does nothing, but it's necessary to avoid some race conditions  in the threading module. See http://code.djangoproject.com/ticket/2330 .
Test whether inotify is enabled and likely to work
No need to update watches when request serves files.  (sender is supposed to be a django.core.handlers.BaseHandler subclass)
New modules may get imported when a request is processed.
Block until an event happens.
If we are here the code must have changed.
get the filename from the last item in the stack
We need to generate a derived key from our base key.  We can do this by  passing the key_salt and our base key through a pseudo-random function and  SHA1 works nicely.
Prefer the stdlib implementation, when available.
Define the old method as a wrapped call to the new method.
Originally from https://bitbucket.org/ned/jslex
slash will mean division
C doesn't grok regexes, and they aren't needed for gettext,  so just output a string instead.
C can't deal with Unicode escapes in identifiers.  We don't  need them for gettext anyway, so replace them with something  innocuous
no caching, just do a statistics update after a successful call
simple caching without ordering or size limit
We capture the arguments to make returning them trivial
backwards compatibility for Python 2
backwards compatibility for Python 2
A class-based view
A function-based view
Build a namespaced resolver for the given parent URLconf pattern.  This makes it possible to have captured parameters in the parent  URLconf pattern.
regex is either a string representing a regular expression, or a  translatable string (using ugettext_lazy) representing a regular  expression.
Don't bother to output the whole list, it can be huge
Merge captured arguments in match with submatch
If there are *any* named groups, ignore all non-named groups.  Otherwise, pass all non-named arguments as positional arguments.
No handler specified in file; use lazy import, since  django.conf.urls imports this file.
SCRIPT_NAME prefixes for each thread are stored here. If there's no entry for  the current thread (which is the only one we ever access), it is assumed to  be empty.
Overridden URLconfs for each thread are stored here.
Convert 'django.views.news.stories.story_detail' to  ['django.views.news.stories', 'story_detail']
If it is a model class or anything else with ._default_manager
If it's a model, use get_absolute_url()
Expand the lazy instance, as it can cause issues when it is passed  further to some Python functions like urlparse.
Handle relative URLs
Next try a reverse URL resolution.  If this is a callable, re-raise.  If this doesn't "feel" like a URL, re-raise.
Finally, fall back and assume it's a URL
Content-Length should contain the length of the body we are about  to receive.
This means we shouldn't continue...raise an error.
For compatibility with low-level network APIs (with 32-bit integers),  the chunk size should be < 2^31, but still divisible by 4.
We have to import QueryDict down here to avoid a circular import.
HTTP spec says that Content-Length >= 0 is valid  handling content-length == 0 before continuing
See if any of the handlers take care of the parsing.  This allows overriding everything if need be.  Check to see if it was handled
Create the data structures to be used later.
Instantiate the parser and stream:
Whether or not to signal a file-completion at the beginning of the loop.
Number of bytes that have been read.  To count the number of keys in the request.  To limit the amount of data read from the request.
We run this at the beginning of the next loop  since we cannot be sure a file is complete until  we hit the next boundary/part of the multipart content.
Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FIELDS.
Avoid reading more than DATA_UPLOAD_MAX_MEMORY_SIZE.
Add two here to make the check consistent with the  x-www-form-urlencoded check that includes '&='.
Since this is only a chunk, any error is an unfixable error.
If the chunk received by the handler is None, then don't continue.
Just use up the rest of this file...
Handle file upload completions on next iteration.
If this is neither a FIELD or a FILE, just exhaust the stream.
Make sure that the request data is all fed
Signal that the upload has completed.
If it returns a file object, then set the files dict.
Free up all file handles.  FIXME: this currently assumes that upload handlers store the file as 'file'  We should document that... (Maybe add handler.free_file to complement new_file)
do the whole thing in one shot if no limit was provided.
otherwise do some bookkeeping to return exactly enough  of the stream and stashing any extra content we get from  the producer
rollback an additional six bytes because the format is like  this: CRLF<boundary>[--CRLF]
Try to use mx fast string search if available. Otherwise  use Python find. Wrap the latter for consistency.
There's nothing left, we should just return and mark as done.
Stream at beginning of header, look for end of header  and parse it if found. The header must fit within one  chunk.
'find' returns the top of these four bytes, so we'll  need to munch them later to prevent them from polluting  the payload.
we find no header, so we just mark this fact and pass on  the stream verbatim
here we place any excess chunk back onto the stream, as  well as throwing away the CRLFCRLF bytes from above.
Eliminate blank lines  This terminology ("main value" and "dictionary of  parameters") is from the Python docs.
Iterate over each part
Lang/encoding embedded in the value (like "filename*=UTF-8''file.ext")  http://tools.ietf.org/html/rfc2231section-4
http://bugs.python.org/issue2193 is fixed in Python 3.3+.
Apply the fix from http://bugs.python.org/issue22775 where  it's not fixed in Python itself  allow assignment of constructed Morsels (e.g. for pickling)
Assume an empty name per  https://bugzilla.mozilla.org/show_bug.cgi?id=169091
unquote using Python's algorithm.
Leave self._reason_phrase unset in order to use the default  reason phrase for status code.
Extract the charset and strip its double quotes
Ensure string is valid in given charset
Convert bytestring using given charset
Ensure string is valid in given charset
Convert unicode string to given charset
Wrapping in str() is a workaround for 12422 under Python 2.
Add one second so the date matches exactly (a fraction of  time gets lost between converting to a timedelta and  then the date string).  Just set max_age - the max_age logic will set expires.
IE requires expires, so set it if hasn't been already.
Handle string types -- we can't rely on force_bytes here because:  - under Python 3 it attempts str conversion first  - when self._charset != 'utf-8' it re-encodes the content
Handle non-string types (16494)
The WSGI server must call this method upon completion of the request.  See http://blog.dscpl.com.au/2012/10/obligations-for-calling-close-on.html
Content is a bytestring. See the `content` property methods.
`streaming_content` should be an iterable of bytestrings.  See the `streaming_content` property methods.
Ensure we can never iterate on "value" more than once.
The encoding used in GET/POST dicts. None means use default setting.
There is no hostname validation when DEBUG=True
Make it an absolute url (but schemeless and domainless) for the  edge case that the path starts with '//'.
Join the constructed URL with the provided location, which will  allow the provided ``location`` to apply query strings to the  base path as well as override the host, if it begins with //
If there are no upload handlers defined, initialize them from settings.
Use already read data
These are both reset in __init__, but is specified here at the class  level so that unpickling will have valid values
query_string normally contains URL-encoded data, a subset of ASCII.  ... but some user agents are misbehaving :-(
It's an IPv6 address without a port.
List of browsers to dynamically create test classes for.  Sentinel value to differentiate browser-specific instances.
quit() the WebDriver before attempting to terminate and join the  single-threaded LiveServerThread to avoid a dead lock if the browser  kept a connection alive.
If tblib isn't installed, pickling the traceback will always fail.  However we don't want tblib to be required for running the tests  when they pass or fail as expected. Drop the traceback when an  expected failure occurs.
The current implementation of the parallel test runner requires  multiprocessing to start subprocesses with fork().  On Python 3.4+: if multiprocessing.get_start_method() != 'fork':
connection.settings_dict must be updated in place for changes to be  reflected in django.db.connections. If the following line assigned  connection.settings_dict = settings_dict, new threads would connect  to the default database instead of the appropriate clone.
In case someone wants to modify these in a subclass.
__init__.py all the way down? give up.
Try discovery if path is a package or directory
Make unittest forget the top-level dir it calculated from this  run, to support running tests from two different top-levels.
Since tests are distributed across processes on a per-TestCase  basis, there's no need for more processes than TestCases.
If there's only one TestCase, parallelization isn't needed.
Maps db signature to dependencies of all it's aliases
If the database is marked as a test mirror, save the alias.
Store a tuple with DB parameters that uniquely identify it.  If we have two aliases with the same values for that tuple,  we only need to create the test database once.
Configure the test mirrors.
removing last children if it is only whitespace  this can result in incorrect dom representations since  whitespace between inline tags like <span> is significant
Removing ROOT element if it's not necessary
To simplify Django's test suite; not meant as a public API
Settings that may not work well when using 'override_settings' (19031)
Reset local time zone cache
Considering the current implementation of the signals framework,  stacklevel=5 shows the line containing the override_settings call.
Set up middleware if needed. We couldn't do this earlier, because  settings weren't available.
sneaky little hack so that we can easily get round  CsrfViewMiddleware.  This makes life easier, and is probably  required for backwards compatibility with external tests against  admin views.
Request goes through middleware.
Simulate behaviors of most Web servers.
Attach the originating request to the response so that it could be  later retrieved.
We're emulating a WSGI server; we must call the close method  on completion.
Not by any means perfect, but good enough for our purposes.
Encode the content so that the byte representation is correct.
Look for a signalled exception, clear the current context  exception data, then re-raise the signalled exception.  Also make sure that the signalled exception is cleared from  the local cache!
Save the client and request that stimulated the response.
Add any rendered template detail to the response.
Attach the ResolverMatch instance to the response
Flatten a single context. Not really necessary anymore thanks to  the __getattr__ flattening in ContextList, but has some edge-case  backwards-compatibility implications.
Update persistent cookie data.
Create a fake request to store login details.
Save the session values.
Prepend the request path to handle relative path redirects
Check that we're not redirecting to somewhere we've already  been to, to prevent loops.
Such a lengthy chain likely also means a loop, but one with  a growing path, changing view, or changing query argument;  20 is the value of "network.http.redirection-limit" from Firefox.
Duplicate dict to prevent subclasses from altering their parent.
Hack used when instantiating from SimpleTestCase.setUpClass.
Duplicate list to prevent subclasses from altering their parent.
When called from SimpleTestCase.setUpClass, values may be  overridden several times; cumulate changes.
If the string is not a complete xml document, we may need to add a  root element. This allow us to compare fragments, like "<foo/><bar/>"
Parse the want and got strings, and compare the parsings.
call test code that consumes from sys.stdin
The class we'll use for the test client self.client.  Can be overridden in derived classes.
Tests shouldn't be allowed to query the database since  this base class doesn't enforce any isolation.
Not a followed redirect
Prepend the request path to handle relative path redirects.
If the response supports deferred rendering and hasn't been rendered  yet, then ensure that it does get rendered before proceeding further.
Put context(s) into a list to simplify processing.
Put error(s) into a list to simplify processing.
Add punctuation to msg_prefix
Put context(s) into a list to simplify processing.
Put error(s) into a list to simplify processing.
use this template with context manager
Use assertTemplateUsed as context manager.
Use assertTemplateNotUsed as context manager.
Assertion used in context manager fashion.  Assertion was passed a callable.
Subclasses can ask for resetting of auto increment sequence before each  test case
Subclasses can enable only a subset of apps for faster tests
Subclasses can define fixtures which will be automatically installed.
Since tests will be wrapped in a transaction, or serialized if they  are not available, we allow queries to be run.
If the test case has a multi_db=True flag, act on all databases,  including mirrors or not. Otherwise, just on the default DB.
Reset sequences
We have to use this slightly awkward syntax due to the fact  that we're using *args and **kwargs together.
Allow TRUNCATE ... CASCADE and don't emit the post_migrate signal  when flushing only a subset of the apps  Flush the database
rollback to avoid trying to recreate the serialized data.
If the backend does not support transactions, we should reload  class data before each test
Assume a class is decorated
Override this thread's database connections with the ones  provided by the main thread.
Create the handler for serving static and media files
Stop the WSGI server
If using in-memory sqlite databases, pass the connections to  the server thread.  Explicitly enable thread-shareability for this connection
Launch the live server's thread
Wait for the live server to be ready  Clean up behind ourselves, since tearDownClass won't get called in  case of errors.
There may not be a 'server_thread' attribute if setUpClass() for some  reasons has raised an exception.  Terminate the live server's thread
Restore sqlite in-memory database connections' non-shareability
Defer saving file-type fields until after the other fields, so a  callable upload_to can use the values from other fields.
Sentinel for fields_for_model to indicate "get the list of  fields from the model"
if we didn't get an instance, instantiate a new one
Build up a list of fields that should be excluded from model field  validation and unique checks.  Exclude fields that aren't on the form. The developer may be  adding these values to the model after form validation.
Exclude fields that failed form validation. There's no need for  the model fields to validate them as well.
Override any validation error messages defined at the model level  with those defined at the form level.
Allow the model generated by construct_instance() to raise  ValidationError and have them handled in the same way as others.
Validate uniqueness if needed.
If committing, save the instance and the m2m data immediately.
If not committing, add a method to the form to allow deferred  saving of m2m data.
Class attributes for the new form class.
Instantiate type(form) in order to use the same metaclass as form.
Set of fields that must be unique among forms of this set.
Set initial values for extra forms
If the queryset isn't already ordered we need to add an  artificial ordering here to make sure that all formsets  constructed from this queryset have the same form order.
Removed queryset limiting here. As per discussion re: 13023  on django-dev, max_num should not prevent existing  related objects/inlines from being displayed.
If the pk is None, it means that the object can't be  deleted again. Possible reason for this is that the  object was already deleted from the DB. Refs 14877.
If we're adding the related instance, ignore its primary key  as it could be an auto-generated default which isn't actually  in the database.
Remove the primary key from the form's data, we are only  creating new instances
Remove the foreign key from the form's data
if there is no value act as we did before.  ensure the we compare the values as equal types.
Can't use iterator() when queryset uses prefetch_related()
This class is a subclass of ChoiceField for purity, but it doesn't  actually use any of ChoiceField's implementation.
Call Field instead of ChoiceField __init__() because we don't need  ChoiceField.__init__().
Need to force a new ModelChoiceIterator to be created, bug 11183
If self._choices is set, then somebody must have manually set  the property self.choices. In this case, just return self._choices.
Since this overrides the inherited ModelChoiceField.clean  we run custom validators here
special field names
default minimum number of forms in a formset
default maximum number of forms in a formset, to prevent memory exhaustion
MIN_NUM_FORM_COUNT and MAX_NUM_FORM_COUNT are output with the rest of  the management form, but only for the convenience of client-side  code. The POST value of them returned from the client is not checked.
return absolute_max if it is lower than the actual total form  count in the data; this is DoS protection to prevent clients  from forcing the server to instantiate arbitrary numbers of  forms
Allow all existing related objects/inlines to be displayed,  but don't allow extra beyond max_num.
Use the length of the initial data if it's there, 0 otherwise.
DoS protection is included in total_form_count()
Don't render the HTML 'required' attribute as it may cause  incorrect validation for extra, optional, and deleted  forms in the formset.
Return a list of form.cleaned_data dicts in the order specified by  the form data.
All the forms on a FormSet are the same, so you only need to  interrogate the first form for media.
XXX: there is no semantic division between forms here, there  probably should be. It might make sense to render each form as a  table row with each field as a td.
Walk through the MRO.  Collect fields from base class.
Field shadowing.
Translators: This is the default suffix added to form field labels
If there aren't any rows in the output, just append the  hidden fields.
Normalize to ValidationError and let its constructor  do the hard work of making sense of the input.
If the form is permitted to be empty, and none of the form data has  changed from the initial data, short circuit any validation.
Initial values are supposed to be clean  value_from_datadict() gets the data from the data dictionaries.  Each widget type knows how to retrieve its own data, because some  widgets split data over several HTML fields.
Always assume data has changed if validation fails.
Get the media property of the superclass, if it exists
Only add the 'value' attribute if a value is non-empty.
An ID attribute was given. Add a numeric index as a suffix  so that the inputs don't all have the same ID attribute.
If the user contradicts themselves (uploads a new file AND  checks the "clear" checkbox), we return a unique marker  object that FileField will turn into a ValidationError.  False signals to clear any existing value, as opposed to just None
Use slightly better defaults than HTML's 20x2 box
Defined at module level so that CheckboxInput is picklable (17976)
check_test is a callable that takes a value and returns True  if the checkbox should be checked for that value.
Only add the 'value' attribute if a value is non-empty.
A missing value means False because HTML form submission does not  send results for unselected checkboxes.
choices can be any iterable, but we may need to render this widget  multiple times. Thus, collapse it into a list so it can be consumed  more than once.
Only allow for a single selection.
Override the default renderer if we were passed one.
See the comment for RadioSelect.id_for_label()
Optional list or tuple of years to use in the "year" select box.
Optional dict of months to use in the "month" select box.
Optional string, list, or tuple to use as empty_label.
The `list` reduce function returns an iterator as the fourth element  that is normally used for repopulating. Since we only inherit from  `list` for `isinstance` backward compatibility (Refs 17413) we  nullify this iterator as it would otherwise result in duplicate  entries. (Refs 23594)
Add an 'invalid' entry to default_error_message if you want a specific  field error message not raised by the field validators.
Tracks each time a Field instance is created. Used to retain order.
Trigger the localization machinery if needed.
Let the widget know whether it should display as required.
Hook into self.widget_attrs() for any Field-specific HTML attributes.
Increase the creation counter, and save our local copy.
For purposes of seeing whether something has changed, None is  the same as an empty string, if the data or initial value we get  is None, replace it with ''.
The HTML attribute is maxlength, not max_length.
The HTML attribute is minlength, not min_length.
Localized number input is not well supported on most browsers
Use exponential notation for small values since they might  be parsed as 0 otherwise. ref 20765
UploadedFile objects should have name and size attributes.
load() could spot a truncated JPEG, but it loads the entire  image in memory, which is a DoS vector. See 3848 and 18520.  verify() must be called immediately after the constructor.
Annotating so subclasses can reuse it for their own validation  Pillow doesn't detect the MIME type of all formats. In those  cases, content_type will be None.
Pillow doesn't recognize it as an image.
urlparse.urlsplit can raise a ValueError with some  misformatted URLs.
If no URL scheme given, assume http://
Assume that if no domain is provided, that the path segment  contains the domain.  Rebuild the url_fields list, since the domain segment may now  contain the path too.
Sometimes data or initial may be a string equivalent of a boolean  so we should run it through to_python first to get a boolean value
Setting choices also sets the choices on the widget.  choices can be any iterable, but we call list() on it because  it will be consumed more than once.
This is an optgroup, so look inside the group for options
Validate that each value in the value list is in self.choices.
Set 'required' to False on the individual fields, because the  required validation will be handled by ComboField, not by those  individual fields.
Set 'required' to False on the individual fields, because the  required validation will be handled by MultiValueField, not  by those individual fields.
Raise a 'required' error if the MultiValueField is  required and any field is empty.
Otherwise, add an 'incomplete' error to the list of  collected errors and skip field cleaning, if a required  field is empty.
Collect all validation errors in a single list, which we'll  raise at the end of clean(), rather than raising a single  exception for the first error we encounter. Skip duplicates.
Prevent unnecessary reevaluation when accessing BoundField's attrs  from templates.
HACK: datetime is an old-style class, create a new-style equivalent  so we can define additional attributes.
Obtain a timezone-aware datetime  Filters must never raise exceptions, and pytz' exceptions inherit  Exception directly, not a specific subclass. So catch everything.
``language`` is either a language code string or a sequence  with the language code as its first item
Either string is malformed, or it's a bug
The self-weakref trick is needed to avoid creating a reference  cycle.
A marker for caching
If DEBUG is on, check that we got a good receiver
Check for **kwargs
We could end up here with NO_RECEIVERS even if we do check this case in  .send() prior to calling _live_receivers() due to concurrent .send() call.
Note, we must cache the weakref versions.
Dereference the weak reference.
Register an event to reset saved queries when a Django request is started.
Register an event to reset transaction state and close connections past  their lifetime.
Older code may be expecting FileField values to be simple strings.  By overriding the == operator, it can remain backwards compatibility.
open() doesn't alter the file's contents, but it does reset the pointer
Save the object because it has changed, unless save is False
Only close the file if it's already open, which we know by the  presence of self._file
FieldFile needs access to its associated model field and an instance  it's attached to in order to work properly, but the only necessary  data to be pickled is the file's name itself. Everything else will  be restored later, by FileDescriptor below.
The instance dict contains whatever was originally assigned  in __set__.
Finally, because of the (some would say boneheaded) way pickle works,  the underlying FieldFile might not actually itself have an associated  file. So we need to reset the details of the FieldFile in those cases.
Make sure that the instance is correct.
That was fun, wasn't it?
The class to wrap instance attributes in. Accessing the file object off  the instance will always return an instance of attr_class.
The descriptor to use for accessing the attribute off of the class.
Need to convert File objects provided via a form to unicode for database insertion
Commit the file to storage prior to saving the model
Clear the image dimensions cache
Attach update_dimension_fields so that dimension fields declared  after their corresponding image field don't stay cleared by  Model.__init__, see bug 11196.  Only run post-initialization dimension update on non-abstract models
Nothing to update if the field doesn't have dimension fields.
getattr will call the ImageFileDescriptor's __get__ method, which  coerces the assigned value into an instance of self.attr_class  (ImageFieldFile in this case).
Nothing to update if we have no file and not being forced to update.
file should be an instance of ImageFieldFile or should be None.  No file, so clear dimensions fields.
Update the width and height fields.
-*- coding: utf-8 -*-
The values to use for "blank" in SelectFields. Will be appended to the start  of most "choices" lists.
Designates whether empty strings fundamentally are allowed at the  database level.
These track each time a Field instance is created. Used to retain order.  The auto_creation_counter is used for fields that Django implicitly  creates, creation_counter is used for all user-specified fields.
Translators: The 'lookup_type' is one of 'date', 'year' or 'month'.  Eg: "Title must be unique for pub_date year"
Field flags
Generic field type description, usually overridden by subclasses
Adjust the appropriate creation counter, and save our local copy.
Needed for @total_ordering
This is needed because bisect does not take a comparison function.
We need to avoid hitting __reduce__, so define this  slightly weird copy construct.
Skip validation for non-editable fields.
This is an optgroup, so look inside the group for  options.
Don't override classmethods with the descriptor. This means that  if you have a classmethod and a field with the same name, then  such fields can't be deferred (we don't have a check for this).
if value is 1 or 0 than it's equal to True or False, but we want  to return a true bool for semantic reasons.
Passing max_length to forms.CharField means that the value's length  will be validated twice. This is considered acceptable since we want  the value in the form field (to pass into widget for example).
No explicit date / datetime value -- no checks necessary
Convert aware datetimes to the default time zone  before casting them to dates (17742).
Casts dates into the format expected by the backend
No explicit date / datetime value -- no checks necessary
Casts datetimes into the format expected by the backend
Method moved to django.db.backends.utils.  It is preserved because it is used by the oracle backend  (django.db.backends.oracle.query), and also for  backwards-compatibility with any external code which may have used  this method.
Discard any fractional microseconds due to floating point arithmetic.
max_length=254 to be compliant with RFCs 3696 and 5321
We do not exclude max_length if it matches default as we want to change  the default in future.
As with CharField, this will cause email validation to be performed  twice.
Passing max_length to forms.CharField means that the value's length  will be validated twice. This is considered acceptable since we want  the value in the form field (to pass into widget for example).
No explicit time / datetime value -- no checks necessary
Not usually a good idea to pass in a datetime here (it loses  information), but this can be a side-effect of interacting with a  database backend (e.g. Oracle), so we'll be accommodating.
Casts times into the format expected by the backend
As with CharField, this will cause URL validation to be performed  twice.
If it's a string, it should be base64-encoded data
The exception can't be created at initialization time since the  related model might not be resolved yet; `rel.model` might still be  a string model reference.
If we've got an old related object, we need to clear out its  cache. This cache also might not exist if the related object  hasn't been accessed yet.
Set the values of the related field.
Set the related instance cache used by __get__ to avoid an SQL query  when accessing the attribute we just set.
If this is a one-to-one relation, set the reverse accessor cache on  the related object to the current instance to avoid an extra SQL  query if it's accessed later on.
The exception isn't created at initialization time for the sake of  consistency with `ForwardManyToOneDescriptor`.
Set the value of the related field to the value of the related object's related field
Set the related instance cache used by __get__ to avoid an SQL query  when accessing the attribute we just set.
Set the forward accessor cache on the related object to the current  instance to avoid an extra SQL query if it's accessed later on.
We use **kwargs rather than a kwarg argument to enforce the  `manager='manager_name'` syntax.
`QuerySet.update()` is intrinsically atomic.
Force evaluation of `objs` in case it's a queryset whose value  could be affected by `manager.clear()`. Refs 19816.
through is provided so that you have easy access to the through  model (Book.authors.through) for inlines, etc. This is done as  a property to ensure that the fully resolved value is returned.
Even if this relation is not to pk, we require still pk value.  The wish is that the instance has been already saved to DB,  although having a pk value isn't a guarantee of that.
We use **kwargs rather than a kwarg argument to enforce the  `manager='manager_name'` syntax.
If this is a symmetrical m2m relation to self, add the mirror entry in the m2m table
Force evaluation of `objs` in case it's a queryset whose value  could be affected by `manager.clear()`. Refs 19816.
We only need to add() if created because if we got an object back  from get() then the relationship already exists.
We only need to add() if created because if we got an object back  from get() then the relationship already exists.
Don't send the signal when we are inserting the  duplicate data row for symmetrical reverse entries.
Don't send the signal when we are inserting the  duplicate data row for symmetrical reverse entries.
source_field_name: the PK colname in join table for the source object  target_field_name: the PK colname in join table for the target object  *objs - objects to remove
Field flags
Reverse relations are always nullable (Django can't enforce that a  foreign key on the related model points to this model).
By default foreign object doesn't relate to any remote field (for  example custom multicolumn joins currently have no remote field).
For multicolumn lookups we need to build a multicolumn where clause.  This clause is either a SubqueryConstraint (for values that need to be compiled to  SQL) or a OR-combined list of (col1 = val1 AND col2 = val2 AND ...) clauses.
Check for recursive relations
Look for an "app.Model" relation
Field flags
Can't cache this property until all the models are loaded.
`f.remote_field.model` may be a string instead of a model. Skip if model name is  not resolved.
rel_opts.object_name == "Target"  If the field doesn't install a backward relation on the target model  (so `is_hidden` returns True), then there are no clashes to check  and we can skip these fields.
Check clashes between accessor or reverse query name of `field`  and any other field name -- i.e. accessor for Model.foreign is  model_set and it clashes with Target.model_set.
Check clashes between accessors/reverse query names of `field` and  any other field accessor -- i. e. Model.foreign accessor clashes with  Model.m2m accessor.
By default related field will not have a column as it relates to  columns from another table.
If this is a callable, do not invoke it here. Just pass  it in the defaults for when the form class will later be  instantiated.
Field flags
Field flags
For backwards compatibility purposes, we need to *try* and set  the to_field during FK construction. It won't be guaranteed to  be correct until contribute_to_class is called. Refs 12190.
Field flags
Override ForeignKey since check isn't applicable here.
Field flags
Class names must be ASCII in Python 2.x, so we forcibly coerce it  here to break early if there's a problem.
Count foreign keys in intermediate model
Validate the given through fields -- they should be actual  fields on the through model, and also be foreign keys to the  expected models.
If this is an m2m-intermediate to self,  the first foreign key you find will be  the source column. Keep searching for  the second foreign key.
Add the descriptor for the m2m relation.
Set up the accessor for the m2m table name for the relation.
Internal M2Ms (i.e., those with a related name ending with '+')  and swapped models don't get a related descriptor.
Set up the accessors for the column names on the m2m table.
A ManyToManyField is not represented by a single column,  so return None.
resolve_expression has already validated the output_field so this  assert should never be hit.
self.output_field is definitely a DateField here.
CAST would be valid too, but the :: shortcut syntax is more readable.
we can't mix TextField (NCLOB) and CharField (NVARCHAR), so convert  all fields to NCLOB when we expect NCLOB
Use CONCAT_WS with an empty separator so that NULLs are ignored.
null on either side results in null for expression, wrap with coalesce
wrap pairs of expressions in successive concat functions  exp = [a, b, c, d]  -> ConcatPair(a, ConcatPair(b, ConcatPair(c, d))))
Postgres' CURRENT_TIMESTAMP means "the time at the start of the  transaction". We use STATEMENT_TIMESTAMP to be cross-compatible with  other databases.
PathInfo is used when converting lookups (fk__somecol). The contents  describe the relation in Model terms (model Options and Fields for both  sides of the relation. The join_field is the field backing the relation.
Connection types
We must promote any new joins to left outer joins so that when Q is  used as an expression, rows aren't filtered due to joins.
To allow for inheritance, check parent class' class_lookups.
This class didn't have any class_lookups
Separator used to split filter strings apart.
Exceptions are special - they've got state that isn't  in self.__dict__. We assume it is all in self.args.
Also ensure initialization is only performed for subclasses of Model  (excluding Model class itself).
Look for an application configuration to attach the model to.
If the model is a proxy, ensure that the base class  hasn't been swapped out.
Add all attributes to the class.
All the fields of any type declared on this model
Only add the ptr field if it's not already present;  e.g. migrations will already have it specified
Pass any non-abstract parent classes onto child.
Abstract base models can't be instantiated and don't appear in  the list of models for an app. We do the final setup for them a  little differently from normal models.
We should call the contribute_to_class method only if it's bound
Step 1: Locate a manager that would have been promoted  to default manager with the legacy system.
If true, uniqueness validation checks will consider this a new, as-yet-unsaved object.  Necessary for correct validation of new instances of objects with explicit (non-auto) PKs.  This impacts validation only; it has no effect on the actual save.
Set up the storage for instance state
The ordering of the zip calls matter - zip throws StopIteration  when an iter throws it. So if the first iter throws it, the second  is *not* consumed. We rely on this, so don't change the order  without changing the logic.
This field wasn't refreshed - skip ahead.
If update_fields is empty, skip the save. We do also check for  no-op saves later on for inheritance cases. This bailout is  still needed for skipping signal sending.
Signal that the save is complete
If this is an excluded field, don't add this check.
These are checks for the unique_for_<date/year/month>.
no value, skip the lookup
no need to check for unique primary key when editing
Exclude the current object from the query if we are editing an  instance (as opposed to creating a new one)
Form.clean() is run even if other validation fails, so do the  same with Model.clean() for consistency.
Skip when the target model wasn't found.
Skip when the relationship model wasn't found.
Check that fields defined in the model don't clash with fields from  parents, including auto-generated fields like multi-table inheritance  child accessors.
Store a list of column names which have already been used by other fields.
In order to avoid hitting the relation tree prematurely, we use our  own fields_map instead of using get_field()
Skip '?' fields.
Convert "-field" to "field".
Skip ordering in the format field1__field2 (FIXME: checking  this format would be nice, but it's a little fiddly).
Skip ordering on pk. This is always a valid order_by field  but is an alias and therefore won't be found by opts.get_field.
Check for invalid or non-existent fields in ordering.
FIXME: It would be nice if there was an "update many" version of update  for situations like this.
Backwards compat - the model was cached directly in earlier versions.
Get the exception class from the class it is attached to:
The maximum number of items to display in a QuerySet.__repr__
Pull into this namespace for backwards compatibility.
extra(select=...) cols are always at the start of the row.
extra(select=...) cols are always at the start of the row.
Reorder according to fields.
Address the circular dependency between `Queryset` and `Manager`.
Force the cache to be fully populated.
The default_alias property may raise a TypeError, so we use  a try/except construct rather than hasattr in order to remain  consistent between PY2 and PY3 (hasattr would swallow  the TypeError on PY2).
The get() needs to be targeted at the write database in order  to avoid potential transaction consistency problems.
The delete is actually 2 queries - one to find related objects,  and one to delete. Make sure that the discovery of related  objects is performed on the same database as the deletion.
Disable non-supported fields.
Clear the result cache, in case this QuerySet gets reused.
This method can only be called once the result cache has been filled.
Shortcut - if there are no extra or annotations, then  the values() clause must be just field names.
values() queryset can only be used as nested queries  if they are set up to select only a single field.
If the query is used as a subquery for a ForeignKey with non-pk  target field, make sure to select the target field in the subquery.
When used as part of a nested query, a queryset will never be an "always  empty" result.
If there is any hinting information, add it to what we already know.  If we have a new hint for an existing key, overwrite with the new value.
We trust that users of values() know what they are doing.
Cache some things for performance reasons outside the loop.
Done iterating the Query. If it has its own cursor, close it.
Adjust any column names which don't match field names  Ignore translations for non-existent column names
Top level, the list of objects to decorate is the result cache  from the primary QuerySet. It won't be for deeper levels.
Prepare main instances
Skip any prefetching, and any object preparation
We assume that objects retrieved are homogeneous (which is the premise  of prefetch_related), so what applies to first object applies to all.
Last one, this *must* resolve to something that supports  prefetching, otherwise there is no point adding it and the  developer asking for it has made a mistake.
Whether or not we're prefetching the last part of the lookup.
We don't want the individual qs doing prefetch_related now,  since we have merged this into the current work.
Arithmetic connectors  The following is a quoted % operator - it is quoted because it can be  used in strings that also have parameter substitution.
Bitwise operators - note that these are generated by .bitand()  and .bitor(), the '&' and '|' are reserved for boolean operator  usage.
everything must be resolvable to an expression
aggregate specific fields
custom logic
order of precedence
order of precedence
The sub-expression `source` has already been resolved, as this is  just a reference to the name of `source`.
We're only interested in the fields of the result expressions.
This is not a complete expression and cannot be used in GROUP BY.
Connection types
Check if this node matches nothing or everything.  First check the amount of full nodes and empty nodes  to make this node empty/full.  Now, check if this node is full/empty using the  counts.
Some backends (Oracle at least) need parentheses  around the inner SQL in the negated case, even if the  inner SQL contains just a single expression.
For example another WhereNode
The contents are a black box - assume no aggregates are used.
Even if aggregates would be used in a subquery, the outer query isn't  interested about those.
Size of each "chunk" for get_iterator calls.  Larger values are slightly faster at the expense of more storage space.
How many results to expect from a cursor.execute call
SQL join types.
The path travelled, this includes the path to the multijoin.
Always execute a new query for a new iterator.  This could be optimized with a cache at the expense of RAM.  If the database can't use chunked reads we need to make sure we  evaluate the entire query up front.
Arbitrary limit for select_related to prevents infinite recursion.
Holds the selects defined by a call to values() or values_list()  excluding annotation_select and extra_select.
A tuple that is a set of model field names and either True, if these  are the fields to defer, or False if these are the only fields to  load.
Work out how to relabel the rhs aliases, if necessary.
Now relabel a copy of the rhs where-clause and add it to the current  one.
Selection columns and extra extensions are those provided by 'rhs'.
It would be nice to be able to handle this, but the queries don't  really make sense (or return consistent value sets). Not worth  the extra complexity when you can write a real query instead.
Ordering uses the 'rhs' ordering, unless it has none, in which case  the current ordering is used.
Even if we're "just passing through" this model, we must add  both the current model's pk and the related reference field  (if it's not a reverse relation) to the things we select.
This is the base table (first FROM entry) - this table  isn't really joined at all in the query, so we should not  alter its join type.  Only the first alias (skipped above) should have None join_type
Join type of 'alias' changed, so re-examine all aliases that  refer to this one.
No clashes between self and outer query should be possible.
Work out the lookup type and remove it from the end of 'parts',  if necessary.
Prevent iterator from being consumed by check_related_objects()
split_exclude() needs to know which joins were generated for the  lookup parts
No support for transforms for relational fields
The field lives on a base class of the current model.  Skip the chain of proxy to the concrete proxied model
First, generate the path for the names
Summarize currently means we are doing an aggregate() query  which is executed as a wrapped subquery if any of the  aggregate() elements reference an existing annotation. In  that case we need to return a Ref to the subquery's annotation.
Generate the inner query.  Try to have as simple as possible subquery -> trim leading joins from  the subquery.
For lookups spanning over relationships, show the error  from the model on which the lookup failed.
Remove any existing deferred names from the current set before  setting the new names.
Replace any existing "immediate load" field names.
We cache this because we call this function multiple times  (compiler.fill_related_selections, query.iterator)
Maps of table alias to how many times it is seen as required for  inner and/or outer joins.
Reference to expression in SELECT clause
References to an expression which is masked out of the SELECT clause
'col' is of the form 'field' or 'field1__field2' or  '-field1__field2__field', etc.
This must come after 'select', 'ordering', and 'distinct' -- see  docstring of get_from_clause() for details.
Finally do cleanup - get rid of the joins we created above.
If there is no slicing in use, then we can safely drop all ordering
The 'seen_models' is used to optimize checking the needed parent  alias for a given field. This also includes None -> start_alias to  be used by local fields.
Extra tables can end up in self.tables, but not in the  alias_map if they aren't in a join. That's OK. We skip them.
Only add the alias if it's not already present (the table_alias()  call increments the refcount, so an alias refcount of one means  this is the only reference).
We've recursed far enough; bail out.
Setup for the case when only particular related fields should be  included in the related selection.
Caller didn't specify a result_type, so just give them back the  cursor to process (and close).
done with the cursor
If we are using non-chunked reads, we return the same data  structure as normally, but ensure it is all read into memory  before going any further.
done with the cursor
A field value of None means the value is raw.
This is an expression, let's compile it.
Some fields (e.g. geo fields) need special munging before  they can be inserted.
Return the common case for the placeholder
list of (sql, [params]) tuples for each object to be saved  Shape: [n_objs][n_fields][2]
tuple like ([sqls], [[params]s]) for each object to be saved  Shape: [n_objs][2][n_fields]
Extract separate lists for placeholders and params.  Each of these has shape [n_objs][n_fields]
Params for each field are still lists, and need to be flattened.
We don't need quote_name_unless_alias() here, since these are all  going to be column names (so we can avoid the extra overhead).
An empty object.
Currently the backends just accept values when generating bulk  queries and generate their own placeholders. Doing that isn't  necessary and it should be possible to use placeholders and  expressions in bulk inserts too.
Skip empty r_fmt to allow subclasses to customize behavior for  3rd party backends. Refs 19096.
Normalize everything to tuples
If the value of option_together isn't valid, return it  verbatim; this will be picked up by the check framework later.
List of all lookups defined in ForeignKey 'limit_choices_to' options  from *other* models. Needed for some admin checks. Internal use only.
A custom app registry to use, if you're making a separate model set.
Don't go through get_app_config to avoid triggering imports.
First, construct the default values for these options.
Store the original user-defined values for each option,  for use when serializing the model definition
verbose_name_plural is a special case because it uses a 's'  by default.
order_with_respect_and ordering are mutually exclusive.
Any leftover attributes must be invalid.
setting not in the format app_label.model_name  raising ImproperlyConfigured here causes problems with  test cleanup code - instead it is raised in get_user_model  or as part of validation.
Used for deprecation of legacy manager inheritance,  remove afterwards. (RemovedInDjango20Warning)
Get the first parent's default_manager_name if there's one.
Due to the way Django's internals work, get_field() should also  be able to fetch a field by attname. In the case of a concrete  field with relation, includes the *_id name too
Due to the way Django's internals work, get_field() should also  be able to fetch a field by attname. In the case of a concrete  field with relation, includes the *_id name too
In order to avoid premature loading of the relation tree  (expensive) we prefer checking if the field is a forward field.
Retrieve field instance by name from cached or just-computed  field map.
Tries to get a link field from the immediate parent  In case of a proxied model, the first link  of the chain to the ancestor is that parent  links
We must keep track of which models we have already seen. Otherwise we  could include the same field multiple times from different models.
Creates a cache key composed of all arguments
In order to avoid list manipulation. Always return a shallow copy  of the results.
In order to avoid list manipulation. Always  return a shallow copy of the results
Store result into cache for later access
Tracks each time a Manager instance is created. Used to retain order.
Set to True for the 'objects' managers that are automatically created.
We capture the arguments to make returning them trivial
using MyQuerySet.as_manager()
Warn the user as soon as possible if they are trying to apply  a bilateral transformation on a nested QuerySet: that won't work.  We need to import QuerySet here so as to avoid circular
Do not call get_db_prep_lookup here as the value will be  transformed before being used for lookup
rhs should be an iterable; use batch_process_rhs() to  prepare/transform those values.
rhs should be an iterable of 2 values, we use batch_process_rhs  to prepare/transform those values
The candidate relations are the ones that come from N-1 and 1-1 relations.  N-N  (i.e., many-to-many) relations aren't candidates for deletion.
Initially, {model: {instances}}, later values become lists.
fast_deletes is a list of queryset-likes that can be deleted without  fetching the objects into memory.
It's something like generic foreign key.
sort instance collections
if possible, bring the models in an order suitable for databases that  don't support transactions or cannot defer constraint checks until the  end of a transaction.  number of objects deleted for each model label
send pre_delete signals
fast deletes
reverse instance collections
Ignore any related fields.
Ignore any non-concrete fields
MySQL stores positive fields as UNSIGNED ints.
http://dev.mysql.com/doc/mysql/en/date-and-time-functions.html  DAYOFWEEK() returns an integer, 1-7, Sunday=1.  Note: WEEKDAY() returns 0-6, Monday=0.
RemovedInDjango20Warning
With MySQLdb, cursor objects have an (undocumented) "_last_executed"  attribute where the exact query sent to the database is saved.  See MySQLdb/cursors.py in the source distribution.
2**64 - 1, as recommended by the MySQL documentation
MySQLism: zero in AUTO_INCREMENT field does not work. Refs 17653.
MySQL doesn't support tz-aware times
Inner import to allow module to fail to load gracefully
Temporary setting db_index to False (in memory) to disable  index creation for FKs (index automatically created by MySQL)
args is None means no string interpolation
Map some error codes to IntegrityError, since they seem to be  misclassified and Django would prefer the more logical place.
Map some error codes to IntegrityError, since they seem to be  misclassified and Django would prefer the more logical place.
Ticket 17671 - Close instead of passing thru to avoid backend  specific behavior.
We need the number of potentially affected rows after an  "UPDATE", not the number of changed rows.
SQL_AUTO_IS_NULL controls whether an AUTO_INCREMENT column on  a recently inserted row will return when the field is tested  for NULL. Disabling this brings this aspect of MySQL in line  with SQL standards.
Override needs_rollback in case constraint_checks_disabled is  nested inside transaction.atomic.
See https://github.com/farcepest/MySQLdb1/issues/24 for the reason  about requiring MySQLdb 1.2.5
Test if the time zone definitions are installed.
This reg-exp is intentionally fairly flexible here.  Needs to be able to handle stuff like:    PostgreSQL ..    EnterpriseDB .    PostgreSQL . beta    PostgreSQL .beta
http://www.postgresql.org/docs/current/static/functions-datetime.htmlFUNCTIONS-DATETIME-TRUNC
http://www.postgresql.org/docs/current/static/functions-datetime.htmlFUNCTIONS-DATETIME-TRUNC
Use UPPER(x) for case-insensitive lookups; it's faster.
Use pg_get_serial_sequence to get the underlying sequence name  from the table name and column name (available since PostgreSQL 8)
http://initd.org/psycopg/docs/cursor.htmlcursor.query  The query attribute is a Psycopg extension to the DB API 2.0.
CREATE DATABASE ... WITH TEMPLATE ... requires closing connections  to the template database.
Register support for inet[] manually so we don't have to handle the Inet()  object on load all the time.
Commit after setting the time zone (see 17062)
Use a psycopg cursor directly, bypassing Django's utilities.
If we're the first column, make the record
TO_CHAR(field, 'D') returns an integer from 1-7, where 1=Sunday.
http://docs.oracle.com/cd/B19306_01/server.102/b14200/functions050.htm
Oracle crashes with "ORA-03113: end-of-file on communication channel"  if the time zone name is passed in parameter. Use interpolation instead.  https://groups.google.com/forum/!msg/django-developers/zwQju7hbG78/9l934yelwfsJ  This regexp matches all time zone names from the zoneinfo database.
Re-convert to a TIMESTAMP because EXTRACT only handles the date part  on DATE values, even though they actually store the time part.
Oracle stores empty strings as null. We need to undo this in  order to adhere to the Django convention of using the empty  string instead of null, but only if the field accepts the  empty string.
http://cx-oracle.sourceforge.net/html/cursor.htmlCursor.statement  The DB API definition does not define this attribute.
Unlike Psycopg's `query` and MySQLdb`'s `_last_executed`, CxOracle's  `statement` doesn't contain the query parameters. refs 20010.
Only one AutoField is allowed per model, so don't  continue to loop
Oracle doesn't support tz-aware times
If we're changing type to an unsupported type we need a  SQLite-ish workaround
Ignore "tablespace already exists" error when keepdb is on.
Statement can fail when acceptable_ora_err is not None
Oracle takes client-side character set encoding from the environment.  This prevents unicode from getting mangled by getting encoded into the  potentially non-unicode database character set.
If connection.operators is looked up before a connection has been  created, transparently initialize connection.operators to avert an  AttributeError.  Creating a cursor will initialize the operators.
Django docs specify cx_Oracle version 4.3.1 or higher, but  stmtcachesize is available only in 4.3.2 and up.  Ensure all changes are preserved even when AUTOCOMMIT is False.
Necessary to retrieve decimal values without rounding error.  Default arraysize of 1 is highly sub-optimal.
Try dict handling; if that fails, treat as sequence
already closed
select for update with limit can be achieved on Oracle, but not with the current backend.
Structure returned by DatabaseIntrospection.get_table_list()
Structure returned by the DB-API cursor.description interface (PEP 249)
If this is an m2m using an intermediate table,  we don't need to reset the sequence.
RemovedInDjango20Warning
Same as prep_for_like_query(), but called for "iexact" matches, which  need not necessarily be implemented using "LIKE" in the backend.
If it's a callable, call it  Run it through the field's get_db_prep_save method so we can send it  to the database.
Add any field index and index_together's (deferred as SQLite3 _remake_table needs it)
Make M2M tables
Handle auto-created intermediary models
Delete the table
The prefix to put on the default database name when creating  the test database.
Don't import django.core.management if it isn't needed.
We report migrate messages at one level lower than that requested.  This ensures we don't get flooded with messages during testing  (unless you really ask to be flooded).
We then serialize the current state of the database into a string  and store it on the connection. This slightly horrific process is so people  who are testing on databases without transactions or who are using  a TransactionTestCase still get a clean database on every test run.
Ensure a connection for the side effect of initializing the test database.
We could skip this call if keepdb is True, but we instead  give it the keepdb param. See create_test_db for details.
When this function is called, the test database has been created  already and its name has been copied to settings_dict['NAME'] so  we don't need to call _get_test_db_name.
if we want to preserve the database  skip the actual destroying piece.
Restore the original database name
Mapping of Field objects to their column types.  Mapping of Field objects to their SQL suffix such as AUTOINCREMENT.  Mapping of Field objects to their SQL for CHECK constraints.
Connection termination related attributes.
Thread-safety related attributes.
A list of no-argument functions to run when the transaction commits.  Each entry is an (sids, func) tuple, where sids is a set of the  active savepoint IDs when this function was registered.
Should we run the on-commit hooks the next time set_autocommit(True)  is called?
Only this branch requires pytz.
A successful commit means that the database connection works.
A successful rollback means that the database connection works.
Savepoints cannot be created outside a transaction
Remove any callbacks registered while this savepoint was active.
If the application didn't restore the original autocommit setting,  don't take chances, drop the connection.
If an exception other than DataError or IntegrityError occurred  since the last commit / rollback, check if the connection works.
Transaction in progress; save for execution on commit.
No transaction in progress and in autocommit mode; execute  immediately.
This should be a string representing the name of the executable  (e.g., "psql"). Subclasses must override this.
connection is an instance of BaseDatabaseWrapper.
Does the backend distinguish between '' and None?
Does the backend allow inserting duplicate NULL rows in a nullable  unique field? All core backends implement this correctly, but other  databases such as SQL Server do not.
Does the backend allow inserting duplicate rows when a unique_together  constraint exists and some fields are nullable but not all of them?
If True, don't use integer foreign keys referring to, e.g., positive  integer primary keys.
Does the default test database allow multiple connections?  Usually an indication that the test database is in-memory
Can an object be saved without an explicit primary key?
Can a fixture contain forward references? i.e., are  FK constraints checked at the end of transaction, or  at the end of each save operation?
Does the backend truncate names properly when they are too long?
Is there a REAL datatype in addition to floats/doubles?
Is there a true datatype for uuid?
Is there a true datatype for timedeltas?
Does the database driver supports same type temporal data subtraction  by returning the type used to store duration field?
Does the database driver support timedeltas as arguments?  This is only relevant when there is a native duration field.  Specifically, there is a bug with cx_Oracle:  https://bitbucket.org/anthony_tuininga/cx_oracle/issue/7/
Do time/datetime fields have microsecond precision?
Does the __regex lookup support backreferencing and grouping?
Can date/datetime lookups be performed using a string?
Can datetimes with timezones be used?
Does the database have a copy of the zoneinfo database?
When performing a GROUP BY, is an ORDER BY NULL required  to remove any ordering?
Does the backend order NULL values as largest or smallest?
Is there a 1000 item limit on query parameters?
Can an object have an autoincrement primary key of 0? MySQL says No.
Do we need to NULL a ForeignKey out, or can the constraint check be  deferred
date_interval_sql can properly handle mixed Date/DateTime fields and timedeltas
Does the backend support tablespaces? Default to False because it isn't  in the SQL standard.
Does the backend reset sequences between tests?
Can the backend determine reliably the length of a CharField?
Can the backend determine reliably if a field is nullable?  Note that this is separate from interprets_empty_strings_as_nulls,  although the latter feature, when true, interferes with correct  setting (and introspection) of CharFields' nullability.  This is True for all core backends.
Can the backend introspect the default value of a column?
Confirm support for introspected foreign keys  Every database can do this reliably, except MySQL,  which can't do it for MyISAM tables
Can the backend introspect an AutoField, instead of an IntegerField?
Can the backend introspect a BigIntegerField, instead of an IntegerField?
Can the backend introspect an BinaryField, instead of an TextField?
Can the backend introspect an DecimalField, instead of an FloatField?
Can the backend introspect an IPAddressField, instead of an CharField?
Can the backend introspect a PositiveIntegerField, instead of an IntegerField?
Can the backend introspect a SmallIntegerField, instead of an IntegerField?
Can the backend introspect a TimeField, instead of a DateTimeField?
Support for the DISTINCT ON clause
Does the backend decide to commit before SAVEPOINT statements  when autocommit is disabled? http://bugs.python.org/issue8145msg109965
Does the backend prevent running SQL queries in broken transactions?
Can we roll back DDL in a transaction?
Can we issue more than one ALTER COLUMN clause in an ALTER TABLE?
Does it support foreign keys?
Does it support CHECK constraints?
Does the backend support 'pyformat' style ("... %(name)s ...", {'name': value})  parameter passing? Note this can be provided by the backend even if not  supported by the Python driver
Does the backend require literal defaults, rather than parameterized ones?
Does the backend require a connection reset after each material schema change?
What kind of error does the backend throw when accessing closed cursor?
Does 'a' LIKE 'A' match?
Does the backend require the sqlparse library for splitting multi-line  statements before executing them?
Suffix for backends that don't support "SELECT xxx;" queries.
If NULL is implied on columns without needing to be explicitly specified
Does the backend support "select for update" queries with limit (and offset)?
Does the backend ignore null expressions in GREATEST and LEAST queries unless  every expression is null?
Can the backend clone databases for parallel test execution?  Defaults to False to allow third-party backends to opt-in.
Dictionary of relations to return
Walk through and look for references to other tables. SQLite doesn't  really have enforced references, but since it echoes out the SQL used  to create the table we can look for REFERENCES statements used there.
Walk through and look for references to other tables. SQLite doesn't  really have enforced references, but since it echoes out the SQL used  to create the table we can look for REFERENCES statements used there.
This will append (column_name, referenced_table_name, referenced_column_name) to key_columns
cid, name, type, notnull, default_value, pk
Not every subexpression has an output_field which is fine  to ignore.
sqlite doesn't support extract, so we fake it with the user-defined  function django_date_extract that's registered in connect(). Note that  single quotes are used because this is a string (and could otherwise  cause a collision with a field name).
sqlite doesn't support DATE_TRUNC, so we fake it with a user-defined  function django_date_trunc that's registered in connect(). Note that  single quotes are used because this is a string (and could otherwise  cause a collision with a field name).
Same comment as in date_extract_sql.
Same comment as in date_trunc_sql.
sqlite doesn't support extract, so we fake it with the user-defined  function django_time_extract that's registered in connect(). Note that  single quotes are used because this is a string (and could otherwise  cause a collision with a field name).
Bypass Django's wrappers and use the underlying sqlite3 connection  to avoid logging this query - it would trigger infinite recursion.  Native sqlite3 cursors cannot be used as context managers.
SQLite doesn't support tz-aware datetimes
SQLite doesn't have a power function, so we fake it with a  user-defined function django_power that's registered in connect().
SQLite doesn't enforce any integer constraints
Restore initial FK setting - PRAGMA values can't be parametrized
Provide isolated instances of the fields to the new model body so  that the existing model's internals aren't interfered with when  the dummy model is constructed.
Work out the new value of unique_together, taking renames into  account
Work out the new value for index_together, taking renames into  account
We need to modify model._meta.db_table, but everything explodes  if the change isn't reversed before the end of this method. This  context manager helps us avoid that situation.
Rename the old table to make way for the new
Create a new table with the updated schema. We remove things  from the deferred SQL that match our table name, too
Delete the old table
Run deferred SQL on correct table  Fix any PK-removed field
Delete the table (and only that)
Special-case implicit M2M tables
Alter by remaking table
Remove the SQLite database file
If database is in memory, closing the connection destroys the  database. To prevent accidental data loss, ignore close requests on  an in-memory db.
sqlite3's internal default is ''. It's different from None.  See Modules/_sqlite/connection.c.  'isolation_level' is a misleading API.  SQLite always runs at the SERIALIZABLE isolation level.
typecast_timestamp returns a date or a datetime without timezone.  It will be formatted as "%Y-%m-%d" or "%Y-%m-%d %H:%M:%S[.%f]"
Override the base class implementations with null  implementations. Anything that tries to actually  do something raises complain; anything that tries  to rollback or undo something raises ignore.
Ticket 17671 - Close instead of passing thru to avoid backend  specific behavior. Catch errors liberally because errors in cleanup  code aren't useful.
Skip `ModelOperation.reduce` as we want to run `references_model`  against self.new_name.
Model options we want to compare and preserve in an AlterModelOptions op
If this migration can be run in reverse.  Some operations are impossible to reverse, like deleting data.
Can this migration be represented as SQL? (things like RunPython cannot)
Should this operation be forced as atomic even on backends with no  DDL transaction support (i.e., does it have no DDL, like RunPython)
Should this operation be considered safe to elide and optimize across?
We capture the arguments to make returning them trivial
We calculate state separately in here since our state functions aren't useful
RunPython objects have no state effect. To add some, combine this  with SeparateDatabaseAndState.
Skip `FieldOperation.reduce` as we want to run `references_field`  against self.new_name.
No support on Python 2 if enum34 isn't installed.
Only iterate over remaining arguments
See if we can import the migrations module directly
-*- coding: utf-8 -*-  Generated by Django %(version)s on %(timestamp)s
Ignore __first__ references to the same app (22325).
Skip internal dependencies
No support on Python 2 if enum34 isn't installed.
Prepend the `b` prefix since we're importing unicode_literals
When len(strings)==0, the empty iterable should be serialized as  "()", not "(,)" because (,) is invalid Python syntax.
Nested operation, trailing comma is handled in upper OperationWriter._write()
Don't use the literal "{%s}" as it doesn't support empty set
Strip the `u` prefix since we're importing unicode_literals
When len(value)==0, the empty tuple should be serialized as "()",  not "(,)" because (,) is invalid Python syntax.
The unwrapped value is returned as the first item of the arguments  tuple.
Anything that knows how to deconstruct itself.
reverse sorting is needed because prepending using deque.extendleft  also effectively reverses values
It's an application with migrations disabled.
None means quit
None means quit
None means quit
Six does not correctly abstract over the fact that  py3 input returns a unicode string, while py2 raw_input  returns a bytestring.
We can't ask the user, so act like the user aborted.
We can't ask the user, so set as not provided.
We can't ask the user, so act like the user aborted.
Internal tracking variable for test assertions about  of loops
Operations to apply during this migration, in order.
Other migrations that should be run before this migration.  Should be a list of (app, migration_name).
Other migrations that should be run after this one (i.e. have  this migration added to their dependencies). Useful to make third-party  apps' migrations run after your AUTH_USER replacement, for example.
Migration names in this app that this migration replaces. If this is  non-empty, this migration will only be applied if all these migrations  are not applied.
Whether to wrap the whole migration in a transaction. Only has an effect  on database backends which support transactional DDL.
Create the forwards plan Django would follow on an empty database
This should only happen if there's a mixed plan
No need to check for `elif all_backwards` here, as that condition  would always evaluate to true.
We remove every migration that we applied from these sets so  that we can bail out once the last migration has been applied  and don't always run until the very end of the migration  process.
Only mutate the state if the migration is actually applied  to make sure the resulting state doesn't include changes  from unrelated migrations.
The state before this migration  The old state keeps as-is, we continue with the new state
Only mutate the state if the migration is actually applied  to make sure the resulting state doesn't include changes  from unrelated migrations.
Test to see if this is an already-applied initial migration
Alright, do it normally  For replacement migrations, record individual statuses
Report progress
Bail if the migration isn't the first one in its app
Bail if it's NOT an initial migration
Handle implicit many-to-many tables created by AddField.
If we get this far and we found at least one CreateModel or AddField migration,  the migration is considered implicitly applied.
Reverse accessors of foreign keys to proxy models are attached to their  concrete proxied model.
Apps to include from main registry, usually unmigrated ones
Get all relations to and from the old model before reloading,  as _meta.apps may change
Include the model itself
Unregister all related models
Gather all models states of those models that will be rerendered.  This includes:  1. All related models of unmigrated apps
2. All related models of migrated apps
Render all models
Not used, but required by AppConfig.__init__
App-label and app-name are not the same thing, so technically passing  in the label here is wrong. In practice, migrations don't care about  the app name, but we need something unique, and the label works fine.
Avoid clearing each model's cache for each change. Instead, clear  all caches when we're finished updating the model instances.
No need to actually clone them, they'll never change
Make sure the default manager is always first since ordering chooses  the default manager.
If the default manager doesn't have `use_in_migrations = True`,  shim a default manager so another manager isn't promoted in its  place.
Construct the new ModelState
Restore managers
Then, make a Model object (apps.register_model is called in __new__)
If this is a type that implements 'deconstruct' as an instance method,  avoid treating this as being deconstructible itself - see 22951
we have a field which also returns a name
The first phase is generating all the operations for each app  and gathering them into a big per-app list.  We'll then go through that list later and order it and split  into migrations to resolve dependencies caused by M2Ms and FKs.
Renames have to come first
Prepare lists of fields and generate through model map
Generate non-rename model operations
we use a stable sort for deterministic tests & general behavior
De-dupe dependencies
Optimize migrations
Don't add operations which modify the database for unmanaged models
Skip here, no need to handle fields for unmanaged models
We might need to depend on the removal of an  order_with_respect_to or index/unique_together operation;  this is safely ignored if there isn't one
unmanaged converted to managed
managed converted to unmanaged
We're already in a transaction; create a savepoint, unless we  were told not to or we're already waiting for a rollback. The  second condition avoids creating useless savepoints and prevents  overwriting needs_rollback until the rollback is performed.
Prematurely unset this flag to allow using commit or rollback.
The database will perform a rollback by itself.  Wait until we exit the outermost block.
Commit transaction  An error during rollback means that something  went wrong with the connection. Drop it.
Bare decorator: @atomic -- although the first argument is called  `using`, it's actually the function being decorated.  Decorator: @atomic(...) or context manager: with atomic(...): ...
Only set the 'errors_occurred' flag for errors that may make  the connection unusable.
Note that we are intentionally not using @wraps here for performance  reasons. Refs 21109.
This backend was renamed in Django 1.9.
If the router doesn't have a method, skip to the next one.
If the router doesn't have a method, skip to the next one.
If the router doesn't have a method, skip to the next one.
These values, if given to validate(), will trigger the self.required check.
Compile the regex if it was not passed pre-compiled.
IP patterns
Check first if the scheme is valid
Then check full URL  Trivial case failed. Try for possible IDN domain
literal form, ipv4 or ipv6 address (SMTP 4.1.3)
Get the proper name for the file, as it will actually be saved.
`filename` may include a path as returned by FileField.upload_to.
This function is only needed to help with the deprecations above and can  be removed in Django 2.0, RemovedInDjango20Warning.
This file has a file path that we can move.
Ooops, the file exists. We need a new file name.
OK, the file save worked. Break out of the loop.
Store filenames with forward slashes, even on Windows.
If the file exists, delete it from the filesystem.  If os.remove() fails with ENOENT, the file may have been removed  concurrently, and it's safe to continue normally.
Safe to use .replace() because UTC doesn't have DST
Macintosh, Unix.
All other platforms: check for same pathname.
There's no reason to move if we don't have to.
If the destination file exists and allow_overwrite is False then raise an IOError
This will happen with os.rename if moving to another filesystem  or when moving opened files on certain operating systems
Certain operating systems (Cygwin and Windows)  fail when deleting opened files, ignore it.  (For the  systems where this happens, temporary files will be auto-deleted  on close anyway.)
Sanitize the file name so that it can't be dangerous.  Just use the basename of the file -- anything else is dangerous.
File names longer than 255 characters can cause problems on older OSes.
Means the file was moved or deleted before the tempfile  could unlink it.  Still sets self.file.close_called and  calls self.file.file.close() before the exception
Since it's in memory, we'll never have multiple chunks.
Adapted from the pyserial project   detect size of ULONG_PTR
Union inside Structure by stackoverflow:3480240
File locking is not supported.
Dummy functions that don't do anything.  File is not locked
File is unlocked
If this is the end of a \n or \r\n line, yield.
Check the content-length header to see if we should  If the post is too large, we cannot use the Memory handler.
Because close can be called during shutdown  we need to cache os.unlink and access it  as self.unlink only
Flag for if it's been compressed or not
Avoid zlib dependency unless compress is being used
Convert the signature from bytes to str only on Python 3
Check timestamp is not older than max_age
-*- coding: utf-8 -*-
... perform checks and collect `errors` ...  or
By default, 'database'-tagged checks are not run as they do more  than mere static code analysis.
Check resolver recursively
This is not a url() instance
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Levels
We need to hardcode ModelBase and Field cases because its __str__  method doesn't return "applabel.modellabel" and cannot be changed.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Short circuit if there aren't any errors.
The or clauses are redundant but work around a bug (25945) in  functools.partial in Python 3 <= 3.5.1 and Python 2 <= 2.7.11.
-*- coding: utf-8 -*-
no invalid tags
We may have previously seen a "all-models" request for  this app (no model qualifier was given). In this case  there is no need adding specific models to the list.
Check that the serialization format exists; this is a shortcut to  avoid collating all the objects and _then_ failing.
Legacy behavior, tablename specified as argument
"key" is a reserved word in MySQL, so use "cache_key" instead.
Keep a count of the installed objects and fixtures
Django's test suite repeatedly tries to load initial_data fixtures  from apps that don't have any fixtures. Because disabling constraint  checks can be expensive on some database (especially MSSQL), bail  out early if no fixtures are found.
Warn if the fixture we loaded contains 0 objects.
Save the fixture_dir and fixture_name for future error messages.
Validation is called explicitly each time the server is reloaded.
We rely on the environment because it's currently the only  way to reach WSGIRequestHandler. This seems an acceptable  compromise considering `runserver` runs indefinitely.
If an exception was silenced in ManagementUtility.execute in order  to be raised in the child process, raise it now.
'shutdown_message' is a stealth option.
Kept for backward compatibility
'table_name_filter' is a stealth option
Add primary_key and unique, if necessary.
Calling `get_field_type` to get the field type string and any  additional parameters and notes.
Custom fields will have a dotted path
Only add the comment if the double underscore was in the original name
This is a hook for data_types_reverse to return a tuple of  (field_type, field_params_dict).
Add max_length for all CharFields.
we do not want to include the u"" or u'' prefix  so we build the string rather than interpolate the tuple
Inspired by Postfix's "postconf -n".
Because settings are imported lazily, we need to explicitly load them.
Work out the list of predecessor migrations
Work out the value of replaces (any squashed ones we're re-squashing)  need to feed their replaces into ours
Write out the new migration file
Remove '.py' suffix  Preserve '.\' prefix on Windows to respect gettext behavior
This check is needed for the case of a symlinked file and its  source being processed inside a single group (locale dir);  removing either of those two removes both.
Ensure last line has its EOL
Strip the header
Need to ensure that the i18n framework is enabled
Build locale list
Account for excluded locales
Build po files for each selected locale
Print warnings
-*- coding: utf-8 -*-
sqlmigrate doesn't support coloring its output but we need to force  no_color=True so that the BEGIN/COMMIT statements added by  output_transaction don't get colored either.
Get the database we're operating from
Load up an executor to get all the migration data
Show begin/end around output only for atomic migrations
Make a plan that represents just the requested migrations and show SQL  for it
Create a test database.
Import the fixture data into the test database.
The following are stealth options used by Django's internals.
Import the 'management' module within each installed app, to register  dispatcher events.
Empty sql_list may signify an empty database and post_migrate would then crash  Emit the post migrate signal. This allows individual applications to  respond as if the database had been migrated from scratch.
no IPython, raise ImportError
Set up a dictionary to serve as the environment for the shell, so  that tab completion works on objects that are imported at runtime.
Execute the command and exit.
Known side effect: updating file access/modified time to current time if  it is writable.
Gather existing directories.
Account for excluded locales
-*- coding: utf-8 -*-
Get the database we're operating from
Load migrations from disk/DB
Generate the plan
Create a random SECRET_KEY to put it in the main settings.
-*- coding: utf-8 -*-
Import the 'management' module within each installed app, to register  dispatcher events.
Get the database we're operating from
Hook for backends needing any database preparation  Work out which apps have migrations and which do not
Raise an error if any migrations are applied before their dependencies.
Get a list of already installed *models* so that references work right.
Note that if a model is unmanaged we short-circuit and never try to install it
Load the current graph state. Pass in None for the connection so  the loader doesn't try to resolve replaced migrations from DB.
Raise an error if any migrations are applied before their dependencies.
Before anything else, see if there's conflicting apps and drop out  hard if there are any and they don't want to merge
If app_labels is specified, filter out conflicting migrations for unspecified apps
If they want to merge and there's nothing to merge, then politely exit
If they want to merge and there is something to merge, then  divert into the merge code
Set up autodetector
Detect changes
Command object passed in.
Load the command object by name.
If the command is already loaded, use it directly.
Don't complete if user hasn't sourced bash_completion file.
Preprocess options to extract --settings and --pythonpath.  These options could affect the commands that are available, so they  must be processed early.
A handful of built-in management commands work without settings.  Load the default settings -- where INSTALLED_APPS is empty.
In all other cases, django.setup() is required to succeed.
Setup a stub settings environment for template rendering
Ignore some files as they cause various breakages.
downloads the file and returns the path
Falling back to content type guessing
Move the temporary file to a filename that has better  chances of being recognized by the archive utils
Giving up
On Jython there is no os.access()
Metadata about this command.
Configuration shortcuts that alter various logic.
No databases are configured (or the dummy one)
isatty is not always implemented, 6223.
For backwards compatibility,  set style for ERROR_OUTPUT == ERROR
ENOENT can happen if the cache file is removed (by another  process) after the os.path.exists check.
Delete a random selection of entries
The exception type to catch from the underlying library for a key  that was not found. This is a ValueError for python-memcache,  pylibmc.NotFound for pylibmc, and cmemcache will return None without  raising an exception.
Using 0 in memcache sets a non-expiring timeout.
Other cache backends treat 0 as set-and-expire. To achieve this  in memcache backends, a negative timeout must be passed.
Python 2 memcache requires the key to be a byte string.
make sure the key doesn't keep its old value in case of failure to set (memcached's 1MB limit)
memcached doesn't support a negative delta
python-memcache responds to incr on non-existent keys by  raising a ValueError, pylibmc by raising a pylibmc.NotFound  and Cmemcache returns None. In all cases,  we should raise a ValueError though.
memcached doesn't support a negative delta
python-memcache responds to incr on non-existent keys by  raising a ValueError, pylibmc by raising a pylibmc.NotFound  and Cmemcache returns None. In all cases,  we should raise a ValueError though.
Global in-memory store of cache data. Keyed by name, to provide  multiple named local memory caches.
To be threadsafe, updates/inserts are allowed to fail silently
Stub class to ensure not passing in a `timeout` argument results in  the default timeout
Memcached does not accept keys longer than this.
ticket 21147 - avoid time.time() related precision issues
Fetch the value again to avoid a race condition if another caller  added a value between the first get() and the add() above.
This is a separate method, rather than just a copy of has_key(),  so that it always has the same functionality as has_key(), even  if a subclass overrides it.
AttributeError if object_list has no count() method.  TypeError if object_list.count() requires arguments  (i.e. is of type list).
The object_list is converted to a list so that if it was a QuerySet  it won't be a database hit per __getitem__.
Special case, return zero if no items.
Special case for the last page because there can be orphans.
PY2 can't pickle naive exception: http://bugs.python.org/issue1692335.
Trigger an AttributeError if this ValidationError  doesn't have an error_dict.
encode() and decode() expect the charset to be a native string.
since size is not None here, len(self.buffer) < size
The WSGI spec says 'QUERY_STRING' may be absent.
mod_wsgi squashes multiple successive slashes in PATH_INFO,  do the same with script_url before manipulating paths (17133).
Under Python 3, non-ASCII values in the WSGI environ are arbitrarily  decoded with ISO-8859-1. This is wrong for Django websites where UTF-8  is the default. Re-encode to recover the original bytestring.
We only assign to this when initialization is complete as it is used  as a flag for initialization being complete.
Setup default url resolver for this thread
If the exception handler returns a TemplateResponse that has not  been rendered, force it to be rendered.
Apply view middleware
Complain if the view returned None (a common error).
If Http500 handler is not installed, re-raise last exception  Return an HttpResponse that displays a friendly error message.
Apply request middleware
This happens when calling quit() on a TLS connection  sometimes, or when the connection was already disconnected  by the server.
We failed silently on open().  Trying to send would be pointless.
Cache the hostname, but do it lazily: socket.getfqdn() can take a couple of  seconds, which slows down the restart of the server.
Don't BASE64-encode UTF-8 messages so that we avoid unwanted attention from  some spam filters.
Default MIME type to use on attachments (if it is not explicitly given  and cannot be guessed).
stdlib uses socket.getfqdn() here instead
On Python 2, use the stdlib since `email.headerregistry` doesn't exist.
On Python 3, an `email.headerregistry.Address` object is used since  email.utils.formataddr() naively encodes the name as ascii (see 25986).
message/rfc822 attachments must be ASCII
the default value of '_charset' is 'us-ascii' on Python 2
Don't bother creating the network connection if there's nobody to  send to.
If mimetype suggests the file is text but it's actually  binary, read() will raise a UnicodeDecodeError on Python 3.
If the previous read in text mode failed, try binary mode.
Bug 18967: per RFC2046 s5.2.1, message/rfc822 attachments  must not be base64 encoded.  convert content into an email.Message first  For compatibility with existing code, parse the message  into an email.Message object if it is not one already.
Encode non-text attachments with base64.
Inheriting from object required on Python 2.  Ignore broken pipe errors, otherwise pass on
Short-circuit parent method to not call socket.getfqdn
Strip all headers with underscores in the name before constructing  the WSGI environ. This prevents header-spoofing based on ambiguity  between underscores and dashes both normalized to underscores in WSGI  env vars. Nginx and Apache 2.4+ both do this as well.
Under Python 3, non-ASCII values in the WSGI environ are arbitrarily  decoded with ISO-8859-1. We replicate this behavior here.  Refs comment in `get_bytes_from_wsgi()`.
Use the C (faster) implementation if possible
Grand-parent super
Map to deserializer error
Look up the model using the model loading mechanism. If this fails,  bail.
Start building a data dictionary from the object.
Also start building a dict of m2m data (this is saved as  {m2m_accessor_attribute : [list_of_related_objects]})
Get the field from the Model. This will raise a  FieldDoesNotExist if, well, the field doesn't exist, which will  be propagated correctly unless ignorenonexistent=True is used.
Return a DeserializedObject so that the m2m data has a place to live.
If there are 'natural' subelements, it must be a natural key
Otherwise, treat like a normal PK value.
expat 1.2
Process the list of models, and get the list of dependencies
Protected types (i.e., primitives like None, numbers, dates,  and Decimals) are passed through as is. All other values are  converted to string first.
Handle each field
skip fields no longer on model
Avoid shadowing the standard library json module
Use JS strings to represent Python Decimal instances (ticket 16850)
Prevent trailing spaces
Grand-parent super
Map to deserializer error
Older, deprecated class name (for backwards compatibility purposes).
Indicates if the implemented serializer is only available for  internal Django use.
prevent a second (possibly accidental) call to save() from saving  the m2m data twice.
RemovedInDjango20Warning: Use old api for non-recursive  loaders.
template needs to be compiled
Django < 1.8 accepted a Context in `context` even though that's  unintended. Preserve this ability but don't rewrap `context`.
If we get here, none of the templates could be loaded
Include a reference to the real function (used to check original  arguments by the template parser, and to bear the 'is_safe' attribute  when multiple decorators are applied).
Set the precision high enough to avoid an exception, see 15789.
Ignore mark_for_escaping deprecation -- this will use  conditional_escape() in Django 2.0.
Unpack list of wrong size (no "maybe" value provided).
Since this package contains a "django" module, this is required on Python 2.
Since this package contains a "django" module, this is required on Python 2.
No templatetags package defined. This is safe to ignore.
Since this package contains a "django" module, this is required on Python 2.
Immutable return value because it's cached and shared by callers.
The joined path was located outside of this template_dir  (it might be inside another one, so this isn't fatal).
Since this package contains a "django" module, this is required on Python 2.
Since this package contains a "django" module, this is required on Python 2.
Null denotation - called in prefix context
Left denotation - called in infix context
Templates shouldn't throw exceptions when rendering.  We are  most likely to get exceptions for things like {% if foo in bar  %} where 'bar' does not support 'in', so default to False
Assign 'id' to each:
IfParser uses Literal in create_var, but TemplateIfParser overrides  create_var so that a proper implementation that actually resolves  variables, filters etc. is used.
Check that we have exhausted all the tokens
First time the node is rendered in template
Apply filters.
Init state storage
Consider multiple parameters.  This automatically behaves  like an OR evaluation of the multiple variables.
The "{% ifchanged %}" syntax (without any variables) compares the rendered output.
render true block if not already rendered
This method is called for each object in self.target. See regroup()  for the reason why we temporarily put the object in the context.
target variable wasn't found in context; fail silently.  List of dictionaries in the format:  {'grouper': 'key', 'list': [list of contents]}.
Try to look up the URL. If it fails, raise NoReverseMatch unless the  {% url ... as var %} construct is used, in which case return nothing.
var and name are legacy attributes, being left in case they are used  by third-party subclasses of this Node.
{% else %} (optional)
{% endif %}
Hard-coded processor for easier use of CSRF protection.
because dictionaries can be put in different order  we have to flatten them like in templates
if it's not comparable return false
Set to the original template -- as opposed to extended or included  templates -- during rendering, see bind_template.
placeholder for context processors output
empty dict for any new modifications  (so that context processors don't overwrite them)
Set context processors according to the template engine's settings.
Unset context processors.
This is for backwards-compatibility: RequestContexts created via  Context.new don't include values from context processors.
The following pattern is required to ensure values from  context override those from template context processors.
what to report as the origin for templates that come from non-loader sources  (e.g. strings)
In some rare cases exc_value.args can be empty or an invalid  unicode string.
Ignore mark_for_escaping deprecation as this will be  removed in Django 2.0.
First argument, filter input, is implied.  Check to see if a decorator is providing the real function.
Not enough OR Too many
First try to treat this variable as a number.  Note that this could cause an OverflowError here that we're not  catching. Since this should only happen at compile time, that's  probably OK.
So it's a float... is it an int? If the original value contained a  dot or an "e" then it was a float, not an int.
"2." is invalid
We're dealing with a variable that needs to be resolved
We're dealing with a literal, so it's already been "resolved"
ValueError/IndexError are for numpy.array lookup on  numpy < 1.9 and 1.9+ respectively
Don't return class attributes if the class is the context:
Reraise an AttributeError raised by a @property
Set this to True for nodes that must be first in the template (although  they can be preceded by text nodes.
Set to True the first time a non-TextNode is inserted by  extend_nodelist().
Unicode conversion can fail sometimes for reasons out of our  control (e.g. exception rendering). In that case, we fail  quietly.
Regex for token keyword arguments
Wrapper for loading templates from eggs via pkg_resources.resource_string.
The joined path was located outside of this template_dir  (it might be inside another one, so this isn't fatal).
RemovedInDjango20Warning: Add template_dirs for compatibility  with old loaders
RemovedInDjango20Warning: Allow loaders to be called like functions.
RemovedInDjango20Warning: Add template_dirs for compatibility with  old loaders
If compiling the template we found raises TemplateDoesNotExist,  back off to returning the source and display name for the  template we were asked to load. This allows for correct  identification of the actual template that does not exist.
This will raise an exception if 'BACKEND' doesn't exist or  isn't a string containing at least one dot.
If importing or initializing the backend raises an exception,  self._engines[alias] isn't set and this code may get executed  again, so we must preserve the original params. See 24265.
Immutable return value because it will be cached and shared by callers.
Dictionary of FIFO queues.
parent is a django.template.Template
parent is a django.template.backends.django.Template
Add the block nodes from this node to the block context
Call Template._render explicitly so the parser context stays  the same.
relative_name is a variable or a literal that doesn't contain a  relative path.
@register.tag()
@register.tag
@register.tag('somename') or @register.tag(name='somename')
register.tag('somename', somefunc)
@register.filter()
@register.filter
@register.filter('somename') or @register.filter(name='somename')
@register.simple_tag(...)
@register.simple_tag
Copy across the CSRF token, if present, because inclusion tags are  often used for forms, and we need instructions for using CSRF  protection to be as simple as possible.
Consider the last n params handled, where n is the  number of defaults.
It would seem obvious to call these next two members 'template' and  'context', but those names are reserved as part of the test Client  API. To avoid the name collision, we use different names.
content argument doesn't make sense here because it will be replaced  with rendered template so we always pass empty string in order to  prevent errors and provide shorter signature.
In order to be able to provide debugging info in the  case of misconfiguration, we use a sentinel value  instead of returning an empty dict.
Return a lazy reference that computes connection.queries on access,  to ensure it contains queries triggered after this function runs.
First, try to get the "index" sitemap URL.
Next, try for the "global" sitemap URL.
This limit is defined by Google. See the index documentation at  http://www.sitemaps.org/protocol.htmlindex.
If protocol is None, the URLs in the sitemap will use the protocol  with which the sitemap was requested.
Determine protocol
Make sure to return a clone; we don't want premature evaluation.
if lastmod is defined for all sites, set header so as  ConditionalGetMiddleware is able to send 304 NOT MODIFIED
Support network-path reference (see 16753) - RSS requires a protocol
if item_pubdate or item_updateddate is defined for the feed, set  header so as ConditionalGetMiddleware is able to send 304 NOT MODIFIED
Titles should be double escaped by default (see 6533)
Check co_argcount rather than try/excepting the function and  catching the TypeError, because something inside the function  may raise the TypeError. This technique is more accurate.
Instantiating the DataSource from the string.
Creating the dictionary.
generate_model.py
Getting the layer corresponding to the layer key and getting  a string listing of all OGR fields in the Layer.
For those wishing to disable the imports.
The model field name.
Getting the keyword args string.
If argument is not a `SpatialReference` instance, use it as parameter  to construct a `SpatialReference` instance.
Initializing the keyword arguments dictionary for both PostGIS  and SpatiaLite.
Creating the spatial_ref_sys model.  Try getting via SRID only, because using all kwargs may  differ from exact wkt/proj in database.
LayerMapping exceptions.
Getting the DataSource and the associated Layer.
Setting the mapping & model attributes.
Checking the layer -- initialization of the object will fail if  things don't check out before hand.
Getting the geometry column associated with the model (an  exception will be raised if there is no geometry column).
Checking the source spatial reference system, and getting  the coordinate transformation object (unless the `transform`  keyword is set to False)
Setting the encoding for OFTString fields, if specified.  Making sure the encoding exists, if not a LookupError  exception will be thrown.
Getting lists of the field names and the field types available in  the OGR Layer.
Getting the string name for the Django field class (e.g., 'PointField').
Getting the coordinate dimension of the geometry field.
Setting the `geom_field` attribute w/the name of the model field  that is a Geometry.  Also setting the coordinate dimension  attribute.
Is the model field type supported by LayerMapping?
Is the OGR field in the Layer?
Otherwise just pulling the SpatialReference from the layer
List of fields to determine uniqueness with
Only a single field passed in.
The keyword arguments for model construction.
Incrementing through each model field and OGR field in the  dictionary mapping.
Verify OGR geometry.
The related _model_, not a field was passed in -- indicating  another mapping for the related Model.
Otherwise, verify OGR Field type.
Setting the keyword arguments for the field name with the  value obtained above.
The encoding for OGR data sources may be specified here  (e.g., 'cp437' for Census Bureau boundary files).
Creating an instance of the Decimal value to use.
Getting the decimal value as a tuple.
Maximum amount of precision, or digits to the left of the decimal.
Getting the digits to the left of the decimal place for the  given decimal.
Attempt to convert any OFTReal and OFTString value to an OFTInteger.
Constructing and verifying the related model keyword arguments.
Downgrade a 3D geom to a 2D one, if necessary.
Constructing a multi-geometry type to contain the single geometry
Transforming the geometry with our Coordinate Transformation object,  but only if the class variable `transform` is set w/a CoordTransform  object.
Returning the WKT of the geometry.
Creating the CoordTransform object
Use `get_field()` on the model's options so that we  get the correct field instance if there's model inheritance.
Getting the default Feature ID range.
Setting the progress interval, if requested.
No unique model exists yet, create.
Attempting to save.
Printing progress information, if requested.
Only used for status output purposes -- incremental saving uses the  values returned here.
Constructing the slice to use for this step; the last slice is  special (e.g, [100:] instead of [90:100]).
Otherwise, just calling the previously defined _save() function.
If no locations specified, then we try to build for  every model in installed applications.
Geo-enabled Sitemap classes.
Database will take care of transformation.
Getting the render function and rendering to the correct.
Creating a template context that contains Django settings  values needed by admin map templates.
Update the template parameters with any attributes passed in.
Defaulting the WKT value to a blank string -- this  will be tested in the JavaScript and the appropriate  interface will be constructed.
Constructing the dictionary of the map options.
Setting the parameter WKT with that of the transformed  geometry.
JavaScript construction utilities for the Bounds and Projection.
Setting the widget with the newly defined widget.
Paths to the city & country binary databases.
Initially, pointers to GeoIP file references are NULL.
Checking the given cache option.
Otherwise, some detective work will be needed to figure out  whether the given database path is for the GeoIP country or city  databases.
GeoLite City database detected.
GeoIP Country database detected.
Cleanup any GeoIP file handles lying around.
Making sure a string was passed in for the query.
Return the query string back to the caller. GeoIP2 only takes IP addresses.
Returning the country code and name
NumPy supported?
Getting the OGR DataSource from the string parameter.
Getting a more specific field type and any additional parameters  from the `get_geometry_type` routine for the spatial backend.
If a string reaches here (via a validation error on another  field) then just reconstruct the Geometry.
Use the official spherical mercator projection SRID when GDAL is  available; otherwise, fallback to 900913.
Try to set the srid
Getting the envelope of the input polygon (used for automatically  determining the zoom level).
Translating the coordinates into a JavaScript array of  Google `GLatLng` objects.
Stroke settings.
Fill settings.
Getting the envelope for automatic zoom determination.
XOR with hash of GIcon type so that hash('varname') won't  equal hash(GIcon('varname')).
Getting the Google Maps API version, defaults to using the latest ("2.x"),  this is not necessarily the most stable.
Can specify the API URL in the `api_url` keyword.
Setting the DOM id of the map, the load function, the JavaScript  template, and the KML URLs array.
Does the user want any GMarker, GPolygon, and/or GPolyline overlays?
If GMarker, GPolygons, and/or GPolylines are used the zoom will be  automatically calculated via the Google Maps API.  If both a zoom  level and a center coordinate are provided with polygons/polylines,  no automatic determination will occur.
Defaults for the zoom level and center coordinates if the zoom  is not automatically calculated.
The `google-multi.js` template is used instead of `google-single.js`  by default.
This is the template used to generate the GMap load JavaScript for  each map in the set.
Running GoogleMap.__init__(), and resetting the template  value with default obtained above.
If a tuple/list passed in as first element of args, then assume
Generating DOM ids for each of the maps in the set.
Overloaded to use the `load` function defined in the  `google-multi.js`, which calls the load routines for  each one of the individual maps in the set.
Constants used for degree to radian conversion, and vice-versa.
Google's tilesize is 256x256, square tiles are assumed.
The number of zoom levels
Multiplying `z` by 2 for the next iteration.
Setting up, unpacking the longitude, latitude values and getting the  number of pixels for the given zoom level.
Calculating the pixel x coordinate by multiplying the longitude value  with the number of degrees/pixel at the given zoom level.
Calculating the pixel y coordinate.
Returning the pixel x, y to the caller of the function.
Getting the number of pixels for the given zoom level.
Calculating the longitude value, using the degrees per pixel.
Calculating the latitude value.
Returning the longitude, latitude coordinate pair.
The given lonlat is the center of the tile.
Getting the pixel coordinates corresponding to the  the longitude/latitude.
Getting the lower-left and upper-right lat/lon coordinates  for the bounding box of the tile.
Constructing the Polygon, representing the tile and returning.
Checking the input type.
Getting the envelope for the geometry, and its associated width, height  and centroid.
Getting the tile at the zoom level.
Otherwise, we've zoomed in to the max.
GeoIP_lib_version appeared in version 1.4.7.
For freeing memory allocated within a record
For retrieving records by name or address.  Checking the pointer to the C structure, if valid pull out elements  into a dictionary.
Now converting the strings to unicode using the proper encoding.
Free the memory allocated for the struct & return.
For opening & closing GeoIP database files.
This is so the string pointer can be freed within Python.
Getting the C `free` for the platform.
Regular expressions for recognizing the GeoIP free database editions.
Paths to the city & country binary databases.
Initially, pointers to GeoIP file references are NULL.
Checking the given cache option.
Cleaning any GeoIP file handles lying around.
Making sure a string was passed in for the query.
Return the query string back to the caller. GeoIP only takes bytestrings.
If an IP address was passed in
If a FQDN was passed in.
Returning the country code and name
Methods for compatibility w/the GeoIP-Python API.
Automatic SRID conversion so objects are comparable
No version parameter
Set parameters as geography if base field is geography
Geometry fields with geodetic (lon/lat) coordinates need special distance functions
Replace boolean param by the real spheroid of the base field
Geometry fields with geodetic (lon/lat) coordinates need length_spheroid
Make srid the resulting srid of the transformation
Some backends do not set the srid on the returning geometry
Always provide the z parameter for ST_Translate (Spatialite >= 3.1)
Geography fields support area calculation, returns square meters.
Getting the area units of the geographic field.
Initializing the procedure arguments.
Is there a geographic field in the model to perform this  operation on?
Setting the procedure args.
The attribute to attach to the model.
Getting the format for the stored procedure.
If the result of this function needs to be converted.
Finally, setting the extra selection attribute with  the format string expanded with the stored procedure  arguments.
Setting up the distance procedure arguments.
If geodetic defaulting distance attribute to meters (Oracle and  PostGIS spherical distances return meters).  Otherwise, use the  units of the geometry field.
The field's _get_db_prep_lookup() is used to get any  extra distance parameters.  Here we set up the  parameters that will be passed in to field's function.
Getting the spatial backend operations.
The `geom_args` flag is set to true if a geometry parameter was  passed in.
Getting whether this field is in units of degrees since the field may have  been transformed via the `transform` GeoQuerySet method.
`transform()` was not used on this GeoQuerySet.
There's no `length_sphere`, and `length_spheroid` also  works on 3D geometries.
Use 3D variants of perimeter and length routines on supported backends.
This geographic field is inherited from another model, so we have to  use the db table for the _parent_ model instead.
Incrementing until the first geographic field is found.
Otherwise, check by the given field name -- which may be  a lookup to a _related_ geographic field.
If the database returns a Decimal, convert it to a float as expected  by the Python geometric objects.  If the units are known, convert value into area measure.
Hacky marker for get_db_converters()
this will be called again in parent, but it's needed now - before  we get the spatial_aggregate_name
Local cache of the spatial_ref_sys table, which holds SRID data for each  spatial database alias. This cache exists so that the database isn't queried  for SRID info each time a distance query is constructed.
The SpatialRefSys model for the spatial backend.
No `spatial_ref_sys` table in spatial backend (e.g., MySQL).
Initialize SRID dictionary for database if it doesn't exist.
This allows operations to be done on fields in the SELECT,  overriding their values -- used by the Oracle and MySQL  spatial backends to get database values as WKT, and by the  `transform` method.
Geodetic units.
Setting the index flag with the value of the `spatial_index` keyword.
Setting the SRID and getting the units.  Unit information must be  easily available in the field instance for distance queries.
Setting the verbose_name keyword argument with the positional  first parameter, so this works like normal fields.
Always include SRID for less fragility; include spatial index if it's  not the default value.
The following functions are used to get the units, their name, and  the spheroid corresponding to the SRID of the BaseSpatialField.  Get attributes from `get_srid_info`.
Some backends like MySQL cannot determine units name. In that case,  test if srid is 4326 (WGS84), even if this is over-simplification.
Assigning the SRID value.
The OpenGIS Geometry name.
Setting the dimension of the geometry field.
Is this a geography rather than a geometry column?
Routines overloaded from Field
Setup for lazy-instantiated Geometry object.
The OpenGIS Geometry Type Fields
Make sure raster fields are used only on backends with raster support.
Prepare raster for writing to database.
Importing GDALRaster raises an exception on systems without gdal.  Setup for lazy-instantiated Raster object. For large querysets, the  instantiation of all GDALRasters can potentially be expensive. This  delays the instantiation of the objects to the moment of evaluation  of the raster attribute.
This manager should be used for queries on related fields  so that geometry columns on Oracle and MySQL are selected  properly.
No need to bother users with the use_for_related_fields  deprecation for this manager which is itself deprecated.
This takes into account the situation where the lookup is a  lookup to a related geographic field, e.g., 'address__point'.
Reversing so list operates like a queue of related lookups,  and popping the top lookup.
Finally, make sure we got a Geographic field and return.
PostGIS band indices are 1-based, so the band index needs to be  increased to be consistent with the GDALRaster band indices.
If rhs is some QuerySet, don't touch it
Unlike BuiltinLookup, the GIS get_rhs_op() implementation should return  an object (SpatialOperator) with an as_sql() method to allow for more  complex computations (where the lhs part can be mixed in).
Alias of same_as
Check if the second parameter is a band index.
Accessed on a class, not an instance
Getting the value of the field.
Otherwise, a geometry or raster object is built using the field's  contents, and the model's corresponding attribute is set.
The geographic type of the field.
For raster fields, assure input is None or a string, dict, or  raster instance.
The geometry type must match that of the field -- unless the  general GeometryField is used.  Assigning the field SRID if the geometry has no SRID.
Set geometries with None, WKT, HEX, or WKB
Setting the objects dictionary with the value, and returning.
Updating the data_types_reverse dictionary with the appropriate  type for Geometry fields.
Geometry fields are stored as BLOB/TEXT and can't have defaults.
Lookup to convert pixel type values from GDAL to PostGIS
Lookup to convert pixel type values from PostGIS to GDAL
Size of the packed value in bytes for different numerical types.  This is needed to cut chunks of band data out of PostGIS raster strings  when decomposing them into GDALRasters.  See https://docs.python.org/3/library/struct.htmlformat-characters
Getting the WKB (in string form, to allow easy pickling of  the adaptor) and the SRID from the geometry or raster.
Does the given protocol conform to what Psycopg2 expects?
Psycopg will figure out whether to use E'\\000' or '\000'.
For rasters, add explicit type cast to WKB string.
Reverse dictionary for PostGIS geometry types not populated until  introspection is actually performed.
The value for the geography type is actually a tuple  to pass in the `geography=True` keyword to the field  definition.
OGRGeomType does not require GDAL and makes it easy to convert  from OGC geom type name to Django field.
Getting any GeometryField keyword arguments that are not the default.
Identifier to mark raster lookups as bilateral.
Get rhs value.
Check which input is a raster.
Using distance_spheroid requires the spheroid of the field as  a parameter.
Getting the distance parameter
Shorthand boolean flags.
Assuming the distance is in the units of the field.
Get the srid for this object
If this is an F expression, then we don't really want  a placeholder and instead substitute in the column  of the expression.
Close out the connection.  See 9437.
Getting the PostGIS version
Routines for getting the OGC-compliant models.
Methods to convert between PostGIS rasters and dicts that are  readable by GDALRaster.
Create geometry columns
Create geometry columns
The positional arguments here extract the hex-encoded srid from the  header of the PostGIS raster string. This can be understood through  the POSTGIS_HEADER_STRUCTURE constant definition in the const module.
Split raster header from data
Parse band data  Get pixel type for this band
Subtract nodata byte from band nodata value if it exists
Convert datatype from PostGIS to GDAL & get pack type and size
Parse band nodata value. The nodata value is part of the  PGRaster string even if the nodata flag is True, so it always  has to be chunked off the data string.
Chunk and unpack band data (pack size times nr of pixels)
If the nodata flag is True, set the nodata value.
Append band data to band list
Store pixeltype of this band in pixeltypes array
Check that all bands have the same pixeltype.  This is required by GDAL. PostGIS rasters could have different pixeltypes  for bands of the same raster.
Return if the raster is null
Hexlify raster header
Get band pixel type in PostGIS notation
Set the nodata flag
Pack band header
Hexlify band data
Add packed header and band data to result
Cast raster to string before passing it to the DB
Check that postgis extension is installed.
Optional geometry representing the bounds of this coordinate  system.  By default, all are NULL in the table.
Fix single polygon orientation as described in __init__()
Fix polygon orientations in geometry collections as described in  __init__()
Associating any OBJECTVAR instances with GeometryField.  Of course,  this won't work right on Oracle objects that aren't MDSYS.SDO_GEOMETRY,  but it is the only object type supported within Django anyways.
dwithin lookups on Oracle require a special string parameter  that starts with "distance=".
No geometry value used for F expression, substitute in  the column name instead.
Routines for getting the OGC-compliant models.
Oracle doesn't allow object names > 30 characters. Use this scheme  instead of self._create_index_name() for backwards compatibility.
Trying to get from WKT first.
Quick booleans for the type of this spatial backend, and  an attribute for the spatial database version tuple (if applicable)
How the geometry column should be selected.
Does the spatial database have a geometry or geography type?
Aggregates
Mapping between Django function names and backend names, when names do not  match; used in spatial_function_name().
Serialization
Constructors
Default conversion functions for aggregates; will be overridden if implemented  for the spatial backend.
For quoting column values, rather than columns.
Routines for getting the OGC-compliant models.
Does the database contain a SpatialRefSys model to store SRID information?
Does the backend support the django.contrib.gis.utils.add_srs_entry() utility?  Does the backend introspect GeometryField to its subtypes?
The following properties indicate if the database backend support  certain lookups (dwithin, left and right, relate, ...)
Does the database have raster support?
Does the database support a unique index on geometry fields?
Specifies whether the Collect and Extent aggregates are supported by the database
Add dynamically properties for each GQS method, e.g. has_force_rhr_method, etc.
No geometry value used for F expression, substitute in  the column name instead.
Adding Transform() to the SQL placeholder.
Routines for getting the OGC-compliant models.
Create geometry columns
Populate self.geometry_sql
Before we get too far, make sure pysqlite 2.5+ is installed.
SpatiaLite can only count vertices in LineStrings
SpatiaLite 4.1+ support initializing all metadata in one transaction  which can result in a significant performance improvement when  creating the database.
Shortcuts
Special methods for arithmetic operations
self must be shorter
self must be shorter
Public list interface Methods    Non-mutating
Mutating
CAREFUL: index.step and step are not the same!  step will never be None
we're not changing the length of the sequence
Keeping a reference to the original geometry object to prevent it  from being garbage collected which could then crash the prepared one  See 21662
If given a file name, get a real handle.
Checking the arguments  If only one geometry provided or a list of geometries is provided   in the first argument.
Ensuring that only the permitted geometries are allowed in this collection  this is moved to list mixin super class
Creating the geometry pointer array.
Checking the index and returning the corresponding GEOS geometry.
MultiPoint, MultiLineString, and MultiPolygon class definitions.
Setting the allowed types here since GeometryCollection is defined before  its subclasses.
Getting the ext_ring and init_holes parameters from the argument list
Getting the current pointer, replacing with the newly constructed  geometry, and destroying the old geometry.
Getting the interior ring, have to subtract 1 from the index.
Polygon Properties   Getting the number of rings
Properties for the exterior ring/shell.
Here a tuple or list was passed in under the `x` parameter.
Here X, Y, and (optionally) Z were passed in individually, as parameters.
Initializing using the address returned from the GEOS   createPoint factory.
can this happen?
Tuple setting and retrieval routines.
The tuple and coords properties
Getting the `free` routine used to free the memory allocated for  string pointers returned by GEOS.
Checking the status code  Double passed in by reference, return its value.
A c_size_t object is passed in by reference for the second  argument on these routines, and its needed to determine the  correct size.  Freeing the memory allocated within GEOS
Getting the string value at the pointer address.  Freeing the memory allocated within GEOS
Prepared geometry constructor and destructors.
Prepared geometry binary predicate support.
Initializing the context handler for this thread with  the notice and error handler.
Defining a thread-local object and creating an instance  to hold a reference to GEOSContextHandle for this thread.
GEOS thread-safe function signatures end with '_r', and  take an additional context handle parameter.  Create a reference here to thread_context so it's not  garbage-collected before an attempt to call this object.
Otherwise, use usual function.
If a context handle does not exist for this thread, initialize one.  Call the threaded GEOS routine with pointer of the context handle  as the first argument.
argtypes property
restype property
errcheck property
Binary & unary predicate factories
GEOSRelate returns a string, not a geometry.
Linear referencing routines
This is the return type used by binary output (WKB, HEX) routines.
ctypes factory classes
HEX & WKB output
Deprecated creation routines from WKB, HEX, WKT
Deprecated output routines
Geometry creation factories
Polygon and collection creation routines are special and will not  have their argument types defined.
Ring routines
Collection Routines
Cloning
Destruction routine.
SRID routines
Object in by reference, return its value.
Coordinate sequence prototype factory classes.
Get routines have double parameter passed-in by reference.
Get/Set ordinate routines have an extra uint parameter.
Coordinate Sequence constructors & cloning.
Getting, setting ordinate
For getting, x, y, z
For setting, x, y, z
These routines return size & dimensions.
The WKB/WKT Reader/Writer structures and pointers
WKTReader routines
WKTWriter routines
WKBReader routines
Although the function definitions take `const unsigned char *`  as their parameter, we use c_char_p here so the function may  take Python strings directly as parameters.  Inside Python there  is not a difference between signed and unsigned characters, so  it is not a problem.
WKBWriter routines
WKB Writing prototypes.
WKBWriter property getter/setter prototypes.
Cleaning up with the appropriate destructor.
Non-public WKB/WKT reader classes for internal use because  their `read` methods return _pointers_ instead of GEOSGeometry  objects.
WKB/WKT Writer Classes
Property for getting/setting the byteorder.
Property for getting/setting the output dimension.
Property for getting/setting the include srid flag.
`ThreadLocalIO` object holds instances of the WKT and WKB reader/writer  objects that are local to the thread.  The `GEOSGeometry` internals  access these instances by calling the module-level functions, defined  below.
These module-level routines return the I/O object that is local to the  thread. If the I/O object does not exist yet it will be initialized.
Initially the pointer is NULL.
Default allowed pointer type.
Pointer access property.  Raise an exception if the pointer isn't valid don't  want to be passing NULL pointers to routines --  that's very bad.
Only allow the pointer to be set with pointers of the  compatible type or None (NULL).
Property for controlling access to the GEOS object pointers.  Using  this raises an exception when the pointer is NULL, thus preventing  the C library from attempting to access an invalid memory location.
If only one argument provided, set the coords array appropriately
If SRID was passed in with the keyword arguments
Creating a coordinate sequence object because it is easier to  set the points using GEOSCoordSeq.__setitem__().
Calling the base geometry initialization with the returned pointer   from the function.
create a new coordinate sequence and populate accordingly
can this happen?
Sequence Properties
LinearRings are LineStrings used within Polygons.
Handling HEXEWKB input.
Handling GeoJSON input.
When the input is a pointer to a geometry (GEOM_PTR).
When the input is a buffer (WKB).
Invalid geometry type.
Setting the pointer object with a valid pointer.
Post-initialization setup.
Setting the SRID, if given.
Setting the coordinate sequence for the geometry (will be None on  geometries that do not have coordinate sequences)
Pickling support  The pickled state is simply a tuple of the WKB (in string form)  and the SRID.
Geometry set-like operations   Thanks to Sean Gillies for inspiration:   http://lists.gispython.org/pipermail/community/2007-July/001034.html  g = g1 | g2
g = g1 & g2
g = g1 - g2
g = g1 ^ g2
Geometry Info
Binary predicates.
A possible faster, all-python, implementation:   str(self.wkb).encode('hex')
short-circuit where source & dest SRIDs match
We don't care about SRID because CoordTransform presupposes  source SRS.
Topology Routines
Other Routines
Custom library path set?
Using the ctypes `find_library` utility to find the path to the GEOS  shared library.  This is better than manually specifying each library name  and extension (e.g., libgeos_c.[so|so.1|dylib].).
The notice and error handler C function callback definitions.  Supposed to mimic the GEOS message handler (C below):   typedef void (*GEOSMessageHandler)(const char *fmt, ...);
Opaque GEOS geometry structures, used for GEOM_PTR and CS_PTR
Pointers to opaque GEOS geometry structures.
Used specifically by the GEOSGeom_createPolygon and GEOSGeom_createCollection   GEOS routines
Returns the string version of the GEOS library. Have to set the restype  explicitly to c_char_p to ensure compatibility across 32 and 64-bit platforms.
Dimensions
Other Methods
Find the first declared geometry field
Add additional argument to force computation if there is no  existing PAM file to take the values from.
Computation of statistics fails for empty bands.
Create ctypes type array generator
Set read mode  Prepare empty ctypes array
Set write mode
Access band
See http://www.gdal.org/gdal_8h.htmla22e22ce0a55036a96f652765793fb7a4
A list of gdal datatypes that are integers.
Preprocess json inputs. This converts json strings to dictionaries,  which are parsed below the same way as direct dictionary inputs.
Create driver (in memory by default)
Check if width and height where specified
Check if srid was specified
Set SRID
Set additional properties if provided
Instantiate the object using an existing pointer to a gdal raster.
Raise an Exception if the value is being changed in read mode.
Create empty ctypes double array for data
Create ctypes double array with input and write data
Get the parameters defining the geotransform, srid, and size of the raster
Get the driver, name, and datatype of the target raster
Set the number of bands
Create target raster
Copy nodata values to warped raster
Select resampling algorithm
Make sure all data is written to file
Convert the resampling algorithm name into an algorithm id
Instantiate target spatial reference system
Set the driver and filepath if provided
Warp the raster into new srid
EPSG integer code was input.
Input is already an SRS pointer.
Creating a new SRS pointer, using the string buffer.
If the pointer is NULL, throw an exception.
Importing from either the user input string or an integer SRID.
Unit Properties
Import Routines
Export Properties
Attempting to import objects that depend on the GDAL library.  The  HAS_GDAL flag will be set to True if the library is present on  the system.
If a string name of the driver was passed in
Checking the alias dictionary (case-insensitive) to see if an  alias exists for the given driver.
Attempting to get the GDAL/OGR driver by the string name.
Making sure we get a valid pointer to the OGR Driver
Only register all if the driver count is 0 (or else all drivers  will be registered over and over again)
Setting the feature pointer and index.
Getting the pointer for this field.
Setting the class depending upon the OGR Field Type (OFT)
OFTReal with no precision should be an OFTInteger.
Field Methods
Field Properties
Default is to get the field as a string.
The Field sub-classes for each OGR Field type.
If this is really from an OFTReal field with no precision,  read as a double and cast as Python int (to prevent overflow).
String & Binary fields, just subclasses
List fields are also just subclasses
Helper routines for retrieving pointers and/or values from  arguments passed in by reference.
For routines that return a string.
Error-code return specified.  Getting the string value  Correctly freeing the allocated memory behind GDAL pointer  with the VSIFree routine.
Envelope checking
Getting the semi_major, semi_minor, and flattening functions.
Morphing to/from ESRI WKT.
Identifying the EPSG
Getting the angular_units, linear_units functions
Memory leak fixed in GDAL 1.5; still exists in 1.4.
SRS Properties
Coordinate transformation
Prepare partial functions that use cpl error codes
Setting the argument types
When a geometry pointer is directly returned.
Error code returned, geometry is returned by-reference.
Use subclass of c_char_p so the error checking routine  can free the memory at the pointer's address.
Error code is returned
`errcheck` keyword may be set to False for routines that  return void, rather than a status code.
Generation routines specific to this module
GetX, GetY, GetZ all return doubles.
Geometry modification routines.
Destroys a geometry
Geometry spatial-reference related routines.
Transformation routines.
For retrieving the envelope of the geometry.
GDAL & SRS Exceptions
Legacy name
Setting the OGR geometry type number.
OGREnvelope (a ctypes Structure) was passed in.
A tuple was passed in.
Individual parameters passed in.   Thanks to ww for the help
Custom library path set?
Windows NT shared libraries
Using the ctypes `find_library` utility  to find the  path to the GDAL library from the list of library names.
This loads the GDAL/OGR C library
On Windows, the GDAL binaries have some OSR routines exported with  STDCALL, while others are not.  Thus, the library will also need to  be loaded up as WinDLL for said OSR functions that require the  different calling convention.
Returns GDAL library version information with the given key.
Set library error handling so as errors are logged
For more information, see the OGR C API source code:   http://www.gdal.org/ogr/ogr__api_8h.html  The OGR_L_* routines are relevant here.
Does the Layer support random reading?
An integer index was given -- we cannot do a check based on the  number of features because the beginning and ending feature IDs  are not guaranteed to be 0 and len(layer)-1, respectively.
A slice was given
ResetReading() must be called before iteration is to begin.
If the Layer supports random reading, return.
Random access isn't supported, have to increment through  each feature until the given feature ID is encountered.  Should have returned a Feature, raise an OGRIndexError.
Map c_double onto params -- if a bad type is passed in it  will be caught here.
For more information, see the OGR C API source code:   http://www.gdal.org/ogr/ogr__api_8h.html  The OGR_G_* routines are relevant here.
If HEX, unpack input to a binary buffer.
Now checking the Geometry pointer before finishing initialization  by setting the pointer for the object.
Assigning the SpatialReference object to the geometry, if valid.
Setting the class depending upon the OGR Geometry Type
Pickle routines
Geometry set-like operations   g = g1 | g2
g = g1 & g2
g = g1 - g2
g = g1 ^ g2
Geometry Properties
The SRID property
Output Methods
Creating the unsigned character buffer, and passing it in by reference.  Returning a buffer of the string at the pointer.
Geometry Methods
Closing the open rings.
Returning the output of the given function with the other geometry's  pointer.
The subclasses for OGR Geometry.
LinearRings are used in Polygons.
Polygon Properties
Summing up the number of points in each ring of the Polygon.
The centroid is a Point, create a geometry for this.
Geometry Collection base class.
Summing up the number of points in each geometry in this collection
Multiple Geometry types.
Initially the pointer is NULL.
Default allowed pointer type.
Pointer access property.  Raise an exception if the pointer isn't valid don't  want to be passing NULL pointers to routines --  that's very bad.
Feature Properties
Retrieving the geometry pointer for the feature.
Getting the geometry for the feature.
Getting the 'description' field for the feature.
We can also increment through all of the fields   attached to this feature.  Get the name of the field (e.g. 'description')
Get the type (integer) of the field, e.g. 0 => OFTInteger
For more information, see the OGR C API source code:   http://www.gdal.org/ogr/ogr__api_8h.html  The OGR_DS_* routines are relevant here.
The write flag.  See also http://trac.osgeo.org/gdal/wiki/rfc23_ogr_unicode
Raise an exception if the returned pointer is NULL
If the backend hasn't been used, no more retrieval is necessary.  If this storage class contained all the messages, no further  retrieval is necessary
Even if there are no more messages, continue iterating to ensure  storages which contained messages are flushed.
Compatibility with previously-encoded messages
uwsgi's default configuration enforces a maximum size of 4kb for all the  HTTP headers. In order to leave some room for other cookies and headers,  restrict the session cookie to 1/2 of 4kb. See 18781.
remove the sentinel value
If we get here (and the JSON decode works), everything is  good. In any other case, drop back and return None.
Mark the data as used (so it gets removed) since something was wrong  with the data.
Check that the message level is not less than the recording level.  Add the message.
Check that the user has delete permission for the actual model
Populate deletable_objects, a data structure of all related objects that  will also be deleted.
Wait for the next page to be loaded
IE7 occasionally returns an error "Internet Explorer cannot  display the webpage" and doesn't load the next page. We just  ignore it.
Prevent the `find_elements_by_css_selector` call from blocking  if the selector doesn't match any options as we expect it  to be the case.
The parameter that should be used in the query string for that filter.
This is to allow overriding the default filters for certain types  of fields with some custom filters. The first found in the list  is used in priority.
When time zone support is enabled, convert "now" to the user's time  zone so Django's definition of "Today" matches what the user expects.
checkboxes should not have a label suffix as the checkbox appears  to the left of the label.
Make self.field look a little bit like a field. This means that  {{ field.name }} must be a useful class name to identify the field.  For convenience, store other field-related data here too.
-*- coding: utf-8 -*-
No database changes; removes auto_add and adds default/editable.
-*- coding: utf-8 -*-
Loop through all the fields (one per cell)
Delete checkbox
Backwards compatibility alias for django.templatetags.static.static().  Deprecation should start in Django 2.0.
if the field is the action checkbox: no sorting and special class
Not sortable
We want clicking on this header to bring the ordering to the  front  o_list_remove - omit
If list_display_links not defined, add the link tag to the first field
Wrapper class used to return items in a list_editable  changelist, annotated with the form object for error  reporting purposes. Needed to maintain backwards  compatibility with existing admin templates.
Text to put at the end of each page's <title>.
Text to put in each page's <h1>.
Text to put at the top of the admin index page.
URL for the "View site" link at the top of each admin page.
Instantiate the admin class to save in the registry
Inner import to prevent django.contrib.admin (app) from  importing django.contrib.auth.models.User (unrelated model).
We add csrf_protect here so this function can be used as a utility  function for any view, without having to repeat 'csrf_protect'.
Since this module gets imported in the application's root package,  it cannot import models from other applications at the module level,  and django.contrib.contenttypes.views imports ContentType.
Since the user isn't logged out at this point, the value of  has_permission must be overridden.
Already logged-in, redirect to admin index
Check whether user has any perm for this module.  If so, add the module to the model_list.
Sort the apps alphabetically.
Sort the models alphabetically within each app.
This global object represents the default admin site, for the common case.  You can instantiate AdminSite in your own code to create a custom admin site.
Changelist settings
Remove all the parameters that are globally and systematically  ignored.
This is simply a custom list filter class.
This is a custom FieldListFilter class for a given field.
This is simply a field name, so use the default  FieldListFilter class that has been registered for  the type of the given field.
Check if we need to use distinct()
Get the number of objects, with admin filters applied.
Get the total number of objects, with no admin filters applied.
Admin actions are shown if there is at least one entry  or if entries are not counted because show_full_result_count is disabled
reverse order if order_field has already "-" as prefix
Add the given query's ordering fields, if any.
First, we collect all the declared list filters.
Then, we let every list filter modify the queryset to its liking.
Finally, we apply the remaining lookup parameters from the query  string (i.e. those that haven't already been processed by the  filters).
Allow certain types of errors to be re-raised as-is so that the  caller can treat them in a special way.
Set ordering.
Apply search results
Remove duplicates from results, if necessary
Note that we're calling MultiWidget, not SplitDateTimeWidget, because  we want to define widgets.
The related object is registered with the same AdminSite
The related object is registered with the same AdminSite
Since this module gets imported in the application's root package,  it cannot import models from other applications at the module level.
Merge FORMFIELD_FOR_DBFIELD_DEFAULTS with the formfield_overrides  rather than simply overwriting.
If the field specifies choices, we don't need to look for special  admin widgets - we just need to use a select widget of some kind.
For any other type of field, just call its formfield() method.
If it uses an intermediary model that isn't auto created, don't show  a field in admin.
use the ContentType lookup if view_on_site is True
This is not a relational field, so further parts  must be transforms.
Either a local field filter, or no fields at all.
Always allow referencing the primary key since it's already possible  to get this information from the change view URL.
Allow reverse relationships to models defining m2m fields if they  target the specified field.
Custom templates (designed to be over-ridden in subclasses)
Actions
Take the custom ModelForm's Meta.exclude into account only if the  ModelAdmin doesn't define its own.  if exclude is an empty list we pass None to be consistent with the  default on modelform_factory
If self.actions is explicitly set to None that means that we don't  want *any* actions enabled on this page.
get_action might have returned None, so filter any of those out.
Convert the actions into an OrderedDict keyed by name.
If the action is a callable, just use it.
Next, look for a method. Grab it off self.__class__ to get an unbound  method instead of a bound one; this ensures that the calling  conventions are the same for functions and methods.
Finally, look for a named method on the admin site
Use only the first item in list_display as link
Redirecting after "Save as new".
There can be multiple action forms on the page (at the top  and bottom of the change list, for example). Get the action  whose button was pushed.
Construct the action form.
If the form's valid we can handle the action.
Perform the action only on the selected objects
Actions may return an HttpResponse-like object, which will be  used as the response from the POST. If not, we'll be a good  little HTTP citizen and redirect back to the changelist page.
We have to special-case M2Ms as a list of comma-separated PKs.
Hide the "Save" and "Save and continue" buttons if "Save as New" was  previously chosen to prevent the interface from getting confusing.  Use the change template instead of the add template.
Check actions to see if any are available on this changelist  Add the action checkboxes if there are any actions available.
If the request was POSTed, this might be a bulk action or a bulk  edit. Try to look up an action or confirmation first, but if this  isn't an action the POST will fall through to the bulk edit check,  below.
If we're allowing changelist editing, we need to construct a formset  for the changelist given all the fields to be edited. Then we'll  use the formset to validate/process POSTed data.
Handle GET -- construct a formset for display.
Build the list of media to be used by the formset.
Build the action form and populate it with available actions.
Populate deleted_objects, a data structure of all related objects that  will also be deleted.
Then get the history for this object.
Take the custom ModelForm's Meta.exclude into account only if the  InlineModelAdmin doesn't define its own.  If exclude is an empty list we use None, since that's the actual  default.
Translators: Model verbose name and instance representation,  suitable to be an item in a list.
The model was auto-created as intermediary for a  ManyToMany-relationship, find the target model
Change url doesn't exist -- don't display link to edit
Display a link to the admin page.
Don't display link to edit, because it either has no  admin or is edited inline.
Generic foreign keys OR reverse relations
field is likely a ForeignObjectRel
-*- coding: utf-8 -*-
Stuff can be put in fields that isn't actually a model field if  it's in readonly_fields, readonly_fields will handle the  validation of such things.
If we can't find a field on the model that matches, it could  be an extra field on the form.
Skip ordering in the format field1__field2 (FIXME: checking  this format would be nice, but it's a little fiddly).
getattr(model, item) could be an X_RelatedObjectsDescriptor
item is option 1
Do not perform more specific checks if the base checks result in an  error.
Skip if `fk_name` is invalid.
Exclude methods starting with these strings from documentation
Display an error message for people without docutils
Non-trivial TEMPLATES settings aren't supported (24125).
Non-trivial TEMPLATES settings aren't supported (24125).
Non-trivial TEMPLATES settings aren't supported (24125).
handle named groups first
handle non-named groups
Some backends (e.g. memcache) raise an exception on invalid  cache keys. If this happens, reset the session. See 17810.
This doesn't handle non-default expiry dates, see 19201
BadSignature, ValueError, or unpickling exceptions. If any of  these happen, reset the session.
Avoids a circular import and allows importing SessionStore when  django.contrib.sessions is not in INSTALLED_APPS.
Save immediately to ensure we have a unique entry in the  database.
Key wasn't unique. Try again.
Some backends (e.g. memcache) raise an exception on invalid  cache keys. If this happens, reset the session. See 17810.
Make sure we're not vulnerable to directory traversal. Session keys  should always be md5s, so they should never contain directory  components.
Remove expired sessions.
Get the session data now, before we start messing  with the file it is stored within.
This will atomically rename the file (os.rename) if the OS  supports it. Otherwise this will result in a shutil.copy2  and os.unlink (for example on Windows). See 9084.
When an expired session is loaded, its file is removed, and a  new file is immediately created. Prevent this by disabling  the create() method.
session_key should not be case sensitive because some backends can store it  on case insensitive file systems.
ValueError, SuspiciousOperation, unpickling exceptions. If any of  these happen, just return an empty dictionary (an empty session).
To avoid unnecessary persistent storage accesses, we set up the  internals directly (loading data wastes time, since we are going to  set it to an empty dict anyway).
Make the difference between "expiry=None passed in kwargs" and  "expiry not passed in kwargs", in order to guarantee not to trigger  self.load() when expiry is provided.
Same comment as in get_expiry_age
Remove any custom expiration for this session.
-*- coding: utf-8 -*-
-*- encoding: utf-8 -*-
Mark value safe so i18n does not break with <sup> or <sub> see 19988
Passed value wasn't a date object
Date arguments out of range
Translators: please keep a non-breaking space (U+00A0)  between count and time unit.
Translators: please keep a non-breaking space (U+00A0)  between count and time unit.
Translators: please keep a non-breaking space (U+00A0)  between count and time unit.
Translators: please keep a non-breaking space (U+00A0)  between count and time unit.
Translators: please keep a non-breaking space (U+00A0)  between count and time unit.
Translators: please keep a non-breaking space (U+00A0)  between count and time unit.
For performance, only add a from_db_value() method if the base field  implements it.
Assume we're deserializing
In.process_rhs() expects values to be hashable, so convert lists  to tuples.
Initializing base_field here ensures that its model matches the model for self.
Register hstore straight away as it cannot be done before the  extension is installed, a subsequent data migration would use the  same connection
See the comment for RadioSelect.id_for_label()
Cast everything to strings for ease.
For purposes of seeing whether something has changed, None is  the same as an empty dict, if the data or initial value we get  is None, replace it w/ {}.
We can't simply concatenate messages since they might require  their associated parameters to be expressed correctly which  is not something `string_concat` does. For example, proxied  ungettext calls require a count parameter and are converted  to an empty string if they are missing it.
FileSystemStorage fallbacks to MEDIA_ROOT when location  is empty, so we restore the empty value.
Handle directory paths and fragments
Special casing for a @font-face hack, like url(myfont.eot?iefix")  http://www.fontspring.com/blog/the-new-bulletproof-font-face-syntax
Ignore absolute/protocol-relative, fragments and data-uri URLs.
Ignore absolute URLs that don't point to a static file (dynamic  CSS / JS?). Note that STATIC_URL cannot be empty.
Strip off the fragment so a path-like fragment won't interfere.
Otherwise the condition above would have returned prematurely.
We're using the posixpath module to mix paths and URLs conveniently.
Determine the hashed name of the target file with the storage backend.
Restore the fragment that was stripped off earlier.
Return the hashed version to the file
don't even dare to process the files if we're in dry run mode
where to store the new paths
build a list of adjustable files
then sort the files by the directory level
use the original, local file, not the copied-but-unprocessed  file, which might be somewhere far away, like S3
generate the hash with the original content, even for  adjustable files.
then get the original's file content..
and then set the cache accordingly
Finally store the processed paths
store the hashed name if there was a miss, e.g.  when the files are still processed
Use the default backend
Only append if urlpatterns are empty
Prefix the relative path if the source storage contains it
Delete broken symlinks
When was the target file modified last time?
The storage doesn't support get_modified_time() or failed
When was the source file modified last time?
May be used to differentiate between handler types (e.g. in a  request_finished signal)
Backwards compatibility alias for django.templatetags.static.static().  Deprecation should start in Django 2.0.
Backwards compatibility alias for django.templatetags.static.do_static().  Deprecation should start in Django 2.0.
To keep track on which directories the finder has searched the static files.
only try to find a file if the source dir actually exists
Make sure we have an storage instance here.
No match.
First attempt to look up the site by host with or without port.
Fallback to looking up site after stripping port from the host.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Defined as class-level attributes to be subclassing-friendly.
No need to check for a redirect for non-404 responses.
No redirect was found. Return the response.
Cache shared by all the get_for_* methods to speed up  ContentType retrieval.
This could raise a DoesNotExist; that's correct behavior and will  make sure that only correct ctypes get stored in the cache dict.
Note it's possible for ContentType objects to be stale; model_class() will return None.  Hence, there is no reliance on model._meta.app_label here, just using the model fields instead.
If the object actually defines a domain, we're done.
Otherwise, we need to introspect the object's relationships for a  relation to the Site object
Fall back to the current site (if possible).
Fall back to the current request's site.
If all that malarkey found an object domain, use it. Otherwise, fall back  to whatever get_absolute_url() returned.
There's no FK to exclude, so no exclusion checks are required.
There's one or more GenericForeignKeys; make sure that one of them  uses the right ct_field and ct_fk_field.
Take the custom ModelForm's Meta.exclude into account only if the  GenericInlineModelAdmin doesn't define its own.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Gracefully fallback if a stale content type causes a  conflict as update_contenttypes will take care of asking the  user what should be done next.
Clear the cache as the `get_by_natual_key()` call will cache  the renamed ContentType instance by its old model name.
Determine whether or not the ContentType model is available.
There's no point in going forward if the initial contenttypes  migration is unapplied as the ContentType model will be  unavailable from this point.  The ContentType model is not available yet.
Always clear the global content types cache.
Field flags
This should never happen. I love comments like this, don't you?
Field flags
Add get_RELATED_order() and set_RELATED_order() to the model this  field belongs to, if the model on the other end of this relation  is ordered with respect to its corresponding GenericForeignKey.
We use **kwargs rather than a kwarg argument to enforce the  `manager='manager_name'` syntax.
`QuerySet.delete()` creates its own atomic block which  contains the `pre_delete` and `post_delete` signal handlers.
Force evaluation of `objs` in case it's a queryset whose value  could be affected by `manager.clear()`. Refs 19816.
-*- coding: utf-8 -*-
Handle script prefix manually because we bypass reverse()
To avoid having to always use the "|safe" filter in flatpage templates,  mark the title and content as already safe (since they are raw HTML  content in the first place).
-*- coding: utf-8 -*-
If a prefix was specified, add a filter
If the provided user is not authenticated, or no user  was provided, filter the list to only public flatpages.
Must have at 3-6 bits in the tag
If there's an even number of bits, there's no prefix
The very last bit must be the context name
If there are 5 or 6 bits, there is a user defined
This override makes get_response optional during the  MIDDLEWARE_CLASSES deprecation.
Return the original response if any errors happened. Because this  is a middleware, we can't assume the errors will be caught elsewhere.
Parse the token
Check that the timestamp/uid has not been tampered with
Check the timestamp is within limit
timestamp is number of days since 2001-1-1.  Converted to  base 36, this gives us a 3 digit string until about 2121
Used for mocking in tests
Active superusers have all permissions.
Otherwise we need to check the backends.
Active superusers have all permissions.
This value in the session is always serialized to a string, so we need  to convert it back to Python whenever we access it.
This backend doesn't accept these credentials as arguments. Try the next one.
This backend says to stop in our tracks - this user should not be allowed in at all.
Annotate the user object with the path of the backend.
The credentials supplied are invalid to all backends, fire signal
To avoid reusing another user's session, create a new, empty  session if the existing session corresponds to a different  authenticated user.
remember language choice saved to session
Stores the raw password if set_password() is called so that it can  be passed to password_changed() after the model is saved.
Password hash upgrades shouldn't be considered password changes.
Set a value that will never be a valid hash
Ensure the user-originating redirection URL is safe.
Security check -- don't allow redirection to a different host.
Redirect to this page until the session has been cleared.
urlsafe_base64_decode() decodes to bytestring on Python 3
Updating the password logs out all other sessions for the user  except the current one.
Avoid a major performance hit resolving permission names which  triggers a content_type load:
See 20078: we don't want to allow any lookups involving passwords.
Always return initial because the widget doesn't  render an input field.
Regardless of what the user provides, return the initial value.  This is done here, rather than on the field, because the  field does not have access to the initial value
Email subject *must not* contain newlines
Don't validate passwords that don't match.
If not provided, create the user with an unusable password  Same as user_data but with foreign keys as fake model instances  instead of raw IDs.
Prompt for username/password, and any other required fields.  Enclose this whole thing in a try/except to catch  KeyboardInterrupt and exit gracefully.
Wrap any foreign keys in fake model instances
Don't validate blank passwords.
This will hold the permissions we're looking for as  (content_type, (codename, name))  The codenames and ctypes that should exist.  Force looking up the content types in the current database  before creating foreign keys to them.
Find all the Permissions that have a content_type for a model we're  looking for.  We don't need to check for codenames since we already have  a list of the ones we're going to create.
KeyError will be raised by os.getpwuid() (called by getuser())  if there is no corresponding entry in the /etc/passwd file  (a very restricted chroot environment, for example).
UnicodeDecodeError - preventive treatment for non-latin Windows.
This file is used in apps.py, it should not trigger models import.
If the User model has been swapped out, we can't make any assumptions  about the default user name.
Run the username validator
Don't return the default username if it is already taken.
Run the default password hasher once to reduce the timing  difference between an existing and a non-existing user (20760).
Create a User object if not already in the database?
-*- coding: utf-8 -*-
Ensure the contenttypes migration is applied before sending  post_migrate signals (which create ContentTypes).
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
db connection state is managed similarly to the wsgi handler  as mod_wsgi may call these functions outside of a request/response cycle
If the hasher didn't change (we don't protect against enumeration if it  does) and the password should get updated, try to close the timing gap  between the work factor of the current encoded password and the default  work factor.
The runtime for Argon2 is too complicated to implement a sensible  hardening algorithm.
Argon2 < 1.3
Hash the password prior to using bcrypt to prevent password  truncation as described in 20138.  Use binascii.hexlify() because a hex encoded bytestring is  Unicode on Python 3.
work factor is logarithmic, adding one doubles the load.
First check if the user has the permission (even anon users)  In case the 403 handler should be called raise the exception  As the last resort, show the login form
-*- coding: utf-8 -*-
Checks might be run against a set of app configs that don't  include the specified user model. In this case we simply don't  perform the checks defined below.
Name of request header to grab username from.  This will be the key as  used in the request.META dictionary, i.e. the normalization of headers to  all uppercase and the addition of "HTTP_" prefix apply.
We are seeing this user for the first time in this session, attempt  to authenticate the user.  User is valid.  Set request.user and persist user in the session  by logging the user in.
backend failed to load
To fix 'item in perms.someapp' and __getitem__ interaction we need to  define __iter__. See 18979 for details.
I am large, I contain multitudes.
The name refers to module.
No namespace hint - use manually provided namespace
For include(...) processing.
Hardcode the class name as otherwise it yields 'Settings'.
update this dict from global settings (but only for ALL_CAPS settings)
store the settings module in case someone later cares
SETTINGS_MODULE doesn't make much sense in the manually configured  (standalone) case.
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  Kept ISO formats as they are in first position
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =  NUMBER_GROUPING =
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
'%d. %B %Y', '%d. %b. %Y',   '25. October 2006', '25. Oct. 2006'
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date  DATETIME_FORMAT =  YEAR_MONTH_FORMAT =  SHORT_DATETIME_FORMAT =  FIRST_DAY_OF_WEEK =
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =  NUMBER_GROUPING =
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  Kept ISO formats as they are in first position
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  Kept ISO formats as they are in first position
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =  NUMBER_GROUPING =
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date  DATETIME_FORMAT =  YEAR_MONTH_FORMAT =  SHORT_DATETIME_FORMAT =  FIRST_DAY_OF_WEEK =
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
-*- encoding: utf-8 -*-
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
'%d. %B %Y', '%d. %b. %Y',   '25. October 2006', '25. Oct. 2006'
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date  DATETIME_FORMAT =  YEAR_MONTH_FORMAT =  MONTH_DAY_FORMAT =  SHORT_DATETIME_FORMAT =  FIRST_DAY_OF_WEEK =
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date  DATETIME_FORMAT =  YEAR_MONTH_FORMAT =  SHORT_DATETIME_FORMAT =  FIRST_DAY_OF_WEEK =
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  Kept ISO formats as they are in first position
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date  DATETIME_FORMAT =  YEAR_MONTH_FORMAT =  MONTH_DAY_FORMAT =  SHORT_DATETIME_FORMAT =  FIRST_DAY_OF_WEEK =
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =  NUMBER_GROUPING =
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  Kept ISO formats as they are in first position
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date  DATETIME_FORMAT =  YEAR_MONTH_FORMAT =  SHORT_DATETIME_FORMAT =  FIRST_DAY_OF_WEEK =
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date  DATETIME_FORMAT =  SHORT_DATETIME_FORMAT =  FIRST_DAY_OF_WEEK =
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =  NUMBER_GROUPING =
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
'%d %B %Y', '%d %b %Y',  '25 octobre 2006', '25 oct. 2006'
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =  NUMBER_GROUPING =
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =  NUMBER_GROUPING =
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =  NUMBER_GROUPING =
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  Kept ISO formats as they are in first position
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date  DATETIME_FORMAT =  SHORT_DATETIME_FORMAT =  FIRST_DAY_OF_WEEK =
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =  NUMBER_GROUPING =
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
'%d. %B %Y', '%d. %b. %Y',   '25. October 2006', '25. Oct. 2006'
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
'%d %b %Y', '%d %b %y',                      '20 jan 2009', '20 jan 09'  '%d %B %Y', '%d %B %y',                      '20 januari 2009', '20 januari 2009'
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  Kept ISO formats as they are in first position
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
'%d. %B %Y', '%d. %b. %Y',   '25. October 2006', '25. Oct. 2006'  Kept ISO formats as one is in first position
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =  NUMBER_GROUPING =
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  Kept ISO formats as they are in first position
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date  DATETIME_FORMAT =  YEAR_MONTH_FORMAT =  SHORT_DATETIME_FORMAT =  FIRST_DAY_OF_WEEK =
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =  NUMBER_GROUPING =
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date  DATETIME_FORMAT =  YEAR_MONTH_FORMAT =  SHORT_DATETIME_FORMAT =  FIRST_DAY_OF_WEEK =
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =  NUMBER_GROUPING =
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
'%d %B %Y', '%d %b. %Y',   '25 Ekim 2006', '25 Eki. 2006'
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date  DATETIME_FORMAT =  SHORT_DATETIME_FORMAT =  FIRST_DAY_OF_WEEK =
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =  NUMBER_GROUPING =
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date  DATETIME_FORMAT =  SHORT_DATETIME_FORMAT =  FIRST_DAY_OF_WEEK =
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =  NUMBER_GROUPING =
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =  NUMBER_GROUPING =
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date  DATETIME_FORMAT =  YEAR_MONTH_FORMAT =  SHORT_DATETIME_FORMAT =  FIRST_DAY_OF_WEEK =
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  Kept ISO formats as they are in first position
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  Kept ISO formats as they are in first position
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
'31/12/2009', '31/12/09'
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
'%d. %B %Y', '%d. %b. %Y',   '25. October 2006', '25. Oct. 2006'
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =  NUMBER_GROUPING =
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =  NUMBER_GROUPING =
This is defined here as a do-nothing function because we can't import  django.utils.translation -- that module depends on the settings.
Whether the framework should propagate raw exceptions rather than catching  them. This is useful under some testing situations and should never be used  on a live site.
Whether to use the "Etag" header. This saves bandwidth but slows down performance.
People who get code error notifications.  In the format [('Full Name', 'email@example.com'), ('Full Name', 'anotheremail@example.com')]
List of IP addresses, as strings, that:    * See debug comments, when DEBUG is true    * Receive x-headers
Hosts/domain names that are valid for this site.  "*" matches anything, ".example.com" matches example.com and all subdomains
Local time zone for this installation. All choices can be found here:  https://en.wikipedia.org/wiki/List_of_tz_zones_by_name (although not all  systems may support all possibilities). When USE_TZ is True, this is  interpreted as the default user time zone.
If you set this to True, Django will use timezone-aware datetimes.
Language code for this installation. All choices can be found here:  http://www.i18nguy.com/unicode/language-identifiers.html
Languages using BiDi (right-to-left) layout
If you set this to False, Django will make some optimizations so as not  to load the internationalization machinery.
Settings for language cookie
If you set this to True, Django will format dates, numbers and calendars  according to user current locale.
Not-necessarily-technical managers of the site. They get broken link  notifications and other various emails.
Default content type and charset to use for all HttpResponse objects, if a  MIME type isn't manually specified. These are used to construct the  Content-Type header.
Encoding of files read from disk (template and initial SQL files).
Email address that error messages come from.
Database connection info. If left empty, will default to the dummy backend.
Classes used to implement DB routing behavior.
The email backend to use. For possible shortcuts see django.core.mail.  The default is to use the SMTP backend.  Third-party backends can be specified by providing a Python path  to a module that defines an EmailBackend class.
Host for sending email.
Port for sending email.
Optional SMTP authentication information for EMAIL_HOST.
List of strings representing installed apps.
Default email address to use for various automated correspondence from  the site managers.
Subject-line prefix for email messages send with django.core.mail.mail_admins  or ...mail_managers.  Make sure to include the trailing space.
Whether to append trailing slashes to URLs.
Whether to prepend the "www." subdomain to URLs that don't have it.
Override the server-derived value of SCRIPT_NAME
A secret key for this particular Django installation. Used in secret-key  hashing algorithms. Set this in your settings, or Django will complain  loudly.
Default file storage mechanism that holds media.
Absolute filesystem path to the directory that will hold user-uploaded files.  Example: "/var/www/example.com/media/"
URL that handles the media served from MEDIA_ROOT.  Examples: "http://example.com/media/", "http://media.example.com/"
Absolute path to the directory static files should be collected to.  Example: "/var/www/example.com/static/"
URL that handles the static files served from STATIC_ROOT.  Example: "http://example.com/static/", "http://static.example.com/"
List of upload handler classes to be applied in order.
Maximum number of GET/POST parameters that will be read before a  SuspiciousOperation (TooManyFieldsSent) is raised.
Directory in which upload streamed files will be temporarily saved. A value of  `None` will make Django use the operating system's default temporary directory  (i.e. "/tmp" on *nix systems).
The numeric mode to set newly-uploaded files to. The value should be a mode  you'd pass directly to os.chmod; see http://docs.python.org/lib/os-file-dir.html.
The numeric mode to assign to newly-created directories, when uploading files.  The value should be a mode as you'd pass to os.chmod;  see http://docs.python.org/lib/os-file-dir.html.
Default formatting for date objects. See all available format strings here:  http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
Default formatting for datetime objects. See all available format strings here:  http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
Default formatting for time objects. See all available format strings here:  http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
Default formatting for date objects when only the year and month are relevant.  See all available format strings here:  http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
Default formatting for date objects when only the month and day are relevant.  See all available format strings here:  http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
Default short formatting for date objects. See all available format strings here:  http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
Default short formatting for datetime objects.  See all available format strings here:  http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
Default formats to be used when parsing dates from input boxes, in order  See all available format string here:  http://docs.python.org/library/datetime.htmlstrftime-behavior  * Note that these format strings are different from the ones to display dates
Default formats to be used when parsing times from input boxes, in order  See all available format string here:  http://docs.python.org/library/datetime.htmlstrftime-behavior  * Note that these format strings are different from the ones to display dates
Default formats to be used when parsing dates and times from input boxes,  in order  See all available format string here:  http://docs.python.org/library/datetime.htmlstrftime-behavior  * Note that these format strings are different from the ones to display dates
First day of week, to be used on calendars  0 means Sunday, 1 means Monday...
Decimal separator symbol
Boolean that sets whether to add thousand separator when formatting numbers
Number of digits that will be together, when splitting them by  THOUSAND_SEPARATOR. 0 means no grouping, 3 means splitting by thousands...
Thousand separator symbol
The tablespaces to use for each model when not specified otherwise.
Default X-Frame-Options header value
List of middleware to use. Order is important; in the request phase, these  middleware will be applied in the order given, and in the response  phase the middleware will be applied in reverse order.
The cache backends to use.
The number of days a password reset link is valid for
Dotted path to callable to be used as view when a request is  rejected by the CSRF middleware.
Settings for CSRF cookie.
Class to use as messages backend
The callable to use to configure logging
Custom logging configuration.
Default exception reporter filter class used in case none has been  specifically assigned to the HttpRequest instance.
The name of the class to use to run the test suite
Apps that don't need to be serialized at test database creation time  (only apps with migrations are to start with)
The list of directories to search for fixtures
A list of locations of additional static files
The default file storage backend used during the build process
List of finder classes that know how to find static files in  various locations.  'django.contrib.staticfiles.finders.DefaultStorageFinder',
Migration module overrides for apps, by app label.
SECURITY MIDDLEWARE
If the default template doesn't exist, use the string template.
Raise if a developer-specified template doesn't exist.
Reraise if it's a missing custom template.
Reraise if it's a missing custom template.
Reraise if it's a missing custom template.
No exception content is passed to the template, to not disclose any sensitive information.
Reraise if it's a missing custom template.
We need this to behave just like the CsrfViewMiddleware, but not reject  requests or log warnings.
Forces process_response to send the cookie
We could just do view_func.csrf_exempt = True, but decorators  are nicer if they don't have side-effects, so we return a new  function.
Compute values (if any) for the requested resource.
Shortcut decorators for common cases based on ETag or Last-Modified only
If template_name isn't specified, it's not a problem --  we just start with an empty list.
Forcing possible reverse_lazy evaluation
If a model has been explicitly provided, use it
If this view is operating on a single object, use  the class of that object
Try to get a queryset and extract the model class  from that
PUT is a valid HTTP verb for creating (with a known URL) or editing an  object, note that browsers only support POST for now.
Add support for browsers which only accept GET and POST for now.
Skip self._make_date_lookup_arg, it's a no-op in this branch.
We need this to be a queryset since parent classes introspect it  to find information about the model.
Use a custom queryset if provided
Filter down a queryset from self.queryset using the date from the  URL. This'll get passed as the queryset to DetailView.get_object,  which'll handle the 404
Bounds of the current interval
If allow_empty is True, the naive result will be valid
Filter out objects in the future if appropriate.  Fortunately, to match the implementation of allow_future,  we need __lte, which doesn't conflict with __lt above.
Snag the first object from the queryset; if it doesn't exist that  means there's no next/previous link available.
Convert datetimes to dates in the current time zone.
Return the first day of the period.
Go through keyword arguments, and either save their values to our  instance, or raise an error.
take name and docstring from class
and possible attributes set by decorators  like csrf_exempt from dispatch
Use a custom queryset if provided; this is required for subclasses  like DateDetailView
Next, try looking up by primary key.
Next, try looking up by slug.
If none of those are defined, it's an error.
Get the single item from the filtered queryset
If template_name isn't specified, it's not a problem --  we just start with an empty list.
If self.template_name_field is set, grab the value of the field  of that name from the object; this is the most specific template  name, if given.
If we still haven't managed to find any template names, we should  re-raise the ImproperlyConfigured to alert the user.
Translations catalog  Language formats for date, time, etc.
paths of requested packages
Translations catalog  Language formats for date, time, etc.
Strip empty path components.
Strip '.' and '..' in path.
Minimal Django templates engine to render the error templates  regardless of the project's TEMPLATES setting.
If the key isn't regex-able, just return as-is.
For fixing 21345 and 23070
Instantiate the default filter for the first time and cache it.
Cleanse all parameters.
Cleanse only the specified parameters.
If value is lazy or a complex object of another kind, this check  might raise an exception. isinstance checks that lazy  MultiValueDicts will have a return value.
Cleanse MultiValueDicts (request.POST is the one we usually care about)
Cleanse all variables
Cleanse specified variables
Potentially cleanse the request and any MultiValueDicts if they  are one of the frame variables.
For good measure, obfuscate the decorated function's arguments in  the sensitive_variables decorator's frame, in case the variables  associated with those arguments were meant to be obfuscated from  the decorated function's frame.
The force_escape filter assume unicode, make sure that works
Get the exception and all its causes
No exceptions were supplied to ExceptionReporter
In case there's just one exception (always in Python 2,  sometimes in Python 3), take the traceback from self.tb (Python 2  doesn't have a __traceback__ attribute on Exception)
If the traceback for current exception is consumed, try the  other exception.
installed_apps is set to None when creating the master registry  because it cannot be populated at that point. Other registries must  provide a list of installed apps and are populated immediately.
Mapping of labels to AppConfig instances for installed apps.
Stack of app_configs. Used to store the current state in  set_available_apps and set_installed_apps.
Whether the registry is populated.
Lock for thread-safe population.
Maps ("app_label", "modelname") tuples to lists of functions to be  called when the corresponding model is ready. Used by this class's  `lazy_model_operation()` and `do_pending_operations()` methods.
Populate apps and models, unless it's the master registry.
populate() might be called by two threads in parallel on servers  that create threads before initializing the WSGI callable.
app_config should be pristine, otherwise the code below won't  guarantee that the order matches the order in INSTALLED_APPS.
Load models.
Is this model swapped out for the model given by to_string?  Is this model swappable and the one given by to_string?
This will be executed after the class corresponding to next_model  has been imported and registered. The `func` attribute provides  duck-type compatibility with partials.
Full Python path to the application eg. 'django.contrib.admin'.
Root module for the application eg. <module 'django.contrib.admin'  from 'django/contrib/admin/__init__.pyc'>.
Last component of the Python path to the application eg. 'admin'.  This value must be unique across a Django project.
Human-readable name for the application eg. "Admin".
Filesystem path to the application directory eg.  u'/usr/lib/python2.7/dist-packages/django/contrib/admin'. Unicode on  Python 2 and a str on Python 3.
Module containing models eg. <module 'django.contrib.admin.models'  from 'django/contrib/admin/models.pyc'>. Set by import_models().  None if the application doesn't have a models module.
Mapping of lower case model names to model classes. Initially set to  None to prevent accidental access before import_models() runs.
If import_module succeeds, entry is a path to an app module,  which may specify an app config class with default_app_config.  Otherwise, entry is a path to an app config class or an error.
Track that importing as an app module failed. If importing as an  app config class fails too, we'll trigger the ImportError again.
Raise the original exception when entry cannot be a path to an  app config class.
If this works, the app module specifies an app config class.
Otherwise, it simply uses the default app config class.
If we're reaching this point, we must attempt to load the app config  class located at <mod_path>.<cls_name>  If importing as an app module failed, that error probably  contains the most informative traceback. Trigger it again.
Check for obvious errors. (This check prevents duck typing, but  it could be removed if it became a problem in practice.)
Obtain app name here rather than in AppClass.__init__ to keep  all error checking for entries in INSTALLED_APPS in one place.
Ensure app_name points to a valid module.
Entry is a path to an app config class.
Dictionary of models for this app, primarily maintained in the  'all_models' attribute of the Apps this AppConfig is attached to.  Injected as a parameter because it gets populated when models are  imported, which might happen before populate() imports models.
Assume both arguments are sanitized -- that is, strings of  length CSRF_TOKEN_LENGTH, all CSRF_ALLOWED_CHARS.
The _accept and _reject methods currently only exist for the sake of the  requires_csrf_token decorator.  Avoid checking the request twice by adding a custom attribute to  request.  This will be relevant when both decorator and middleware  are used.
Cookie token needed to be replaced;  the cookie needs to be reset.  Use same token next time.
Wait until request.META["CSRF_COOKIE"] has been manipulated before  bailing out, so that get_token still works
Make sure we have a valid URL for Referer.
Ensure that our Referer is also secure.
Here we generate a list of all acceptable HTTP referers,  including the current host since that has been validated  upstream.
No CSRF cookie. For POST requests, we insist on a CSRF cookie,  and in this way we can avoid all CSRF attacks, including login  CSRF.
Fall back to X-CSRFToken, to make things easier for AJAX,  and possible for PUT/DELETE.
Allow sys.exit() to actually exit. See tickets 1023 and 4701
Get the exception info now, in case another exception is thrown later.
We don't need to update the cache, just return.
Don't cache responses that set a user-specific (and maybe security  sensitive) cookie in response to a cookie-less request.
try and get the cached GET response
hit, return cached response
Don't set it if it's already in the response
Don't set it if they used @xframe_options_exempt
This override makes get_response optional during the  MIDDLEWARE_CLASSES deprecation.
Insert language after the script prefix and before the  rest of the URL
Check for denied User-Agents
Check if a slash should be appended
Return a redirect if necessary
If the given URL is "Not Found", then check if we should redirect to  a path with a slash appended.
Different subdomains are treated as different domains.
The referer is empty.
APPEND_SLASH is enabled and the referer is equal to the current URL  without a trailing slash indicating an internal redirect.
A '?' in referer is identified as a search engine source.
The referer is equal to the current URL, ignoring the scheme (assumed  to be a poorly implemented bot).
It's not worth attempting to compress really short responses.
Avoid gzipping if we've already got a content-encoding.
Delete the `Content-Length` header for streaming content, because  we won't know the compressed size until we stream it.
-*- coding: utf-8 -*-
A utf-8 verbose name (Ångström's Articles) to test they are valid.
Test models with non-default primary keys / AutoFields 5218
Intentionally broken (invalid start byte in byte string).
Chained foreign keys with to_field produce incorrect query 18432
this line will raise an AttributeError without the accompanying fix
NOTE: Part of the regression test here is merely parsing the model  declaration. The verbose_name, in particular, did not always work.  An empty choice field should return None for the display name.
Empty strings should be returned as Unicode
Models with broken unicode methods should still have a printable repr
this is the actual test for 18432
Following queryset should return the most recent revision:
Note: the extra ordering must appear in select clause, so we get two  non-distinct results here (this is on purpose, see 7070).
Define a temporary environment for the subprocess
Backup original state
Original state hasn't changed
Variable was correctly set
Restore original state
Only 4 characters, all of which could be in an ipv6 address
Uses only characters that could be in an ipv6 address
Check a warning is emitted
It's possible one outputs on stderr and the other on stdout, hence the set
If the Exception is not CommandError it should always  raise the original exception.
If the Exception is CommandError and --traceback is present  this command should raise the original CommandError as if it  were not a CommandError.
Test connections have been closed
running again..
running again..
-*- coding: utf-8 -*-
Regression for 13368. This is an example of a model  that imports a class that has an abstract base class.
-*- coding: utf-8 -*-
Accessing "name" doesn't trigger a new database query. Accessing  "value" or "text" should.
Test for 12163 - Pickling error saving session with unsaved model  instances.
Regression for 15790 - only() broken for proxy models
Test for 17485.
use a non-default name for the default manager
For testing when upper case letter in app name; regression for 4057
Models to regression test 7572, 20820
Subclass of a model with a ManyToManyField for test_ticket_20820
Models to regression test 22421
Models to regression test 11428
Check for forward references in FKs and M2Ms with natural keys
Person doesn't actually have a dependency on store, but we need to define  one to test the behavior of the dependency resolution algorithm.
ome models with pathological circular dependencies
Model for regression test of 11101
Fake the dependency for a circularity
-*- coding: utf-8 -*-  Unittests for fixtures.
Load a fixture that uses PK=1
Just for good measure, run the same query again.  Under the influence of ticket 7572, this will  give a different result to the previous call.
Output order isn't guaranteed, so check for parts
Get rid of artifacts like '000000002' to eliminate the differences  between different Python versions.
Order between M2MComplexA and M2MComplexB doesn't matter. The through  model has dependencies to them though, so it should come last.
The dependency sorting should not result in an error, and the  through model should have dependencies to the other models and as  such come last in the list.
relative_prefix is something like tests/fixtures_regress or  fixtures_regress depending on how runtests.py is invoked.  All path separators must be / in order to be a proper regression test on  Windows, so replace as appropriate.
Since this package contains a "jinja2" directory, this is required to  silence an ImportWarning warning on Python 2.
Check that context processors run
Check that context overrides context processors
Incorrect: APP_DIRS and loaders are mutually incompatible.
Customized refresh_from_db() reloads all deferred fields on  access of any of them.
only hooks registered during successful savepoints execute
With no content and allow_empty_first_page=True, 1 is a valid page number
Prepare a list of objects for pagination.
adding a default part to our car - no signal listener installed
Install a listener on the two m2m relations.
Membership object defined as a class
A set of models that use an non-abstract inherited model as the 'through' model.
Using to_field on the through model
This field name is intentionally 2 characters long (16080).
Make sure the correct queryset is returned
Make sure the correct queryset is returned
Null/not null queries
Make sure the correct queryset is returned
Regression for 17830
Make sure the correct queryset is returned
Make sure that correct filters are returned with custom querysets
Make sure the correct queryset is returned
Make sure the correct queryset is returned
FK relationship
Make sure the correct queryset is returned
M2M relationship
Make sure the correct queryset is returned
Make sure the correct queryset is returned
Make sure the correct queryset is returned
Make sure the correct queryset is returned
Make sure the correct queryset is returned
Make sure that the first option is 'All'
Make sure the correct queryset is returned
Look for books in the 1980s
Make sure the correct queryset is returned
Look for books in the 1990s
Make sure the correct queryset is returned
Look for books in the 2000s
Make sure the correct queryset is returned
Make sure the correct queryset is returned
Make sure the correct queryset is returned
Make sure the correct queryset is returned
Make sure the correct queryset is returned
Make sure the correct queryset is returned
Make sure the correct queryset is returned
Add the URL of our custom 'add_view' view to the front of the URLs  list.  Remove the existing one(s) first
Anne is friends with Bill and Chuck
David is friends with Anne and Chuck - add in reverse direction
Bill is already friends with Anne - add Anne again, but in the  reverse direction
Remove Anne from Bill's friends
Clear Anne's group of friends
Who is friends with Anne?
Reverse relationships should also be gone  Who is friends with Chuck?
Who is friends with David?
David is idolized by Anne and Chuck - add in reverse direction
Ann idolizes David
David is idolized by Anne
Ann idolizes herself
The "full_name" property hasn't provided a "set" method.
And cannot be used to initialize the class.
But "full_name_2" has, and it can be used to initialize the class.
default serializable on Python 3 only
To test pickling we need a class that isn't defined on module, but  is still available from app-cache. So, the Container class moves  SomeModel outside of module level
Ticket 17776
Exceptions are not equal to equivalent instances of themselves, so  can't just use assertEqual(original, unpickled)
With related field (14515)
First pickling
Second pickling
Happening.when has a callable default of datetime.datetime.now.
Translators: This comment should be extracted
This comment should not be extracted
This file has a literal with plural forms. When processed first, makemessages  shouldn't create a .po file with duplicate `Plural-Forms` headers
This string is intentionally duplicated in test.html
-*- encoding: utf-8 -*-
Populate _format_cache with temporary values
Now with a long
Inexisting context...
here we rely on .split() being called inside the _fetch()  in trans_real.translation()
make sure sideeffect_str actually added a new translation
This unusual grouping/force_grouping combination may be triggered by the intcomma filter (17414)
Even a second time (after the format has been cached)...
Even a second time (after the format has been cached)...
Checking for the localized "products_delivered" field
Non-strings are untouched
Suspicion that user entered dot as decimal separator (22171)
Should return the correct default module when no setting is set
When the setting is a string, should return the given module and  the default module
When setting is a list of strings, should return the given  modules and the default module
This test assumes there won't be a Django translation to a US  variation of the Spanish language, a safe assumption. When the  user sets it as the preferred language, the main 'es'  translation should be selected instead.
This test assumes there won't be a Django translation to a US  variation of the Spanish language, a safe assumption. When the  user sets it as the preferred language, the main 'es'  translation should be selected instead.
Original translation.
Different translation.  Force refreshing translations.
Doesn't work because it's added later in the list.
Force refreshing translations.
Unless the original is removed from the list.
A language with no translation catalogs should fallback to the  untranslated string.
Regression test for 21473
Specifying encoding is not supported (Django enforces UTF-8)
Make sure the cache is empty before we are doing our tests.
Make sure we will leave an empty cache for other testcases.
Namespaced URL
language from outside of the supported LANGUAGES list
We only want one redirect, bypassing CommonMiddleware
-*- encoding: utf-8 -*-
keep the diff friendly - remove 'POT-Creation-Date'
Sample project used by test_extraction.CustomLayoutExtractionTests
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
`call_command` bypasses the parser; by calling  `execute_from_command_line` with the help subcommand we  ensure that there are no issues with the parser itself.
-*- encoding: utf-8 -*-
: .\path\to\file.html:123
: path/to/file.html:123
Check that the temporary file was cleaned up
{% trans %}
{% blocktrans %}
16903 -- Standard comment with source file relative path should be present
`call_command` bypasses the parser; by calling  `execute_from_command_line` with the help subcommand we  ensure that there are no issues with the parser itself.
A user-defined format
Translators: This comment should be extracted
This comment should not be extracted
Use iterator() with datetimes() to return a generator that lazily  requests each result one at a time, to save memory.
-*- coding: utf-8 -*-
Non-required fields aren't required
If you want cleaning an empty value to return a different type, tell the field
-*- coding: utf-8 -*-
hangs "forever" if catastrophic backtracking in ticket:11198 not fixed
a second test, to make sure the problem is really addressed, even on  domains that don't fail the domain label length check in the regex
Valid IDN
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
As with any widget that implements get_value_from_datadict(), we must  accept the input from the "as_hidden" rendering as well.
With Field.show_hidden_initial=True
Test null bytes (18982)
assertIsInstance or assertRaises cannot be used because UnicodeEncodeError  is a subclass of ValueError
Return an empty dictionary if max_length and min_length are both None.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
This can also cause weirdness: be careful (bool(-1) == True, remember)
-*- coding: utf-8 -*-
No file was uploaded and no initial data.
A file was uploaded and no initial data.
A file was not uploaded, but there is initial data
A file was uploaded and there is initial data (file identity is not dealt  with here)
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Check for runaway regex security problem. This will take a long time  if the security fix isn't in place.
Make sure we're compatible with MySQL, which uses 0 and 1 for its  boolean values (9609).
-*- coding: utf-8 -*-
Rendering the default state with empty_label setted as string.
Even with an invalid date, the widget should reflect the entered value (17401).
w2 ought to be independent of w1, since MultiWidget ought  to make a copy of its sub-widgets when it is copied.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Check that the empty value is a QuerySet  While we're at it, test whether a QuerySet is returned if there *is* a value.
Boundary conditions on a PositiveIntegerField
Formfield initial values   If the model has default values for some fields, they are used as the formfield  initial values.
Test that saving a form with a blank choice results in the expected  value being stored in the database.
nl/formats.py has customized TIME_INPUT_FORMATS:  ['%H:%M:%S', '%H.%M:%S', '%H.%M', '%H:%M']
Parse a time in an unaccepted format; get an error
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip
Parse a time in a valid, but non-default format, get a parsed result
Check that the parsed result does a round trip to default format
ISO formats are accepted, even if not specified in formats.py
Parse a time in an unaccepted format; get an error
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a time in an unaccepted format; get an error
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip
Parse a time in a valid, but non-default format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a time in an unaccepted format; get an error
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a time in an unaccepted format; get an error
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a time in a valid, but non-default format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a time in an unaccepted format; get an error
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a time in an unaccepted format; get an error
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a time in an unaccepted format; get an error
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
ISO formats are accepted, even if not specified in formats.py
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip
Parse a date in a valid, but non-default format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip
Parse a date in a valid, but non-default format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a date in a valid, but non-default format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
ISO formats are accepted, even if not specified in formats.py
Check that the parsed result does a round trip
Parse a date in a valid, but non-default format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
Check that the parsed result does a round trip to the same format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Check that the parsed result does a round trip to the same format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Check that the parsed result does a round trip to the same format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip
Parse a date in a valid, but non-default format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Check that the parsed result does a round trip to the same format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Check that the parsed result does a round trip to the same format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a date in a valid, but non-default format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
Check that the parsed result does a round trip to the same format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
Check that the parsed result does a round trip to the same format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
-*- coding: utf-8 -*-
FormSet allows us to use multiple instance of the same form on 1 page. For now,  the best way to create a FormSet is by using the formset_factory function.
Let's define a FormSet that takes a list of favorite drinks, but raises an  error if there are any duplicates. Used in ``test_clean_hook``,  ``test_regression_6926`` & ``test_regression_12878``.
Used in ``test_formset_splitdatetimefield``.
If a FormSet was not passed any data, its is_valid and has_changed  methods should return False.
FormSet instances has_changed method will be True if any data is  passed to his forms, even if the formset didn't validate
invalid formset test
valid formset test
We can also display more than 1 empty form passing min_num argument  to formset_factory. It will (essentially) increment the extra argument
Min_num forms are required; extra forms can be empty.
We can also display more than 1 empty form passing min_num argument
If we remove the deletion flag now we will have our validation back.
If max_num is 0 then no form is rendered at all.
One form from initial and extra=3 with max_num=2 should result in the one  initial form and one extra.
Regression tests for 16455 -- formset instances are iterable
confirm iterated formset yields formset.forms
confirm indexing of formset
Formsets can override the default iteration order
Regression tests for 16479 -- formsets form use ErrorList instead of supplied error_class
But we still only instantiate 3 forms  and the formset isn't valid
Four forms are instantiated and no exception is raised
Regression test for 11160  If non_form_errors() is called without calling is_valid() first,  it should ensure that full_clean() is called.
Regression test for 11418
Empty forms should be unbound
The empty forms should be equal.
-*- coding: utf-8 -*-
Tests to prevent against recurrences of earlier bugs.
Unicode decoding problems...
Deep copying translated text shouldn't raise an error)
There once was a problem with Form fields called "data". Let's make sure that  doesn't come back.
A form with *only* hidden fields that has errors is going to be very unusual.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Should be "\nTst\n" after 19251 is fixed
-*- coding: utf-8 -*-
Pass a dictionary to a Form's __init__().
For DateFields, it's set to None.
Any Field can have a Widget class passed to its constructor:
The 'widget' parameter to a Field can also be an instance:
For a form with a <select>, use ChoiceField:
You can set a ChoiceField's choices after the fact.
Add widget=RadioSelect to use that widget with a ChoiceField.
You can iterate over any BoundField, not just those with widget=RadioSelect.
MultipleChoiceField is a special case, as its data is required to be a list:
DateTimeField rendered as_hidden() is special too
Validation errors are HTML-escaped when output as HTML.
Ensure that the newly added list of errors is an instance of ErrorList.
Trigger validation.
Check that update_error_dict didn't lose track of the ErrorDict type.
It's possible to construct a Form dynamically by adding to the self.fields  dictionary in __init__(). Don't forget to call Form.__init__() within the  subclass' __init__().
Instances of a dynamic Form do not persist fields from one Form instance to  the next.
Similarly, changes to field attributes do not persist from one Form instance  to the next.
A corner case: It's possible for a form to have only HiddenInputs.
Some Field classes have an effect on the HTML attributes of their associated  Widget. If you set max_length in a CharField and its associated widget is  either a TextInput or PasswordInput, then the widget's rendered HTML will  include the "maxlength" attribute.
If you specify a custom "attrs" that includes the "maxlength" attribute,  the Field's max_length attribute will override whatever "maxlength" you specify  in "attrs".
You can specify the label for a field by using the 'label' argument to a Field  class. If you don't specify 'label', Django will use the field name with  underscores converted to spaces, and the initial letter capitalized.
If a label is set to the empty string for a field, that field won't get a label.
If label is None, Django will auto-create the label from the field name. This  is default behavior.
If a Form defines 'initial' *and* 'initial' is passed as a parameter to Form(),  then the latter will get precedence.
We need to define functions that get called later.)
Test that field raising ValidationError is always in changed_data
BoundField is also cached
Nix microseconds (since they should be ignored). 22502
You can specify descriptive text for a field by using the 'help_text' argument)
You can subclass a Form to add fields. The resulting form subclass will have  all of the fields of the parent Form, plus whichever fields you define in the  subclass.
Yes, you can subclass multiple forms. The fields are added in the order in  which the parent classes are listed.
By default, forms append a hyphen between the prefix and the field name, but a  form can alter that behavior by implementing the add_prefix() method. This  method takes a field name and returns the prefixed field, according to  self.prefix.
Prefix can be also specified at the class level.
NullBooleanField is a bit of a special case because its presentation (widget)  is different than its data. This is handled transparently, though.
FileFields are a special case because they take their data from the request.FILES,  not request.POST.
Sometimes (pretty much in formsets) we want to allow a form to pass validation  if it is completely empty. We can accomplish this by using the empty_permitted  argument to a form constructor.
Now let's show what happens when empty_permitted=True and the form is empty.
If a field is not given in the data then None is returned for its data. Lets  make sure that when checking for empty_permitted that None is treated  accordingly.
However, we *really* need to be sure we are checking for None as any data in  initial that returns False on a boolean call needs to be treated literally.
Fake json.loads
without anything: just print the <label>
passing just one argument: overrides the field's label
Passing attrs to add extra attributes on the <label>
Return a different dict. We have not changed self.cleaned_data.
-*- coding: utf-8 -*-
Can take a non-string.
-*- coding: utf-8 -*-
A widget can exist without a media definition
Regression check for 12879: specifying the same CSS or JS file  multiple times in a single Media instance should result in that file  only being included once.
If a widget extends another but defines media, it extends the parent widget's media by default
MultiWidgets have a default media definition that gets all the  media from the component widgets
Forms can also define media, following the same rules as widgets.
Custom managers
Custom manager
No custom manager on this class to make sure the default case doesn't break.
Managers from base classes are inherited and, if no manager is specified  *and* the parent has a manager specified, the first one (in the MRO) will  become the default.
Should be the default manager, although the parent managers are  inherited.
RelatedManagers
Accessing the manager on an abstract model with an custom  manager should raise an attribute error with an appropriate  message.
Accessing the manager on an abstract model with an explicit  manager should raise an attribute error with an appropriate  message.
Make sure related managers core filters don't include an  explicit `__exact` lookup that could be interpreted as a  reference to a foreign `exact` field. refs 23940.
Shouldn't issue any warnings, since GeoManager itself will be  deprecated at the same time as use_for_related_fields, there  is no point annoying users with this deprecation.
With the new base_manager_name API there shouldn't be any warnings.
With the new base_manager_name API there shouldn't be any warnings.
With the new base_manager_name API there shouldn't be any warnings.
Shouldn't complain since the inherited manager  is basically the same that would have been created.
Should create 'objects' (set as default) and warn that  it will no longer be the case in the future.
When there is an inherited manager we shouldn't get any warning  and 'objects' shouldn't be created.
With `manager_inheritance_from_future = True` 'objects'  shouldn't be created.
This will return a different result when it's run within or outside  of a git clone: 1.4.devYYYYMMDDHHMMSS or 1.4.
The normal case  Same thing, via an update
Won't work because force_update and force_insert are mutually  exclusive
Try to update something that doesn't have a primary key in the first  place.
Won't work because we can't insert a pk of the same value.
Trying to update should still fail, even with manual primary keys, if  the data isn't in the database already.
Add an author to the book.  The book should have one author.
Try get_or_create again, this time nothing should be created.  And the book should still have one author.
Add a second author to the book.
The book should have two authors now.
Create an Author not tied to any books.
There should be three Authors in total. The book object should have two.
Try creating a book through an author.
Now Ed has two Books, Fred just one.
Use the publisher's primary key value instead of a model instance.
Try get_or_create again, this time nothing should be created.
The publisher should have three books.
If we execute the exact same statement, it won't create a Person.
This dollar is ok as it is escaped
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
test register as function
The resolver is checked recursively (examining url()s in include()).
-*- coding: utf-8 -*-
Selected author ages are 57 and 46
The value doesn't matter, we just need any negative  constraint on a related model that's a noop.
Try to generate query tree
Check internal state
If the backend needs to force an ordering we make sure it's  the only "ORDER BY" clause present in the query.
Create a plain expression
Instantiate the Form and FormSet to prove  you can create a form with no data
Instantiate the Form and FormSet to prove  you can create a form with no data
Testing the inline model's relation
Testing the inline model's relation
Instantiate the Form and FormSet to prove  you can create a formset with an instance of None
To save a formset as new, it needs a new hub instance
reload database
reload database
reload database
verify no "odd" PKs left
Even if the "us" object isn't in the DB any more, the form  validates.
max_age parameter can also accept a datetime.timedelta object
-*- coding: utf-8 -*-
Should not set mode to None if it is not present.  See 14681, stdlib gzip module crashes if mode is set to None
Set chunk size to create a boundary after \n:  b'one\n...         ^
Set chunk size to create a boundary between \r and \n:  b'one\r\n...         ^
Set chunk size to create a boundary after \r:  b'one\r...         ^
file_move_safe should raise an IOError exception if destination file exists and allow_overwrite is False
should allow it and continue on if allow_overwrite is True
Use HTTP responses different from the defaults
Models for 19425
Models for 18263
The heading for the m2m inline block uses the right text  The "add another" label is correct  The '+' is dropped from the autogenerated form prefix (Author_books+)
Here colspan is "4": two fields (title1 and title2), one hidden field and the delete checkbox.
Add parent object view should have the child inlines section  The right callable should be used for the inline readonly_fields  column cells
Get the ID of the automatically created intermediate model for the Author-Book m2m
No change permission on books, so no inline
No permissions on Inner2s, so no inline
No change permission on books, so no inline
No permissions on Inner2s, so no inline
No change permission on Books, so no inline
Add an inline
Check that the objects have been created in the database
Add a few inlines
admin for 18433
admin for 19425 and 18388
admin for 19524
admin and form for 18263
Test bug 12561 and 12778  only ModelAdmin media  ModelAdmin and Inline media  only Inline media
Security tests
Reversing None should raise an error, not return the last un-named view.
Regression for 20022, adjusted for 24013 because ~ is an unreserved  character. Tests whether % is escaped.
Pick a resolver from a namespaced URLconf
We don't really need a model; just something with a get_absolute_url
modules that are not listed in urlpatterns should not be importable
Views added to urlpatterns using include() should be reversible.
Test legacy support for extracting "function, args, kwargs"
... and for legacy purposes:
View is not a callable (explicit import; arbitrary Python object)
Regex contains an error (refs 6170)
passing a callable should return the callable
no app_name -> deprecated
app_name argument to include -> deprecated
3-tuple -> deprecated
This is non-reversible, but we shouldn't blow up when parsing it.
Security tests
A URLs file that doesn't use the default  from django.conf.urls import *  import pattern.
I just raise an AttributeError to confuse the view loading mechanism
-*- encoding: utf-8 -*-
Bug 16494: HttpResponse should behave consistently with non-strings
test content via property
test iter content via property
test odd inputs  '\xde\x9e' == unichr(1950).encode('utf-8')
304 responses should not have content/content-type
iterating over the response itself yields bytestring chunks.
and the response can only be iterated once.
streaming responses don't have a `content` attribute.
and you can't accidentally assign to a `content` attribute.
but they do have a `streaming_content` attribute.
coercing a streaming response to bytes doesn't return a complete HTTP  message like a regular response does. it only gives us the headers.
and this won't consume its content.
additional content cannot be written to the response.
and we can't tell the current position.
Disable the request_finished signal during this test  to avoid interfering with the database connection.
file isn't closed until we close the response.
when multiple file are assigned as content, make sure they are all  closed with the response.
file isn't closed until we close the response.
If ticket 1578 ever slips back in, these models will not be able to be  created (the field names being lower-cased versions of their opposite  classes is important here).
Protect against repetition of 1839, 2415 and 2536.
Multiple paths to the same model (7110, 7125)
Test related objects visibility.
Check that implied __exact also works.
You can specify filters containing the explicit FK value.
Regression for 12876 -- Model methods that include queries that  recursive don't cause recursion depth problems under deepcopy.
Same twice
Same as each other
Look up the object again so that we get a "fresh" object.
Accessing the related object again returns the exactly same object.
But if we kill the cache, we get a new object.
Assigning a new object results in that object getting cached immediately.
Assigning None succeeds if field is null=True.
bestchild should still be None after saving.
bestchild should still be None after fetching the object again.
Assigning None will not fail: Child.parent is null=False.
You also can't assign an object of the wrong type here
You can assign None to Child.parent during object creation.
But when trying to save a Child with parent=None, the database will  raise IntegrityError.
Creation using keyword argument should cache the related object.
Regression for 12190 -- Should be able to instantiate a FK outside  of a model, and interrogate its related field.
Only one school is available via all() due to the custom default manager.
Make sure the base manager is used so that an student can still access  its related school even if the default manager doesn't normally  allow it.
The exception raised on attribute access when a related object  doesn't exist should be an instance of a subclass of `AttributeError`  refs 21563
The first time autodiscover is called, we should get our real error.
Calling autodiscover again should raise the very same error it did  the first time, not an AlreadyRegistered error.
We can't test the DEFAULT_TABLESPACE and DEFAULT_INDEX_TABLESPACE settings  because they're evaluated when the model class is defined. As a consequence,  @override_settings doesn't work, and the tests depend
The unmanaged models need to be removed after the test in order to  prevent bad interactions with the flush operation in other tests.
1 for the table  1 for the index on the primary key
1 for the table + 1 for the index on the primary key
No tablespace-related SQL
1 for the table  1 for the primary key + 1 for the index on code
1 for the table + 1 for the primary key + 1 for the index on code
1 for the index on reference
No tablespace-related SQL
The ManyToManyField declares db_tablespace, its indexes go there.
model_options is the name of the application for this test.
coding: utf-8
filter refs the annotated value
None indicates not to create a column in the database.
-*- encoding: utf-8 -*-
primary_key must be True. Refs 12467.
Prevent Django from autocreating `id` AutoField, which would  result in an error, because a model must have exactly one  AutoField.
-*- encoding: utf-8 -*-
Prevent checks from being run on the 'other' database, which doesn't have  its check_field() method mocked in the test.
-*- encoding: utf-8 -*-
There would be a clash if Model.field installed an accessor.
Model names are resolved when a model is being created, so we cannot  test relative fields in isolation and we need to attach them to a  model.
Implicit symmetrical=False.
Explicit symmetrical=True.
Explicit symmetrical=True.
Note that both fields are not unique.
A model that can be, but isn't swapped out. References to this  model should *not* raise any validation error.
Python 2 crashes on non-ASCII strings.
Python 2 crashes on non-ASCII strings.
New tests should not be included here, because this is a single,  self-contained sanity check, not a test of everything.
-*- encoding: utf-8 -*-
unique_together tests are very similar to index_together tests.
unique_together can be a simple tuple
A model with very long name which will be used to set relations to.
Models used for setting `through` in M2M field.
Here we have two clashed: id (automatic field) and clash, because  both parents define these fields.
This field doesn't result in a clash.
This field clashes with parent "f_id" field.
In lieu of any other connector, an existing OneToOneField will be  promoted to the primary key.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
This field is intentionally 2 characters long (16080).
If we specify the fields argument, fieldsets_add and fieldsets_change should  just stick the fields into a formsets structure and return it.
Using `fields`.
Using `fieldsets`.
Using `exclude`.
You can also pass a tuple to `exclude`.
Using `fields` and `exclude`.
Middleware examples that do the right thing
Sample middlewares that raise exceptions
Sample middlewares that omit to return an HttpResonse
Test client intentionally re-raises any exceptions being raised  during request handling. Hence actual testing that exception was  properly handled is done by relying on got_request_exception  signal being sent.
Check that the right middleware methods have been invoked
Repopulate the list of middlewares since it's already been populated  by setUp() before the MIDDLEWARE setting got overridden.
Removing ROOT_URLCONF is safe, as override_settings will restore  the previously defined settings.
DATA fields
VIRTUAL fields
GFK fields
GR fields
DATA fields
VIRTUAL fields
GFK fields
GR fields
DATA fields
VIRTUAL fields
GFK fields
GR fields
ForeignKey to BasePerson
ForeignKey to Person
ForeignKey to ProxyPerson
ManyToManyField to BasePerson
ManyToManyField to Person
ParentListTests models
Running unit test twice to ensure both non-cached and cached result  are immutable.
The apps.clear_cache is setUp() should have deleted all trees.  Exclude abstract models that are not included in the Apps registry  and have no cache.
On first access, relation tree should have populated cache.
AbstractPerson does not have any relations, so relation_tree  should just return an EMPTY_RELATION_TREE.
All the other models should already have their relation tree  in the internal __dict__ .
!/usr/bin/env python
Create a specific subdirectory for the duration of the test suite.  Set the TMPDIR environment variable in addition to tempfile.tempdir  so that children processes inherit it.
Removing the temporary TMPDIR. Ensure we pass in unicode so that it will  successfully remove temp trees containing non-ASCII filenames on Windows.  (We're assuming the temp dir name itself only contains ASCII characters.)
Need to add the associated contrib app to INSTALLED_APPS in some cases to  avoid "RuntimeError: Model class X doesn't declare an explicit app_label  and isn't in an application in INSTALLED_APPS."
GIS tests are in nested apps
Force declaring available_apps in TransactionTestCase for faster tests.
Load all the ALWAYS_INSTALLED_APPS.
Load all the test model apps.
Reduce given test labels to just the app module path
if the module (or an ancestor) was named on the command line, or  no modules were named (i.e., run all), import  this module and add it to INSTALLED_APPS.  exact match or ancestor match
Restore the old settings.
This doesn't work before django.setup() on some databases.
Run the test suite, including the extra validation tests.
Make sure the bisection point isn't in the test list  Also remove tests that need to be run in specific combinations
Make sure the constant member of the pair isn't in the test list  Also remove tests that need to be run in specific combinations
mock is a required dependency
Allow including a trailing slash on app_labels for tab completion convenience
This should never receive unrendered content.
process_template_response must not be called for HttpResponse
-*- coding: utf-8 -*-
Test uncached access  Test cached access
Test uncached access  Test cached access
Test uncached access  Test cached access
Test uncached access
Test cached access: no changes
Test cached access: add a module
originally from https://bitbucket.org/ned/jslex
implement specific and obviously wrong escaping  in order to be able to tell for sure when it runs
2:30 happens twice, once before DST ends and once after
2:30 never happened due to DST
-*- encoding: utf-8 -*-
str(s) raises a TypeError on python 3 if the result is not a text type.  python 2 fails when it tries converting from str to unicode (via ASCII).
Reserved chars remain unescaped.
Test idempotency.
Valid UTF-8 sequences are decoded.
Test idempotency.
See ticket 20212
This would fail with "TypeError: can't pickle instancemethod objects",  only on Python 2.X.
Try the variant protocol levels.
This would fail with "TypeError: expected string or Unicode object, NoneType found".
dt is ambiguous in Europe/Copenhagen. LocalTimezone guesses the  offset (and gets it wrong 50% of the time) while pytz refuses the  temptation to guess. In any case, this shouldn't crash.
Regression test for 18951
3h30m to the west of UTC
Ticket 16924 -- We don't need timezone support to test this
An importable child
A child that exists, but will generate an import error if loaded
A child that doesn't exist
A child that doesn't exist, but is the name of a package on the path
A module which doesn't have a __path__ (so no submodules)
An importable child
A child that exists, but will generate an import error if loaded
A child that doesn't exist
A child that exists, but will generate an import error if loaded
A child that doesn't exist
__contains__ doesn't work when the haystack is a string and the needle a LazyObject
Copying a list works and returns the correct objects.
Copying a list doesn't force evaluation.
Copying a class works and returns the correct objects.
Copying a class doesn't force evaluation.
Deep copying a list works and returns the correct objects.
Deep copying doesn't force evaluation.
Deep copying a class works and returns the correct objects.
Deep copying doesn't force evaluation.
By inheriting from LazyObjectTestCase and redefining the lazy_wrap()  method which all testcases use, we get to make sure all behaviors  tested in the parent testcase also apply to SimpleLazyObject.
First, for an unevaluated SimpleLazyObject  __repr__ contains __repr__ of setup function and does not evaluate  the SimpleLazyObject
-*- encoding: utf-8 -*-
docstring should be preserved
check that it is cached
check that it returns the right thing
check that state isn't shared between instances
check that it behaves like a property when there's no instance
check that overriding name works
NOTE: \xa0 avoids wrapping between value and unit
Refs 23664
AttributeError: ImmutableList object is immutable.
AttributeError: Object is immutable!
-*- coding: utf-8 -*-
caused infinite loop on Pythons not patched with  http://bugs.python.org/issue20288
defines __html__ on its own
overrides __unicode__ and is marked as html_safe
defines __html__ on its own
overrides __str__ and is marked as html_safe
-*- encoding: utf-8 -*-
-*- encoding: utf-8 -*-
-*- coding: utf-8 -*-
Always start off in TEST_DIR.
Make sure that get_current() does not return a deleted Site object.
Test that an exception is raised if the sites framework is installed  but there is no matching Site
Host header without port
Host header with port - match, no fallback without port
Host header with port - no match, fallback without port
Host header with non-matching domain
Delete the site created as part of the default migration process.
does nothing
The data is ignored, but let's check it doesn't crash the system  anyway.
Check that the response was a 302 (redirect)
Check if parameters are intact
Check that the response was a 301 (permanent redirect)
Check that the response was a 302 (non-permanent redirect)
Check that the response was a 302, and that  the attempt to get the redirection location returned 301 when retrieved
Check that the response was a 404, and that the content contains MAGIC
Check that the multi-value data has been rolled out ok
Check that the response was a 404
Check that the path in the response includes it (ignore that it's a 404)
Get the page without logging in. Should result in 302.
Log in
Get the page without logging in. Should result in 302.
Log in
Get the page without logging in. Should result in 302.
Log in
Get the page without logging in. Should result in 302.
Log in
Get the page without logging in. Should result in 302.
Log in
Get the page without logging in. Should result in 302.
Log in
Get the page without logging in. Should result in 302.
Log in
Log in
Log out
Request a page that requires a login
Log in
Log out
Request a page that requires a login
Get the page without logging in. Should result in 302.
Log in
Get the page without logging in. Should result in 302.
Log in
Log in with wrong permissions. Should result in 302.
Get the page without logging in. Should result in 403.
Log in
Log in with wrong permissions. Should result in 403.
Get the page without logging in. Should result in 302.
Log in
Log in with wrong permissions. Should result in 302.
Session value isn't set initially
Check that the session was modified
Try the same assertion, a different way
Check some response details
The normal client allows the post
The CSRF-enabled client rejects it
Do nothing for 200 responses.
Strip content for HEAD requests.
Don't bother validating the formset unless each form is valid
Deactivate translation when set to true
Leaves locale from settings when set to false
raise an error if some --parameter is flowing from options to args
Use a simple string for forward declarations.
You can also explicitly specify the related app.
Create a Parent
Create some children
Set the best child  No assertion require here; if basic assignment and  deletion works, the test passes.
Writes in the outer block are rolled back too.
atomic block shouldn't rollback, but force it.
The tests access the database after exercising 'atomic', initiating  a transaction ; a rollback is required before restoring autocommit.
The third insert couldn't be roll back. Temporarily mark the  connection as not needing rollback to check it.  The second insert couldn't be roll back. Temporarily mark the  connection as not needing rollback to check it.
The first block has a savepoint and must roll back.
The third insert couldn't be roll back. Temporarily mark the  connection as not needing rollback to check it.  The second block has a savepoint and must roll back.
Make sure autocommit wasn't changed.
The transaction is marked as needing rollback.
Mark the transaction as no longer needing rollback.
The connection is closed and the transaction is marked as  needing rollback. This will raise an InterfaceError on databases  that refuse to create cursors on closed connections (PostgreSQL)  and a TransactionManagementError on other databases.  The connection is usable again .
We cannot synchronize the two threads with an event here  because the main thread locks. Sleep for a little while.  2) ... and this line deadlocks. (see below for 1)
This is the thread-local connection, not the main connection.
Double atomic to enter a transaction and create a savepoint.  1) This line locks... (see above for 2)
Must not raise an exception
Expect an error when rolling back a savepoint that doesn't exist.  Done outside of the transaction block to ensure proper recovery.
Start a plain transaction.
Swallow the intentional error raised in the sub-transaction.
Start a sub-transaction with a savepoint.
This is expected to fail because the savepoint no longer exists.
Add permissions auth.add_customuser and auth.change_customuser
LogEntry.user column doesn't get altered to expect a UUID, so set an  integer manually to avoid causing an error.
-*- coding: utf-8 -*-
Uses a mocked version of PasswordResetTokenGenerator so we can change  the value of 'today'
This will put a 14-digit base36 timestamp into the token, which is too large.
Usernames to be passed in REMOTE_USER for the test_known_user test case.
Another request with same user should not create any new users.
Set last_login to something so we can determine if it changes.
REMOTE_USER strings with email addresses for the custom backend to  clean.
The custom_perms test messes with ContentTypes, which will  be cached; flush the cache to ensure there are no side effects  Refs 14975, 14925
Re-set the password, because this tests overrides PASSWORD_HASHERS
The get_group_permissions test messes with ContentTypes, which will  be cached; flush the cache to ensure there are no side effects  Refs 14975, 14925
user_login_failed signal is sent.
Get a session for the test user
Prepare a request object
Remove NewModelBackend  Get the user from the request
Assert that the user retrieval is successful and the user is  anonymous as the backend is not longer available.
Prepare a request object
*After* rendering, we check whether the session was accessed
This line is only required to render the password reset with is_admin=True
For testing that auth backends can be referenced using a convenience import
-*- coding: utf-8 -*-
To verify that the login form rejects inactive users, use an authentication  backend that allows them.
The user is inactive, but our custom form policy allows them to log in.
Just check we can create it
Use the form to construct the POST data
The password field should be readonly, so anything  posted here should be ignored; the form will be  valid, and give back the 'initial' value for the  password field.
original hashed password contains $
When rendering the bound password field,  ReadOnlyPasswordHashWidget needs the initial  value to render correctly
This cleanup is necessary because contrib.sites cache  makes tests interfere with each other, see 11505
The form itself is valid, but no email is sent
Rendering the widget with value set to None  mustn't raise an exception.
-*- coding: utf-8 -*-
On some platforms (e.g. OpenBSD), crypt.crypt() always return None.
Check that no upgrade is triggered.
Revert to the old rounds count and ...
... check if the password would get updated to the new count.
Increasing rounds from 4 to 6 means an increase of 4 in workload,  therefore hardening should run 3 times to make the timing the  same (the original encode() call already ran once).
Assert that the unusable passwords actually contain a random part.  This might fail one day due to a hash collision.
Check that no upgrade is triggered
Revert to the old iteration count and ...
... check if the password would get updated to the new iteration count.
Encode should get called once ...
Check that no upgrade is triggered
Correct password supplied, no hardening needed
Wrong password supplied, hardening needed
Generate hash with attr set to 1
Check that no upgrade is triggered.
Revert to the old rounds count and ...
... check if the password would get updated to the new count.
the is_active attr is provided by AbstractBaseUser
Maybe required?
Admin required fields
The extension user is a simple extension of the built-in user class,  adding a required date_of_birth field. This allows us to check for  any hard references to the name "User" in forms/handlers etc.
-*- coding: utf-8 -*-
optional multipart text/html email has been added.  Make sure original,  default functionality is 100% the same
redirect to a 'complete' page:
Let's munge the token in the path, but keep the same length,  in case the URLconf will reject a different length.
Ensure that we get a 200 response for a non-existent user, not a 404
Ensure that we get a 200 response for a base36 user id that overflows int
Check the password has not been changed
Check the password has been changed
Check we can't use the link again
16919 -- The ``password_reset_confirm`` view should pass the user  object to the ``SetPasswordForm``, even on GET requests.  For this test, we render ``{{ form.user }}`` in the template  ``registration/password_reset_confirm.html`` so that we can test this.
However, the view should NOT pass any user object on a form if the  password reset link was invalid.
instead of fixture
if the hash isn't updated, retrieving the redirection page will fail.
15198  the custom authentication form used by this login asserts  that a request is passed to the form successfully.
Use POST request to log in
Check the CSRF token switched
If no password change, session key should not be flushed.
Bug 14377
Bug 11223
Create a new session with language
Redirect in test_user_change_password will fail if session auth hash  isn't updated after password change (21649)
Make me a superuser before logging in.
The LogEntry.user column isn't altered to a UUID type so it's set to  an integer manually in CustomUserAdmin to avoid an error. To avoid a  constraint error, delete the entry before constraints are checked  after the test.
getpass on Windows only supports prompt as bytestring (19807)
'Julia' with accented 'u':
'Julia' with accented 'u':
created password should be unusable
created password should be unusable
We can use the management command to create a superuser  We skip validation because the temporary substitution of the  swappable User model messes with validation.
Returns '1234567890' the first two times it is called, then  'password' subsequently.
The first two passwords do not match, but the second two do match and  are valid.
The first two passwords are empty strings, but the second two are  valid.
add/change/delete permission by default + custom permission
custom permission only since default permissions is empty
User not in database
Valid user with correct password
correct password, but user is inactive
Valid user with incorrect password
User not in database
Valid user with correct password'
Valid user with incorrect password
User not in database
Create a silo'd admin site for just the user/group admins.
According to  http://tools.ietf.org/html/rfc3696section-3  the "@" symbol can be part of the local part of an email address
you can set the attribute - but it will not save  there should be no problem saving - but the attribute is not saved  the attribute is always true for newly retrieved instance
Tests for user equality.  This is hard because User defines  equality in a non-duck-typing way  See bug 12060
-*- coding: utf-8 -*-
Check username getter
Check API-based user creation with no password
Backwards-compatibility callables
Ensure there were no more failures.
The log_out function will still trigger the signal for anonymous  users.
-*- coding:utf-8 -*-
Reset warnings state.
Ensure that AdminEmailHandler does not get filtered out  even with DEBUG=True.
Backup then override original filters
Restore original filters
Monkeypatches
Revert Monkeypatches
Text email
HTML email
validate is just an example command to trigger settings configuration
An explicit link to the parent (we can control the attribute name).
The parent_link connector need not be the pk on the model.
Test parent_link connector can be discovered in abstract classes.
Check that abstract classes don't get m2m tables autocreated.
Check concrete -> abstract -> concrete inheritance
Check concrete + concrete -> concrete -> concrete
Create a child-parent chain with an explicit parent link
Check that no extra parent objects have been created.
You can also update objects when using a raw save.
This should delete both Restaurants, plus the related places, plus  the ItalianRestaurant.
the child->parent link
All fields from an ABC, including those inherited non-abstractly  should be available on child classes (7588). Creating this instance  should work without error.
Check that a subclass of a subclass of an abstract model doesn't get  its own accessor.
... but it does inherit the m2m from its parent
Regression test for 11369: verbose_name_plural should be inherited  from an ABC even when there are one or more intermediate  abstract models in the inheritance chain, for consistency with  verbose_name.
-*- encoding: utf-8 -*-
.GET and .POST should be QueryDicts
and FILES should be MultiValueDict
Slight time dependency; refs 23450
A compat cookie may be in use -- check that it has worked  both as an output string, and using the cookie attributes
Consume enough data to mess up the parsing:
Shouldn't use the X-Forwarded-Port header
Should use the X-Forwarded-Port header
Invalid hostnames would normally raise a SuspiciousOperation,  but we have DEBUG=True, so this check is disabled.
//// is needed to create a request with a path beginning with //
//// is needed to create a request with a path beginning with //
Assign a unicode string as name to make sure the intermediary model is  correctly created. Refs 20207
Models to test correct related_name inheritance
Adding an object of the wrong type raises TypeError
Excluding a related item works as you would expect, too (although the SQL  involved is a little complex).
Ensure that querysets used in m2m assignments are pre-evaluated  so their value isn't affected by the clearing operation in  ManyRelatedManager.set() (19816).
Ensure that querysets used in M2M assignments are pre-evaluated  so their value isn't affected by the clearing operation in  ManyRelatedManager.set() (19816).
Avoid validation
Disable reverse relation  Set unique to enable model cache.
Add virtual relation to the ArticleTranslation model.
Model for index_together being used only with single list
Ensure the index name is properly quoted
Test for using index_together with a single list (22172)
unique=True and db_index=True should only create the varchar-specific  index (19441).
-*- encoding: utf-8 -*-
Override the STATIC_ROOT for all tests from setUp to tearDown  rather than as a context manager  Same comment as in runtests.teardown.
Override settings
Restore original settings
Backup original environment variable
If contrib.staticfiles isn't configured properly, the exception  should bubble up to the main thread.
Restore original environment variable
skip it, as setUpClass doesn't call its parent either
Intentionally empty method so that the test is picked up by the  test runner and the overridden setUpClass() method is executed.
run collectstatic again
If this string is in the collectstatic output, it means the warning we're  looking for was emitted.
Make sure the warning went away again.
Clear hashed files to avoid side effects among tests.
collect the additional file
delete the original file form the app, collect with clear
Don't run collectstatic command in this test class.
Store a version of the original raster field for testing the exception  raised if GDAL isn't installed.
Test GeometryColumns when available
Test spatial indices when available
Test spatial indices when available
Test GeometryColumns when available
Test spatial indices when available
Test spatial indices when available
Test GeometryColumns when available
Throwing a curveball w/`db_column` here.
All the transformations are to state plane coordinate systems using  US Survey Feet (thus a tolerance of 0 implies error w/in 1 survey foot).
Relations more than one level deep can be queried.
This combines the Extent and Union aggregates into one query
This combines the Extent and Union aggregates into one query
The second union aggregate is for a union  query that includes limiting information in the WHERE clause (in other  words a `.filter()` precedes the call to `.aggregate(Union()`).
Regression test for 9752.
Now creating a second Parcel where the borders are the same, just  in different coordinate systems.  The center points are also the  same (but in different coordinate systems), and this time they  actually correspond to the centroid of the border.
Should return the second Parcel, which has the center within the  border.
This time center2 is in a different coordinate system and needs  to be wrapped in transformation SQL.
Should return the first Parcel, which has the center point equal  to the point in the City ForeignKey.
This time the city column should be wrapped in transformation SQL.
Count annotation should be 2 for the Dallas location now.
Even though Dallas and Ft. Worth share same point, Collect doesn't  consolidate -- that's why 4 points in MultiPoint.
This triggers TypeError when `get_default_columns` has no `local_only`  keyword.  The TypeError is swallowed if QuerySet is actually  evaluated as list generation swallows TypeError in CPython.
Reference mapping of city name to its altitude (Z value).
Interstate (2D / 3D and Geographic/Projected variants)
The VRT is 3D, but should still be able to map sans the Z.
The city shapefile is 2D, and won't be able to fill the coordinates  in the 3D model -- thus, a LayerMapError is raised.
3D model should take 3D data just fine.
Making sure LayerMapping.make_multi works right, by converting  a Point25D into a MultiPoint25D.
KML should be 3D.  `SELECT ST_AsKML(point, 6) FROM geo3d_city3d WHERE name = 'Houston';`
KML should be 3D.  `SELECT ST_AsKML(point, 6) FROM geo3d_city3d WHERE name = 'Houston';`
Path where reference test data is located.
Shapefile is default extension, unless specified otherwise.
Converting lists to tuples of certain keyword args  so coordinate test cases will match (JSON has no  concept of tuple).
Load up the test geometry data from fixture into global.
Get array from raster
Assert data is same as original input
Prepare tempfile
Prepare tempfile and nodata value
Transform raster into srid 4326.
Reload data from disk
Check that statistics are persisted into PAM file on band close
Close band and remove file if created
Open raster in read mode
Setting attributes in write mode raises exception in the _flush method
Set nodata value
Prepare data for setting values in subsequent tests
Loading up the data source
Making sure the layer count is what's expected (only 1 layer in a SHP file)
Making sure GetName works
Making sure the driver name matches up
Making sure indexing works
Incrementing through each layer, this tests DataSource.__iter__  Making sure we get the number of features we expect
Making sure we get the number of fields we expect
Now checking the field names.
Negative FIDs are not allowed.
Testing `Layer.get_fields` (which uses Layer.__iter__)
Using the first data-source because the same slice  can be used for both the layer and the control values.
See ticket 9448.  This DataSource object is not accessible outside this  scope.  However, a reference should still be kept alive  on the `Layer` returned.
Making sure we can call OGR routines on the Layer returned.
Same issue for Feature/Field objects, see 18640
Incrementing through each layer  Incrementing through each feature in the layer  Making sure the number of fields, and the geometry type  are what's expected.
Making sure the fields match to an appropriate OFT type.  Making sure we get the proper OGR Field instance, using  a string value index for the feature.
Testing Feature.__iter__
Incrementing through each layer and feature.
Making sure we get the right Geometry name & type
When not set, it should be None.
Must be set a/an OGRGeometry or 4-tuple.
Clearing the spatial filter by setting it to None.  Now  should indicate that there are 3 features in the Layer.
Using *.dbf from Census 2010 TIGER Shapefile for Texas,  which has land area ('ALAND10') stored in a Real field  with no precision.  Reference value obtained using `ogrinfo`.
OGRGeomType should initialize on all these inputs.
Should throw TypeError on this input
'Geometry' initialization implies an unknown geometry type.
In GDAL 1.8, the non-conformant GML tag  <gml:GeometryCollection> was  replaced with <gml:MultiGeometry>.
Constructing w/HEX
Constructing w/WKB.
Testing `from_bbox` class method
Testing equivalence
Creating a geometry w/spatial reference
Ensuring that SRS is propagated to clones.
Ensuring all children geometries (polygons and their rings) all  return the assigned spatial reference as well.
Using the `srid` property.
srs/srid may be assigned their own values, even when srs is None.
Testing use of the `clone` keyword.
Making sure the coordinate dimension is still 2D.
Can't insert a Point into a MultiPolygon.
Same as `City` above, but for testing model inheritance.
Mapping dictionaries for the models above.  ForeignKey's use another mapping dictionary for the _related_ Model (State in this case).
-*- coding: utf-8 -*-
Dictionaries to hold what's expected in the county shapefile.
Model field that does not exist.
Shapefile field that does not exist.
Nonexistent geographic field type.
Incrementing through the bad mapping dictionaries and  ensuring that a LayerMapError is raised.
A LookupError should be thrown for bogus encodings.
Setting up for the LayerMapping.
There should be three cities in the shape file.
Comparing the geometries.
When the `strict` keyword is set an error encountered will force  the importation to stop.
This LayerMapping should work b/c `strict` is not set.
Two interstate should have imported correctly.
Verifying the values in the layer w/the model.
Only the first two features of this shapefile are valid.
Everything but the first two decimal digits were truncated,  because the Interstate model's `length` field has decimal_places=2.
Should only be one record b/c of `unique` keyword.
Multiple records because `unique` was not set.
Telling LayerMapping that we want no transformations performed on the data.
Specifying the source spatial reference system via the `source_srs` keyword.
Unique may take tuple or string parameters.
No source reference system defined in the shapefile, should raise an error.
There exist no State models for the ForeignKey mapping to work -- should raise  a MissingForeignKey exception (this error would be ignored if the `strict`  keyword is not set).
Now creating the state models so the ForeignKey mapping may work.
A reference that doesn't use the unique keyword; a new database record will  created for each polygon.
The county helper is called to ensure integrity of County models.
Function for clearing out all the counties before testing.
Initializing the LayerMapping object to use in these tests.
Bad feature id ranges should raise a type error.
Features IDs 3 & 4 are for Galveston County, Texas -- only  one model is returned because the `unique` keyword was set.
Features IDs 5 and beyond for Honolulu County, Hawaii, and  FID 0 is for Pueblo County, Colorado.
Testing the `step` keyword -- should get the same counties  regardless of we use a step that divides equally, that is odd,  or that is larger than the dataset.
Parent model has geometry field.
Grandparent has geometry field.
-*- coding: utf-8 -*-
No city database available, these calls should fail.
Non-string query should raise TypeError
Some databases have only unaccented countries
Getting the database identifier used by OGR, if None returned  GDAL does not have the support compiled in.
Writing shapefiles via GDAL currently does not support writing OGRTime  fields, so we need to actually use a database
Some backends may have srid=-1
Ensure that GDAL library has driver support for the database.
SQLite/Spatialite in-memory databases
Build the params of the OGR database connection string
Don't add the parameter if it is not in django's settings
This is an inherited model from City
Testing on a Point
Now setting with a compatible GEOS Geometry, saving, and ensuring   the save took, notice no SRID is explicitly set.
Ensuring that the SRID is automatically set to that of the   field after assignment, but before saving.
Ensuring the point was saved correctly after saving
Creating a State object using a built Polygon
San Antonio in 'WGS84' (SRID 4326)
Creating San Antonio.  Remember the Alamo.
If the GeometryField SRID is -1, then we shouldn't perform any  transformation if the SRID of the input geometry is different.
Creating a Pennsylvanian city.
All transformation SQL will need to be performed on the  _parent_ table.
Getting Texas, yes we were a country -- once ;)
Testing `contains` on the states using the point for Lawrence.
Getting the borders for Colorado & Kansas
Note: Wellington has an 'X' value of 174, so it will not be considered   to the left of CO.
Creating a state with a NULL boundary.
Querying for both NULL and Non-NULL values.
Puerto Rico should be NULL (it's a commonwealth unincorporated territory)
Saving another commonwealth w/a NULL geometry.
To make things more interesting, we will have our Texas reference point in  different SRIDs.
Not passing in a geometry as first param should  raise a type error when initializing the GeoQuerySet
Testing within relation mask.
XXX For some reason SpatiaLite does something screwy with the Texas geometry here.  Also,  XXX it doesn't like the null intersection.
Should be able to execute the queries; however, they won't be the same  as GEOS (because Oracle doesn't use GEOS internally like PostGIS or  SpatiaLite).
Reference query:  `SELECT ST_extent(point) FROM geoapp_city WHERE (name='Houston' or name='Dallas');`    =>  BOX(-96.8016128540039 29.7633724212646,-95.3631439208984 32.7820587158203)
Only PostGIS and SpatiaLite support GeoJSON.
Precision argument should only be an integer
Reference queries and values.  SELECT ST_AsGeoJson("geoapp_city"."point", 8, 0)  FROM "geoapp_city" WHERE "geoapp_city"."name" = 'Pueblo';
SELECT ST_AsGeoJson("geoapp_city"."point", 8, 2) FROM "geoapp_city"  WHERE "geoapp_city"."name" = 'Houston';  This time we want to include the CRS by using the `crs` keyword.
SELECT ST_AsGeoJson("geoapp_city"."point", 8, 1) FROM "geoapp_city"  WHERE "geoapp_city"."name" = 'Houston';  This time we include the bounding box by using the `bbox` keyword.
SELECT ST_AsGeoJson("geoapp_city"."point", 5, 3) FROM "geoapp_city"  WHERE "geoapp_city"."name" = 'Chicago';  Finally, we set every available keyword.
Should throw a TypeError when trying to obtain KML from a   non-geometry field.
Both 'countries' only have two geometries.
Oracle and PostGIS 2.0+ will return 1 for the number of  geometries on non-collections.
Oracle cannot count vertices in Point geometries.
XXX This seems to be a WKT-translation-related precision issue?
Because floating-point arithmetic isn't exact, we set a tolerance  to pass into GEOS `equals_exact`.
Pre-transformed points for Houston and Pueblo.
XXX The low precision is for SpatiaLite
Ensuring the georss element was added to each item in the feed.
Making sure the box got added to the second GeoRSS feed.
Ensuring the georss element was added to each entry in the feed.
-*- encoding: utf-8 -*-
contrived example, but need a geo lookup paired with an id__in lookup
.count() should not throw TypeError in __eq__
verify types -- shouldn't be 0/1  verify values
Only PostGIS and SpatiaLite support GeoJSON.
Precision argument should only be an integer
Reference queries and values.  SELECT ST_AsGeoJson("geoapp_city"."point", 8, 0)  FROM "geoapp_city" WHERE "geoapp_city"."name" = 'Pueblo';
SELECT ST_AsGeoJson("geoapp_city"."point", 8, 2) FROM "geoapp_city"  WHERE "geoapp_city"."name" = 'Houston';  This time we want to include the CRS by using the `crs` keyword.
SELECT ST_AsGeoJson("geoapp_city"."point", 8, 1) FROM "geoapp_city"  WHERE "geoapp_city"."name" = 'Houston';  This time we include the bounding box by using the `bbox` keyword.
SELECT ST_AsGeoJson("geoapp_city"."point", 5, 3) FROM "geoapp_city"  WHERE "geoapp_city"."name" = 'Chicago';  Finally, we set every available keyword.
Should throw a TypeError when trying to obtain KML from a  non-geometry field.
SpatiaLite and Oracle do something screwy with the Texas geometry.
When the intersection is empty, Spatialite and MySQL return None
When the intersection is empty, Oracle returns an empty string
Both 'countries' only have two geometries.
Oracle and PostGIS return 1 for the number of geometries on  non-collections, whereas MySQL returns None.
Spatialite and MySQL can only count points on LineStrings
Oracle cannot count vertices in Point geometries.
Because floating-point arithmetic isn't exact, we set a tolerance  to pass into GEOS `equals_exact`.
Oracle does something screwy with the Texas geometry.
Pre-transformed points for Houston and Pueblo.
The low precision is for SpatiaLite
For some reason SpatiaLite does something screwy with the Texas geometry here.  Also, it doesn't like the null intersection.
Should be able to execute the queries; however, they won't be the same  as GEOS (because Oracle doesn't use GEOS internally like PostGIS or  SpatiaLite).
Returning a simple tuple for the geometry.
This time we'll use a 2-tuple of coordinates for the box.
The following feeds are invalid, and will raise exceptions.
Ensuring the right sitemaps namespace is present.
Getting the relative URL since we don't have a real site.
-*- coding: utf-8 -*-
Naive check to see if there is DNS available to use.  Used to conditionally skip fqdn geoip checks.  See 25407 for details.
No city database available, these calls should fail.
Non-string query should raise TypeError
Some databases have only unaccented countries
-*- coding: utf-8 -*-
this would work:  self._list = self._mytype(items)  but then we wouldn't be testing length parameter
Also works with a custom IndexError
Creating a WKTReader instance
read() should return a GEOSGeometry
Should only accept six.string_types objects.
Creating a WKTWriter instance, testing its ptr property.
Creating a WKBReader instance
read() should return a GEOSGeometry on either a hex string or  a WKB buffer.
Ensuring bad byteorders are not accepted.  Equivalent of `wkb_w.byteorder = bad_byteorder`
Setting the byteorder to 0 (for Big Endian)
Back to Little Endian
Now, trying out the 3D and SRID flags.
Now setting the output dimensions to be 3
Telling the WKBWriter to include the srid in the representation.
Testing out GEOSBase class, which provides a `ptr` property  that abstracts out access to underlying C pointers.
This one only accepts pointers to floats
Default ptr_type is `c_void_p`.  Default ptr_type is C float pointer
These assignments are OK -- None is allowed because  it's equivalent to the NULL pointer.
Because pointers have been set to NULL, an exception should be  raised when we try to access it.  Raising an exception is  preferable to a segmentation fault that commonly occurs when  a C method is given a NULL memory reference.  Equivalent to `fg.ptr`
For testing HEX(EWKB).  `SELECT ST_AsHEXEWKB(ST_GeomFromText('POINT(0 1)', 4326));`  `SELECT ST_AsHEXEWKB(ST_GeomFromEWKT('SRID=4326;POINT(0 1 2)'));`
OGC-compliant HEX will not have SRID value.
HEXEWKB should be appropriate for its dimension -- have to use an  a WKBWriter w/dimension set accordingly, else GEOS will insert  garbage into 3D coordinate if there is none.
Same for EWKB.
Redundant sanity check.
string-based
Bad WKB
Some other object  None
Other tests use `fromfile()` on string filenames so those  aren't tested here.
Making sure that the point's X, Y components are what we expect
Centroid operation on point should be point itself
Now testing setting the x and y
Setting via the tuple/coords property
Point individual arguments
Testing the geometry equivalence  Should not be equal to previous geometry
Testing __iter__
Testing polygon construction.
Polygon(shell, (hole1, ... holeN))
Polygon(shell_tuple, hole_tuple1, ... , hole_tupleN)
Getting a polygon with interior rings, and pulling out the interior rings
These deletes should be 'harmless' since they are done on child geometries
Deleting the polygon
Access to these rings is OK since they are clones.
Constructing the polygon and getting the coordinate sequence
Checks __getitem__ and __setitem__
Constructing the test value to set the coordinate sequence with
Making sure every set point matches what we expect
The buffer we expect
Can't use a floating-point for the number of quadsegs.
Constructing our buffer
GEOS may get the SRID from HEXEWKB  'POINT(5 23)' at SRID=4326 in hex form -- obtained from PostGIS  using `SELECT GeomFromText('POINT (5 23)', 4326);`.
Testing that geometry SRID could be set to its own value
Testing the mutability of Polygons
Should only be able to use __setitem__ with LinearRing geometries.
Assigning polygon's exterior ring w/the new shell
Testing the assignment
Distance to self should be 0.
Distance should be 1
Distance should be ~ sqrt(2)
Points have 0 length.
Should be ~ sqrt(2)
Should be sum of each element's length in collection.
Testing len() and num_geom.
Should construct ok from WKT
Should also construct ok from individual geometry arguments.
And, they should be equal.
Testing use of the `clone` keyword.
Using both pickle and cPickle -- just 'cause.
Original geometry deletion should not crash the prepared one (21662)
_set_single
_set_list
Create test raster and geom.
Create query filter combinations.
Apply this query filter.
Evaluate normal filter qs.
Evaluate on conditional Q expressions.
Create test raster and geom.
Filter raster with different lookup raster formats.
Filter in an unprojected coordinate system.
Filter raster by geom.
Filter geom by raster.
Filter through related model.
Filter through related model with band index transform
Filter through different lookup.
From proj's "cs2cs -le" and Wikipedia (semi-minor only)
Some of the authority names are borked on Oracle, e.g., SRID=32140.   also, Oracle Spatial seems to add extraneous info to fields, hence the   the testing with the 'startswith' flag.
No proj.4 and different srtext on oracle backends :(
Can't get 'NAD83 / Texas South Central' from PROJ.4 string  on SpatiaLite
Getting the ellipsoid and precision parameters.
Getting our spatial reference and its ellipsoid
`GeoQuerySet.distance` is not allowed geometry fields.
Regression test for 14060, `~=` was never really implemented for PostGIS.
There is a similar test in `layermap` that uses the same data set,  but the County model here is a bit different.
Reference county names, number of polygons, and state names.
Decorators to disable entire test functions for specific  spatial backends.
MySQL spatial indices can't handle NULL geometries.
Expected cities for Australia and Texas.
Now performing the `dwithin` queries on a geodetic coordinate system.
Testing using different variations of parameters and using models  with different projected coordinate systems.
Original query done on PostGIS, have to adjust AlmostEqual tolerance  for Oracle.
Testing geodetic distance calculation with a non-point geometry  (a LineString of Wollongong and Shellharbour coords).
We'll be using a Polygon (created by buffering the centroid  of 77005 to 100m) -- which aren't allowed in geographic distance  queries normally, however our field has been transformed to  a non-geographic system.
Reference query:  SELECT ST_Distance(ST_Transform("distapp_censuszipcode"."poly", 32140),    ST_GeomFromText('<buffer_wkt>', 32140))  FROM "distapp_censuszipcode";
Retrieving the cities within a 20km 'donut' w/a 7km radius 'hole'  (thus, Houston and Southside place will be excluded as tested in  the `test02_dwithin` above).
Not enough params should raise a ValueError.
Cities that are either really close or really far from Wollongong --  and using different units of distance.
Reference query (should use `length_spheroid`).  SELECT ST_length_spheroid(ST_GeomFromText('<wkt>', 4326) 'SPHEROID["WGS 84",6378137,298.257223563,    AUTHORITY["EPSG","7030"]]');
Does not support geodetic coordinate systems.
Now doing length on a projected coordinate system.
Running on points; should return 0.
Original query done on PostGIS, have to adjust AlmostEqual tolerance  for Oracle.
Testing geodetic distance calculation with a non-point geometry  (a LineString of Wollongong and Shellharbour coords).
We'll be using a Polygon (created by buffering the centroid  of 77005 to 100m) -- which aren't allowed in geographic distance  queries normally, however our field has been transformed to  a non-geographic system.
Reference query:  SELECT ST_Distance(ST_Transform("distapp_censuszipcode"."poly", 32140),    ST_GeomFromText('<buffer_wkt>', 32140))  FROM "distapp_censuszipcode";
Reference query (should use `length_spheroid`).  SELECT ST_length_spheroid(ST_GeomFromText('<wkt>', 4326) 'SPHEROID["WGS 84",6378137,298.257223563,    AUTHORITY["EPSG","7030"]]');
Running on points; should return 0.
Form fields, by default, are required (`required=True`)
This will clean None as a geometry (See 10660).
map_srid in operlayers.html template must not be localized.
Force deserialize use due to a string value
Ensure that resulting geometry has srid set
mark the name to show that this was called
This returns a different result each time,  to make sure it only gets called once.
-*- coding: utf-8 -*-
Tests for TZ-aware time methods need pytz.
Changing TIME_ZONE may issue a query to set the database's timezone,  hence TestCase.
Set up a second temporary directory which is ensured to have a mixed  case name.
Check for correct behavior under both USE_TZ=True and USE_TZ=False.  The tests are similar since they both set up a situation where the  system time zone, Django's TIME_ZONE, and UTC are distinct.
dt should be aware, in UTC
dt and now should be the same effective time.
Use a fixed offset timezone so we don't need pytz.  At this point the system TZ is +1 and the Django TZ  is -5.
dt should be naive, in system (+1) TZ
Check that OSErrors aside from EEXIST are still raised.
An object without a file has limited functionality.
Testing truncation.
But it shouldn't be deleted, even if there are no more objects using  it.
CustomValidNameStorage.get_valid_name() appends '_valid' to the name
Create sample file
Test passing StringIO instance as content argument to save
Simulate call to f.save()
Repeat test with a callable.  Return a non-normalized path on purpose.
Simulate call to f.save()
Membership objects have access to their related Person if both  country_ids match between them
Membership objects returns DoesNotExist error when the there is no  Person with the same id and country_id
Creating a valid membership because it has the same country has the person
Creating an invalid membership because it has a different country has the person
Creating an invalid membership
Creating an invalid membership
Creating an invalid membership
We start out by making sure that the Group 'CIA' has no members.
We start out by making sure that Bob is in no groups.
Bob should be in the CIA and a Republican
We start out by making sure that the Group 'CIA' has no members.
Something adds jane to group CIA but Jane is in Soviet Union which isn't CIA's country
There should still be no members in CIA
We start out by making sure that Jane has no groups.
Something adds jane to group CIA but Jane is in Soviet Union which isn't CIA's country
Jane should still not be in any groups
Note that we use ids instead of instances. This is because instances on ForeignObject  properties will set all related field off of the given instance
ForeignObjects should not have any form fields, currently the user needs  to manually deal with the foreignobject relation.
order mismatches the Contact ForeignObject.
Anything with as_sql() method works in get_extra_restriction().
Table Column Fields
Table Column Fields
Table Column Fields
Table Column Fields
Relation Fields
Integer value can be queried using string
Each QuerySet gets iterator(), which is a generator that "lazily"  returns results using database-level iteration.
iterator() can be used on any QuerySet.
Date and date/time lookups can also be done with strings.
Underscores, percent signs and backslashes have special meaning in the  underlying SQL code, but Django handles the quoting of them automatically.
An invalid nested lookup on a related field raises a useful error.
-*- coding: utf-8 -*-
Doesn't insert a token or anything
Doesn't insert a token or anything
Doesn't insert a token or anything
token_view calls get_token() indirectly
token_view calls get_token() indirectly
This test failed in 16715 because in some cases INNER JOIN was selected  for the second foreign key relation instead of LEFT OUTER JOIN.
This failed.
This failed.
These all work because the second foreign key in the chain has null=True.
This test failed in 16715 because in some cases INNER JOIN was selected  for the second foreign key relation instead of LEFT OUTER JOIN.
This failed.
-*- coding: utf-8 -*-
Total hack, but it works, just want an attribute that's always true.
equal html contains each other
HACK: This depends on internals of our TestCase subclasses  Detect fixture loading by counting SQL queries, should be zero
context manager form of assertRaisesMessage()
callable form
callable_obj was a documented kwarg in Django 1.8 and older.
override to avoid a second cls._rollback_atomics() which would fail.  Normal setUpClass() methods won't have exception handling so this  method wouldn't typically be run.
Simulate a broken setUpTestData() method.
setUpTestData() should call _rollback_atomics() so that the  transaction doesn't leak.
self.available_apps must be None to test the serialized_rollback  condition.
with a mocked call_command(), this doesn't have any effect.
Implied dependencies
Implicit dependencies
reordering aliases shouldn't matter
Transaction support should be properly initialized for the 'other' DB  And all the DBs should report that they support transactions
All others can follow in unspecified order, including doctests
Import all the models from subpackages
Generic inline with unique_together
Generic inline with can_delete=False
-*- coding: utf-8 -*-
Set DEBUG to True to ensure {% include %} will raise exceptions.  That is how inlines are rendered and 9498 will bubble up if it is an issue.
inline data
Regression test for 10522.
Regression test for 12340.
Create a formset with default arguments
Create a formset with custom keyword arguments
Test that get_fieldsets is called when figuring out form fields.  Refs 18681.
Ensure author is always accessible in clean method
Optional secondary author
this is purely for testing the data doesn't matter here :)
models for testing unique_together validation when a fk is involved and  using inlineformset_factory.
models for testing callable defaults (see bug 7975). If you define a model  with a callable default value, you cannot rely on the initial value in a  form.
models for testing a null=True fk to a parent
Models for testing custom ModelForm save methods in formsets and inline formsets
Models for testing UUID primary keys
Make sure this form doesn't pass validation.
Make sure this form doesn't pass validation.
Simulate deletion of an object that doesn't exist in the database
The formset is valid even though poem.pk + 1 doesn't exist,  because it's marked for deletion anyway
Test the behavior of min_num with model formsets. It should be  added to extra.
Test the behavior of min_num with existing objects.
change the name to "Vladimir Mayakovsky" just to be a jerk.
As you can see, 'Les Paradis Artificiels' is now a book belonging to  Charles Baudelaire.
The save_as_new parameter lets you re-associate the data to a new  instance.  This is used in the admin for save_as functionality.
Test inline formsets where the inline-edited object has a custom  primary key that is not the fk to the parent object.
change the name to "Brooklyn Bridge" just to be a jerk.
The Poet instance is saved after the formset instantiation. This  happens in admin's changeform_view() when adding a new object and  some inlines in the same request.
Now test the same thing without the validate_max flag to ensure  default behavior is unchanged
inlineformset_factory tests with fk having null=True. see 9462.  create some data that will exhibit the issue
-*- encoding: utf-8 -*-
-*- encoding: utf-8 -*-
Python 3 allows non-ASCII identifiers
-*- coding: utf-8 -*-  Unit and doctests for specific database backends.
Check that '%' chars are escaped for query execution.
If the backend is Oracle, test that we can call a standard  stored procedure through our cursor wrapper.
If the backend is Oracle, test that we can pass cursor variables  as query parameters.
If the backend is Oracle, test that the client encoding is set  correctly.  This was broken under Cygwin prior to r14781.
Helper mocks
psycopg2 < 2.0.12 code path
Invalidate timezone name cache, because the setting_changed  handler cannot know about new_connection.
Fetch a new connection with the new_tz as default  time zone, run a query and rollback.
Now let's see if the rollback rolled back the SET TIME ZONE.
Open a database connection.
Check the level on the psycopg2 connection, not the Django wrapper.
Start a transaction so the isolation level isn't reported as 0.  Check the level on the psycopg2 connection, not the Django wrapper.
Create an object with a manually specified PK
Reset the sequences for the database
If we create a new object now, it should have a PK greater  than the PK we specified manually.
This test needs to run outside of a transaction, otherwise closing the  connection would implicitly rollback and cause problems during teardown.
Unfortunately with sqlite3 the in-memory test database cannot be closed,  and so it cannot be re-opened during testing.
Test executemany with params=[] does nothing 4765
same test for DebugCursorWrapper
same test for DebugCursorWrapper
As password is probably wrong, a database exception is expected
Both InterfaceError and ProgrammingError seem to be used when  accessing closed cursor (psycopg2 has InterfaceError, rest seem  to use ProgrammingError).  cursor should be closed, so no queries should be possible.
There isn't a generic way to test that cursors are closed, but  psycopg2 offers us a way to check that by closed attribute.  So, run only on psycopg2 for that reason.
Initialize the connection and clear initialization statements.
Create a Reporter.
Map connections by id because connections with identical aliases  have the same hash.
Passing django.db.connection between threads doesn't work while  connections[DEFAULT_DB_ALIAS] does.  Allow thread sharing so the connection can be closed by the  main thread.
Map connections by id because connections with identical aliases  have the same hash.
Allow thread sharing so the connection can be closed by the  main thread.
Finish by closing the connections opened by the other threads (the  connection opened in the main thread will automatically be closed on  teardown).
Without touching allow_thread_sharing, which should be False by default.  Forbidden!
If explicitly setting allow_thread_sharing to False  Forbidden!
If explicitly setting allow_thread_sharing to True  All good
First, without explicitly enabling the connection for sharing.
The exception was raised
Then, with explicitly enabling the connection for sharing.
Enable thread sharing
No exception was raised
Get a copy of the default connection. (Can't use django.db.connection  because it'll modify the default connection itself.)
Create a book on the default database using create()
Create a book on the default database using a save
Create a book on the second database
Create a book on the default database using a save
Create a book and author on the default database
Create a book and author on the other database
Save the author relations
Retrieve related object by descriptor. Related objects should be database-bound
Save the author relations
Save the author relations
Create a second book on the other database
Create a book and author on the default database
Create a book and author on the other database
Set a foreign key set with an object from a different database
Add to an m2m with an object from a different database
Set a m2m with an object from a different database
Add to a reverse m2m with an object from a different database
Set a reverse m2m with an object from a different database
Delete the object on the other database
Delete the object on the other database
Create a book and author on the default database
Save the author's favorite books
Retrieve related object by descriptor. Related objects should be database-bound
Save the author relations
Create a book and author on the default database
Create a book and author on the other database
Set a foreign key with an object from a different database
Set a foreign key set with an object from a different database
Add to a foreign key set with an object from a different database
Delete the person object, which will cascade onto the pet
Retrieve related objects; queries should be database constrained
Retrieve related object by descriptor. Related objects should be database-bound
Create a user and profile on the default database
Create a user and profile on the other database
Set a one-to-one relation with an object from a different database
BUT! if you assign a FK object when the base object hasn't  been saved yet, you implicitly assign the database for the  base object.
assigning a profile requires an explicit pk as the object isn't saved
initially, no db assigned
old object comes from 'other', so the new object is set to use 'other'...
This also works if you assign the O2O relation in the constructor
Create a book and author on the other database
Reget the objects to clear caches
Retrieve related object by descriptor. Related objects should be database-bound
Create a book and author on the other database
Set a foreign key with an object from a different database
Add to a foreign key set with an object from a different database
BUT! if you assign a FK object when the base object hasn't  been saved yet, you implicitly assign the database for the  base object.  initially, no db assigned
Delete the Book object, which will cascade onto the pet
Retrieve the Person using select_related()
The editor instance should have a db state
When you call __str__ on the query object, it doesn't know about using  so it falls back to the default. If the subquery explicitly uses a  different database, an error should be raised.
Evaluating the query shouldn't work, either
Init with instances instead of strings
Make the 'other' database appear to be a replica of the 'default'
Add the auth router to the chain. TestRouter is a universal  synchronizer, so it should have no effect.
Now check what happens if the router order is reversed.
Create a book and author on the other database
An update query will be routed to the default database
By default, the get query will be directed to 'other'
But the same query issued explicitly at a database will work.
Check that the update worked.
get_or_create is a special case. The get needs to be targeted at  the write database in order to avoid potential transaction  consistency problems
A delete query will also be routed to the default database
Create a book and author on the default database
Create a book and author on the other database
Set a foreign key with an object from a different database
... but they will when the affected object is saved.
This isn't a real primary/replica database, so restore the original from other
Set a foreign key set with an object from a different database
This isn't a real primary/replica database, so restore the original from other
Add to a foreign key set with an object from a different database
This isn't a real primary/replica database, so restore the original from other
old object comes from 'other', so the new object is set to use the  source of 'other'...
This also works if you assign the FK in the constructor
For the remainder of this test, create a copy of 'mark' in the  'default' database to prevent integrity errors on backends that  don't defer constraints checks until the end of the transaction
This moved 'mark' in the 'default' database, move it back in 'other'
Create books and authors on the inverse to the usual database
Now save back onto the usual database.  This simulates primary/replica - the objects exist on both database,  but the _state.db is as it is for all other tests.
Set a m2m set with an object from a different database
Reset relations
Add to an m2m with an object from a different database
Reset relations
Set a reverse m2m with an object from a different database
Reset relations
Add to a reverse m2m with an object from a different database
If you create an object through a M2M relation, it will be  written to the write database, even if the original object  was on the read database
Same goes for get_or_create, regardless of whether getting or creating
Create a user and profile on the default database
Create a user and profile on the other database
... but they will when the affected object is saved.
Create a book and author on the default database
Create a book and author on the other database
Set a generic foreign key with an object from a different database
... but they will when the affected object is saved.
This isn't a real primary/replica database, so restore the original from other
Add to a generic foreign key set with an object from a different database
... but they will when the affected object is saved.
BUT! if you assign a FK object when the base object hasn't  been saved yet, you implicitly assign the database for the  base object.  initially, no db assigned
Dive comes from 'other', so review3 is set to use the source of 'other'...
When you call __str__ on the query object, it doesn't know about using  so it falls back to the default. Don't let routing instructions  force the subquery to an incompatible database.
If you evaluate the query, it should work, running on 'other'
Create one user using default allocation policy
Create another user, explicitly specifying the database
The second user only exists on the other database
The second user only exists on the default database
No objects will actually be loaded
Make a receiver  Connect it
Create the models that will be used for the tests
test add  test remove  test clear  test setattr  test M2M collection
test related FK collection
We use default here to ensure we can tell the difference  between a read request and a write request for Auth objects
A router that only expresses an opinion on writes
-*- coding: utf-8 -*-
we need to register a custom ModelAdmin (instead of just using  ModelAdmin) because the field creator tries to find the ModelAdmin  for the related model
should be ordered by name (as defined by the model)
should be ordered by name (as defined by the model)
should be ordered by rank (defined by the ModelAdmin)
No Articles yet, so we should get a Http404 error.
get_object_or_404 can be passed a Model to query.
We can also use the Article manager through an Author object.
No articles containing "Camelot".  This should raise a Http404 error.
Custom managers can be used too.
QuerySets can be used too.
Using an empty QuerySet raises a Http404 error.
get_list_or_404 can be used to get lists of objects
Http404 is returned if the list is empty.
Custom managers can be used too.
QuerySets can be used too.
Calling an internal method purely so that we can trigger a "raw" save.
8285: signals can be any callable
Assigning and removing to/from m2m shouldn't generate an m2m signal.
-*- coding: utf-8 -*-
The generic relations regression test needs two different model  classes with the same PK value, and there are some (external)  DB backends that don't work nicely when assigning integer to AutoField  column (MSSQL at least).
Models for ticket 21150
tests that this query does not raise a DatabaseError due to the full  subselect being (erroneously) added to the GROUP BY parameters  force execution of the query
Implicit ordering is also ignored
Check that all of the objects are getting counted (allow_nulls) and  that values respects the amount of objects
Bad field requests in aggregates are caught and reported
Old-style count aggregations can be mixed with new-style
Non-ordinal, non-computed Aggregates over annotations correctly  inherit the annotation's internal type if the annotation is ordinal  or computed
Regression for 10089: Check handling of empty result sets with  aggregates
Regression for 11256 - duplicating a default alias raises ValueError.
Regression for 11256 - providing an aggregate name  that conflicts with a field name on the model raises ValueError
Regression for 11256 - providing an aggregate name  that conflicts with an m2m name on the model raises ValueError
Regression for 11256 - providing an aggregate name  that conflicts with a reverse-related name on the model raises ValueError
Regression for 10197 -- Queries with aggregates can be pickled.  First check that pickling is possible at all. No crash = success
Note: intentionally no order_by(), that case needs tests, too.
Regression for 10666 - inherited fields work with annotations and  aggregations
Regression for 10766 - Shouldn't be able to reference an aggregate  fields in an aggregate() call.
Regression for 11789
Check that the query executes without problems.
A query with an existing annotation aggregation on a relation should  succeed.
Force re-evaluation
It doesn't matter that we swapped out user for permission;  there's no validation. We just want to check the setting stuff works.
It doesn't matter that we swapped out user for permission;  there's no validation. We just want to check the setting stuff works.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Disable auto loading of this model as we load it on our own
Disable auto loading of this model as we load it on our own
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Reset applied-migrations state.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Test operation in non-atomic migration is not wrapped in transaction
from 0001  from 0002
Rebuild the graph to reflect the new DB state
Rebuild the graph to reflect the new DB state
Rebuild the graph to reflect the new DB state
Migrate backwards -- This led to a lookup LookupErrors because  lookuperror_b.B2 is not in the initial state (unrelated to app c)
Rebuild the graph to reflect the new DB state
Apply PK type alteration
Rebuild the graph to reflect the new DB state
Because we've now applied 0001 and 0002 both, their squashed  replacement should be marked as applied.
Because 0001 and 0002 are both applied, even though this migrate run  didn't apply anything new, their squashed replacement should be  marked as applied.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Silence warning on Python 2: Not importing directory  'tests/migrations/migrations_test_apps/without_init_file/migrations':  missing __init__.py
Yes, it doesn't make sense to use a class as a default for a  CharField. It does make sense for custom fields though, for example  an enumfield that takes the enum class as an argument.
No explicit managers defined. Migrations will fall back to the default
The ordering we really want is objects, mgr1, mgr2
First, test rendering individually
We shouldn't be able to render yet
Once the parent model is in the app registry, it should be fine
We shouldn't be able to render yet
Check that the relations between the old models are correct
Same test for deleted model
At this point the model would be rendered twice causing its related  M2M through objects to point to an old copy and thus breaking their  attribute lookup.
If we just stick it into an empty state it should fail
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Test atomic operation in non-atomic migration is wrapped in transaction
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Run through arrange_for_graph
Right number/type of migrations?
Right number of migrations?
IntegerField intentionally not instantiated.
Make state
Make state
Make state
Make state
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
And test reversal
Run delete queries to test for parameter substitution failure  reported in 23426
Make sure there's no table  Test the database alteration
And test reversal
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Ensure we've included unmigrated apps in there too
Empty database: use squashed migration
Starting at 1 or 2 should use the squashed migration too
However, starting at 3 to 5 cannot use the squashed migration
Starting at 5 to 7 we are passed the squashed migrations
Empty database: use squashed migration
Starting at 1 or 2 should use the squashed migration too
Starting at 5 to 7 we are passed the squashed migrations
-*- coding: utf-8 -*-
Fake an apply  Unmigrate everything
Cleanup by unmigrating everything
Cannot generate the reverse SQL unless we've applied the migration.
Cleanup by unmigrating everything
unmigrated_app.SillyModel has a foreign key to 'migrations.Tribble',  but that model is only defined in a migration, so the global app  registry never sees it and the reference is left dangling. Remove it  to avoid problems in subsequent tests.
Rollback changes
Check for empty __init__.py file in migrations folder
Check for existing 0001_initial.py file in migration folder
Meta.verbose_name  Meta.verbose_name_plural
Python 3 importlib caches os.listdir() on some platforms like  Mac OS X (23850).
If a migration fails to serialize, it shouldn't generate an empty file. 21280
Check for existing 0001_initial.py file in migration folder
Remove all whitespace to check for empty dependencies and operations
Output the expected changes directly, without asking for defaults
Normal --dry-run output
Migrations file is actually created in the expected path.
Command output indicates the migration is created.
Check for existing migration file in migration folder
generate an initial migration
Python 3 importlib caches os.listdir() on some platforms like  Mac OS X (23850).
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Unit tests for cache framework  Uses whatever cache backend is set in the test settings file.
functions/classes for complex data type tests
Unpicklable using the default pickling protocol on Python 2.
Simple cache set/get works
Test for same cache key conflicts between shared backend
should not be set in the prefixed cache
Non-existent cache keys return as None/default  get with non-existent keys
The in operator can be used to inspect cache contents
Binary strings should be cacheable
Make sure a timeout given as a float doesn't crash anything.
mimic custom ``make_key`` method being defined since the default will  never show the below warnings
Shouldn't fail silently if trying to cache an unpicklable type.
Simulate cache.add() failing to add a value. In that case, the  default value should be returned.
Spaces are used in the table name to ensure quoting/escaping is working
The super calls needs to happen first for the settings override.
The super call needs to happen first because it uses the database.
Use another table name to avoid the 'table already exists' message.
LocMem requires a hack to make the other caches  share a data store with the 'normal' cache.
memcached backend isn't guaranteed to be available.  To check the memcached backend, the test settings file will  need to contain at least one cache backend setting that points at  your memcache server.
Regression test for 22845
Regression test for 22845
culling isn't implemented, memcached deals with it.
culling isn't implemented, memcached deals with it.
small_value should be deleted, or set if configured to accept larger values
Caches location cannot be modified through override_settings / modify_settings,  hence settings are manipulated directly here and the setting_changed signal  is triggered manually.
Call parent first, as cache.clear() may recreate cache base directory
This fails if not using the highest pickling protocol on Python 2.
this key is both longer than 250 characters, and has spaces
The 5 minute (300 seconds) default expiration time for keys is  defined in the implementation of the initializer method of the  BaseCache type.
Expect None if no headers have been set yet.  Set headers to an empty list.
Expect None if no headers have been set yet.  Set headers to an empty list.  Verify that the querystring is taken into account.
Make sure that the Vary header is added to the key hash
Initial Cache-Control, kwargs to patch_cache_control, expected Cache-Control parts
Regression test for 17476
cache with non empty request.GET
i18n tests
This test passes on Python < 3.3 even without the corresponding code  in UpdateCacheMiddleware, because pickling a StreamingHttpResponse  fails (http://bugs.python.org/issue14288). LocMemCache silently  swallows the exception and doesn't store the response in cache.
If no arguments are passed in construction, it's being used as middleware.
Now test object attributes against values defined in setUp above
If arguments are being passed in construction, it's being used as a decorator.  First, test with "defaults":
Value of DEFAULT_CACHE_ALIAS from django.core.cache
Next, test with custom values:
Put the request through the request middleware
Now put the response through the response middleware
Repeating the request should result in a cache hit
The same request through a different middleware won't hit
The same request with a timeout _will_ hit
decorate the same view with different cache decorators
Request the view once
Request again -- hit the cache
Requesting the same view with the explicit cache should yield the same result
Requesting with a prefix will hit a different cache key
Hitting the same view again gives a cache hit
And going back to the implicit cache will hit the same cache
Requesting from an alternate cache won't hit cache
But a repeated hit will hit cache
And prefixing the alternate cache yields yet another cache entry
But if we wait a couple of seconds...
... the default cache will still hit
... the default cache with a prefix will still hit
... the explicit default cache will still hit
... the explicit default cache with a prefix will still hit
.. but a rapidly expiring cache won't hit
.. even if it has a prefix
Inserting a CSRF cookie in a cookie-less request prevented caching.
Expect None if no headers have been set yet.  Set headers to an empty list.
Verify that a specified key_prefix is taken into account.
1 query to fetch all children of 0 (1 and 2)  1 query to fetch all children of 1 and 2 (none)  Should not require additional queries to populate the nested graph.
One for Location, one for Guest, and no query for EventGuide
NOTE: cannot use @property decorator, because of  AttributeError: 'property' object has no attribute 'short_description'
safestring should not be escaped
normal strings needs to be escaped
-*- coding: utf-8 -*-
Make sure custom action_flags works
If the log entry doesn't have a content type it should still be  possible to view the Recent Actions part (10275).
order_with_respect_to points to a model with a OneToOneField primary key.
Hook to allow subclasses to run these tests with alternate models.
The ordering can be altered
Swap the last two items in the order list
By default, the ordering is different from the swapped version
Some JVM GCs will execute finalizers in a different thread, meaning  we need to wait for that to complete before we go on looking for the  effects of that.
Collecting weakreferences can take two collections on PyPy.
Note that dead weakref cleanup happens as side effect of using  the signal's receivers through the signals API. So, first do a  call to an API method to force cleanup.
Disconnect after reference check since it flushes the tested cache.
The first two models represent a very simple null FK ordering case.
These following 4 models represent a far more complex ordering case.
We can't compare results directly (since different databases sort NULLs to  different ends of the ordering), but we can check that all results are  returned.
We have to test this carefully. Some databases sort NULL values before  everything else, some sort them afterwards. So we extract the ordered list  and check the length. Before the fix, this list was too short (some values  were omitted).
You can't proxy a swapped model
Related field filter on proxy
Select related + filter on proxy
Proxy of proxy, select_related + filter
Select related + filter on a related proxy field
Select related + filter on a related proxy of proxy field
We need to set settings.DEBUG to True so we can capture the output SQL  to examine.
This is executed in autocommit mode so that code in  run_select_for_update can see this data.
We need another database connection in transaction to test that one  connection issuing a SELECT ... FOR UPDATE will block.
Roll back the blocking transaction.
Examine the SQL that was executed to determine whether it  contains the 'SELECT..FOR UPDATE' stanza.
We need to enter transaction management again, as this is done on  per-thread basis
This method is run in a separate thread. It uses its own  database connection. Close it without waiting for the GC.
First, let's start the transaction in our thread.
Now, try it again using the ORM's select_for_update  facility. Do this in a separate thread.
Check the person hasn't been updated. Since this isn't  using FOR UPDATE, it won't block.
When we end our blocking transaction, our thread should  be able to continue.
Check the thread has finished. Assuming it has, we should  find that it has updated the person's name.
We must commit the transaction to ensure that MySQL gets a fresh read,  since by default it runs in REPEATABLE READ mode
This method is run in a separate thread. It uses its own  database connection. Close it without waiting for the GC.
don't allow this field to be used in form (real use-case might be  that you know the markup will always be X, but it is among an app  that allows the user to say it could be something else)  regressed at r10062
Model for 13776
Model for 639
Support code for the tests; this keeps track of how many times save()  gets called on each instance.
Models for 24706
A model with ForeignKey(blank=False, null=True)
form is valid because required=False for field 'character'
Make sure the exception contains some reference to the  field responsible for the problem.
Should have the same result as before,  but 'fields' attribute specified differently
Should have the same result as before,  but 'fields' attribute specified differently
This Price instance generated by this form is not valid because the quantity  field is required, but the form is valid because the field is excluded from  the form. This is for backwards compatibility.
The form should still have an instance of a model that is not complete and  not saved into a DB yet.
First class with a Meta class wins...
Can't create new form
Even if you provide a model instance
Ensure all many-to-many categories appear in model_to_dict  Ensure many-to-many relation appears as a list
Ensure all many-to-many categories appear in model_to_dict  Ensure many-to-many relation appears as a list
You can create a form over a subset of the available fields  by specifying a 'fields' argument to form_for_instance.
Manually save the instance
The instance doesn't have m2m data yet
Save the m2m data on the form
Here, we define a custom ModelForm. Because it happens to have the same fields as  the Category model, we can just call the form's save() to apply its changes to an  existing Category instance.
Add a Category object *after* the ModelChoiceField has already been  instantiated. This proves clean() checks the database during clean() rather  than caching it at time of instantiation.
Delete a Category object *after* the ModelChoiceField has already been  instantiated. This proves clean() checks the database during clean() rather  than caching it at time of instantiation.
len can be called on choices
To allow the widget to change the queryset of field1.widget.choices correctly,  without affecting other forms, the following must hold:
Delete a Category object *after* the ModelMultipleChoiceField has already been  instantiated. This proves clean() checks the database during clean() rather  than caching it at time of instantiation.
BetterWriter model is a subclass of Writer with an additional `score` field
WriterProfile has a OneToOneField to Writer
author object returned from form still retains original publication object  that's why we need to retrieve it from database again
Delete the current file since this is not done by Django.
Delete the current file since this is not done by Django.
Delete the current file since this is not done by Django.
It's enough that the form saves without error -- the custom save routine will  generate an AssertionError if it is called more than once during save.
Fake a POST QueryDict and FILES MultiValueDict.
Check the savecount stored on the object (see the model).
Delete the "uploaded" file to avoid clogging /tmp.
Delete the current file since this is not done by Django, but don't save  because the dimension fields are not null=True.  Override the file by uploading a new one.
Delete the current file since this is not done by Django, but don't save  because the dimension fields are not null=True.
Delete the current file since this is not done by Django, but don't save  because the dimension fields are not null=True.
Test the non-required ImageField  Note: In Oracle, we expect a null ImageField to return '' instead of  None.
Delete the current file since this is not done by Django.
'created', non-editable, is excluded by default
Choices on CharField and IntegerField
form.instance.left will be None if the instance was not constructed  by form.full_clean().
Without a widget should not set the widget to textarea
With a widget should not set the widget to textarea
A bad callback provided by user still gives an error
Use a fast hasher to speed up tests.
compose(f, g)(*args, **kwargs) == f(g(*args, **kwargs))
django.views.decorators.http
django.views.decorators.vary
django.views.decorators.cache
django.contrib.auth.decorators  Apply user_passes_test twice to check 9474
django.contrib.admin.views.decorators
django.utils.functional
For testing method_decorator, a decorator that assumes a single argument.  We will get type arguments if there is a mismatch in the number of arguments.
For testing method_decorator, two decorators that add an attribute to the function
Sanity check myattr_dec and myattr2_dec
Decorate using method_decorator() on the method.
Decorate using method_decorator() on both the class and the method.  The decorators applied to the methods are applied before the ones  applied to the class.
Decorate using an iterable of decorators.
Test for argumented decorator
The order should be consistent with the usual order in which  decorators are applied, e.g.     @add_exclamation_mark     @add_question_mark     def func():         ...
Since the real purpose of the exempt decorator is to suppress  the middleware's functionality, let's make sure it actually works...
No related name is needed here, since symmetrical relations are not  explicitly reversible.
Regression for 11956 -- a many to many to the base class
A related_name is required on one of the ManyToManyField entries here because  they are both addressable as reverse relations from Tag.
Two models both inheriting from a base model with a self-referential m2m field
Many-to-Many relation between models, where one of the PK's isn't an Autofield
Regression for 11226 -- A model with the same name that another one to  which it has a m2m relation. This shouldn't cause a name clash between  the automatically created m2m intermediary table FK field names when  running migrate
Regression for 24505 -- Two ManyToManyFields with the same "to" model  and related_name set to '+'.
Get same manager twice in a row:
Get same manager for different instances
Regression for 19236 - an abstract class with a 'split' method  causes a TypeError in add_lazy_relation
Test that the deferred class does not remember that gender was  set, instead the instance should remember this.
Save is skipped.  Signals were skipped, too...
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Getting a single item should work too:
This final query should only have seven tables (port, device and building  twice each, plus connection once). Thus, 6 joins plus the FROM table.
Still works if we defer an attribute on the inherited class
Also works if you use only, rather than defer
The select_related join wasn't promoted as there was already an  existing (even if trimmed) inner join to state.
The select_related join was promoted as there is already an  existing join.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
We can also do the above query using UTF-8 strings.
Test that the searches do not match the subnet mask (/32 in this case)
Regression test for 7530, 7716.
Each of these individually should return the item.
Regression test for 15823.
models for test_q_object_or:
For testing 13085 fix, we also use Note model defined above
Fails with another, ORM-level error
__nonzero__() returns False -- This actually doesn't currently fail.  This test validates that
Saving model with GenericForeignKey to model instance with an  empty CharField PK
deleting the Related cascades to the Content cascades to the Node,  where the pre_delete signal should fire and prevent deletion.
see 9258
-*- coding: utf-8 -*-
Override any settings on the model admin
Construct the admin, and ask it for a formfield
"unwrap" the widget wrapper, if needed
Return the formfield so that other tests can continue
This should result in an error message, not a server exception.
FK to a model not registered with admin site. Raw ID widget should  have no magnifying glass link. See 16542
see 9258
M2M relationship with model not registered with admin site. Raw ID  widget should have no magnifying glass link. See 16542
Used to fail with a name error.
Open a page that has a date and time picker widgets
Open a page that has a date and time picker widgets
fill in the birth date.
Click the calendar icon
get all the tds within the calendar
make sure the first and last 6 cells have class nonday
Open a page that has a date and time picker widgets
fill in the birth date.
Click the calendar icon
get all the tds within the calendar
verify the selected cell
Open a page that has a date and time picker widgets
Click the calendar icon
get all the tds within the calendar
verify there are no cells with the selected class
Enter test data
Get the expected caption
Test with every locale
Click on the "today" and "now" shortcuts.
Check that there is a time zone mismatch warning.  Warning: This would effectively fail if the TIME_ZONE defined in the  settings has the same UTC offset as "Asia/Singapore" because the  mismatch warning would be rightfully missing from the page.
Submit the form.
Make sure that "now" in javascript is within 10 seconds  from "now" on the server side.
The above tests run with Asia/Singapore which are on the positive side of  UTC. Here we test with a timezone on the negative side.
Choose some options
Check the title attribute is there for tool tips: ticket 20821
Check the tooltip is still there after moving: ticket 20821
Choose some more options
Unselect the options
Choose some more options
Unselect the options
Pressing buttons shouldn't change the URL.
Save and check that everything is properly stored in the database
Save and check that everything is properly stored in the database
self.selenium.refresh() or send_keys(Keys.F5) does hard reload and  doesn't replicate what happens when a user clicks the browser's  'Refresh' button.
No value has been selected yet
The field now contains the selected band's id
The field now contains the other selected band's id
No value has been selected yet
Help text for the field is displayed
The field now contains the selected band's id
The field now contains the two selected bands' ids
The field now contains the new user
Click the Change User button to change it
-*- coding: utf8 -*-
Test for ticket 12059: TimeField wrongly handling datetime.datetime object.
MySQL backend does not support timezone-aware datetimes.
Verify we didn't break DateTimeField behavior  We need to test this way because datetime.datetime inherits  from datetime.date:
If the value for the field doesn't correspond to a valid choice,  the value itself is provided as a display value.
Although test runner calls migrate for several databases,  testing for only one of them is quite sufficient.  we need to test only one call of migrate
The migration isn't applied backward.
-*- coding: utf-8 -*-
Call the "real" save() method
Call the "real" delete() method
Content-object field
Assume business logic forces every person to have at least one house.
Without the ValueError, an author was deleted due to the implicit  save of the relation assignment.
Without the ValueError, a book was deleted due to the implicit  save of reverse relation assignment.
Not ambiguous.
Control lookups.
Control lookups.
Control lookups.
Control lookups.
1 for TaggedItem table, 1 for Book table, 1 for Reader table
1 for Comment table, 1 for Book table
The custom queryset filters should be applied to the queryset  instance returned by the manager.
Regression for 18090: the prefetching query must include an IN clause.  Note that on Oracle the table name is upper case in the generated SQL,  thus the .lower() call.
The following two queries must be done in the same order as written,  otherwise 'primary_house' will cause non-prefetched lookups
parent link
Set main_room for each house before creating the next one for  databases where supports_nullable_unique_constraints is False.
Article objects have access to their related Reporter objects.
Ensure that querysets used in reverse FK assignments are pre-evaluated  so their value isn't affected by the clearing operation in  RelatedManager.set() (19816).
QuerySet with an __init__() method that takes an additional argument.
Public methods are copied  Private methods are not copied
Methods with queryset_only=False are copied even if they are private.  Methods with queryset_only=True aren't copied even if they are public.
Each model class gets a "_default_manager" attribute, which is a  reference to the first manager defined in the class.
If the hidden object wasn't seen during the save process,  there would now be two objects in the database.
All of the RestrictedModel instances should have been  deleted, since they *all* pointed to the RelatedModel. If  the default manager is used, only the public one will be  deleted.
NB: be careful to delete any sessions created; stale sessions fill up  the /tmp (with some backends) and eventually overwhelm it after lots  of runs (think buildbots)
Need to reset these to pretend we haven't accessed it:
Custom session expiry  A normal session has a max age equal to settings
So does a custom session with an idle expiration time of 0 (but it'll  expire at browser close)
Mock timezone.now, because set_expiry calls it on this code path.
Tests get_expire_at_browser_close with different settings and different  set_expiry calls
check that the failed decode is logged
this doesn't work with JSONSerializer (serializing timedelta)
provided unknown key was cycled, not reused
Create new session.
Logout in another context.
Modify session in first context.  This should throw an exception as the session is deleted, not  resurrect the session.
Create a session
One object in the future
One object in the past
Set the account ID to be picked up by a custom session storage  and saved to a custom session model database column.
Make sure that the customized create_model_instance() was called.
Make the session "anonymous".
Make sure that save() on an existing session did the right job.
21000 - CacheDB backend should respect SESSION_CACHE_ALIAS.
Don't need DB flushing for these tests, so can use unittest.TestCase as base class
Make sure the file backend checks for a good storage dir
Ensure we don't allow directory-traversal.  This is tested directly on _key_to_file, as load() will swallow  a SuspiciousOperation in the same way as an IOError - by creating  a new session, making it unclear whether the slashes were detected.
Ensure we don't allow directory-traversal
One object in the future
One object in the past
One object in the present without an expiry (should be deleted since  its modification time + SESSION_COOKIE_AGE will be in the past when  clearsessions runs).
Three sessions are in the filesystem before clearsessions...  ... and two are deleted.
Re-initialize the session backend to make use of overridden settings.
Simulate a request the modifies the session
Handle the response through the middleware
Simulate a request the modifies the session
Simulate a request the modifies the session
Handle the response through the middleware
Simulate a request the modifies the session
Handle the response through the middleware
Check that the value wasn't saved above.
Handle the response through the middleware. It will try to save the  deleted session which will cause an UpdateError that's caught and  results in a redirect to the original page.
Check that the response is a redirect.
Before deleting, there has to be an existing cookie
Simulate a request that ends the session
Handle the response through the middleware
Before deleting, there has to be an existing cookie
Simulate a request that ends the session
Handle the response through the middleware
Simulate a request that ends the session
Handle the response through the middleware
A cookie should not be set.  The session is accessed so "Vary: Cookie" should be set.
Don't need DB flushing for these tests, so can use unittest.TestCase as base class
The cookie backend doesn't handle non-default expiry dates, see 19201
signed_cookies backend should handle unpickle exceptions gracefully  by creating a new session
Check each of the allowed method names
Check the case view argument is ok if predefined on the class...  ...but raises errors otherwise.
Let the cache expire and test again
we can't use self.rf.get because it always sets QUERY_STRING
the test_name key is inserted by the test classes parent
test that kwarg overrides values assigned higher up
Checks 'pony' key presence in dict returned by get_context_date
Checks 'object' key presence in dict returned by get_context_date 20234
Don't pass queryset as argument
Overwrite the view's queryset with queryset from kwarg
-*- coding: utf-8 -*-
Create/UpdateView
Useful for testing redirects
Ensures get_context_object_name() doesn't reference self.object.
Dummy object, but attr is required by get_template_name()
-*- coding: utf-8 -*-
Custom pagination allows for 2 orphans on a page size of 5
Custom pagination allows for 2 orphans on a page size of 5
-*- coding: utf-8 -*-
Regression test for 18354
Since allow_empty=False, next/prev years must be valid (7164)
Regression test for 18354
Since allow_empty=False, next/prev months must be valid (7164)
allow_empty = False, empty month
allow_future = False, future month
Since allow_future = True but not allow_empty, next/prev are not  allowed to be empty months (7164)
Since allow_empty=False, next/prev weeks must be valid
allow_empty = False, empty week
Since allow_future = True but not allow_empty, next/prev are not  allowed to be empty weeks
Since allow_empty=False, next/prev days must be valid.
allow_empty = False, empty month
allow_future = False, future month
allow_future but not allow_empty, next/prev must be valid
Should raise exception -- No redirect URL provided, and no  get_absolute_url provided
-*- coding: utf-8 -*-
don't use the manager because we want to ensure the site exists  with pk=1, regardless of whether or not it already exists.
no 'django.contrib.flatpages.middleware.FlatpageFallbackMiddleware'
no 'django.contrib.flatpages.middleware.FlatpageFallbackMiddleware'
This cleanup is necessary because contrib.sites cache  makes tests interfere with each other, see 11505
A package that raises an ImportError that can be shared among test apps and  excluded from test discovery.
M2M described on one of the models
field order is deliberately inverted. the target field is "invitee".
Since this isn't a symmetrical relation, Tony's friend link still exists.
Truncate the first character so that the hash is invalid.
Set initial data.  Test that the message actually contains what we expect.
Test deletion of the cookie (storing with an empty value) after the messages have been consumed
Set initial (invalid) data.  Test that the message actually contains what we expect.
Remove the is_safedata flag from the messages in order to imitate  the behavior of before 1.5 (monkey patching).
Decode the messages in the old format (without is_safedata)
Set initial data.  Test that the message actually contains what we expect.
Set initial cookie data.
Overwrite the _get method of the fallback storage to prove it is not  used (it would cause a TypeError: 'NoneType' object is not callable).
Test that the message actually contains what we expect.
Overwrite the _get method of the fallback storage to prove it is not  used (it would cause a TypeError: 'NoneType' object is not callable).
Test that the message actually contains what we expect.
Set initial cookie and session data.
Test that the message actually contains what we expect.
Set initial cookie and session data.
Test that the message actually contains what we expect.
Set initial cookie and session data.
When updating, previously used but no longer needed backends are  flushed.
Overwrite the _store method of the fallback storage to prove it isn't  used (it would cause a TypeError: 'NoneType' object is not callable).
LEVEL_TAGS is a constant defined in the  django.contrib.messages.storage.base module, so after changing  settings.MESSAGE_TAGS, we need to update that constant too.
there shouldn't be any messages on second GET request
After iterating the storage engine directly, the used flag is set.  The data does not disappear because it has been iterated.
get_level works even with no storage on the request.
get_level returns the default level if it hasn't been set.
Only messages of sufficient level get recorded.
mixed Text and Char
mixed Text and Char wrapped
Check nodes counts
Multiple compilations should not alter the generated query.
if alias is null, set to first 5 lower characters of the name
Convert to target timezone before truncation
otherwise, truncate to year
If there was a daylight saving transition, then reset the timezone.
Load fixture1 which has a site, two articles, and a category
Load fixture 4 (compressed), using format specification
Load back in fixture 1, we need the articles from it
object list is unaffected
! -*- coding: utf-8 -*-
encodestring is a deprecated alias on Python 3
The second value is normalized to an empty name by  MultiPartParser.IE_sanitize()
A small file (under the 5M quota)
A big file (over the quota)
AttributeError: You cannot alter upload handlers after the upload has been processed.
Check that the files got actually parsed.
Check that the fd closing logic doesn't trigger parsing of the stream
install the custom handler that tries to access request.POST
CustomUploadError is the error that should have been raised
We're not actually parsing here; just checking if the parser properly  instantiates with empty upload handlers.
If a file is posted, the dummy client should only post the file name,  not the full path.
Adding large file to the database should succeed
Check to see if unicode name came through properly.
Cleanup the object with its exotic file name immediately.  (shutil.rmtree used elsewhere in the tests to clean up the  upload directory has been seen to choke on unicode  filenames on Windows.)
Check that the no template case doesn't mess with the template assertions
Should redirect to get_view
The redirect target responds with a 301 code, not 200
The redirect target responds with a 301 code, not 200
The chain of redirects stops once the cycle is detected.
The chain of redirects will get back to the starting point, but stop there.
Create a second client, and log in.
Get a redirection page with the second client.
At this points, the self.client isn't logged in.  Check that assertRedirects uses the original client, not the  default client.
We need two different tests to check URLconf substitution -  one to check  it was changed, and another one (without self.urls) to check it was reverted on  teardown. This pair of tests relies upon the alphabetical ordering of test execution.
This test needs to run *after* UrlconfSubstitutionTests; the zz prefix in the  name is to ensure alphabetical ordering.
The session doesn't exist to start.
This request sets a session variable.
Check that the session has been modified
Log in
Session should still contain the modified value
A HEAD request doesn't return any content.
A GET-like request can pass a query string as part of the URL
Data provided in the URL to a GET-like request is overridden by actual form data
apart from the next line the three tests are identical
This test is executed after the previous one
Special attribute that won't be present on a plain HttpRequest
There is a bug in sqlite < 3.7.0, where placeholder order is lost.  Thus, the above query returns  <condition_value> + <result_value>  for each matching case instead of <result_value> + 1 (24148).
fails on sqlite if output_field is not set explicitly on all  Values containing timedeltas
fails on sqlite if output_field is not set explicitly on all  Values containing times
Test related objects visibility.
Add a Waiter to the Restaurant.
One-to-one fields still work if you create your own primary key
place should not cache restaurant
The bug in 9023: if you access the one-to-one relation *before*  setting to None and deleting, the cascade happens anyway.
Assigning None doesn't throw AttributeError if there isn't a related  UndergroundBar.
Look up the objects again so that we get "fresh" objects
Accessing the related object again returns the exactly same object
But if we kill the cache, we get a new object
Reassigning the Restaurant object results in an immediate cache update  We can't use a new Restaurant because that'll violate one-to-one, but  with a new *instance* the is test below will fail if 6886 regresses.
Assigning None succeeds if field is null=True.
Assigning None will not fail: Place.restaurant is null=False
You also can't assign an object of the wrong type here
Creation using keyword argument should cache the related object.
Creation using keyword argument and unsaved related instance (8070).
Use a fresh object without caches
Use a fresh object without caches
When there's no instance of the origin of the one-to-one
When there's one instance of the origin  (p.undergroundbar used to return that instance)
Several instances of the origin are only possible if database allows  inserting multiple NULL rows for a unique constraint
When there are several instances of the origin
Assigning a reverse relation on an unsaved object is allowed.
Only one school is available via all() due to the custom default manager.
Only one director is available via all() due to the custom default manager.
Make sure the base manager is used so that the related objects  is still accessible even if the default manager doesn't normally  allow it.
Make sure the base manager is used so that an student can still access  its related school even if the default manager doesn't normally  allow it.
The exception raised on attribute access when a related object  doesn't exist should be an instance of a subclass of `AttributeError`  refs 21563
'managed' is True by default. This tests we can set it explicitly.
To re-use the many-to-many intermediate table, we need to manually set up  things up.
Firstly, we need some models that will create the tables, purely so that the  tables are created. This is a test setup, not a requirement for unmanaged  models.
Unmanaged with an m2m to unmanaged: the intermediary table won't be created.
Here's an unmanaged model with an m2m to a managed one; the intermediary  table *will* be created (unless given a custom `through` as for C02 above).
... and pull it out via the other set.
-*- coding: utf-8 -*-
Default views
a view that raises an exception for the debug view
Static views
-*- coding: utf-8 -*-
-*- coding:utf-8 -*-
Special URLs for particular regression cases.
json response
Make sure that a callable that raises an exception in the stack frame's  local vars won't hijack the technical 500 response. See:  http://code.djangoproject.com/ticket/15025
We need to inspect the HTML generated by the fancy 500 debug view but  the test client ignores it, so we send it explicitly.
We need to inspect the HTML generated by the fancy 500 debug view but  the test client ignores it, so we send it explicitly.
If we do not specify a template, we need to make sure the debug  view doesn't blow up.
Make sure datetime and Decimal objects would be serialized properly
-*- coding:utf-8 -*-
Force a language via GET otherwise the gettext functions are a noop!
The test cases use fixtures & translations from these apps.
-*- coding:utf-8 -*-
The url() & view must exist for this to work as a regression test.
And reverse
And reverse
Force a language via GET otherwise the gettext functions are a noop!
The test cases use fixtures & translations from these apps.
strip() to prevent OS line endings from causing differences
This is 24h before max Unix time. Remember to fix Django and  update this test well before 2038 :)
-*- coding: utf-8 -*-  This coding header is significant for tests, as the debug view is parsing  files to search for such a header to decode the source file content
Ensure that when DEBUG=True, technical_500_template() is called.
May need a query to initialize MySQL connection
No template directories are configured, so no templates will be found.
Ensure that when DEBUG=True, technical_500_template() is called.
All POST parameters are shown.
All POST parameters' names are shown.  Non-sensitive POST parameters' values are shown.
Sensitive POST parameters' values are not shown.
All POST parameters' names are shown.  No POST parameters' values are shown.
All POST parameters are shown.
All POST parameters' names are shown.  Non-sensitive POST parameters' values are shown.
Custom exception handler, just pass it into ExceptionReporter
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
This is the same as in the default project template
(False and False) or True == True   <- we want this one, like Python  False and (False or True) == False
True or (False and False) == True   <- we want this one, like Python  (True or False) and False == False
(1 or 1) == 2  -> False  1 or (1 == 2)  -> True   <- we want this one
-*- coding: utf-8 -*-
Test that the decorators preserve the decorated function's docstring, name and attributes.
A test middleware that installs a temporary URLConf
explicit baking
response is not re-rendered without the render call
rebaking doesn't change the rendered content
but rendered content can be overridden by manually  setting content
unrendered response raises an exception on iteration
iteration works for rendered responses
unrendered response raises an exception when content is accessed
rendered response content can be accessed
When the content is rendered, all the callbacks are invoked, too.
But if we render the response, we can pickle it.
...and the unpickled response doesn't have the  template-related attributes, so it can't be re-rendered
...and requesting any of those attributes raises an exception
But if we render the response, we can pickle it.
...and the unpickled response doesn't have the  template-related attributes, so it can't be re-rendered
...and requesting any of those attributes raises an exception
Let the cache expire and test again
Let the cache expire and test again
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Fake views for testing url reverse lookup
coding: utf-8
Failures
A single equals sign is a syntax error.
For this to act as a regression test, it's important not to use  foo=True because True is (not None)
Raise exception if we don't have 3 args, last one an integer
coding: utf-8
coding: utf-8
Inherit from a template with block wrapped in an {% if %} tag  (in parent), still gets overridden
The super block will still be found.
Ignore numpy deprecation warnings (23890)
Raise TemplateSyntaxError when trying to access a variable  containing an illegal character.
Filtered variables should reject access of attributes beginning with  underscores.
Variables should reject access of attributes beginning with  underscores.
Variables should raise on non string type
Correct number of arguments  One optional  Not supplying all
Render another path that uses the same templates from the cache
Render a second time from cache
We can't access ``my_doodad.value`` in the template, because  ``my_doodad.__call__`` will be invoked first, yielding a dictionary  without a key ``value``.
We can confirm that the doodad has been called
Double-check that the object was really never called during the  template rendering.
Double-check that the object was really never called during the  template rendering.
Double-check that the object was really never called during the  template rendering.
-*- coding: utf-8 -*-
Run a second time from cache
fill the template cache
The two templates should not have the same content
UTF-8 bytestrings are permitted.  Unicode strings are permitted.
when testing deprecation warnings, it's useful to run just one test since  the message won't be displayed multiple times
numerous tests make use of an inclusion tag  add this in here for simplicity
Make Engine.get_default() raise an exception to ensure that tests  are properly isolated from Django's global settings.  Set up custom template tag libraries if specified
These two classes are used to test auto-escaping of unicode output.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
lowercase e umlaut
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
The above test fails because of Python 2's float handling. Floats  with many zeroes after the decimal point should be passed in as  another type such as unicode or Decimal.
-*- coding: utf-8 -*-
uppercase E umlaut
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Test that push() limits access to the topmost dict
update only a
update both to check regression
make contexts equals again
The stack should now contain 3 items:  [builtins, supplied context, context processor, empty dict]
Create an engine without any context processors.
test comparing RequestContext to prevent problems if somebody  adds __eq__ in the future
View returning a template response
A view that can be hard to find...
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Scene/Character/Line models are used to test full text search. They're  populated with content from Monty Python and the Holy Grail.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Ensure CreateExtension quotes extension names by creating one with a  dash in its name.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
This checks that get_prep_value is deferred properly
Regression for 22907
The inner CharField is missing a max_length.
The inner CharField is missing a max_length.
This should not raise a validation error
A OneToOneField is just a ForeignKey unique=True, so we don't duplicate  all the tests; just one smoke test to ensure on_delete works for it as  well.
This model is used to test a duplicate query regression (25685)
Testing DO_NOTHING is a bit harder: It would raise IntegrityError for a normal model,  so we connect to pre_delete and set the fk to a known value.
RelToBase should not be queried.
1 (select related `T` instances)  + 1 (select related `U` instances)  + 2 (delete `T` instances in batches)  + 1 (delete `s`)
Attach a signal to make sure we will not do fast_deletes.
Attach a signal to make sure we will not do fast_deletes.
TEST_SIZE // batch_size (select related `T` instances)  + 1 (select related `U` instances)  + TEST_SIZE // GET_ITERATOR_CHUNK_SIZE (delete `T` instances in batches)  + 1 (delete `s`)
One query for the Avatar table and a second for the User one.
1 query to fast-delete the user  1 query to delete the avatar
1 to delete f, 1 to fast-delete m2m for f
1 to delete t, 1 to fast-delete t's m_set
We should have the debug flag in the template.
And now we should not
Check we have not actually memoized connection.queries  Check queries for DB connection 'other'
F expressions cannot be used to update attributes which are foreign  keys, or attributes which involve joins.
Test that reverse multijoin F() references and the lookup target  the same join. Pre 18375 the F() join was generated first, and the  lookup couldn't reuse that join.
Reuse the same F-object for another queryset  The original query still works correctly
LH Addition of floats and integers
LH Subtraction of floats and integers
Multiplication of floats and integers
LH Division of floats and integers
LH Modulo arithmetic on integers
LH Bitwise ands on integers
LH Bitwise or on integers
Right hand operators
RH Multiplication of floats and integers
RH Division of floats and integers
RH Modulo arithmetic on integers
Test data is set so that deltas and delays will be  strictly increasing.
Ticket 21643 - Crash when compiling query more than once  Intentionally no assert
Simulate http.server.BaseHTTPRequestHandler.parse_request handling of raw request
On Python 3, %E9 is converted to the unicode replacement character by parse_qsl
If would be nicer if request.COOKIES returned unicode values.  However the current cookie parser doesn't do this and fixing it is  much more work than fixing 20557. Feel free to remove force_str()!
We don't test COOKIES content, as the result might differ between  Python version because parsing invalid content became stricter in  latest versions.
Expect "bad request" response
Regression test for 23173  Test first without PATH_INFO
This method intentionally doesn't work for all cases - part  of the test for ticket 20278
Save it into the database. You have to call save() explicitly.
You can also mix and match position and keyword arguments, but  be sure not to duplicate field information.
You can use 'in' to test for membership...  ... but there will often be more efficient ways if that is all you need:
Can't be instantiated
A hacky test for custom QuerySet subclass - refs 17271
Value based on PK  No PK value -> unhashable (because save() would then change  hash)
Create an Article.  Save it into the database. You have to call save() explicitly.
Change values by changing the attributes, then calling save().
Article.objects.all() returns all the articles in the database.
Lookup by a primary key is the most common case, so Django  provides a shortcut for primary-key exact lookups.  The following is identical to articles.get(id=a.id).
pk can be used as a shortcut for the primary key name in any query.
Model instances of the same type and same ID are considered equal.
Create a very similar object
Do not delete a directly - doing so alters its state.
Make sure the _update method below is in fact called.
This is not wanted behavior, but this is how Django has always  behaved for databases that do not return correct information  about matched rows for UPDATE.
MySQL < 5.6.4 removes microseconds from the datetimes which can cause  problems when comparing the original value to that loaded from DB
The old related instance was thrown away (the selfref_id has  changed). It needs to be reloaded on access, so one query  executed.
Regression test for 12560
Regression test for 12560
Regression test for 12132
Should work without errors
Should work without errors
-*- encoding: utf-8 -*-
Mock out get_wsgi_application so we know its return value is used
This is just to test finding, it doesn't have to be a real WSGI callable
Use a name that avoids collision with the built-in year lookup.
mult3__div3 always leads to 0
Same as age >= average_rating
The non-optimized version works, too.
Use a name that avoids collision with the built-in year lookup.
equals now.replace(tzinfo=utc)
Regression for 17414
As 24h of difference they will never be the same
Regression for 18504  This is 2012-03-08HT19:30:00-06:00 in America/Chicago
Because of the DST change, 2 days and 6 hours after the chosen  date in naive arithmetic is only 2 days and 5 hours after in  aware arithmetic.
-*- coding: utf-8 -*-
Attach to parent as a string
Verify that the child message header is not base64 encoded
Verify that the child message header is not base64 encoded
Verify that the child message header is not base64 encoded
Ticket 18861 - Validate emails when using the locmem backend
ignore decode error in SSL/TLS connection tests as we only care  whether the connection attempt was made
Find the actual message
Ensure that the message only contains CRLF and not combinations of CRLF, LF, and CR.
Messages are stored in an instance variable for testing.
-*- coding: utf-8 -*-
no content_type field
missing object_id field
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
At this point, a lookup for a ContentType should hit the DB
Once we clear the cache, another lookup will again hit the DB
Make sure stale ContentTypes can be fetched like any other object.  Before Django 1.6 this caused a NoneType error in the caching mechanism.  Instead, just return the ContentType object and let the app detect stale states.
View docstring
Overridden because non-trivial TEMPLATES settings aren't supported  but the page shouldn't crash (24125).
Overridden because non-trivial TEMPLATES settings aren't supported  but the page shouldn't crash (24125).
method docstrings
foreign keys
foreign keys with help text
-*- encoding: utf-8 -*-
The 'complex_filter' method supports framework features such as  'limit_choices_to' which normally take a single dictionary of lookup  arguments but need to support arbitrary queries via Q objects too.
Try some arg queries with operations other than filter.
Can't run this test under SQLite, because you can't  get two connections to an in-memory database.
Create a second connection to the default database
Close down the second connection.
Start a transaction on the main connection.
Delete something using another database connection.
In the same transaction on the main connection, perform a  queryset delete that covers the object deleted with the other  connection. This causes an infinite loop under MySQL InnoDB  unless we keep track of already deleted objects.
first two asserts are just sanity checks, this is the kicker:
first two asserts just sanity checks, this is the kicker:
Create an Image
Get the Image instance as a File
An Image deletion == File deletion
The Image deletion cascaded and *all* references to it are deleted.
Get the Image as a Photo
A File deletion == Image deletion
The File deletion should have cascaded and deleted *all* references  to it.
Create an Image (proxy of File) and FooFileProxy (proxy of FooFile,  which has an FK to File)
microseconds are lost during a round-trip in the database
interpret the naive datetime in local time to get the correct value
interpret the naive datetime in local time to get the correct value
interpret the naive datetime in local time to get the correct value
interpret the naive datetime in local time to get the correct value
naive datetimes are interpreted in local time
naive datetimes are interpreted in local time
microseconds are lost during a round-trip in the database  naive datetimes are interpreted in local time
microseconds are lost during a round-trip in the database
Regression test for 17294
Clear cached properties, after first accessing them to ensure they exist.
Clear cached properties, after first accessing them to ensure they exist.
Depending on the yaml dumper, '!timestamp' might be absent
Transform a list of keys in 'datetimes' to the expected template  output. This makes the definition of 'results' more readable.
the actual value depends on the system time zone of the host
this is obviously a bug
this is obviously a bug
Datetime inputs formats don't allow providing a time zone.
-*- coding: utf-8 -*-
Use a utf-8 bytestring to ensure it works (see 11710)
we replicate Color to register with another ModelAdmin
a base class for Recommender and Recommendation
Proxy model to test overridden fields attrs on Post model so as not to  interfere with other tests.
`db_index=False` because MySQL cannot index large CharField (21196).
Models for 23329
Models for 23431
Models for 23604 and 23915
Model for 23839  Don't point any FK at this model.
Models for 23934
Models for 25622
-*- coding: utf-8 -*-
inline data
started with 3 articles, one was deleted.
Test we can override with query string
Should have 5 columns (including action checkbox col)
Check order
Check sorting - should be by name
Spanning relationships through a nonexistent related object (Refs 16716)
Regression test for 18530
Filters should be allowed if they involve a local field without the  need to whitelist them in list_filter or date_hierarchy.
23839 - Primary key should always be allowed, even if the referenced model isn't registered.
23915 - Specifying a field referenced by another model though a m2m should be allowed.
23604, 23915 - Specifying a field referenced through a reverse m2m relationship should be allowed.
23329 - Specifying a field that is not referred by any other model directly registered  to this admin site but registered through inheritance should be allowed.
23431 - Specifying a field that is only referred to by a inline of a registered  model should be allowed.
Filters should be allowed if they are defined on a ForeignKey pointing to this model
Check the format of the shown object -- shouldn't contain a change link
Test custom add form template
this would be the usual behaviour  this is the overridden behaviour
Setup permissions, for our users who can add, change, and delete.
Establish a valid admin session
Logging in with non-admin user fails
Establish a valid admin session
8509 - if a normal user is already logged in, it is possible  to change user into the superuser without error  Check and make sure that if user expires, data still persists  make sure the view removes test cookie
Not logged in: we should see the login page.
we shouldn't get a 500 error caused by a NoReverseMatch
Ticket 12707: Saving inline editable should not show admin  action warnings
test a filtered page
test a searched page
Same data as above: Forbidden because of unique_together!
Same data as above: Forbidden because of unique_together!
Same data also.
test if non-form errors are handled; ticket 12716
Ensure that the form processing understands this as a list_editable "Save"  and not an action "Go".
test if non-form errors are correctly handled; ticket 12878
NB: The order values must be changed so that the items are reordered.
Ensure that the form processing understands this as a list_editable "Save"  and not an action "Go".
Successful post will redirect
List editable changes should not be executed if the action "Go" button is  used to submit the form.
List editable changes should be executed if the "Save" button is  used to submit the form - any action choices should be ignored.
confirm the search returned 1 object
confirm the search returned one object
confirm the search returned zero objects
confirm the search returned one object
confirm the search returned zero objects
confirm the search returned one object
confirm the search returned one object
make sure we have no duplicate HTML names
No 500 caused by NoReverseMatch  The page shouldn't display a link to the nonexistent change page
Two different actions selected on the two forms...  ...but we clicked "go" on the top form.
Send mail, don't delete.
Insert some data
create 2 Person objects
Check that the PK link exists on the rendered form
Check that the PK link exists on the rendered form
Check that the PK link exists on the rendered form
Check that the PK link exists on the rendered form
Check that the PK link exists on the rendered form
NB: The order values must be changed so that the items are reordered.
Successful post will redirect
Save the object
Checking readonly field.  Checking readonly field in inline.
The reverse relation also works if the OneToOneField is null.
The allowed option should appear twice; the limited option should not appear.
Find the link
Find the link
Find the link
Don't depend on a warm cache, see 17377.
Ensure no queries are skipped due to cached content type for Group.
The tabular inline
The builtin tag group exists
A builtin tag exists in both the index and detail
An app tag exists in both the index and detail
The admin list tag group exists
An admin list tag exists in both the index and detail
The builtin filter group exists
A builtin filter exists in both the index and detail
no day-level links
no day/month-level links
Test inequality.
Ignore scheme and host.
Ignore ordering of _changelist_filters.
Get the `change_view`.
Get the `add_view`.
just verifying the parent form failed validation, as expected --  this isn't the regression test
just verifying the parent form failed validation, as expected --  this isn't the regression test
Restore the original values for the benefit of other tests.
we have registered two models from two different apps
-*- coding: utf-8 -*-
A method with the same name as a reverse accessor.
Order by a field that isn't in list display, to be able to test  whether ordering is preserved.
Corner case: Don't call parent implementation
Disable change_view, but leave other urls untouched
Register core models we need in our tests
A custom index view.
used for testing password change on a user not in queryset
This cleanup is necessary because contrib.sites cache  makes tests interfere with each other, see 11505
Making sure there's only one `channel` element w/in the  `rss` element.
Find the last build date
Ensure the content of the channel is correct
Check feed_url is passed
Find the pubdate of the first feed item
Assert that <guid> does not have any 'isPermaLink' attribute
Check feed_url is passed
this feed has a `published` element with the latest date
this feed has an `updated` element with the latest date
Naive date times passed in get converted to the local time zone, so  check the received zone offset against the local offset.
No last-modified when feed has no item_pubdate
Defining a template overrides any item_title definition
Provide a weird offset so that the test can know it's getting this  specific offset and not accidentally getting on from  settings.TIME_ZONE.
Testing choices= Iterable of Iterables    See: https://code.djangoproject.com/ticket/20430
Models created as unmanaged as these aren't ever queried
Models created as unmanaged as these aren't ever queried
Models created as unmanaged as these aren't ever queried
All our models should validate properly  Validation Tests:    * choices= Iterable of Iterables        See: https://code.djangoproject.com/ticket/20430    * related_name='+' doesn't clash with another '+'        See: https://code.djangoproject.com/ticket/21375
Oracle can have problems with a column named "date"
Add custom method with allow_tags attribute
There's only one Group instance
There's only one Group instance
There's only one Concert instance
There's only one Quartet instance
There's only one ChordsBand instance
Two children with the same name
Make sure distinct() was called
Make sure distinct() was called
There's only one Concert instance
Test with user 'noparents'
Test with user 'parents'
Add "show all" parameter to request
When no order is defined at all, everything is ordered by '-pk'.
When an order field is defined but multiple records have the same  value for that field, make sure everything gets ordered by -pk as well.
When no order is defined at all, use the model's default ordering (i.e. 'number')
When an order field is defined but multiple records have the same  value for that field, make sure everything gets ordered by -pk as well.
assuming we have exactly `objects_count` objects
setting page number and calculating page range
The "Add" button inside the object-tools shouldn't appear.
Rendering should be u'' since this templatetag just logs,  it doesn't render any string.
Test amount of rows in the Changelist
Test current selection
Create an object for sitemap content.
This cleanup is necessary because contrib.sites cache  makes tests interfere with each other, see 11505
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Load a fixture that doesn't exist
The master app registry is always ready when the tests run.  Non-master app registries are populated in __init__.
App label is case-sensitive, Model name is case-insensitive.
LazyModelA shouldn't be waited on since it's already registered,  and LazyModelC shouldn't be waited on until LazyModelB exists.
Test that multiple operations can wait on the same model
Now we are just waiting on LazyModelC.
Everything should be loaded - make sure the callback was executed properly.
Temporarily add two directories to sys.path that both contain  components of the "nsapp" package.
Avoid having two "id" fields in the Child1 subclass
Check for ticket 13839
An explicit link to the parent (we can control the attribute name).
Since Student does not subclass CommonInfo's Meta, it has the effect  of completely overriding it. So ordering by name doesn't take place  for Students.
However, the CommonInfo class cannot be used as a normal model (it  doesn't exist as a model).
Even though p.supplier for a Place 'p' (a parent of a Supplier), a  Restaurant object cannot access that reverse relation, since it's not  part of the Place-Supplier Hierarchy.
The Post model doesn't have an attribute called  'attached_%(class)s_set'.
The Post model doesn't have a related query accessor based on  related_name (attached_comment_set).
Low-level test for related_val  Higher level test for correct query values (title foof not  accidentally found).
Equality doesn't transfer in multitable inheritance.
Filters against the parent model return objects of the parent's type.
This won't work because the Demon Dogs restaurant is not an Italian  restaurant.
An ItalianRestaurant which does not exist is also a Place which does  not exist.
MultipleObjectsReturned is also inherited.
This won't work because the Place we select is not a Restaurant (it's  a Supplier).
Test that the field was actually deferred
The correct level gets the message.
Incorrect levels shouldn't have any messages.
WSGIRequestHandler closes the output file; we need to make this a  no-op so we can still read its contents.
We don't need to check stderr, but we don't want it in test output  instantiating a handler runs the request as side effect
Backup original environment variable
Just the host is not accepted
The host must be valid
Restore original environment variable
put it in a list to prevent descriptor lookups in test
skip it, as setUpClass doesn't call its parent either
We're out of ports, LiveServerTestCase correctly fails with  a socket error.  Unexpected error.
We've acquired a port, ensure our server threads acquired  different addresses.
See ticket 16570.
See ticket 18389.
Set up a temp directory for file storage.
-*- coding: utf-8 -*-
When an extra clause exists, the boolean conversions are applied with  an offset (13293).
If the field has a relation, there should be only one of the  4 cardinality flags available.
Test classes are what we expect
Ensure all m2m reverses are m2m
Test classes are what we expect
Ensure all o2m reverses are m2o
Test classes are what we expect
Ensure all m2o reverses are o2m
Test classes are what we expect
Ensure all o2o reverses are o2o
null isn't well defined for a ManyToManyField, but changing it to  True causes backwards compatibility problems (25320).
Pillow not available, create dummy classes (tests will be skipped anyway)
Person model to use for tests.  File class to use for file instances.
Get a "clean" model instance  It won't have an opened file.
After asking for the size, the file should still be closed.
TestImageField value will default to being an instance of its  attr_class, a  TestImageFieldFile, with name == None, which will  cause it to evaluate as False.
Test setting a fresh created model instance.
If image assigned to None, dimension fields should be cleared.
A new file should update dimensions.
Field and dimensions should be cleared after a delete.
Dimensions should get set if file is saved.
If we assign a new image to the instance, the dimensions should  update.  Dimensions were recalculated, and hence file should have opened.
exercises ForeignKey.get_db_prep_value()
Need a TransactionTestCase to avoid deferring FK constraint checking.
This should not crash.
Other model with different datetime.
-*- coding:utf-8 -*-
To test fix for 11263
To test fix for 7551
Original list of tags:
shouldn't had changed the tag
Recall that the Mineral class doesn't have an explicit GenericRelation  defined. That's OK, because you can create TaggedItems explicitly.  However, excluding GenericRelations means your lookups have to be a  bit more explicit.
One update() query.
Test that GenericRelation by default isn't usable from  the reverse side.
Simple tests for multiple GenericForeignKeys  only uses one model, since the above tests should be sufficient.
Create directly
If we delete cheetah, Comparisons with cheetah as 'first_obj' will be  deleted since Animal has an explicit GenericRelation to Comparison  through first_obj. Comparisons with cheetah as 'other_obj' will not  be deleted.
GenericForeignKey should not use the default manager (which may filter objects) 16048
AllowsNullGFK doesn't require a content_type, so None argument should  also be allowed.  TaggedItem requires a content_type but initializing with None should  be allowed.
second would clash with the __second lookup.
Exact query with value None returns nothing ("is NULL" in sql,  but every 'id' field has a value).
The same behavior for iexact query.
Valid query, but fails because foo isn't a keyword
Can't use None on anything other than __exact and __iexact
Related managers use __exact=None implicitly if the object hasn't been saved.
Sqlite had a problem where all the same-valued models were  collapsed to one insert.
SQLite had a problem with more than 500 UNIONed selects in single  query.
Regression for 13227 -- having an attribute that  is unpicklable doesn't stop you from cloning queries  that use objects of that type as an argument.
A complex ordering specification. Should stress the system a bit.
Ticket 23721
db_table names have capital letters to ensure they are quoted in queries.
Create these out of order so that sorting by 'id' will be different to sorting  by 'info'. Helps detect some problems later.
Ordering by 'rank' gives us rank2, rank1, rank3. Ordering by the Meta.ordering  will be rank3, rank2, rank1.
Checking that no join types are "left outer" joins.
Excluding from a relation that cannot be NULL should not use outer joins.
Using remote model default ordering can span multiple models (in this  case, Cover is ordered by Item's default, which uses Note's default).
This is also a good select_related() test because there are multiple  Note entries in the SQL. The two Note items should be different.
The *_id version is returned by default.
You can also pass it in explicitly.
...or use the field name.
Nullable dates
Make sure querysets with related fields can be pickled. If this  doesn't crash, it's a Good Thing.
Complex objects should be converted to strings before being used in  lookups.
There were "issues" when ordering and distinct-ing on fields related  via ForeignKeys.
Pickling of QuerySets using datetimes() should work.
When bailing out early because of an empty "__in" filter, we need  to set things up correctly internally so that subqueries can continue properly.
Testing an empty "__in" filter with a generator as the value.
The subquery result cache should not be populated
The subquery result cache should not be populated
The subquery result cache should not be populated
Ordering by related tables should accommodate nullable fields (this  test is a little tricky, since NULL ordering is database dependent.  Instead, we just count the number of results).
Querying direct fields with isnull should trim the left outer join.  It also should not create INNER JOIN.
Querying across several tables should strip only the last outer join,  while preserving the preceding inner joins.
Combining queries should not re-populate the left outer join
A negated Q along with an annotated queryset failed in Django 1.4
Count should work with a partially read result set.
This shouldn't create an infinite loop.
An error should be raised when QuerySet.datetimes() is passed the  wrong type of field.
Updates that are filtered on the model being updated are somewhat  tricky in MySQL. This exercises that case.
Avoid raising an EmptyResultSet if an inner query is probably  empty (and hence, not executed).
Once upon a time, select_related() with circular relations would loop  infinitely if you forgot to specify "depth". Now we set an arbitrary  default upper bound.
The parent object should have been deleted as well.
Ordering by model related to nullable relations(!) should use outer  joins, so that all results are included.
This example is tricky because the parent could be NULL, so only checking  parents with annotations omits some results (tag t1, in this case).
The annotation->tag link is single values and tag->children links is  multi-valued. So we have to split the exclude filter in the middle  and then optimize the inner query without losing results.
Nested queries are possible (although should be used with care, since  they have performance problems on backends like MySQL.
The all() method on querysets returns a copy of the queryset.
Using an empty generator expression as the rvalue for an "__in"  lookup is legal.
Evaluate the Note queryset, populating the query cache  Use the note queryset in a query, and evaluate  that query in a way that involves cloning.
14366 -- Calling .values() on an empty QuerySet and then cloning  that should not cause an error
19151 -- Calling .values() or .values_list() on an empty QuerySet  should return an empty QuerySet and not cause an error.
Using an offset without a limit is also possible.
ticket 12192
If you're not careful, it's possible to introduce infinite loops via  default ordering on foreign keys in a cycle. We detect that.
Note that this doesn't cause an infinite loop, since the default  ordering on the Tag model is empty (and thus defaults to using "id"  for the related field).
... but you can still order in a non-recursive fashion among linked  fields (the previous test failed because the default ordering was  recursive).
Create a few Orders.
Check that the inner queryset wasn't executed - it should be turned  into subquery above
Test for 19895 - second iteration over invalid queryset  raises errors.
Demote needed for the "a" join. It is marked as outer join by  above filter (even if it is trimmed away).
Evaluating the children query (which has parents as part of it) does  not change results for the parents query.
The query below should match o1 as it has related order_item  with id == status.
Passing incorrect object type
Passing an object of the class on which query is done.
parent objects
QuerySet related object type checking shouldn't issue queries  (the querysets aren't evaluated here, hence zero queries) (23266).
Make sure there is a left outer join without the filter.
Protect against annotations being passed to __init__ --  this'll make the test suite get angry if annotations aren't  treated differently than fields.
last_name isn't given, but it will be retrieved on demand
First Iteration
Second Iteration
Indexing on RawQuerySets
Because no Articles exist yet, earliest() raises ArticleDoesNotExist.
Ensure that earliest() overrides any other ordering specified on the  query. Refs 11283.
Because no Articles exist yet, latest() raises ArticleDoesNotExist.
Ensure that latest() overrides any other ordering specified on the query. Refs 11283.
And it does not matter if there are any records in the DB.
XXX check Content-Length and truncate if too many bytes written?
Return a blob of data that is 1.5 times the maximum chunk size.
SimpleTestCase can be decorated by override_settings, but not ut.TestCase
inner's __exit__ should have restored the settings of the outer  context manager, not those when the class was instantiated
Force evaluation of the lazy object.
-*- coding: utf-8 -*-
The XML serializer handles everything as strings, so comparisons  need to be performed on the stringified value
-*- coding: utf-8 -*-
SerializerDoesNotExist is instantiated with the nonexistent format
Serialize the test database to a stream
Serialize normally for a comparison
Check that the two are the same
Prior to saving, old headline is in place
After saving, new headline is in place
Check the class instead of using isinstance() because model instances  with deferred fields (e.g. Author_Deferred_name) will pass isinstance.
Regression for 12524 -- dates before 1000AD get prefixed  0's on the year
The deserialization process needs to run in a transaction in order  to test forward reference handling.
Create the books.
Serialize the books.
Delete one book (to prove that the natural key generation will only  restore the primary keys of books found in the database via the  get_natural_key manager method).
Dynamically register tests for each serializer
Because Oracle treats the empty string as NULL, Oracle is expected to fail  when field.empty_strings_allowed is True and the value is None; skip these  tests.
Regression test for 8651 -- a FK to an object with PK of 0  This won't work on MySQL since it won't let you create an object  with an autoincrement primary key of 0,
Create all the objects defined in the test data
Get a count of the number of objects created for each class
Add the generic tagged objects to the object list
Serialize the test database
Assert that the deserialized data is the same  as the original source
Assert that the number of objects deserialized is the  same as the number that was serialized.
One to one field can't be null here, since it is a PK.
-*- coding: utf-8 -*-
clear out cached serializers to emulate yaml missing
clear out cached serializers to clean out BadSerializer instances
yaml.safe_load will return non-string objects for some  of the fields we are interested in, this ensures that  everything comes back as a string
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Make sure this form doesn't pass validation.
Make sure this form doesn't pass validation.
Regression test for 9171.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Use the name of the primary key, rather than pk.
pk can be used as a substitute for the primary key.  The primary key can be accessed via the pk property on the model.  Or we can use the real attribute name for the primary key:
Primary key may be unicode string
Regression for 10785 -- Custom fields can be used for primary keys.
Based on tests/reserved_names/models.py
local_models should contain test dependent model classes that will be  automatically removed from the app cache on test tear down.
Weird field that saves the count of items in its value
Make sure the field isn't nullable
Make sure the field isn't nullable
Make sure the field isn't nullable
This will fail if DROP DEFAULT is inadvertently executed on this  field which drops the id sequence, at least on PostgreSQL.
model requires a new PK
Ensure unique constraint works.
"Alter" the field. This should not rename the DB table to itself.
Remove the M2M table again  Ensure there's no m2m table there
Make sure the model state is coherent with the table one now that  we've removed the tags field.
Create the table  Find the properly shortened column name
Ensure the table is there and has an index on the column
Copy those methods from ManyToManyField because they don't call super() internally
Caution: this is only safe if you are certain that headline will be  in ASCII.
-*- coding: utf-8 -*-
On Python 2, the default str() output will be the UTF-8 encoded  output of __unicode__() -- or __str__() when the  python_2_unicode_compatible decorator is used.
-*- coding: utf-8 -*-
(validator, value, expected),
assertRaises not used, so as to be able to produce an error message  containing the tested value
Delete all permissions and content_types
Re-run migrate. This will re-build the permissions and content types.
end of file
remove blank lines at end
remove docstrings and comments
no matching package--search for possibles, and limit to 15  results
make sure we only get unique package names
if only one package was found, return it
print(cand.__name__)
only include if there are missing arguments in the docstring (fewer false positives)  and there are at least some documented arguments
res[b][a] = group['values'].sum()
allhits = set()  files now holds a list of paths to files
drop python from our environment to avoid  passing this onto sub-processes
get current files
file logging
main loop
result = np.empty((ak + bk, len(result_index)), dtype=np.float64)  lib.take_axis0(av, rindexer, out=result[:ak].T)  lib.take_axis0(bv, lindexer, out=result[ak:].T)
result = np.empty((ak + bk, len(result_index)), dtype=np.float64)  lib.take_axis0(av, rindexer, out=result[:ak].T)  lib.take_axis0(bv, lindexer, out=result[ak:].T)
join(a, b, avf, bvf, how=kind)
slower!
no recursion.
data.join(to_join, on=['key1', 'key2'])
!/usr/bin/env python  -*- coding: utf-8 -*-
we're not in a dev dir with already processed files
ignore cython generation timestamp header
recent
pre eb2234231 , ~ 0.7.0,
save to HDF5Store
Remote name with the PR
Remote name where results pushed
Prefix added to temporary branches
py2.6 does not have subprocess.check_output
The string "Closes %s" string is required for GitHub to correctly close  the PR
classes whose members to check
make sure Series precedes DataFrame, Panel, and Panel4D
class members
try checking equality directly with '=' operator,  as comparison may have been overriden for the left  hand object
check for None-ness otherwise we could end up  comparing a numpy array vs None
could not compare them directly, so try comparison  using the 'is' operator
We do this so that we can provide a more informative  error message about the parameters that we are not  supporting in the pandas implementation of 'fname'
set(dict) --> set of the dictionary's keys
Check that the total number of arguments passed in (i.e.  args and kwargs) does not exceed the length of compat_args
Check there is no overlap with the positional and keyword  arguments, similar to what is done in actual Python functions
Determine the OS/platform and set the copy() and paste() functions  accordingly.
Determine which command/module is installed, if any.
Check it gtk is installed.
Check for either PyQt4 or PySide
Set one of the copy & paste functions.
pandas aliases
if package name determination failed, just default to pandas
otherwise, reverse to get correct order and return
don't include the outer egg directory
cap verbosity at 3 because nose becomes *very* verbose beyond that
reset doctest state on every run
default based on if we are released
set testing_mode
set the testing mode filters
reset the testing mode filters
reset the display options
https://docs.python.org/3/library/unittest.htmldeprecated-aliases
don't generate tempfile if using a path with directory specified
instance validation
class / dtype comparison
MultiIndex special comparison for little-friendly error messages  cannot use get_level_values here because it can change dtype
get_level_values may change dtype
return Index as it is to include values in the error message
np.nan
datetimetz on rhs may raise TypeError
sorting does not change precisions
instance validation  to show a detailed erorr message when classes are different  both classes must be an np.ndarray
count up differences
compare shape and values
This could be refactored to use the NDFrame.equals method
instance validation
index comparison
we want to check only if we have compat dtypes  e.g. integer and M|m are NOT compat, but we can simply check  the values in that case
metadata comparison
instance validation
index comparison
column comparison
SparseIndex comparison
trade-off?
make index
make series
make frame
build default names
pass None to index constructor for no name
make singelton case uniform
convert tuples to index
5 row, 3 columns, default names on both, single index on both axis
make the data a random int between 1 and 100
2-level multiindex on rows with each label duplicated  twice on first level, default names on both axis, single  index on both axis
DatetimeIndex on row, index with unicode labels on columns  no names on either axis
4-level multindex on rows with names provided, 2-level multindex  on columns with default labels and default names.
by default, generate data based on location
below is cribbed from scipy.sparse  generate a few more to ensure unique values
or this e.errno/e.reason.errno
and conditionally raise on these exception types
dict comprehensions break 2.6
don't return anything if used in function form
Failed, so allow Exception to bubble up
make sure that we are clearning these warnings  if they have happened before  to guarantee that we will catch them
needed for window's python in cygwin's xterm!
include margin for titles
must be convert here to get index levels for colorization
Allow for both boolean or callable known failure conditions.
Local import to avoid a hard nose dependency and only incur the  import time overhead at actual test-time.
discard the top level
could be passed a Series object with no 'columns'
If there are no values and the table is a series, then there is only  one column in the data. Compute grand margin and return it.
populate grand margin
we cannot reshape, so coerce the axis
need to "interleave" the margins
we are going to mutate this, so need to copy!
we cannot reshape, so coerce the axis
slight hack
need to "interleave" the margins
but they still will be counted in the output
Post-process
Normalize core
Fix Margins
because we want to coerce to numeric if possible,  do not use _shallow_copy_with_infer
being a bit too dynamic  pylint: disable=E1101
The or v[0] == '0' is because their versioneer is  messed up on dev
Compat with mp 1.5, which uses cycler.
alias so the names are same as plotting method parameter names
no gaps between subplots
workaround because `c='b'` is hardcoded in matplotlibs scatter method
Deal with the diagonal by drawing a histogram there.
if all ticks are int
Take the rest of the coefficients and resize them  appropriately. Take a copy of amplitudes as otherwise numpy  deletes the element from amplitudes itself.
Generate the harmonics and arguments for the sin and cos  functions.
random.sample(ndarray, int) fails on python 3.3, sigh
workaround because `c='b'` is hardcoded in matplotlibs scatter method
if we get an axis, the users should do the visibility  setting...
need to know for format_date_labels since it's rotated to 30 by  default
support series.plot(color='green')
secondary axes may be passed via ax kw
if it has right_ax proparty, ``ax`` must be left axes
if it has left_ax proparty, ``ax`` must be right axes
if all data is plotted on secondary, return right axes
no empty frames or series allowed
prevent style kwarg from going to errorbar, where it is  unsupported
key-matched DataFrame
key-matched dict
raw error values
broadcast errors to each data series
check axes coordinates to estimate layout
hide the matplotlib default for size, in case we want to change  the handling of this argument later
plot a colorbar only if a colormap is provided or necessary
pandas uses colormap, matplotlib uses cmap.
this is slightly deceptive
accept x to be consistent with normal plot func,  x is not passed to tsplot as it uses data.index as x coordinate  column_num must be in kwds for stacking purpose
set date formatter, locators and rescale limits
stacker may not be initialized for subplots
irregular TS rotated 30 deg. by default  probably a better place to check / set this.
use smaller alpha to distinguish overlap
get data from the line to get coordinates for fill_between
Do not call LinePlot.__init__ which may fill nan
create common bin edge
ignore style
y is required for KdePlot
leglabels is used for legend labels
namedtuple to hold results
Do not call LinePlot.__init__ which may fill nan
Disable label ax sharing. Otherwise, all subplots shows last  column label
Boxplot fails with empty arrays, so need to add a NaN    if any cols are empty  GH 8181
get standard colors for default  use 2 colors by default, for box/whisker and median  flier colors isn't needed here  because it can be specified by ``sym`` kw
Other types are forwarded to matplotlib  If None, use default colors
converted to series actually. copy to not modify
validate return_type:
Return axes in multiplot case, maybe revisit later  985
workaround because `c='b'` is hardcoded in matplotlibs scatter method
allowed to specify mpl default with 'default'
Return axes in multiplot case, maybe revisit later  985
Just a figure and one subplot
Create empty object array to hold all axes.  It's easiest to make it 1-d  so we can just append subplots upon creation, and then
Create first subplot separately, so we can share it if requested
Reshape the array to have the final desired dimension (nrow,ncol),  though discarding unneeded dimensions that equal 1.  If we only have  one subplot, just return it instead of a 1-element array.
returned axis array will be always 2-d, even if nrows=ncols=1
first find out the ax layout,  so that we can correctly handle 'gaps"
only the last row of subplots should get x labels -> all  other off layout handles the case that the subplot is  the last in the column, because below is no subplot/gap.
if gridspec is used, ax.rowNum and ax.colNum may different  from layout shape. in this case, use last_row logic
only the first column should get y labels -> set all other to  off as we only have labels in teh first column and we always  have a subplot there, we can skip the layout test
this works
no rows
GH 12017
h3h
it works
1978, 1979
mixed type handling
what to test here
append empty
overlap
different join types
these are actual copies
axis=0
axis=1
GH10698
append
it works
invalid concatente of mixed dims
left join
inner join
if things are a bit misbehaved
2257
it works!
if things are a bit misbehaved
preserve series names, 2489
1649
to join with union  these two are of different length!
generator ok though
exclude a couple keys for fun
Join on string value
merge column not p resent
overlap
GH12081
corner cases
no overlapping blocks
generated an exception in 0.4.3
it works!
smoke test
GH13169  this really should be bool
it works!
result will have object dtype
result will have object dtype
12411
GH 4993  appending with datetime will incorrectly convert datetime64
2649, 10639
concat
Concat'ing two UTC times
Concat'ing two London times
Concat'ing 2+1 London times
Concat'ing 1+2 London times
name will be reset
No side effects
Check with custom name
Check if working name in df
Check for name conflict with custom name
Merge on multiple columns
compare left vs right merge with multikey
~ 40000000 possible unique groups
just to hit the label compression code path
it works!
check that left merge w/ sort=False maintains left frame order
confirm that this is checking what it is supposed to check
add duplicates to left frame
add duplicates & overlap with left to the right frame
shuffle left & right frames
manually compute outer merge
as in GH9092 dtypes break with outer/right join
invalid cases
some smoke tests
filter
get rid of suffixes, if any
put in the right order...
all systems should have at least a single locale
coerces to float
bool is regarded as numeric
NOTE: this binning code is changed a bit from histogram for var(x) == 0
handle empty arrays. Can't determine range, so use 0-1.  rng = (0, 1)
rounded up or down
left_index=left_index, right_index=right_index,
May have passed ndarray
note this function has side effects
if we have an all missing left_indexer  make sure to just use the right values
avoid key upcast in corner case (length-0)
Hm, any way to make this logic less complicated??
bind `sort` arg. of _factorize_keys
get left & right join labels and num. of levels at each location
get flat i8 keys from label lists
factorize keys to a dense i8 space  `count` is the num. of unique keys  set(lkey) | set(rkey) == range(count)
preserve left frame order if how == 'left' and sort == False
this is a bit kludgy
bind `sort` argument
get flat i8 join keys
factorize keys to a dense i8 space
if asked to sort or there are 1-to-many matches
left frame preserves order & length of its index
NA group
tuplesafe
how many levels can be done without overflow
densify current keys to avoid overflow
consolidate data & figure out what our result ndim is going to be
consolidate
filter out the empties if we have not multi-index possibiltes  note to keep empty Series as it affect to result columns / name
Need to flip BlockManager axis in the DataFrame special case
if we have mixed ndims, then convert to highest ndim  creating column numbers as needed
doing a row-wise concatenation so need everything  to line up
note: this is the BlockManager axis (since DataFrame is transposed)
series only
combine as columns in a frame
ufff...
make sure that all of the passed indices have the same nlevels
also copies
also copies
construct labels
concat Series with axis 1
filter empty arrays  1-d dtypes always are included here
these are mandated to handle empties as well
we have all empties, but may need to coerce the result dtype to  object if we have non-numeric type operands (numpy would otherwise  cast this to float)
let numpy coerce
coerce to object
coerce to object dtype
convert to object type and perform a regular concat
we could have object blocks and categoricals here  if we only have a single categoricals then combine everything  else its a non-compat categorical
validate the categories
must be single dtype
need to coerce to object
coerce to native type
input may be sparse / dense mixed and may have different fill_value  input must contain sparse at least 1
densify and regular concat
sparsify if inputs are sparse and dense numerics  first sparse input's fill_value and SparseIndex is used
coerce to object if needed
make myself hashable
we are called as an empty constructor  generally for pickle compat
format the tz
make myself hashable
define abstract base classes to enable isinstance type checking on our  objects
nodes that we don't support directly but are needed for parsing
these nodes are low priority or won't ever be supported (e.g., AST)
we're adding a different assignment in some cases to be equality comparison  and we don't want `stmt` and friends in their so get only the class whose  names are capitalized
the kind of the operator (is actually an instance)
must be two terms and the comparison operator must be ==/!=/in/not in
if there are any strings or lists in the expression
pop the string variable out of locals and replace it with a list  of one string, kind of a hack
right is a float32 array, left is a scalar
left is a float32 array, right is a scalar
all date ops must be done in python bc numexpr doesn't work  well with NaT
"in"/"not in" ops are always evaluated in python
evaluate "==" and "!=" in python if either of our operands  has an object return type
a Term instance
an Op instance
Check if this is a supported function name  Raise original error
base case: we have something like a CMP b
ast.Call signature changed on 3.5,  conditionally change which methods is named  visit_Call depending on Python version, 11097
resolve the rhs (and allow it to be None)
string quoting
if too many values to create the expression, use a filter instead
equality conditions
if self.condition is not None:     self.condition = "~(%s)" % self.condition  return self
convert values if we are in the table
equality conditions
use a filter after reading
resolve the value
try to get the value to see if we are another expression
something like datetime.datetime where scope is overriden
try to be back compat
capture the environment if needed
make sure we have an op at least
get our (possibly passed-in) scope
construct the engine and evaluate the parsed expression
if returning a copy, copy only on the first assignment
existing resolver needs updated to handle  case of mutating existing column in copy
we have > NPY_MAXARGS terms in our expression
if it's a variable name (otherwise a constant)
potentially very slow for large, mixed dtype frames
ndarray
scalar
clobber types to bool if the op is a boolean operator
has to be made a list for python3
handle truediv
recurse over the left/right nodes
recurse over the left/right nodes
base cases
do not upcast float32s to float64 un-necessarily
make sure no names in resolvers and locals/globals clash
convert the expression to a valid numexpr expression
python 3 compat kludge
single unary operand
we don't have any pandas objects
initial axes are the axes of the largest-axis'd term
flatten the parse tree (a nested list, really)
if all resolved variables are numeric scalars
perform the main alignment
the minimum prod shape that we will use numexpr
set/unset to use numexpr
if we are using numexpr, set the threads to n  otherwise reset
required min elements (otherwise we are adding overhead)
allowed are a superset
we were originally called by a reversed op  method
turn myself on
odd failure on win32 platform, so skip
try:  self.assertRaises(Exception, pd.eval, ex,  engine=self.engine, parser=self.parser)  except AssertionError:  raise
ValueError: series frame or frame series align  TypeError, AttributeError: series or frame with scalar align
direct numpy comparison
make sure the other engines work the same as this one
~
bool
float
float
int
ValueError: series frame or frame series align  TypeError, AttributeError: series or frame with scalar align
multiple assignees
with a local name overlap
multiple assignment
GH 9297
default for inplace will change
but don't warn without assignment
xref https://github.com/pydata/pandas/issues/12293
Did not test complex64 because DataFrame is converting it to  complex128. Due to https://github.com/pydata/pandas/issues/10952
get the hex repr of the binary char and remove 0x and pad by pad_size  zeros  bytes literals masquerade as ints when iterating in py3
interpret as a pointer since that's what really what id returns
shallow copy because we don't want to keep filling this up with what  was there before if there are multiple calls to Scope/_ensure_scope
assumes that resolvers are going from outermost scope to inner
only look for locals in outer scope
not a local variable so check in resolvers if we have them
if we're here that means that we have no locals and we also have  no resolvers
last ditch effort we look in temporaries  these are created when parsing indexing expressions  e.g., df[df > 0]
won't remove it, but DECREF it  in Py3 this probably isn't necessary since frame won't be  scope after the loop
add sl frames to the scope starting with the  most distant and overwriting with more current  makes sure that we can capture variable scope
add to inner most scope
only increment if the variable gets put in the scope
either utf-8 or we replace errors
have to handle empty styles like ['']
formatter must be callable, so '{}' are converted to lambdas
single scalar to format all cells with
like tee
-*- coding: utf-8 -*-  pylint: disable=W0141
Levels are added in a newline
level infos are added to the end and in a new line, like it is done  for Categoricals
Series uses mode=center because it has single value columns  DataFrame uses mode=left
re-calculate padding space per str considering East Asian Width
Column of which first element is used to determine width of a dot col
Cut the data to the information actually printed
Format only rows and columns that could potentially fit the  screen
Add ... to signal truncated
the whole frame
Size of last col determines dot col size. See  `self._to_str_columns
Call again _chk_truncate to cut frame appropriately  and then generate string representation
if we have a Float level, they don't use leading space at all
self.str_columns = str_columns
Note: this is only used by to_string() and to_latex(), not by  to_html().
empty space for columns
write nothing
GH3547
GH3547
prevents crash in _csv
GH3457
validate mi options
update columns to include possible multiplicity of dupes  and make sure sure cols is just a list of labels
save it
preallocate data 2d list
original python implem. of df.to_csv  invoked by df.to_csv(engine=python)
to be removed in 0.13
write out the mi
write out the names for each level, then ALL of the values for  each level
we need at least 1 index column to write our col names
name is the first column
add blanks for the columns, so that we  have consistent seps
write out the index label line
write in chunksize bites
self.data is a preallocated list
Format multi-index as a merged cells.
write index_values
check for aliases
MultiIndex columns require an extra row  with index names (blank if None) for  unambigous round-trip, unless not merging,  in which case the names all go on one row Issue 11328
if index labels are not empty go ahead and dump
Format hierarchical rows as merged cells.
object dtype
float_format is expected to be a string  formatter should be used to pass a function
the float_format parameter supersedes self.float_format
if we have a fixed_width, we'll need to try different float_format
separate the wheat from the chaff
There is a special default string when we are fixed-width  The default is otherwise to use str instead of a formatting string
this is pretty arbitrary for now  large values: more that 8 characters including decimal symbol  and first digit, hence > 1e6
shortcut
periods may contains different freq
return a boolean if we are only dates (and don't have a timezone)
leave one 0 after the decimal points if need be.
try again for something better
when all else fails. this will usually be "ascii"
GH3360, save the reported defencoding at import time  MPL backends may change it. Make available for debugging.
deprecated.
sane defaults for interactive non-shell terminal  match default for width,height in config_init
pure terminal
Note if the User sets width/Height to None (auto-detection)  and we're in a script (non-inter), this will return (None,None)  caller needs to deal.
self.use_eng_prefix = True
self.use_eng_prefix = False
Change the class of the array to be the subclass type.
if float fill_value is being included in dense repr,  convert values to float
caller must pass SparseIndex
Inplace operators
Python 2 division operators
caching not an option, leaks memory
return scalar
if com.is_integer(key):     self.values[key] = value  else:     raise Exception("SparseArray does not support seting non-scalars  via setitem")
x = self.values  x[slobj] = value  self.values = x
scalar
ndarray
the arr is a SparseArray
Force alignment, no copy necessary
pickling
set the fill value if we are filling as a scalar with nothing special  going on
Scalar
always return a SparseArray!
.take returns SparseArray
from BlockManager perspective
this is pretty fast
use unaccelerated ops for sparse objects
pylint: disable=W0611  flake8: noqa
deprecation 11157
pre-filter, if necessary
do we want to fill missing ones?
do nothing when DataFrame calls this method
return dense values
DataFrame's index
DataFrame's columns / "items"
xs cannot handle a non-scalar key, so just reindex here
values are stacked column-major
have full set of observations for each item
for each item, take mask values at index locations for those sparse  values, and use that to select values
still column major
maybe unnecessary
Sparse objects opt out of numexpr
this is pretty fast
increment by offset
as of v0.15.0  this is now identical (but not is_a )
construct no data
construct from nested dict
assert level parameter breaks reindex
no index or columns
raise on level argument
wrong length index / columns
GH 9272
cross-sectional operations
1585 select multiple columns
2227
axis = 0
axis = 1
slicing
boolean indexing
insert ndarray
insert ndarray wrong size
agg / broadcast
df.T breaks
just test that it works
this changes internal SparseArray repr  tm.assert_sp_series_equal(result, expected.to_sparse(fill_value=0))
propagate CORRECT fill value
with copy=False
for now
int is coerced to float dtype
win32 don't check dtype
2220
note : no error without nan
note that 2 ** df works fine, also df ** 1
GH 8822
one or both is empty
block extend beyond end
block overlap
check versus Series...
-*- coding: utf-8 -*-
tests for first / last / nth
fill_value = np.nan
fill_value = 0
1st fill_value will be used
fill_value = np.nan
fill_value = 0
fill_value = np.nan
fill_value = 0
fill_value = np.nan
fill_value = 0
arithmetic tests
deprecation GH11157
-*- coding: utf-8 -*-
GH 10648
GH 10648
two corner cases from Series
check numpy compat
nan-based
nan-based
zero-based
zero-based
deprecation TimeSeries, 10890
it works!
printing & access
blocking
GH2803
use passed name
Sparse time series works
pass Series
don't copy the data by default
but can make it copy!
GH 9272
test that data is copied
correct fill value
no deep copy
exception handling
index not contained
negative indices
scalar value
zero-based
with dense
skipping for now
special cases
GH12723
GH 12908
GH 12908
GH 12908
GH 13114  test it doesn't raise error. Formatting is tested in test_format
convert to dense and compare  or compare directly as difference of sparse  assert(abs(A - A_result).max() < 1e-12)  max is failing in python  2.6
dense array
sparse array (actuary it coerces to normal Series)
dense array
sparse array (actuary it coerces to normal Series)
GH 9467
dense array
sparse array (actuary it coerces to normal Series)
dense array
sparse array (actuary it coerces to normal Series)
dense array
sparse array (actuary it coerces to normal Series)
all fill_value
dense array
sparse array (actuary it coerces to normal Series)
need to be override to use different label
exceeds the bounds
dense array
sparse array (actuary it coerces to normal Series)
exceeds the bounds
dense array
sparse array (actuary it coerces to normal Series)
dense array
sparse array (actuary it coerces to normal Series)
strip special method names, e.g. `__add__` needs to be `add` when  passed to _sparse_series_op
we are called internally, so short-circuit
extract the SingleBlockManager
array-like
create/copy the manager
create a sparse array
currently, unicode is same as repr...fixes infinite loop
pickling
recreate the ndarray
create a sparse array
recreate
Could not hash item, must be array-like?
to handle MultiIndex labels
no special handling of fill values yet
backwards compatiblity  deprecation TimeSeries, 10890
index and column levels must be a partition of the index
from the SparseSeries: get the labels and data for non-null entries
if index has labels (that are not None) use those,  else use the level location
to keep things simple, only rely on integer indexing (not labels)
is there a better constructor method to use here?
Let users know if they're missing any of our hard dependencies
numpy compat
let init-time option registration happen
define the testing framework
use the closest tagged version if possible
not writeable when instantiated with string, doesn't handle unicode well  always writeable
have to explicitly put builtins into the namespace
list-producing versions of the major Python iterating functions
Python 2
import iterator versions of these functions
Python 2-builtin ranges produce lists
only python 2 has bound/unbound method issue    functions largely based / taken from the six module
Definition of East Asian Width  http://unicode.org/reports/tr11/  Ambiguous width can be changed by option
encoding is for compat with PY2
callable reintroduced in later versions of Python
dateutil brokenness
Can't use functools.wraps() here because of bootstrap issues
can't use 'key in mapping' with defaultdict
support subclasses that define __missing__
reuses stored hash values if possible
turn off all numpy warnings
is_list_like
Currently, numpy (v1.11) has backwards compatibility checks  in place so that this 'kwargs' parameter is technically  unnecessary, but in the long-run, this will be needed.
the Panel class actual relies on the 'axes' parameter if called  via the 'numpy' library, so let's make sure the error is specific  about saying that the parameter is not supported for particular  implementations of 'transpose'
compat
py3 compat
compat
override because the m parameter is introduced in Python 3.4
categories and ordered can't be part of attributes,  as these are properties
we are going to look things up with the codes themselves
filling in missing if needed
coerce to a regular index here!
.reindex returns normal Index. Revert to CategoricalIndex if  all targets are included in my categories
we will try to coerce to integers
isscalar, generators handled in coerce_to_ndarray
do not cache or you'll create a memory leak
don't coerce ilocs to integers
e.g. fails in numpy 1.6 with DatetimeIndex 1681
allow integer / object dtypes to be passed, but coerce to float64
coerce to float64 for storage
if we are not a slice, then we are done
translate to locations
if other is a sequence this throws a ValueError
should only need to catch ValueError here but on numpy  1.7 .item() can raise IndexError when NaNs are present
pylint: disable=E1101,E1103,W0232
initialize to zero-length tuples to make everything work
we've already validated levels and labels, so shortcut here
handles name validation
remove me in 0.14 and change to read only property
remove me in 0.14 and change to readonly property
discards freq
we are formatting thru the attributes
set the name
count the times name equals an element in self.names.
reconstruct the multi-index
Note: levels are zero-based
fml
to disable groupby tricks
isnull is not implemented for MultiIndex
somewhat broken encapsulation
Label-based
generator/iterator-like
we have some NA
GH3547  use value of sparsify as sentinel,  unless it's an obvious  "Truthey" value  little bit of a kludge job for 1217
Assumes that each label is divisible by n_shuffle
I think this is right? Not quite sure...
work around some kind of odd cython bug
cannot be sure whether the result will be sorted
if all(isinstance(x, MultiIndex) for x in other):
set nan if needed
we have a directed ordering via ascending
level ordering
GH6552: preserve names when reindexing to non-named target  (i.e. neither Index nor Series).
GH7774: preserve dtype/tz if target is empty and not an Index.  target may be an iterator
hopefully?
This function adds nothing to its parent implementation (the magic  happens in get_slice_bound method), but it adds meaningful doc.
short circuit
kludgearound
no dropping here
kludge for 1796
we have a multiple selection here
everything
we have a partial slice (like looking up a partial date  string)
we have a slice for start and/or stop  a partial date slicer on a DatetimeIndex generates a slice  note that the stop ALREADY includes the stopped point (if  it was a string sliced)
need to have like semantics here to right  searching as when we are using a slice  so include the stop+1 (so we include stop)
sorted, so can return slice object -> view
sorted, so can return slice object -> view
indexer  this is the list of all values that we want to select
a boolean indexer, must be the same length!
ignore not founds
no matches we are done
empty slice
a slice, include BOTH of the labels
a single label
empty indexer
have to insert into level  must insert at end otherwise you have to recompute all the  other labels
RangeIndex
we are formatting thru the attributes
Method hint: linear Diophantine equation  solve intersection problem  performance hint: for identical step sizes, could use  cheaper alternative
check whether element sets intersect
adjust index to limiting interval
note: could return RangeIndex in more circumstances
complete missing slice information
delegate non-integer slices
convert indexes to values
fall back to Int64Index
alppy if we have an override
we don't have a representable op  so return a base index
for compat with numpy / Int64Index  even if we can represent as a RangeIndex, return  as a Float64Index if we have float-like descriptors
convert to Int64Index ops
simplify
To hand over control to subclasses
Cython methods
prioritize current class for _shallow_copy_with_infer,  used to infer integers as datetime-likes
categorical
index-like
return an actual float index
_asarray_tuplesafe does not always copy underlying data,  so need to make sure that this happens
don't support boolean explicity ATM
only when subarr has the same tz
guard when called from IndexOpsMixin
use something other than None to be clearer
other iterable of some kind
no data provided, just attributes
max_seq_items = get_option('display.max_seq_items')  if len(self) > max_seq_items:     space = "\n%s" % (' ' * (len(klass) + 1))
do we want to justify (only do so for non-objects)
are we a truncated display
adj can optionaly handle unicode eastern asian width
adjust all values to max length if needed
remove trailing space of last line
last value: no sep added + 1 space of width used for trailing ','
remove initial space
to disable groupby tricks in MultiIndex
how to represent ourselves to matplotlib
we want to raise KeyError on string/mixed here  technically we *could* raise a TypeError  on anything but mixed though
if we are not a slice, then we are done
potentially cast the bounds to integers
figure out if this is a positional indexer
missing values are flagged as -1 by get_indexer and negative  indices are already converted to positive indices in the  above if-statement, so the negative flags are changed to  values outside the range of indices so as to trigger an  IndexError in maybe_convert_indices
property, for now, slow to look up
work around some kind of odd cython bug
There's no custom logic to be implemented in __getslice__, so it's  not overloaded intentionally.
This case is separated from the conditional above to avoid  pessimization of basic indexing.
shouldn't reach to this condition by checking hasnans beforehand
coerces to object
could have nans
incomparable objects
worth making this faster? a very unusual case
for subclasses
duplicates
if we have something that is Index-like, then  use this, e.g. DatetimeIndex
invalid type as an indexer
generator/iterator-like
checks that level number is actually just 1
override this method on subclasses
trying to reindex on an axis with duplicates
GH6552: preserve names when reindexing to non-named target  (i.e. neither Index nor Series).
a unique indexer
see GH5553, make sure we use the right indexer
we have a non_unique selector, need to use the original  indexer here
need to retake to have the same size as the indexer
reset the new indexer to account for the new size
try to figure out the join level  GH3662
have the same levels/names so a simple join
join on the level
make the indices into mi's that match
flip if join method is right or left
2 multi-indexes
find indexers of begining of each set of  same-key labels w.r.t all but last level
missing values are placed first; drop them!
left_indexers are w.r.t masked frame.  reverse to original frame!
we are trying to find integer bounds on a non-integer based index  this is rejected (generally .loc gets you here)
np.searchsorted expects ascending sort order, have to reverse  everything for it to work (element ordering, search side and  resulting value).
For datetime indices label may be a string that has to be converted  to datetime boundary according to its resolution.
we need to look up the label  raise the original KeyError
If it's a reverse slice, temporarily swap bounds.
i == -1 triggers ``len(self) + i`` selection that points to the  last element, not before-the-first one, subtracting len(self)  compensates that.
no need to care metadata other than name  because it can't have freq if
we may need to directly compare underlying  representations
technically we could support bool dtyped Index  for now just return the indexing array directly
if we are an inheritor of numeric,  but not actually numeric (e.g. DatetimeIndex/PeriodInde)
higher up to handle
higher up to handle
if we are a reversed non-communative op
return NumPy type
must check for exactly list here because of strict type  check in clean_index_list  2200 ?
get the functional keywords here
how is a keyword that if not-None should be in kwds
also allow 'rolling' as key
Run simple OLS.
Run rolling simple OLS with window of size 10.
Set up LHS and RHS for data across all items
Run panel OLS.
Run expanding panel OLS with window 10 and entity clustering.
print('nw_overlap is True and newey_west generated a non positive '                'semidefinite matrix, so using newey_west with max_lags of %d.'                % new_max_lags)
no intercept
has_intercept = np.abs(self._resid_raw.sum()) < _FP_ERR
XXX
XXX
Use transformed (demeaned) Y, X variables
var_beta has not been newey-west adjusted
Compute the P-value for each pair
no intercept
x should be ones
expanding case
XXX: what's the best way to determine where to start?
group by
assumed no NaN values
encode the stuff, create unique label
only add intercept when no time effects
Filter x's without y (so we can make a prediction)
convert to DataFrame
has_intercept = np.abs(self._resid_raw.sum()) < _FP_ERR
cleans up the names of the generated dummies
renames the dummies if a conversion dict is provided
Non-transformed X
X'X
X'X - (T'T)^-1 (T'X)
check that sparse version is the same
Check resid if we have a time index specified
GH 5233/5250
works
it works!
.flat is flatiter instance
.flat is flatiter instance
.flat is flatiter instance
.flat is flatiter instance
For now, just check that it doesn't crash
Check resid if we have a time index specified
test a function that doesn't aggregate
reference._stats is tuple
does it work?
does it work?
truncate data to save time
HACK XXX
these strings will be replaced by git during git-archive.  setup.py/versioneer.py will grep for the variable names, so they must  each be defined on a line of their own. _version.py will just call  get_keywords().
remember shell=False, so use git.cmd on windows, not just git
parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]  TAG might have hyphens.
look for -dirty suffix
distance: number of commits since tag
commit: short hex revision ID
HEX: no tags
exception 1
exception 1
exception 1
exception 1
exception 1
exception 1
versionfile_source is the relative path from the top of the source  tree (where the .git directory might live) to this file. Invert  this to find the root from __file__.
support of bool dtype indexers
if a passed freq is None, don't infer automatically
convert if not already
no need to infer if freq is None
update name when delta is index
to make our life easier, "sort" the two ranges
Only need to "adjoin", not overlap
to make our life easier, "sort" the two ranges
to make our life easier, "sort" the two ranges
try converting tolerance now, so errors don't get swallowed by  the try/except clauses below
given a key, try to figure out a location for a partial slice
try to convert if possible
fall back to object index
this also converts strings
return an type that can be compared
...so it must be a scalar value. Return scalar.
import after tools, dateutil check
timedelta path
normalize_date returns normal datetime
sentinel class for catching the apply error to return NotImplemented
default for prior pickles
timedelta is used for sub-daily plural offsets and all singular  offsets relativedelta is used for plural offsets of daily length or  more nanosecond(s) are handled by apply_wraps
sub-daily offset - use timedelta (tz-aware)
perform calculation in UTC
bring tz back from UTC calculation
relativedelta/_offset path only valid for base DateOffset
timedelta
relativedelta with other keywords
XXX, see 1395
Default (slow) method for determining if some date is a member of the  date range generated by this offset. Subclasses may have this  re-implemented in a nicer way.
when subtracting, dates on start roll to prior
when adding, dates on end roll to next
way to get around weirdness with rule_code
default _from_name calls cls with no args
calcurate here because offset is not immutable
Valid BH can be on the different BusinessDay during midnight  Distinguish by the time spent from previous opening time
used for moving to next businessday
trust that calendar.holidays and holidays are  consistent
we don't want to actually pickle the calendar object  as its a np.busyday; we recreate on deserilization
used for moving to next businessday
as if rolled forward already
First move to month offset  Find this custom month offset
handle zero case. arbitrarily rollforward
First move to month offset  Find this custom month offset
handle zero case. arbitrarily rollforward
'BQ'
make sure you roll forward, so negate
after start, so come back an extra period as if rolled forward
freq_month = self.startingMonth
n == 0, roll forward
convert month anchor to annual period tuple
n == 0, roll forward
We have to check the year end of "this" cal year AND the previous
This is identical to DateOffset.__hash__, but has to be redefined here  for Python 3, because we've redefined __eq__.
faster than cur + offset
faster than cur + offset
maybe need to upcast (ints)
blow up if we operate on categories
return the result as a Series, which is by definition a copy
setting this object will show a SettingWithCopyWarning/Error
setting this object will show a SettingWithCopyWarning/Error
This class is never instantiated, and exists solely for the benefit of  the Series.dt class property. For Series objects, .dt will always be one  of the more specific classes above.
adjust for leap year
support of bool dtype indexers
data are already in UTC  so need to localize
if a passed freq is None, don't infer automatically
other iterable of some kind
the tz's must match
make sure that we have a index/ndarray like (and not a  Series)
tz aware
Convert tz-naive to UTC
if dtype is provided, coerce here
naive dates
naive dates
vzone sholdn't be None if value is non-datetime like  convert to Timestamp as np.datetime64 doesn't have tz attr
I somewhat believe this should never be raised externally and  therefore should be a `PandasError` but whatever...
This can't happen with external-facing code, therefore  PandasError
how to represent ourselves to matplotlib
< 0.15 compat
provide numpy < 1.7 compat
adding a timedeltaindex to a datetimelike
no need to infer if freq is None
update name when delta is Index
preserve the tz & copy
Superdumb, punting on any optimizing
we know it conforms; skip check
to make our life easier, "sort" the two ranges
Only need to "adjoin", not overlap
if we are comparing an offset that does not propagate timezones  this will raise
to make our life easier, "sort" the two ranges
to make our life easier, "sort" the two ranges
These resolution/monotonicity validations came from GH3931,  GH3452 and GH2369.
_partial_date_slice doesn't allow microsecond resolution, but  _parsed_string_to_bounds allows it.
a monotonic (sorted) series can be sliced
try to find a the dates
needed to localize naive datetimes
try converting tolerance now, so errors don't get swallowed by  the try/except clauses below
needed to localize naive datetimes
alias to offset
b/c datetime is represented as microseconds since the epoch, make  sure we can't have ambiguous indexing
fall back to object index
tz naive, use tz_localize
No conversion since timestamps are all UTC to begin with
cannot just use e = Timestamp(end) + 1 because arange breaks when  stride is too large, see GH10887  end.tz == start.tz by this point due to _generate implementation
utc = len(dates) > 0 and dates[0].tzinfo is not None
this also converts strings
well, technically not a "class" anymore...oh well
construct period for day 1/1/1 and get the first second
other iterable of some kind
freq must be provided
This should be TypeError, but TypeError cannot be raised  from here because numpy catches.
mult1 can't be negative or 0
how to represent ourselves to matplotlib
raise when input doesn't have freq
result must be Int64Index or Float64Index
b/c data is represented as ints make sure we can't have ambiguous  indexing
we cannot construct the Period  as we have an invalid type
box
overwrites method from DatetimeIndexOpsMixin
< 0.15 compat
backcompat
to the groupby descriptor
API compat of disallowed attributes
compat for deprecated
for compat with prior versions, we want to  have the warnings shown here and just have this work
try the key selection
panel grouper
we have a non-reducing function  try to evaluate
groupby & aggregate methods
.resample(..., how='sum')
.resample(..., how=lambda x: ....)
show the prior function call
initialize our GroupByMixin object with  the resampler attributes
this is how we are actually creating the bins
reset to the new freq
do we have a regular frequency
let's do an asfreq
we are downsampling  we want to call the actual grouper method here
we may have a different kind that we were asked originally  convert if needed
Cannot have multiple of periods, convert to timestamp
convert to timestamp
we may need to actually resample as if we are timestamps
Start vs. end of period
we may need to actually resample as if we are timestamps
Start vs. end of period
Get the fill indexer
We are actually downsampling  but are in the asfreq path  GH 12926
always sort time groupers
create the resampler and return our binner
return an ordering of the transformed group labels,  suitable for multi-grouping, e.g the labels for  the resampled intervals
since we may have had to sort  may need to reorder groups here
GH 12037  use first/last directly instead of call replace() on them  because replace() will swallow the nanosecond part  thus last bin maybe slightly before the end if the end contains  nanosecond part and lead to `Values falls after last bin` error
a little hack
general version, knowing nothing about relative frequencies
if we end up with more labels than bins  adjust the labels  GH4076
intraday values on last day
Addresses GH 10530
1165
hack!
roll back
roll forward
already the end of the road
start of the road
roll forward
Mon-Fri are 0-4
Mon-Fri are 0-4
Don't process unnecessary holidays
if we are adding a non-vectorized value  ignore the PerformanceWarnings:
Used inferred freq is possible, need a test case for inferred
set date formatter, locators and rescale limits
resample against axes freq if necessary
Convert DatetimeIndex to PeriodIndex
clear current axes and data
for tsplot
get frequency from data
use axes freq if no data freq
get the period frequency
tsplot converts automatically, but don't want to convert index  over and over for DataFrames
x and y coord info
round the local times
compare
keep freq in PeriodIndex, reset otherwise
quick check
quick check
delta operation
immutable so OK
display as values, not quoted
convert tz if needed
defined in period.pyx  note that these are different from freq codes
Return day freq code against longer freq than day
e.g., freqstr = (2000, 1)
e.g., freqstr = ('T', 5)
Note that _rule_aliases is not 1:1 (d[BA]==d[A@DEC]), and so traversal  order matters when constructing an inverse. we pick one. 2331  Used in get_legacy_offset_name
hack to handle WOM-1MON
handles case where there's no suffix (and will TypeError if too  many '-')
bad prefix or suffix  cache
do not return cache because it's mutable
period frequency constants corresponding to scikits timeseries  originals  Annual freqs with various fiscal year ends.  eg, 2005 for A-FEB runs Mar 1, 2004 to Feb 28, 2005
Additional aliases
hack
This moves the values, which are implicitly in UTC, to the  the timezone so they are in local time
Weekly
Business daily. Maybe
Only attempt to infer up to WOM-4. See 9425
get which week
Weekly
sanity check that the behavior didn't change  GH7206
monotonic
non-monotonic
Return NaT
GH9094
GH9094
offset
offset
monotonic
non-monotonic
Return NaT
offset
offset
multiply
don't allow division by NaT (make could in the future)
scalars
with dti
unequal length
random indexes
create repeated values, 'n'th element is repeated by n+1 times
GH 10295
monotonic
GH9094
GH 6527  GH9094
GH 10295
GH 9903
period shift doesn't accept freq
Ensure that Timestamp.min is a valid Timestamp
Ensure that Timestamp.max is a valid Timestamp
confirm base representation is correct
only with timestring
re-creation shouldn't affect to internal value
should preserve tz
GH 7833
confirm base representation is correct
only with timestring
re-creation shouldn't affect to internal value
should preserve tz
should convert to UTC
This should be 2013-11-01 05:00 in UTC  converted to Chicago tz
GH 11630
GH 9255
this can cause the tz field to be populated, but it's redundant to  information in the datestring
See issue 13057
By definition we can't go out of bounds in [ns], so we  convert the datetime64s to [us] so we can go out of bounds
No error for the min/max datetimes
One us less than the minimum is an error
One us more than the maximum is an error
9000
that we are int/long like
https://github.com/dateutil/dateutil/issues/217
odd comparisons across version  let's just skip
compare with dateutil result
we don't support dayfirst/yearfirst here:
Without coercing, the presence of any invalid dates prevents  any values from being converted
GH 7878
GH 10041
GH 6873
GH 11718
datetime
Assert on the types resulting from Timestamp +/- various date/time  objects  build a timestamp with a frequency, since then it supports  addition/subtraction of integers
Timestamp + datetime not supported, though subtraction is supported  and yields timedelta more tests in tseries/base/tests/test_base.py
Timestamp +/- datetime64 not supported, so not tested (could possibly  assert error raised?)
compat for pandas-like methods
str/repr
bin numeric ops
unary numeric ops
getitem compat
same as prior versions for DataFrame
technically this is allowed
A should not be referenced as a bad column...  will have to rethink regex if you change message!
getting
setting
before use
after grouper is initialized is ok
both resample and groupby should work w/o aggregation
this is a non datetimelike index
need to test for ohlc from GH13083
GH13212
count retains dimensions too
construct expected val
GH2763 - return in put dtype if we can
see gh-12811
from daily
to weekly
Test for issue 3020
check all cython functions work
Same issue should apply to .size() since it goes through    same code path
from daily
GH 10530
1327
Ensure left closing works
it works
it works!
1596
it works!
Upsample by factor 3 with reindex() and resample() methods:
s10_2, r10, r10_2, rl should all be equal
this is a bug, this *should* return a PeriodIndex  directly  GH 12884
this is ok
GH12774
GH12770
GH5430
1 day later
Create the expected series  Index is moved back a day with the timezone conversion from UTC to  Pacific
GH5430
1 day later
conforms, but different month
subset the data.
it works
2245
1465
it works!
2070
reduction
Errors
it works!
GH 9925
unparseable
try to create an out-of-bounds result timestamp; if we can't create  the offset skip  Using 10000 in BusinessHour fails in tz check because of DST  difference
make sure that we are returning a Timestamp
make sure that we are returning NaT
test tz when input is datetime or Timestamp
but be changed when normalize=True
but be changed when normalize=True
when normalize=True, onOffset checks time is 00:00:00
In default BusinessHour (9:00-17:00), normalized time  cannot be in business hour range
normalize=True
This code was executed once on v0.15.2 to generate the pickle:  with open(pickle_path, 'wb') as f: pickle.dump(offsets, f)
equivalent in this special case
root cause of 456
equivalent in this special case
root cause of 456
2014 Calendar to check custom holidays    Sun Mon Tue Wed Thu Fri Sat   6/22  23  24  25  26  27  28     29  30 7/1   2   3   4   5      6   7   8   9  10  11  12
equivalent in this special case
2014/07/01 is Tuesday, 06/30 is Monday(holiday)
2014/6/30 and 2014/6/27 are holidays
equivalent in this special case
root cause of 456
equivalent in this special case
equivalent in this special case
root cause of 456
Saturday
Test On that day
Test on that day
root cause of 456
root cause of 456
corner
corner
From Micron, see:  http://google.brand.edgar-online.com/?sym=MU&formtypeID=7
INTC (extra week in Q1)  See: http://www.intc.com/releasedetail.cfm?ReleaseID=542844
End of long Q1
Start of long Q1
End of year before year with long Q1
Other long years
From Micron, see:  http://google.brand.edgar-online.com/?sym=MU&formtypeID=7
corner
corner
not equals
https://github.com/pydata/pandas/issue/9688
should be cached - this is kind of an internals test...
it works
take a Timestamp and compute total hours of utc offset
one microsecond before the DST transition
expect the signular offset value to match between tstart and t
the offset should be the same as if it was done in UTC
also testing datetime64 dtype (GH8614)
Matplotlib's time representation using floats cannot distinguish  intervals smaller than ~10 microsecond in the common range of years.
GH9012
only really care that it works
32-bit vs. 64-bit platforms
overlapping
non-overlapping, gap in middle
non-overlapping, no gap
order does not matter
overlapping, but different offset
overlapping
non-overlapping, gap in middle
non-overlapping, no gap
overlapping, but different offset
non-overlapping
freq
GH 456
GH 770
GH 2906
GH 2906  Use maybe_get_tz to fix filename in tz under dateutil.
GH12409
test with default frequency, UTC
only really care that it works
32-bit vs. 64-bit platforms
overlapping
non-overlapping, gap in middle
non-overlapping, no gap
order does not matter
overlapping, but different offset
overlapping
non-overlapping, gap in middle
non-overlapping, no gap
overlapping, but different offset
GH 456
axes freq
GH11858
it works!
tsplot
tsplot
change xlim
verify tick labels
GH 2960
plot the left section of the irregular series, then the right section
GH 3490 - non-timeseries with secondary y
GH 3490 - mixed frequency timeseries with secondary y
a downsample should not have changed either limit
GH 3490 - irregular-timeseries with secondary y
plot higher-x values on secondary axis  ensure secondary limits aren't overwritten by plot on primary
Test for issue 11477
Fix holiday behavior found in 11477  where holiday.dates returned dates outside start/end date  or observed rules could not be applied as the holiday  was not in the original date range (e.g., 7/4/2015 -> 7/3/2015)
Test for issue 10278
Testing to make sure holiday is not incorrectly observed before 1986
pylint: disable-msg=E1101,W0612
2563
new index
duplicate some values in the list
it works!
GH 3448 (ranges)
partial ranges
single values
GH 3070, make sure semantics work on Series/Frame
setting
this is a single date, so will raise
don't carry freq through irregular slicing
Bugs in 1396
2538
neither monotonic increasing or decreasing
it works!
GH 10636, default is now 'raise'
2699
this is only locale tested with US/None locales
GH 12300
empty string
ints
GH 3888 (strings)
5/25/2012
GH 1062
test asfreq
1645
midnight, everything
midnight, everything
GH11818
extra fields from DatetimeIndex like quarter and week
make sure weeks at year boundaries are correct
1475
pre-1900
GH 3042
NumPy 1.6.1 weak ns support
blow up, don't loop forever
GH 2938
GH 2938
1624
1646
it works!
e.g. datetime.min
2689, 2627
it works!
GH 11349
2624
it works!
testing the settings before calling .asfreq() and .resample()
does .asfreq() set .freq correctly?
does .resample() set .freq correctly?
GH4606
GH11002  don't infer freq
Assuming all datetimes are in bounds, to_datetime() returns  an array that is equal to Timestamp() parsing
A list of datetimes where the last one is out of bounds
With errors='ignore', out of bounds datetime64s  are converted to their .item(), which depending on the version of  numpy is either a python datetime.datetime or datetime.date
xref 8260
xref 8260
tz coerceion
dict-like
coerce back to int
extra columns
GH2658
it works
2252
non-conforming
if we already have a tz and its not the same, then raise
raise TypeError for now
cannot test array because np.datetime('nat') returns today's date
Check pd.NaT is handles as the same as np.nan
GH 8890
GH11086
non-monotonic
GH 7880
empty same freq GH2129
1644
round
floor
ceil
invalid
GH 7299
reset freq to None
either depeidnig on numpy version
tz must be preserved
GH10747
this test no longer makes sense as series is by default already  M8[ns]
nan
test value to string and back conversions  further test accessors
5-18-2012 00:00:00.000
1404
1404
1404
case where ndim == 0
make sure we can compare Timestamps on the right AND left hand side  GH4982
no nats
nats
compare to timestamp with series containing nats
compare to nat with series containing nats
this is a KeyError as we don't do partial string selection on  multi-levels
GH 4294  partial slice on a series mi
GH 1063, multiple of same base
For historical reasons.
with NaT
string with NaT
GH 10178
GH 10834
8904  exact kw
Whether the format is explicitly passed, it is inferred, or  it is not inferred, the results should all be the same
When the format is inconsistent, infer_datetime_format should just  fallback to the default parsing
The month names will vary depending on the locale, in which  case these wont be parsed properly (dateutil can't parse them)
pylint: disable-msg=E1101,W0612
infortunately, too much has changed to handle these legacy pickles  class TestLegacySupport(unittest.TestCase):
test get_legacy_offset_name
currently invalid as it has a - on the hhmmdd part (only allowed on  the days)
only leading neg signs are allowed
no units specified
str does not normally display nanos
invalid
invalid
this is NOT equal and cannot be roundtriped (because of the nanos)
invalid
invalid multiply with another timedelta
can't operate with integers
that we are int/long like
GH 10050
invalid
invalid
empty string
ints
single element conversion
scalar
m
ms
these will error
time not supported ATM
GH4984  make sure ops return Timedelta
GH 6462  consistency in returned values for sum
std
invalid ops
GH 10040  make sure NaT is properly handled by median()
GH 9442
the computation is converted to float so might be some loss of  precision
GH5438
GH 9273
GH 11129
python timedeltas drop ns resolution
Beyond lower limit, a NAT before the Overflow
Same tests using the internal nanosecond values
it works
with nat
test Series
with nat
non-conforming freq
GH10025
series
tdi
raise TypeError for now
cannot test array because np.datetime('nat') returns today's date
Check pd.NaT is handles as the same as np.nan
reset freq to None
either depeidnig on numpy version
GH4226
higher reso
pylint: disable-msg=E1101,W0612
Construct a timezone object from a string. Overridden in subclass to  parameterize tests.
Construct a timezone string from a string. Overridden in subclass to  parameterize tests.
Compare two timezones. Overridden in subclass to parameterize  tests.
Values are unmodified
Values are unmodified
DST ambiguity, this should fail  Is this really how it should fail??
DST ambiguity, this should fail  Is this really how it should fail??
4 hours before DST transition
spring forward, + "7" hours
4 hours before DST transition
spring forward, + "7" hours
DST transition time
a more unusual time zone, 1946
DateRange with naive datetimes
compare vs a localized tz
datetimes with tzinfo set
March 13, 2011, spring forward, skip from 2 AM to 3 AM
after dst transition, it works
November 6, 2011, fall back, repeat 2 AM hour
UTC is OK
November 6, 2011, fall back, repeat 2 AM hour
Test constructor
Test duplicate times where infer_dst fails
When the sizes are incompatible, make sure error is raised
When sizes are compatible and there are repeats ('infer' won't work)
construction with an ambiguous end-point  GH 11626
left dtype is  datetime64[ns, US/Eastern]  right is datetime64[ns, tzfile('/usr/share/zoneinfo/US/Eastern')]
test utility methods
dates around a dst transition
1673
it works!
it works!
it works!
Standard -> Daylight Savings Time
it works
it works! 2443
2621
Skipped on win32 due to dateutil bug
from system utc to real utc  check that the time hasn't changed.
from system utc to real utc  check that the time hasn't changed.
skip utc as it's a special case in dateutil
skip timezones that dateutil doesn't know about.
non-overlapping
freq with mult should not affect to the result
Test for GH 11738
Test properties on Periods with annually frequency.
Test properties on Periods with hourly frequency.
frequency conversion tests: from Weekly Frequency
U was used as undefined period
GH13079
GH13067
MPL kludge
GH 7228
GH  1211
GH 4390, iat incorrectly indexing
columns
invalid axis
PeriodIndex.to_timestamp always use 'infer'
monotonic
not monotonic
infer freq from first element
infer freq from first element
it works!
GH 6716  Confirm DatetimeIndex and PeriodIndex works identically
getitem against index should raise ValueError
GH7116  these show deprecations as we are trying  to slice with non-integer indexers  with tm.assertRaises(IndexError):     idx[v]
GH 6716
changed to TypeError in 1.12  https://github.com/numpy/numpy/pull/6271
changed to TypeError in 1.12  https://github.com/numpy/numpy/pull/6271
GH 5407
not in order
not in order
non-monotonic
empty same freq
year, month, day, hour, minute  second, weekofyear, week, dayofweek, weekday, dayofyear, quarter  qyear
unicode
should return an array
preserve element types
dtype should be object
lastly, values should compare equal
1705
1815
drops index
freq is Tick
error message differs between PY2 and 3
GH 13071
GH 13071
different base freq
different mult
currently Period is stored as object dtype, not as NaT
dtype will be object because of original dtype
different base freq
different base freq
malformed
Tests for 9064
check roundtrip
Only supports freq up to WOM-4. See 9425
Only attempts to infer up to WOM-4. See 9425
All of these dates are on same day of week and are 4 or 5 weeks apart
GH 10822  odd error message on conversions to datetime for unicode
a non-convertible string
Since these are private methods from dateutil, it is safely imported  here so in case this interface changes, pandas will just fallback  to not using the functionality
The StringIO(str(_)) is for dateutil 2.2 compatibility
In case the datetime can't be parsed, its format cannot be guessed
In case the datetime string can't be split, its format cannot  be guessed
If a given attribute has been placed in the format string, skip  over other formats for that same underlying attribute (IE, month  can be represented in multiple different ways)
Only consider it a valid guess if we have a year, month and day
Either fill in the format placeholder (like %Y)
Or just the token separate (IE, the dashes in "01-01-2013")  If the token is numeric, then we likely didn't parse it  properly, so our guess is wrong
rebuild string, capturing any inferred padding
Try to guess the format based on the first non-NaN element
There is a special fast-path for iso8601 formatted  datetime strings, so in those cases don't use the inferred  format because this path makes process slower in this  special case
shortcut formatting here
replace passed unit with _unit_map
m is case significant
we allow coercion to if errors allows
calculate the actual result
try intlike / strings that are ints
a float with actual np.nan
string with NaN-like
Put the found format in front
Excel has a bug where it thinks the date 2/29/1900 exists  we just reject any date before 3/1/1900.
constants
time formatter
Datetime Conversion
matplotlib.dates._UTC has no _utcoffset called by pandas
For mpl > 2.0 the format strings are controlled via rcparams  so do not mess with them.  For mpl < 2.0 change the second  break point and add a musec break point
if no data have been set, this will tank with a ValueError
We went through the whole loop without breaking, default to 1
save this for later usage
Case 1. Less than a month
requires matplotlib >= 0.98.0
don't actually use the locs. This is just needed to work with  matplotlib. Force to use vmin, vmax
if slicer is a name, get the object
build the klass
setup the axes
combine labels to form new axes
set as NonImplemented operations which we don't support
add the aggregate operations
walk the nested dict
must at least 1 arg deal with constraints later
default to false
walk the nested dict
bind the functions with their docstrings into a Callable  and use that as the functions exposed in pd.api
the default value should be legal
walk the nested dict, creating dicts as needed along the path
save the option metadata
short-circuit for exact key
else look through all of them
Functions/offsets to roll dates forward
Functions to check where a date lies
convert dates
we take an aggressive stance and convert to datetime64[ns]
if we are all nans then leave me alone
convert timedeltas
if we are all nans then leave me alone
convert to numeric
if we are all nans then leave me alone
soft-conversion
List or scalar
If not object, do not attempt conversion
Soft conversions
Object check to ensure only run if previous did not convert
If all NaNs, then do not-alter
masked recarray
GH10856  raise ValueError if only scalars in dict
prefilter if columns passed
no obvious "empty" int column
1783
zero len case (GH 2234)
helper to create the axes as indexes  return axes or defaults
we could have a categorical type passed or coerced to 'category'  recast this to an _arrays_to_mgr
by definition an array here  the dtypes will be coerced to a single dtype
if we don't have a dtype specified, then try to convert objects  on the entire block; this is to convert if we have datetimelike's  embedded in an object type
exceed max columns
used by repr_html under IPython notebook or scripts ignore terminal  dims
check at least the column row for excessive width
when auto-detecting, so width=None and not in ipython front end  check whether repr fits horizontal by actualy checking  the width of the rendered repr
only care about the stuff we'll actually print out  and to_string on entire frame may be expensive
min of two, where one may be None
use integer indexing because of possible duplicate column names
fallback to regular tuples
Make a copy of the input columns so we can modify it
array of tuples to numpy cols. copy copy copy
minor axis must be sorted
preserve names, if any
create new axes
create new manager
Get defaults from the pandas config
hack
old unpickling
set using a non-recursive method & reset the cache
a location index by definition
if we are a copy, mark as such
need to return view
if the values returned are not the same length  as the index (iow a not found value), iget returns  a 0-len ndarray. This is effectively catching  a numpy error (as numpy should really raise)
this is a cached value, mark it so
shortcut if we are an actual column
see if we can slice the rows
either boolean or fancy integer index
get column
duplicate columns & possible reduce dimensionality
when res is multi-dimensional loc raises, but this is sometimes a  valid query
convert the myriad valid dtypes object to a single representation
can't both include AND exclude!
see if we can slice the rows
set column
support boolean setting with DataFrame input, e.g.  df[df > df2] = 0
check if we are modifying a copy  try to set first as we want an invalid  value exeption to occur first
do all calculations first...
... and then assign
GH 4107
duplicate axis
other
possibly infer to datetimelike
upcast the scalar
return internal types directly
append all but the last column so we don't have to modify  the end of this loop
clear up memory usage
to ndarray and maybe infer different dtype
sort by the index
make sure that the axis is lexsorted to start  if not we need to reconstruct to get the correct indexer
GH11080 - Check monotonic-ness before sort an index  if monotonic (already sorted), return None or copy() according  to 'inplace'
one but not both
unique
non-unique
Ambiguous case, use _series so works with DataFrame
unique
non-unique
sorts if possible
don't overwrite columns unecessarily  DO propagate if this column is not in the intersection
if we have different dtypes, possibily promote
see if we need to be represented as i8 (datetimelike)  try to keep us at this dtype
convert_objects just in case
don't overwrite columns unecessarily
skip if we are mixed datelike and trying reduce across axes  GH6125
try to reduce first (by default)  this only matters if the reduction in values is of different dtype  e.g. if we want to apply to a SparseFrame, then can't directly reduce
so will work with MultiIndex
make sure i is defined
if we have a dtype == 'M8[ns]', provide boxed values
For SparseDataFrame's benefit
join indexes only using concat
Dispatch to Series.round
mask missing values
demeaned data
Since we have mixed types, calling notnull(frame.values) might  upcast everything to object
But use the speedup when we have homogeneous dtypes
We're transposing the mask rather than frame to avoid potential  upcasts to object, which induces a ~20x slowdown
Undo our earlier transpose
exclude timedelta/datetime unless we are uniform types
try by-column first
this can end up with a non-reduction  but not always. if the types are mixed  with datelike then need to make sure a series
try to coerce to the original dtypes item by item if we can
figure out the index, if necessary
don't force copy because getting jammed in an ndarray anyway
from BlockManager perspective
drop subclass info, do not copy data
last ditch effort
create the manager
list of lists
assure that they are of the base dict class and not of derived  classes
caller's responsibility to check for this...
Forces alignment. No need to copy data since we  are putting it into an ndarray later
replace but return a numpy array  use a Series because it handles dtype conversions properly
localize to UTC
unorderable in py3 if mixed str/int
reset tz
handle Categorical and sparse,  datetime tz can be handeled in ndarray path
ndarray path. pass original to handle DatetimeTzBlock
convert the keys back to the dtype we came in
must sort because hash order isn't necessarily defined.
reverse indices
dispatch to internal type takes
at this point, it's guaranteed that dtype can hold both the arr values  and the fill_value
need to make sure that we account for na for datelike/timedelta  we don't actually want to subtract these i8 numbers
Working around NumPy ticket 1542
this is the NaT pattern
box
Working around NumPy ticket 1542
this is the NaT pattern
box
shape compat
Object arrays can contain None, NaN and NaT.  string dtypes must be come to this path for NumPy 1.7.1 compat
pd.isnull considers NaN and None to be equivalent.
NaNs can occur in float and complex arrays.
numpy will will not allow this type of datetimelike vs integer comparison
M8/m8
NaNs cannot occur otherwise.
messy. non 0/1 integers do not get converted.
a 1-element ndarray
if we passed an array here, determine the fill value by dtype
we need to change to object type as our  fill_value is of object type
in case we have a string that looked like a number
we are forced to change the dtype of the result as the input  isn't compatible
we have a scalar or len 0 ndarray  and its nan and we are changing some values
we have an ndarray and the masking has nans in it
try to upcast here
don't allow upcasts here (except if empty)
if we don't have any elements, just astype it
do a test on the first element, if it fails then we are done
if we have any nulls, then we are done
a comparable, e.g. a Decimal may slip in here
comparison of an object dtype with a number type could  hit here
should still pass if we don't have a datelike
handles cases like _get_dtype(int)  i.e., python objects that are valid dtypes (unlike user-defined  types, in general)  TypeError handles the float16 typecode of 'e'  further handle internal types
if we have multiples coming back, box em
return the value
check datetime64[ns]/timedelta64[ns] are valid  otherwise try to coerce
our NaT doesn't support tz's  this will coerce to DatetimeIndex with  a matching dtype below
have a scalar array-like (e.g. NaT)
we have a non-castable dtype that was passed
catch a datetime/timedelta that is not of ns variety  and no coercion specified
only do this if we have an array and the dtype of the array is not  setup already we are not an integer/object, so don't bother with this  conversion
safe coerce to datetime64
we might have a sequence of the same-datetimes with tz's  if so coerce to a DatetimeIndex; if they are not the same,  then these stay as object dtype
will try first with a string & object conversion
do a quick inference for perf
try timedelta first to avoid spurious datetime conversions  e.g. '00:00:01' is a timedelta but technically is also a datetime
input may not be sliceable
Python 3
Making a 1D array that safely contains tuples is a bit tricky  in numpy, leading to the following  we have a list-of-list
python 3 generators have __next__ instead of next
invalid comparison  object == category will hit this
return if we have an i8 convertible and numeric comparison
return if we have an i8 convertible and object comparsion
exclude object as its a mixed dtype
this isn't even a dtype
in Py3 that's str, in Py2 that's unicode
in py3, timedelta64[ns] are int64
allow frequency conversions
work around NumPy brokenness, 1987
special case to prevent duplicate plots when catching exceptions when  forwarding methods from NDFrames
if a level is given it must be a mi level or  equivalent to the axis name
we accept no other args
If the original grouper was a tuple
turns out it wasn't a tuple
need to setup the selection  as are not passed directly but in the grouper
a little trickery for aggregation functions that need an axis  argument
preserve the name so we can detect it when calling plot methods,  to avoid duplicates
special case otherwise extra plots are created when catching the  exception below
related to : GH3688  try item-by-item  this can be called recursively, so need to raise  ValueError  if we don't have this method to indicated to aggregate to  mark this column as an error
this is needed so we don't try and wrap strings. If we could  resolve functions to their callable functions prior, this  wouldn't be needed
ignore SettingWithCopy here in case the user mutates
since we are masking, make sure that we have a float object
reset the identities of the components  of the values to prevent aliasing
possible MI return case
GH5610, returns a MI, with the first level being a  range index
mask fails to broadcast when passed to where; broadcast manually.
10177
defined here for API doc
old behaviour, but with all and any support for DataFrames.  modified in GH 7559 to have better perf
get a new grouper for our dropped obj
we don't have the grouper info available  (e.g. we have selected out  a column that is not in the current object)
create a grouper with the original parameters, but on the dropped  object
set the results which don't meet the criteria
filled in by Cython
provide "flattened" iterator for multi-group setting
group might be modified
For many items in each group this is much faster than  self.size().max(), in worst case marginally slower
return if my group orderings are monotonic
see if there is a fused-type version of function  only valid for numeric
otherwise find dtype-specific version, falling back to object
a sub-function
need to curry our sub-function
temporary storange for running-total type tranforms
punting for now
punting for now
check binner fits data
linear scan, presume nothing about values/binner except that it fits ok
count values in current bin, advance to next bin
this is mainly for compat  GH 3881
right place for this?
pre-computed
all levels may not be observed
handle NAs
a passed Categorical
must have an ordered categorical
fix bug GH8868 sort=False being ignored in categorical  groupby
we make a CategoricalIndex out of the cat grouper  preserving the categories / ordered attributes
a passed Grouper like
we are done
already have a BaseGrouper, just return it
if the actual grouper should be obj[key]
if the the grouper is obj[name]
create the Grouping  allow us to passing the actual Grouping as the gpr
create the internals grouper
Make class defs of attributes on SeriesGroupBy whitelist
_level handled at higher
indicated column order
reset the cache so that we  only include the named selection
let higher level handle
GH 6265
GH 823
possible that Series -> DataFrame by applied function
GH 6265
reg transform
may need to astype
Interpret np.nan as False.
group boundaries are where group ids change  unique observations are where sorted values change
1st item of each group is a new unique observation
we might have duplications among the bins
scalar bins cannot be done at top level  in a backward compatible way
groupby removes null keys from groupings
bins[:-1] for backward compat;  o.w. cat.categories could be better
group boundaries are where group ids change
new values are where sorted labels change
num. of times each group should be repeated
for compat. with algos.value_counts need to ensure every  bin is present at every index level, null filled with zeros
build the multi-index w/ full levels
kludge
more kludge
Make sure block manager integrity check passes.
see if we can cast the block back to the original dtype
undoing kludge from below
grouper specific aggregations
GH6337
GH12824.
GH9684. If all values are None, then this will throw an error.  We'd prefer it return an empty dataframe.
reorder the values
don't use the key indexer
GH3596  provide a reduction (Frame -> Series) if groups are  unique
assign the name to this series
GH2893  we have series in the values array, we want to  produce a series:  if any of the sub-series are not indexed the same  OR we don't have a multi-index and we have only a  single values
still a series  path added as of GH 5545
GH 8467
GH1738: values is list of arrays of unequal lengths fall  through to the outer else caluse
only coerce dates if we find at least 1 datetime
Handle cases like BinGrouper
broadcasting
a reduction transform
nuiscance columns
if there were groups with no observations (Categorical only?)  try casting data to original dtype
if we make it here, test if we can use the fast path
iterate through columns
Make class defs of attributes on DataFrameGroupBy whitelist.
zip in reverse so we can always insert at loc 0
kludge
Sorted labels
Counting sort indexer
must return keys::list, values::list, mutated::bool  fails when all -1
this is the BlockManager
this is sort of wasteful but...
how many levels can be done without overflow:
compute flat ids for the first `nlev` levels
compress what has been done so far in order to avoid overflow  to retain lexical ranks, obs_ids should be sorted
reconstruct labels  at some point group indices are factorized,  and may not be deconstructed here! wrong path!
obs ids are deconstructable! take the fast route!
we are already a Categorical
create the Categorical
specially handle Categorical
note, group labels come out ascending (ie, 1,2,3 etc)
sorter is index where elements ought to go
reverse_indexer is where elements came from
move labels to right locations (ie, unsort ascending labels)
sort observed ids
flake8: noqa
we want to transform an object array  ValueError message to the more typical TypeError  e.g. this is normally a disallowed function on  object arrays that contain strings
wrap the 0's if needed
prefer to treat inf/-inf as NA, but must compute the func  twice :(
Bottleneck chokes on datetime64
bottleneck does not properly upcast during the sum  so can overflow
if it doesn't support infs, then it can't have infs
need the max int here
get our fill value (in case we need to provide an alternative  dtype for it)
promote if needed
return a platform independent precision dtype
raise if we have a timedelta64[ns] which is too large
an array from a frame  there's a non-empty array to apply over otherwise numpy raises
otherwise return a scalar value
Return variance as np.float64 (the datatype used in the accumulator),  unless we were dealing with a float array, in which case use the same  precision as the original values array.
floating point error
floating point error
if ``denom`` is a scalar, check these corner cases first before  doing division
NOTE: Only frame cares about default_axis, specifically: special methods  have default axis None, whereas flex methods have default axis 'columns'  if we're not using numexpr, then don't pass a str_rep
this makes sure that we are aligned like the input  we are updating inplace so we want to ignore is_copy
need to make sure that we are aligning the data
2 timedeltas
datetime and timedelta/DateOffset
2 datetimes
have a timedelta, convert to to ns here
datetimes require views
pass thru on the na_op
with tz, convert to UTC
otherwise it's a timedelta
convert Tick DateOffset to underlying delta
if we need to mask the results
datetime64[ns]/timedelta64[ns] masking
decide if we can do it
if dtype is object, try elementwise op
scalars
dispatch to the categorical if we have a categorical  in either operand
we want to compare like types  we only want to convert to integer like if  we are not NotImplemented, otherwise  we would allow datetime64 (but viewed as i8) against  integer comparisons
we have a datetime/timedelta and may need to convert
Validate the axis parameter
cats are a special case as get_values() would return an ndarray,  which would then not take categories ordering into account  we can go directly to op, as the na_op would just test again and  dispatch to it.
always return a full value series here
let null fall thru
GH 353, NumPy 1.5.1 workaround
casted = self._constructor_sliced(other, index=self.columns)
casted = self._constructor_sliced(other,                                    index=self.index)
casted = self._constructor_sliced(other,                                    index=self.columns)
casted = self._constructor(other, index=self.index,                             columns=self.columns)
straight boolean comparisions we want to allow all columns  (regardless of dtype to pass thru) See 4537 for discussion.
copied from Series na_op above, but without unnecessary branch for  non-scalar
Validate the axis parameter
the supported indexers
"null slice"
the public IndexSlicerMaker
we need to return a copy of ourselves
for perf reasons we want to try _xs first  as its basically direct indexing  but will fail when the index is not present  see GH5667
invalid indexer type vs 'other' indexing errors
if we are accessing via lowered dim, use the last dim  a scalar
if we are accessing via lowered dim, use the last dim
should check the stop slice?
should check the elements?
maybe partial set
if there is only one block/type, still have to take split path  unless the block is one-dimensional or it can hold the value
if we have any multi-indexes that have non-trivial slices  (not null slices) then we must take the split path, xref  GH 10360
reindex the axis to the new value  and set inplace
if this is the items axes, then take the main missing  path first  this correctly sets the dtype and avoids cache issues  essentially this separates out the block that is needed  to possibly be modified
add a new item with the dtype setup
reindex the axis to the new value  and set inplace
we have a coerced indexer, e.g. a float  that matches in an Int64Index, so  we will not create a duplicate index, rather  index to that element  e.g. 0.0 -> 0  GH12246
no columns and scalar
append a Series
a list-list
must have conforming columns
set using setitem (Panel and > dims)
set
align and set the values
if we have a partial multiindex, then need to adjust the plane  indexer here
require that we are setting the right number of values that  we are indexing
make sure we have an ndarray
reset the sliced object if unique
equal len list/ndarray
we need an iterable, with a ndim of at least 1  eg. don't pass through np.array(0)
we have an equal len Frame
we have an equal len ndarray/convertible to our labels
setting with a list, recoerces
we have an equal len list/ndarray
per label values
scalar
check for chained assignment
actually do the set
flatten np.ndarray indexers
frame
panel
we have a frame, with multiple indexers on both axes; and a  series, so need to broadcast (see GH5206)
single indexer
2 dims
>2 dims
reindex along the matching dimensions
panel
need to conform to the convention  as we are not selecting on the items axis  and we have a single indexer  GH 7763
by definition we are indexing on the 0th axis  a passed in dataframe which is actually a transpose  of what is needed
no multi-index, so validate all of the indexers
ugly hack for GH 836
no shortcut needed
ugly hack for GH 836
want Index objects to pass through untouched
asarray can be unsafe, NumPy strings are weird
fast path for series or for tup devoid of slices
slices are unhashable
raise the error if we are not sorted
we can directly get the axis result since the axis is specified
we may have a nested tuples indexer here
we maybe be using a tuple to represent multiple dimensions here
to avoid wasted computation  df.ix[d1:d2, 0] -> columns first (True)  df.ix[0, ['C', 'B', A']] -> rows first (False)
we have yielded a scalar ?
we're in the middle of slicing through a MultiIndex  revise the key wrt to `section` by inserting an _NS
unfortunately need an odious kludge here because of  DataFrame transposing convention
This is an elided recursive call to iloc/loc/etc'
we have too many indexers for our dim, but have at least 1  multi-index dimension, try to see if we have something like  a tuple passed to a series with a multi-index
this is a series with a multi-index specified a tuple of  selectors
handle the multi-axis by taking sections and reducing  this is iterative
if we have a scalar, we are done
has the dim of the obj changed?  GH 7199
GH 7516  if had a 3 dim and are going to a 2d  axes are reversed on a DataFrame
maybe coerce a float scalar to integer
this is the fallback! (for a non-float, non-integer index)
want Index objects to pass through untouched
asarray can be unsafe, NumPy strings are weird
have the index handle the indexer and possibly return  an indexer or raising
this is not the most robust, but...
existing labels are unique and indexer are unique
Series
existing labels are non-unique
reindex with the specified axis
try to find out correct indexer, if not type correct raise
but we will allow setting
see if we are positional in nature
a positional
if we are setting and its not a valid location  its an insert which fails by definition
always valid
a positional
want Index objects to pass through untouched
The index may want to handle a list indexer differently  by returning an indexer or raising
this is not the most robust, but...
take all
unique index
non-unique (dups)
allow a not found key only if we are a setter
scalar callable may return tuple
mi is just a passthru
Convert key '2016-01-01' to  ('2016-01-01'[, slice(None, None, None)]+)
an iterable multi-selection
fall thru to straight lookup
if the dim was reduced, then pass a lower-dim the next time
try to get for the next axis
a single integer or a list of integers
validate list bounds
force an actual list
validate the location
make need to convert a float key
we could have a convertible item here (e.g. Timestamp)
scalar callable may return tuple
allow arbitrary setting
32-bit floating point machine epsilon
we are an actual column
We might have a datetimelike string that we can translate to a  slice here via partial string indexing
is_bool_indexer has already checked for nulls in the case of an  object array key, so no check needed here
a missing key (but not a tuple indexer)
If list is empty, np.array will return float and cause indexing  errors.
check for a compatiable nested tuple and multiindexes among the axes
are we nested tuple of: tuple,list,slice
allow a list_like, but exclude NamedTuples which can be indexers
select a label or row
default to column slice, like DataFrame  ['A', 'B'] -> IndexSlices[:, ['A', 'B']]
true when slice does *not* reduce
a 1-d slice, like df.loc[1]
legacy
pylint: disable=E1103,W0231,W0212,W0621
extract axis for remaining axes & create the slicemap
require an arg for each axis and the value
how to make this logic simpler?
a dup selection will yield a full ndim
could check that everything's the same size, but forget it
a reduction
xs by position
shaped like the return DataFrame  size = mask.sum()
size = N * K
2d-slabs
1d
all the iteration points
construct the object
increment the indexer
empty object
construct slabs, in 2-d this is a DataFrame result
need to assume they are the same
have a dict, so top-level is +1 dim
scalar
same as self  return the construction dictionary for these axes
sliced
check if a list of axes was passed in instead as a  single *args element
NumPy strings are a pain, convert to object
caller differs dict/ODict, presered type
handles discrepancy between numpy and numexpr on division/mod  by 0 though, given that these are generally (always?)  non-scalars, I'm not sure whether it's worth it at the moment
add `div`, `mul`, `pow`, etc..
legacy  deprecation, 10892
deprecation, 10892
pylint: disable=E1101,E1103  pylint: disable=W0703,W0622,W0613,W0201
when index includes `nan`, need to lift levels/strides by 1
make the mask
place the values
Will also convert negative level numbers and check if out of bounds.
Workaround the edge case where 0 is one of the column names,  which interferes with trying to sort based on the first  level
more efficient way to go about this? can do the whole masking biz but  will only save a small amount of time...
frame is a copy
asanyarray will keep the columns as an Index
validate prefixes and separator to avoid silently dropping cols
validate separators
Series avoids inconsistent NaN handling
if all NaN
if dummy_na, we just fake a nan level. drop_first will drop it again
Blank entries if not dummy_na and code == -1, GH4446
remove first categorical level to avoid perfect collinearity  GH12042
reset NaN GH4446
remove first GH12042
these must apply directly
GH 12373: rolling functions error on float32 data
calculation function
only default unset
only default unset
only default unset
only default unset
special case in order to handle duplicate column names
Symmetric case
mask out values, this also makes a common index...
this is a pd.Categorical, but is not  a valid type for astypeing
don't want to print out all of the items here
we cannot coerce the underlying object, so  make an ObjectBlock
turn it off completely
single block handling
try to cast all non-floats here
ndim > 1
item-by-item  this is expensive as it splits the blocks items-by-item
may need to convert to categorical  this is only called for non-categoricals
astype processing
force the copy here
use native type formatting for datetime/tz/timedelta
astype formatting
_astype_nansafe works fine with 1-d only
may need to change the dtype here
coerce None values, if appropriate
coerce args
cast the values to a type that can hold nan (if necessary)
slice
empty indexers  8669 (empty)
setting a single element for each dim and with a rhs that could  be say a list  GH 6043
set
may have to soft convert_objects here
cast to the passed dtype if possible  otherwise raise the original error  e.g. we are uint32 and our value is uint64  this is for compat with older numpies
if we are passed a scalar None, convert it here
maybe upcast me
need to go column by column
need a new block
type of the new block
we need to explicitly astype here to make a copy
Put back the dimension that was taken from it and make  a block out of the result.
Only FloatBlocks will contain NaNs.  timedelta subclasses IntBlock
a fill na type method
try an interp method
if we are coercing, then don't force the conversion  if the block can't hold the type
only deal with floats
process a 1-d slice, returning it  should the axis argument be handled below in apply_along_axis?  i.e. not an arg to missing.interpolate_1d
interp each column independently
make sure array sent to np.roll is c_contiguous
restore original order
coerce/transpose the args if needed
get the result, may need to transpose the other
avoid numpy warning of comparisons again None
avoid numpy warning of elementwise comparisons to object
error handler if we have an issue operating with the function
return the values
get the result
if we have an invalid shape/broadcast error  GH4576, so raise instead of allowing to pass through
technically a broadcast error in numpy can 'work' by returning a  boolean False
differentiate between an invalid ndarray-ndarray comparison  and an invalid type comparison
transpose if needed
try to cast if requested
our where function
return the values
see if we can operate on the entire block, or need item-by-item  or if we are a single block (ndim == 1)
try to cast if requested
create the array of na_values  2d len(values) * len(qs)
older numpies don't handle an array for q
Placement must be converted to BlockPlacement via property setter  before ndim logic, because placement may be a slice which doesn't  have a length.
kludgetastic
when inserting a column should not coerce integers to floats  unnecessarily
scalar
FIXME:  should use the formats.format.Timedelta64Formatter here  to figure what format to pass to the Timedelta  e.g. to not show the decimals say
attempt to create new type blocks
GH6026
split and convert the blocks
to_replace is regex compilable
regex is regex compilable
only one will survive
if regex was passed as something that can be a regex (rather than a  boolean)
try to get the pattern attribute (compiled re) or it's a string
if the pattern is not empty and to_replace is either a string or a  regex  if the thing to replace is not a string or compiled regex call  the superclass method -> to_replace is some kind of object
deal with replacing values with objects (strings) that match but  whose replacement is not a string (numeric, nan, object)
value is guaranteed to be a string here, s can be either a string  or null if it's null it gets returned
convert
coerce to categorical if we can
slice the category  return same dims as we currently have
GH12564: CategoricalBlock is 1-dim only  while returned results could be any dim
we may need to upcast our fill to match our dtype
axis doesn't matter; we are really a single-dim object  but are passed the axis depending on the calling routing  if its REALLY axis 0, then this will be a reindex and not a take
if we are a 1-dim object, then always place at 0
Categorical is always one dimension
we are expected to return a 2-d ndarray
if we are passed a datetime64[ns, tz]
delegate
coercion issues  let higher levels handle
Workaround for numpy 1.6 bug
allow passing of > 1dim if its trivial
move to UTC & take
return np.nan
we may need to upcast our fill to match our dtype
reset the sparse values
kludgy, but SparseBlocks cannot handle slices, where the  output is 0-item, so let's convert it to a dense block: it  won't take space since there's 0 items, plus it will preserve  the dtype.
taking on the 0th axis always here
preserve dtype if possible
Python3 compat
make items read only for now
First three elements of the state are to maintain forward  compatibility with 0.13.1.
numpy < 1.7 pickle compat
discard anything after 3rd, support beta pickling format for a  little while longer
note that some DatetimeTZ, Categorical are always ndim==1
multiple blocks that are reduced
reset the placement to the original
single block
compute the orderings of our original data
figure out our mask a-priori to avoid repeated replacements
Warning, consolidation needs to get checked upstairs
Warning, consolidation needs to get checked upstairs
Warning, consolidation needs to get checked upstairs
FIXME: optimization potential
take by position
could be an array indexer!
we must copy here as we are mixed type
non-unique (GH4726)
allow a single nan location indexer
fastpath shortcut for select a single-dim from a 2-dim BM
FIXME: this may return non-upcasted types?
FIXME: use Index.delete as soon as it uses fastpath=True
categorical/spares/datetimetz
This item wasn't present, just insert at end
Remove blocks & update blknos accordingly
This code (ab-)uses the fact that sparse blocks contain only  one item.
unfit_val_locs contains BlockPlacement objects
Newly created block's dtype may already be present.
Should this be a different kind of error??
insert to the axis; this could possibly raise a TypeError
np.append is a lot faster (at least in numpy 1.7.1), let's use it  if we can.
some axes don't allow reindexing with dups
Otherwise, slicing along items axis is necessary.  A non-consolidatable block, it's easy, because there's  only one item and each mgr loc is a copy of that single  item.
canonicalize block order, using a tuple combining the type  name and then mgr_locs because there might be unconsolidated  blocks (say, Categorical) which can only be distinguished by  the iteration order
passed from constructor, single block, single axis
create the block here
provide consolidation to the interleaved_dtype
if we are the same and don't copy, just return
fill if needed
put "leftover" items in float bucket, where else?  generalize?
HACK 2355 definite overflow
empty items -> dtype object
group by dtype
fml
if we are mixing unsigned and signed, then return  the next biggest int type (if we can)
return 1 bigger on the itemsize if unsinged
sort by _can_consolidate, dtype
FIXME: optimization potential in case all mgrs contain slices and  combination of those slices is a slice, too.
no merge
work around NumPy 1.6 bug
numpy deprecation warning to have i8 vs integer comparisions
numpy deprecation warning if comparing numeric vs string-like
Create observation selection vector using major and minor  labels, for converting to panel format.
FIXME: blk_count is unused, but it may avoid the use of dicts in cython
n should be the length of the mask or a scalar here
see if we are only masking values that if putted  will work in the current dtype
make sure that we have a nullable type  if we have nulls
Null blocks should not influence upcast class selection, unless there  are only null blocks, when same upcasting rules must be applied to  null upcast classes.
Concatenating join units along ax0 is handled in _merge_blocks.
Only one block, nothing to concatenate.
Calculate post-reindex shape , save for item axis which will be separate  for each block anyway.
Fastpath detection of join unit not  needing to reindex its block: no ax0  reindexing took place and block  placement was sequential before.  Slow-ish detection: all indexer locs  are sequential (and length match is  checked above).
Omit indexer if no item reindexing is required.
trim_join_unit updates unit in place, so only  placement needs to be sliced to skip min_len.
Passing shape explicitly is required for cases when block is None.
FIXME: cache results of indexer == -1 checks.
No upcasting is necessary
we want to avoid filling with np.nan if we are  using None; we already know that we are all  nulls
preserve these for validation in _concat_compat
External code requested filling/upcasting, bool values must  be upcasted to object to avoid being upcasted to numeric.
No dtype upcasting is done here, it will be performed during  concatenation itself.
If there's no indexing to be done, we want to signal outside  code that this array must be copied explicitly.  This is done  by returning a view and checking `retval.base`.
Handle empty arr case separately: numpy 1.6 chokes on that.
Should be overwritten by base classes
no memory_usage attribute, so fall back to  object's 'sizeof'
don't overwrite existing methods/properties
this ensures that Series.str.<method> is well defined
not ok  {'ra' : { 'A' : 'mean' }}
set the final keys
nested renamer
some selection on the object
we are a Series like object,  but may have multiple aggregations
we are selecting the same set as we are aggregating
we are a DataFrame, with possibly multiple aggregations
no selection
we are aggregating expecting all 1d-returns  but we have 2d
caller can react
degenerate case
make sure we find a good name
create a new object to prevent aliasing
Python 2 compat
Python 3 compat
ndarray compatibility
copy numpy's message here because Py26 raises an IndexError
needs coercion on the key (DatetimeIndex does already)
numpy elementwise comparison warning
if x is a string and arr is not, then we get False and we must  expand the mask to size arr.shape
numpy elementwise comparison warning
asfreq is compat for resampling
have to call np.asarray(xvalues) since xvalues could be an Index  which cant be mutated
if not issubclass(xvalues.dtype.type, np.datetime64):
ignores some kwargs that could be passed along.
GH 5975, scipy.interp1d can't hande datetime64s
GH 10633
return the method for compat with scipy version & backwards compat
Scipy earlier than 0.17.0 missing axis
reshape a 1 dim if needed
reshape back
for test coverage
for test coverage
GH 7325, mask and nans must be broadcastable (also: PR 9308)  Raveling and then reshaping makes np.putmask faster
fallback
we are called internally, so short-circuit
data is an ndarray, index is defined
need to copy to avoid aliasing issues
coerce back to datetime objects for lookup
GH12574: Allow dtype=category only, otherwise error
handle sparse passed here (and force conversion)
return a sparse series here
types
return self._subtyp in ['time_series', 'sparse_time_series']
need to set here becuase we changed the index
we want to call the generic version and not the IndexOpsMixin
complex
coercion
recreate the ndarray
backwards compat
recreate
dispatch to the values if we need
we need to box if we have a non-unique index here  otherwise have inline ndarray/lists
kludge
we can try to coerce the indexer (or this will raise)
pragma: no cover
handle the dup indexing case (GH 4246)
[slice(0, 5, None)] will break if you convert to ndarray,  e.g. as requested by np.median  hack
mpl hackaround
If key is contained, would have returned by now
reassign a null value to iNaT
do the setitem
XXX ignoring the "order" keyword.
set using a non-recursive method
set name if it was passed, otherwise, keep the previous name
ndarray compat
scalar
one but not both
When name is None, __finalize__ overwrites current name
GH 5856/5853
easier to ask forgiveness than permission  if kind==mergesort, it can fail for object dtype  stable sort not available for object dtype  uses the argsort default quicksort
Validate that 'axis' is consistent with Series's single axis.
be subclass-friendly
check/convert indicies here
result is only a string if no path provided, otherwise None
do nothing
coerce datetimelike types
perf shortcut as this is the most common case
GH 846
don't coerce Index types  e.g. indexes can have different conversions (so don't fast path  them)  GH 6140
scalar like
figure out the dtype from the value (upcast if necessary)  need to possibly convert the value here
the result that we want
a 1-element ndarray
This is to prevent mixed-type Series getting all casted to  NumPy string type, e.g. NaN --> '-1IND'.
backwards compatiblity  deprecation TimeSeries, 10890
Add arithmetic!
allow categorical vs object dtype array comparisons for equality  these are only positional comparisons
For comparisons, so that numpy uses our implementation if the compare  ops, which raise
fast path
sanitize input
we are either a Series or a CategoricalIndex
raise, as we don't have a sortable data structure and so  the user should give us one by specifying categories
FIXME
make sure that we always have the same type here, no matter what  we get passed in
NaNs in cats deprecated in 0.17,  remove in 0.18 or 0.19 GH 10748
categories is an Index, which is immutable -> no need to copy
remove all _codes which are larger and set to -1/NaN
GH 10156
0.16.0 ordered change
>=15.0 < 0.16.0
if we are a datetime and period index, return Index to keep metadata
pad / bfill
filling must always be None/nan here  but is passed thru internally
only allow 1 dimensional slicing, but can  in a 2-d case be passd (slice(None),....)
Strip all leading spaces, which format_array adds for columns...
0 = no breaks
replace to simple save space by
require identical categories set
no assignments of values not in categories, but it's always ok to set  something to np.nan
set by position
slicing in Series or Categorical
is this reached?
pylint: disable=W0231,E1101
a compound dtype
unicode representation based upon iterating over self  (since, by definition, `PandasContainers` are iterable)
typ
indexing support
setup the actual axis
addtl parms
construct the args
look for a argument by position
index or columns
prefix with 'i' or 'c' depending on the input axis  e.g., you must do ilevel_0 for the 0th level of an unnamed  multiiindex
put the index/columns itself in the dict
we do it this way because if we have reversed axes, then  the block manager shows then reversed
we must have unique axes
renamer function if passed a dict
start in the axis order to eliminate too many copies
inv fails with 0 len
compat
set in the order of internal names  to avoid definitional recursion  e.g. say fill_value needing _data to be  defined
old pickling format, for compatibility
> 2 dims
add to our internal names set
for a chain
we are trying to reference a dead referant, hence  a copy
this could be a view  but only in a single-dtyped view slicable case
This technically need not raise SettingWithCopy if both are view  (which is not  generally guaranteed but is usually True.  However,  this is in general not a good practice and we recommend using .loc.
see if the copy is not actually refererd; if so, then disolve  the copy weakref
we might be a false positive
a custom message
If the above loop ran and didn't delete anything because  there was no match, this call should raise the appropriate  exception:
delete from the caches
maybe set copy if we didn't actually change the index
convert to a label indexer if needed
create the tuple of the indexer
may need to box a datelike-scalar  if we encounter an array-like and we only have 1 dim  that means that their are list/ndarrays inside the Series!  so just return them (GH 6394)
this could be a view  but only in a single-dtyped view slicable case
check if we are a multi reindex
perform the reindex on the axes
reindex doing multiple operations on different axes if indicated
Process random_state argument
Check weights for compliance
If a series, align with frame
If has nan, set to zero.
Check for negative sizes
allow an actual np.nan thru
compat
Must combine even after consolidation, because there may be  sparse items which are never consolidated into one block.
set the default here, so functions examining the signaure  can detect if something was set (e.g. in groupby) (GH9221)
> 3d
3d
2d or less
passing a single value that is scalar like  when value is None (GH5319), for compat
passed a nested dict/Series
need a non-zero len on all axes
create/use the index  prior default
Tick-like, e.g. 3 weeks
if numeric_only is None, and we can't get anything, we try with  numeric_only=True
defaults
other must be always DataFrame
series/series compat, other must always be a Series
try to align
align with me
if we are NOT aligned, raise as we cannot where index
slice me out of the other
try to set the same dtype as ourselves
we can end up comparing integers and m8[ns]  which is a numpy no no
we need to use the new dtype
GH 2745 / GH 4192  treat like a scalar
GH 3235  match True cond to other
try to not change dtype at first (if try_quick)
let's create a new (if we failed at the above  or not try_quick
we are the same shape, so create an actual object for alignment
if we have a date index, convert to dates, otherwise  treat like a slice
get them all to be in [0, 1]
median should always be included
sort and check for duplicates
when some numerics are found, keep only numerics
set a convenient order for rows
install the indexes
should really _check_ for NA
Do this first, to make sure it happens even if the re.compile  raises below.
This is the new behavior of str_match.
the regex must contain capture groups.
CPython optimized implementation
CPython optimized implementation
save orig to blow up categoricals to the right type
infer from ndim if expand is not specified
if expand is False, result should have the same name  as the original otherwise specified  do not use logical or, _orig may be a DataFrame  which has "name" column
Wait until we are sure result is a Series or Index before  checking attributes (GH 12180)  if result is a boolean np.array, return the np.array  instead of wrapping it into a boolean Index (GH 8875)
Must be a Series
string methods
this is our datetime.pxd
import datetime C API
initialize numpy
We can call the memoryview version of the code
We can call the memoryview version of the code
We can call the memoryview version of the code
GH 2778
GH 2778
GH 2778
GH 2778
single value is NaN
not nan
not nan
not nan
not nan
not nan
not nan
not nan
not nan
not nan
not nan
not nan
not nan
end of the road
end of the road
end of the road
end of the road
end of the road
end of the road
we are done
we are done
('platform_int', 'INT', 'int_'),
exclude NA group
array of each previous indexer seen
Put `generated.pyx` in the same directory as this file
alias for compatibility to simplejson/marshal/pickle.
-*- coding: utf-8 -*-
pass whatever functions you normally would to assertRaises (after the  Exception kind)
other shouldn't be mutated
shallow copy should be the same too
setting should not be allowed
Delegate does not implement memory_usage.  Check that we fall back to in-built `__sizeof__`  GH 12924
don't test boolean / int64 index
if a filter, skip if it doesn't match
freq raises AttributeError on an Int64Index because its not  defined we mostly care about Series hwere anyhow
reverse version of the binary ops
bug brought up by 1079  changed from TypeError in 0.17.0
if we have a datetimelike dtype then needs a view to work  but the user is responsible for that
comparing tz-aware series with np.array results in  TypeError
GH 7261
check DatetimeIndex monotonic path
check DatetimeIndex non-monotonic path
create repeated values, 'n'th element is repeated by n+1 times  freq must be specified because repeat makes freq ambiguous
resets name from Index
attach name to klass
resets name from Index
attach name to klass
create repeated values, 'n'th element is repeated by n+1  times  freq must be specified because repeat makes freq  ambiguous
numpy_array_equal cannot compare arrays includes nan
sort ascending
bins
returned dtype differs depending on original
with NaT
timedelta64[ns]
factorize explicitly resets name
don't test boolean
GH 4060
special case
has_duplicates
GH 11343  though Index.fillna and Series.fillna has separate impl,  test here to confirm these works as the same
value for filling
special assign to the numpy array
freq must be specified because repeat makes freq  ambiguous
check shallow_copied
if there are objects, only deep will pick them up
sys.getsizeof will call the .memory_usage with  deep=True, and add on some GC overhead
See gh-12238
self.assertTrue("__frozen" not in dir(t))
'--with-coverage', '--cover-package=pandas.core'],
with dup column support this method was taken out  GH3679
coerce None
test trying to create block manager with overlapping ref locs
GH2431
reset to False on load
view assertion
what to test here?
compare non-numeric
noops
mgr is not consolidated, f8 & f8-2 blocks
create_single_mgr('sparse', N),
import pudb; pudb.set_trace()
fancy indexer
take/fancy indexer
-*- coding: utf-8 -*-  pylint: disable-msg=W0612,E1101
GH12402 Add a new parameter `drop_first` to avoid collinearity  Basic case
Test the case that categorical variable only has one level.
-*- coding: utf-8 -*-
np.dtype doesn't know about our new dtype
dtypes
numpy compat
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
complex agg
corner cases
A should not be referenced as a bad column...  will have to rethink regex if you change message!
it works!
validate first
PR 9090, related to issue 8979  test nth on MultiIndex, should match .first()
GH  1048
GH 3911, mixed frame non-conversion
5592 revisited, with datetimes
scalar outputs
nothing to group, all NA
single series
group frame by function name
regression
GH 8430
this SHOULD be an int
explicity return a float from my function
correct result
should fail (not the same number of levels)
should fail (incorrect shape)
get attribute
make sure raises error
aggregate
iterate
groups / group_indices
this code path isn't used anywhere else  not sure it's useful
test it works
aggregate
iterate
it works!
corner case
as_index= False, much easier
as_index=True, (used to be different)
test that having an all-NA column doesn't mess you up
weirdo
names are different
weirdo
2113
raise exception for non-MultiIndex
PR8618 and issue 8015
GH 12902
GH 8467  first show's mutation indicator  second does not, but should yield the same results
Low level tinkering could be unsafe, make sure not
return view
it works! 2605
GH 5755 - cumsum is a transformer and should ignore as_index
this is by definition a mutating operation!
GH 6124
Provide a different name for each Series.  In this case, groupby  should not attempt to propagate the Series name since they are  inconsistent.
it works!
accessing the index elements causes segfault
base will be length 0
pathological case of ambiguity
grouped.groups keys are np.datetime64 with system tz  not to be affected by tz, only compare values
confirm obj is not filtered
verify this is testing what it is supposed to test!
GH 995
GH 1268
GH 7929
make sure all these work
when categories is ordered, group is ordered by category's order
this is an unordered categorical, but we allow this
when categories is ordered, group is ordered by category's order
GH9049: ensure backward compatibility
it works!
GH9311, GH6620
len(bins) != len(series) here
compare the results
reset_index changes columns dtype to object
don't die
GH 11682  Timezone info lost when broadcasting scalar datetime to DataFrame
GH 6908 change target column's order
single grouping
with index
Filter DataFrame
Filter Series
^ made manually because this can get confusing!
Transform (a column from) DataFrameGroupBy
Filter DataFrame
Filter Series
^ made manually because this can get confusing!
Transform (a column from) DataFrameGroupBy
Filter DataFrame
Filter Series
^ made manually because this can get confusing!
Transform (a column from) DataFrameGroupBy
Filter DataFrame
Filter Series
^ made manually because this can get confusing!
Transform (a column from) DataFrameGroupBy
Filter DataFrame
Filter Series
^ made manually because this can get confusing!
Transform (a column from) DataFrameGroupBy
previously didn't have access to col A ????
some methods which require DatetimeIndex
'nlargest', 'nsmallest',
e.g., to_csv
np.argsort(items) places NaNs last  np.argsort(items2) may not place NaNs first
GH 2785; due to a regression in NumPy1.6.2
gets called in Cython to check that raising calls the method
GH 4095
with nans
series  print(data.head())
DataFrame - Single and MultiIndex,  group by values, index level, columns
whitelisted methods set the selection before applying  bit a of hack to make sure the cythonized shift  is equivalent to pre 0.17.1 behavior
GH 4095
Grouping on a single column
Tests to make sure no errors if apply function returns all None  values. Issue 9684.
see gh-12811
-*- coding: utf-8 -*-
wrapper to test single str
Make sure each row has this ... in the same place
in non interactive mode, there can be no dependency on the  result of terminal auto size detection
Out off max_columns boundary, but no extending  since not exceeding width
out vertical bounds can not result in exanded repr
Wrap around with None
Truncate with auto detection.
Wrap around with None  Truncate vertically
it works!
it works even if sys.stdin in None
Emable Unicode option
truncate
this should work
12045
GH 12211
it works
representing infs poses no problems
escape embedded tabs in string  GH 2038
big mixed
but not all exactly zero
it works!
big mixed
must be the same result as normal index
python2 default encoding is ascii, so an error should be raised
it works!
GH Bug 9402
GH 781
same but for an index
same for a multi-index
same for a multi-index
empty string
Emable Unicode option
nat in summary
index
https://github.com/pydata/pandas/issues/12125  smoke test for _translate
-*- coding: utf-8 -*-
list of strings / unicode
GH 9129
questionable
questionable
make sure the total items covered by the ranges are a complete cover
exhaustively test all possible mask sequences of length 8
base cases
all new-style classes are hashable by default
numpy.array is no longer collections.Hashable as of  https://github.com/numpy/numpy/pull/5326, just test  pandas.common.is_hashable()
old-style classes in Python 2 don't appear hashable to  collections.Hashable but also seem to support hash() by default
Check with seed
Check with random state object
check with no arg random state
Error for floats or strings
Conversion to Int64Index:
Did we get back our own DF class?
Do we get back our own Series class after selecting a column?
Do we get back our own DF class after slicing row-wise?
GH9776
GH 11808
same but for empty slice of df
mixed casting
to object
datetimelike  Test str and unicode on python 2.x and just str on python 3.x
pylint: disable-msg=W0612,E1101
column order not necessarily sorted
will raise SyntaxError if trying to create namedtuple
mixed type
mixed type
reg name
set_index
reset_index
drop_duplicates
sort
sort_index
sortlevel
fillna
replace
rename
Series
reset_index
fillna
replace
rename
name tracking
omit values
From a mixed type dataframe
From a dataframe with incorrect data type for fill_value
Fill with non-category results in NaN entries similar to above
When mixed types are passed and the ints are not level  names, raise
check reversibility
check composability of unstack
GH 2929
Stacking a single level should not make any all-NaN rows,  so df.stack(level=level, dropna=False) should be the same  as df.stack(level=level, dropna=True).
all
bad input
limit and value
with datelike  GH 6344
it works!
it works
Series treated same as dict
disable this for now
scipy route
all good
'--with-coverage', '--cover-package=pandas.core']
boolean with the duplicate raises
not-comparing like-labelled
testing iget
See gh-12344
new object, single-column
inplace, single
create new object, multi-column
inplace
corner case
Series
multiple columns
convert index to series
assignt to frame
keep the timezone
list of datetimes with a tz
it works!
index
have to pass something
no change, position
no change, labels
preserve column names
test resetting in place
GH5818
Check that set_index isn't converting a MultiIndex into an Index
Check actual equality
Check that [MultiIndex, MultiIndex] yields a MultiIndex rather  than a pair of tuples
Check equality
must raise
GH 10174
GH 10174
exclude datetime
compare the results
first dataframe
Cython code should be unit-tested directly
Same index, copies values but not index if copy=False
length zero
pass non-Index
copy with no axes
length zero
axis=0
axis=1
reindex_axis
GH4746, reindex on duplicate index error messages
reindex fails
test other non-float types
try to align dataframe to series along bad axis
align dataframe to series with broadcast or not
empty left
empty right
both empty
GH 910
these must be the same results (but flipped)
series + frame
like
shouldn't remove anything
pass in None
objects
regex
doesn't have to be at beginning
homogeneous
neg indicies
axis = 1
mixed-dtype
neg indicies
axis = 1
by dtype
ints are weird
no-op case
Ensure copy, do I want this?
triggers in-place consolidation
unconsolidated
mixed lcd
guess all ints are cast to uints....
check dtypes
'dt3' : date_range('20130101 00:00:01',periods=3,freq='s'),
these work (though results may be unexpected)
10822  invalid error message on dt inference
API/ENH 9607
use the default copy=True, change a column
make sure we did not change the original DataFrame
API/ENH 9607
use the copy=False, change a column
make sure we did change the original DataFrame
copy objects
buglet
empty
tz frame
force numeric conversion
via astype, but errors
(wesm) Unclear how exactly this is related to internal matters  uncommenting these makes the results match  for col in xrange(100, 200):     wasCol[col] = 1     df[col] = nan
dtypes other than float64 1761
it works!
when dtypes of pandas series are different  then ndarray will have dtype=object,  so it need to be properly handled
min_periods no NAs (corner case)
regular
make sure order does not matter
Top value is a boolean value that is False
corner case
GH 423
mixed types (with upcasting happening)
GH 676
axis = 0
axis = 1
it works
fix issue
axis = 0
axis = 1
it works
fix issue
set one entry to a non-number str
axis = 0
axis = 1
works
fix issue
axis = 0
axis = 1
fix issue
ints
ints32
integers
mixed-type frames
bottom
top
bottom
top
Neither of these should raise an error because they  are explicit keyword arguments in the signature and  hence should not be swallowed by the kwargs parameter
set some NAs
HACK: win32
check dtypes
bad axis  make sure works on mixed-type frame
min
max
results in an object array
excludes numeric
works when only those columns are selected
ensure this works, bug report
xs sum mixed type, just want to know it works...
don't blow up
bad axis
partial overlapping columns
cols and index:
against regular index
single column
deprecate take_last
consider everything
in this case only
GH 11864
single column
multiple columns
single column
single column
deprecate take_last
multi column
single column
deprecate take_last
multi column
single column
single column
single column
consider everything
in this case only
Test that rounding an empty DataFrame does nothing
Here's the test frame we'll be working with
This should also work with np.round (since np.round dispatches to  df.round)
Round with a list
Dict with unknown elements
float input to `decimals`
String input
List input
Non integer Series inputs
nan in Series round
Rounding with decimal is a ValueError in Python < 2.7
Make sure this doesn't break existing Series.round
GH11763  Here's the test frame we'll be working with
GH 2747
Check alignment
Check series argument
can pass correct-length arrays
it works
GH10844
force these all to int64 to avoid platform testing issues
lambda syntax
original is unmodified
Non-Series array-like  original is unmodified
overwrite
lambda
GH 4107, more descriptive error message
preserve columns name field
original frame
result
GH4032, inserting a column and renaming causing errors
smaller hits python, larger hits numexpr
ops as strings
these are commutative
these are not
GH 11485
no local variable c
no column named 'c'
we don't pick up the local 'sin'
"index" should refer to the index
test against a scalar
can't reference ourself b/c we're a local so @ is necessary
don't have the pandas parser
GH 12467  combining datetime tz-aware and naive DataFrames
dict
can append when name set
Empty df append empty df
disjoint
same index
overlap
reverse overlap
corner cases
no nats
nats
GH3590, modulo as ints
numpy has a slightly different (wrong) treatement
integer div, but deal with the 0's (GH 9144)
numpy has a slightly different (wrong) treatement
operator.neg is deprecated in numpy >= 1.9
GH4947  bool comparisons should return bool
GH4604, automatic casting here
ndim >= 3
corner cases
list/tuple
special case for some reason
cases which will be refactored after big arithmetic refactor
empty
out of order
mix vs float64, upcast
mix vs mix
with int
Series
TimeSeries
10890  we no longer allow auto timeseries broadcasting  and require explict broadcasting
empty but with non-empty index
gt
==
not shape compatible
trivial
corner cases
trivial
corner cases
add
sub
GH 5104  make sure that we are actually changing the object
dtype change
test roundtrip
no index
tz, 8260
GH3454
we wrote them in a different order  so compare them in that order
wrote in the same order
dupe cols
dupe cols with selection
GH3437
N=35000
read_Csv disambiguates the columns by  labeling them dupe.1,dupe.2, etc'. monkey patch columns
test roundtrip with inf, -inf, nan, as full columns and mix
round trip
needed if setUP becomes a classmethod
try multiindex with dates
do not load index
no index
needed if setUP becomes classmethod
writing with no index
whatsnew example
invalid options
catch invalid headers
write with cols
empty
Commas inside fields should be correctly escaped when saving as CSV.
add in some nans
date cols
GH3457
read_csv will rename the dups columns
don't check_names as t != 1
GH 8215  Make sure we return string for consistency with  Series.to_csv()
test the round trip - to_csv -> read_csv
test the round trip - to_csv -> read_csv
test the round trip - to_csv -> read_csv
explicitly make sure file is xzipped
zip compression is not supported and should raise ValueError
Check that the data was put in the specified format
Check that columns get converted
Columns don't get converted to ints by read_csv
test NaTs
make sure we are not failing on transitions
assert working
GH 3010, constructing with odd arrays
this is ok
this is not ok
this is ok
GH 3243
mixed floating and integer coexinst in the same frame
add lots of types
GH 622
GH10952
col2 is padded with NaN
Corner cases
empty dict plus index
GH10856  dict with scalar values should raise error, even if columns passed
passing an empty array with columns specified.
embedded data frames
GH 1491
GH 10160
GH 10160
mat: 2d matrix with shpae (3, 2) to input. empty - makes sized  objects  2-D input
0-length axis
masked int promoted to float  2-D input
masked bool promoted to object  2-D input
cast type
call assert_frame_equal for all selections of 3 arrays
fill the comb
specify columns
specify index
empty but with specified dtype
does not error but ends up float
1783 empty dtype object
used to be in test_matrix.py
no data specified
Empty generator: list(empty_gen()) == []
GH 3783  collections.Squence like
see gh-13304
the first dict element sets the ordering for the DataFrame,  even if there are conflicting orders from subsequent ones
2234
this is a bit non-intuitive here; the series collapse down to arrays
name
no name
it works! 2079
GH 9428
GH 7594  don't coerce tz-aware
GH 7822  preserver an index with a tz on dict construction
test list of lists/ndarrays
GH 2751 (construction with no index specified), make sure we cast to  platform values
allow single nans to succeed
multiple nans should fail
without names, it should go to last ditch
demo missing data
MultiIndex
GH 2623
this may fail on certain platforms because of a numpy issue  related GH6140
construction with a null in a recarray  GH 6140
tuples (lose the dtype info)
created recarray and with to_records recarray (have dtype info)
list of tupels (no dtype info)
tuples is in the order of the columns
test exclude parameter & we are casting the results here (as we don't  have dtype info to recover)
columns is in a different order here than the actual items iterated  from the dict
should pass
should fail
2633
mixed type
simplest cases  regex -> value  obj frame
mixed
everything with compiled regexs as well
mixed
simplest cases  regex -> value  obj frame
mixed
mixed
mixed frame to make sure this doesn't break things
empty
in this case, should be the same as the not nested version
dtypes
bools
empty with index
small one
columns but no index
no columns or index
big one
columns are not sortable
GH 12182
it works!
GH11761
memory usage is a lower bound, so print it as XYZ+ MB
excluded column with object dtype, so estimate is accurate
Ensure df size is as expected  (cols * rows * bytes) + index size
assert deep works only on object
sys.getsizeof will call the .memory_usage with  deep=True, and add on some GC overhead
int dtype
GH 4533
naive shift
shift by 0
shift by DateOffset
Regression test for 8019
PeriodIndex
DatetimeIndex
neither specified
both specified
start specified
end specified
test does not blow up on length-0 DataFrame
GH12800
ufunc
aggregator
empty
2476
Ensure that x.append hasn't been called
no reduction
the result here is actually kind of ambiguous, should it be a Series  or a DataFrame?
GH 465, function returning tuples
GH 2909, object conversion to float in constructor?
9816 deprecated
sortlevel
axis=0
by column
GH4839
check for now
test with multiindex, too
also, Series!
use .sort_values 9816
use .sort_values 9816
use .sort_values 9816  multi-column 'by' is separate codepath
multi-column 'by' is separate codepath
convert tuples to a list of tuples  use .sort_values 9816
slicing
GH 12533
boolean indexing
test that Series work
test that Series indexers reindex  we are producing a warning that since the passed boolean  key is not the same as the given index, we will reindex  not sure this is really necessary
test df[df > 0]
add back other columns to compare
where dtype conversions  GH 3733
2096
both of these should succeed trivially
1942
this is partially a view (e.g. some blocks are view)  so raise/warn
dtype changing GH4204
a df that needs alignment first
indexed with same shape but rows-reversed df
upcast
set existing column
difficulties with mixed-type data
created as float type
self.assertIsNotNone(dm.objects)
cache it
slice rows with labels, inclusive!
slice columns
get view
this is OK
so is this
case 2
case 3: slicing rows, etc.
case 5
case 6: slice rows with labels, inclusive!
case 7: slice columns
slice indices
slice with labels
get view with single block  setting it triggers setting with copy
tmp correctly sets the dtype  so match the exp way
labels that aren't contained
1432
return self if no slicing...for now
positional xs
single column
return view
slice of mixed-frame
case 1: set cross-section for indices
case 2, set a section of a column
case 3: full xs
single column
individual value
individual value
from 2d, set with booleans
boolean index misaligned labels
loc_float changes this to work properly
positional slicing only via iloc!
float slicing
check our dtypes
allow this syntax
allow this syntax
1201
1201
2199
1943
10711, deprecated
slice
verify slice is view  setting it makes it raise/warn
list of integers
10711, deprecated
slice
verify slice is view  and that we are setting a copy
list of integers
reindex by these causes different MultiIndex levels
xs get column
view is returned if possible
only add to the numeric items
dtypes
check getting
other is a frame
check other is ndarray
integers are upcast, so don't check the dtypes
invalid conditions
where inplace
aligining
GH 3311
GH 9736
Upcast needed
DataFrame vs DataFrame
GH8801
GH4071
setitem
-*- coding: utf-8 -*-  pylint: disable-msg=W0612,E1101
no op
mixed
min elements
ok, we only check on first part of expression
-*- coding: utf-8 -*-
handle timedelta dtypes
some nanops handle object dtypes better than their numpy  counterparts, so the numpy functions need to be given something  else
Test numeric ndarray
Test object ndarray
Test non-convertible string ndarray
Samples from a normal distribution.
Generate some sample data.
The unbiased estimate.
The underestimated variance.
The overestimated variance.
Regression test for GH 10242 (test data taken from GH 10489). Ensure  that variance is stable.
Test data + skewness value (computed with scipy.stats.skew)
Test data + kurtosis value (computed with scipy.stats.kurtosis)
!/usr/bin/env python  coding: utf-8
GH 6106, GH 9322
GH 9184
GH3638
iter must yield a Series
indices of each yielded Series should be equal to the index of  the original Series
each element of the series is either a basestring/str or nan
desired behavior is to iterate until everything would be nan on the  next iter so make sure the last element of the iterator was 'l' in  this case since 'wikitravel' is the longest string
nothing to iterate over so nothing defined values should remain  unchanged
single array
unicode
case insensitive without regex
unicode
unicode
unicode
mixed
unicode
mixed
unicode
Old match behavior, deprecated (but still default) in 0.13
mixed
unicode
mixed
unicode
Contains tests like those in test_match and some others.
mixed
unicode
only non-capturing groups
one group, no matches
Contains tests like those in test_match and some others.
mixed
unicode
only non-capturing groups
GH9980, GH8028
mixed
mixed
unicode
mixed
unicode
mixed
unicode
mixed
unicode
If fillchar is not a charatter, normal str raises TypeError  'aaa'.ljust(5, 'XY')  TypeError: must be char, not str
unicode
re.split 0, str.split -1
unicode
2119
mixed
unicode
mixed
unicode
mixed
unicode
https://github.com/pydata/pandas/issues/10673
regular instance creation
casting
corner case
GH 6274  infer freq of same
GH 5460issuecomment-44474502  it should be possible to convert any object that satisfies the numpy  ndarray interface directly into an Index
with arguments
these are ok
with arguments
it works!
pass on name
different length
Must also be an Index
GH 7256  validate neg/pos inserts
test 0th element
test Nth element that follows Python list behavior
test loc +/- neq (0, -1)
test empty
either depeidnig on numpy version
index
pd.DatetimeIndex is excluded, because it overrides getitem and should  be tested separately.
np.ndarray only accepts ndarray of int & bool dtypes, so should  Index.
Corner cases
Corner cases
empty
from bug report
doesn't fail test unless there is a check before `+=`
different names
same names
with empty
with everythin
__xor__ syntax
windows has different precision on datetime.datetime.now (it doesn't  include us since the default for Timestamp shows these but Index  formating does not we are skipping
bug I fixed 12/20/2011
GH10411
errors='ignore'
intersection broken?  needs to be 1d like idx1 and idx2
union broken
set
empty, return dtype bool
Float64Index overrides isin, so must be checked separately
GH 9910
right_idx in this case because DatetimeIndex has join precedence over  Int64Index
GH6552
Must preserve name even if dtype changes.
GH7774
GH7774
suppress flake8 warnings
Emable Unicode option
this is testing for pickle compat
need an object to create with
GH8083 test the base class for shift
GH11193, when an existing index is passed, and a new name is not  specified, the new index should inherit the previous object name
boolean context compat
test for validity
don't tests a MultiIndex here (as its tested separated)
rename in place just leaves tuples and other containers alone
don't tests a MultiIndex here (as its tested separated)
9816 deprecated
separately tested
separate
GH 10791
non-iterable input
12591 deprecated
test 0th element
tested in class
either depending on numpy version
GH9947, GH10637
raise TypeError or ValueError (PeriodIndex)  PeriodIndex behavior should be changed in future version
coerces to float (e.g. np.sin)
raise AttributeError or TypeError
raise TypeError or ValueError (PeriodIndex)
results in bool array
GH 11343, added tests for hasnans / isnans
cases in indices doesn't include NaN
in general not true for RangeIndex
truediv under PY3
GH 8608  add/sub are overriden explicity for Float/Int Index
interops with numpy arrays
GH 9244
invalid
representable by slice [0:2:2]  self.assertRaises(KeyError, idx.slice_locs, np.nan)
downcast
object
pass list, coerce fine
from iterable
scalar raise Exception
copy
this should not change index
preventing casting
coerce things
but not if explicit dtype passed
not monotonic
no guarantee of sortedness, so sort for comparison purposes
monotonic
can't
shouldn't
turn me to an Index
specify dtype
these are generally only equal when the categories are reordered
GH 10039  set ops (+/-) raise TypeError
invalid
assert codes NOT in index
append cats with the same categories
empty
with objects
invalid objects
test 0th element
test Nth element that follows Python list behavior
test empty
invalid
either depeidnig on numpy version
this IS equal, but not the same class
determined by cat ordering
non-unique
non-unique, slicable
formatting
long format  this is not reprable
Emable Unicode option
fill by value not in categories raises ValueError
set names for multiple levels
side note - you probably wouldn't want to use levels and labels  directly like this - but it is possible.
level changing [w/o mutation]
level changing [w/ mutation]
level changing specific level [w/o mutation]
level changing multiple levels [w/o mutation]
label changing [w/o mutation]
label changing [w/ mutation]
label changing specific level [w/o mutation]
label changing multiple levels [w/o mutation]
shouldn't scalar data error, instead should demand list-like
shouldn't scalar data error, instead should demand list-like
shouldn't scalar data error, instead should demand list-like
should have equal lengths
should have equal lengths
should have equal lengths
make sure level setting works  non-inplace doesn't kill _tuples [implementation detail]  and values is still same too
inplace should kill _tuples
and again setting inplace should kill _tuples, etc
names are assigned in __init__
external API
deprecated properties
levels should be (at least, shallow copied)
labels doesn't matter which way copied
names doesn't matter which way copied
sort order should be copied
names should be applied to levels
changing names should change level names on object
but not on copies
and copies shouldn't change original
Check that code branches for boxed values produce identical results
empty
this blows up
0.7.3 -> 0.8.0 format manage
MultiIndex is never numeric
scalar
slice
works  should there be a test case here???
need to construct an overflow
the fact that is works means it's consistent
after < before
pass non-MultiIndex
create index with duplicates
GH1538
levels are different
some of the labels are different
names are metadata, they don't change id
corner case, pass self or empty thing:
corner case, pass self
empty intersection: disjoint
empty, but non-equal
raise Exception called with non-MultiIndex
name from empty array
error='ignore'
compare the results
key not contained in all levels
key wrong length
FIXME data types changes to float because  of intermediate nan insertion;
pare down levels
handle int64 overflow if possible
no dups
no overflow
overflow possible
formatting
long format
tested elsewhere
should not segfault GH5123  NOTE: if MI representation changes, may make sense to allow  isnull(MI)
if this fails, probably didn't reset the cache correctly.
GH6552
GH5620
GH9785
indexing with IndexSlice
Even though this syntax works on a single index, this is somewhat  ambiguous and we don't want to extend this behavior forward to work  in multi-indexes. This would amount to selecting a scalar from a  column.
partial string match on year only
partial string match on date
partial string match on date and hour, from middle
partial string match on secondary index
tuple selector with partial string match on date
Slicing date on first level should break (of course)
incompat tz/dtype
GH 13149, GH 13209
GH 13149, GH 13209
GH 13149, GH 13209
other is ndarray or Index
GH 8367  round-trip of timezone
GH 11343
GH 13149, GH 13209
GH 13149, GH 13209
GH 11343
GH 13149, GH 13209
GH 13149, GH 13209
GH 13149, GH 13209
normal ops are also tested in tseries/test_timedeltas.py
GH 11343
we don't allow on a bare Index
pass thru w and w/o copy
an invalid range
GH12288
truediv under PY3
__mul__
__pow__
invalid passed type
test 0th element
either depending on numpy version
we don't allow object dtype for RangeIndex
join with Int64Index
join with RangeIndex
Join with non-RangeIndex
no guarantee of sortedness, so sort for comparison purposes
Join two RangeIndex
Join with Int64Index
Join withRangeIndex
Join with Int64Index
Join withRangeIndex
memory savings vs int index
constant memory usage
can't
shouldn't
GH 8608  add/sub are overriden explicity for Float/Int Index
interops with numpy arrays
RangeIndex() is a valid constructor
scalar indexing
slicing  slice value completion
positive slice values
negative slice values
stop overshoot
reverse
unicode
bytes for python3
raises
reverse
reverse
reverse
reverse
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
No exceptions should be thrown
We need to pass an expression on the stack to ensure that there are  not extra references hanging around. We cannot rewrite this test as    buf = b[:-3]    as_stolen_buf = move_into_mutable_buffer(buf)  because then we would have more than one reference to buf.
materialize as bytearray to show that it is mutable
string array of bytes
object array of bytes
too many characters
make sure record array works
see gh-13320
GH 7431  cannot infer more than this as only a single element
GH 8974
test that we can detect many kinds of integers
-*- coding: utf-8 -*-
doc example reshaping.rst
nan still maps to na_sentinel when sort=False
get_labels appends to the vector  to_array resizes the vector
resizing to empty is a special case
GH 9431
GH 9431
preserve order?
corner case
need to use a stable sort
in python we can force the default encoding for this test
restore the former encoding
preserver if non-object
force numeric conversion
preserver if non-object
names match, preserve
names don't match, don't preserve
similiarly for .cat, but with the twist that str and dt should be  there if the categories are of that type first cat and str
HACK: By doing this in two stages, we avoid 2to3 wrapping the call  to .keys() in a list()
assert is lazy (genrators don't define reverse, lists do)
default deep is True
Did not modify original Series
we DID modify the original Series
GH 11794  copy of tz-aware
default deep is True
it works!
test numpy compat with Series as sub-class of NDFrame
.item()
using an ndarray like function  assert_series_equal(result,expected)
ravel
result empty Index(dtype=object) as the same as original
result empty Float64Index as the same as original
str accessor only valid with string values
ffill
invalid axis
GH12723
try time interpolation on a non-TimeSeries  Only raises ValueError if there are NaNs.
'values' is synonymous with 'index' for the method kwarg
Provide 'forward' (the default) explicitly here.
raises an error even if no limit is specified.
These tests are for issue 9218 -- fill NaNs in both directions.
Check that this works on a longer series of nans.
These test are for issue 10420 -- flow back to beginning.
These test are for issue 11115 -- limit ends properly.
scipy
non-scipy
GH 7173
GH 10633
'--with-coverage', '--cover-package=pandas.core']
wrong type
wrong length
works
dict
check inplace
no change, position
no change, labels
object dtype
datetime64[ns] dtype
timedelta64[ns] dtype
GH7661
GH 10174
test with and without interpolation keyword
GH 10174
GH 10174
interpolation other than linear
object dtype
all nan/empty
datetimeindex with tz
we test freq below
trying to set a copy
dtype may be S10 or U10 depending on python version
GH 8689
GH 9322
GH 844
GH 6915  overflowing on the smaller int dtypes
test with integers, test failure
1 - element series with ddof=1
1 - element series with ddof=1
no longer works as the return type of np.diff is now nd.array
add some NaNs
idxmax, idxmin, min, and max are valid for dates
skipna or no
check the result is correct
xref 9422  bottleneck >= 1.0 give 0.0 for an allna Series sum
dtype=object with None, it works!
check date range
check on string data
Invalid axis.
Unimplemented numeric_only parameter.
with missing values
Alternative types, with implicit 'object' dtype.
Check skipna, with implicit 'object' dtype.
bool_only is not implemented with level option.
bool_only is not implemented alone.
GH 9144
float
timedelta64[ns]
full overlap
partial overlap
No overlap
all NA
these methods got rewritten in 0.8
full overlap
partial overlap
No overlap
all NA
min_periods
Check index alignment
Check series argument
714 also, dtype=float
rankdata returns a ndarray
GH 6270  looks like a buggy np.maximum.accumulate for numpy 1.6.1, py 3.2
datetime64[ns]
fails on dtype conversion in the first place
timedelta64[ns]
index min/max
GH 2982  with NaT
abs
max/min
add some NaNs
skipna or no
all NaNs
datetime64[ns]
argmin is aliased to idxmin
add some NaNs
skipna or no
all NaNs
argmax is aliased to idxmax
it works! 1807
9816 deprecated
GH 5856/5853  Series.sort_values operating on a view
descending
For 11402
sort_index
compat on axis
9816 deprecated
something object-type  no failure
not supported on some archs  Series([3., 2, 1, 2, 5], dtype='complex256'),
GH 9416
GH 4554
check DatetimeIndex outputs the same result
check DatetimeIndex outputs the same result
most dtypes are tested in test_base.py
check CategoricalIndex outputs the same result
check CategoricalIndex outputs the same result
nothing used from the input
Holes filled from input
mixed types
corner case
GH 3217
round-tripping with self & like self
it works!
GH 9144
series on the rhs
scalar Timestamp on rhs
roundtrip
inplace
GH 4532  operate with pd.offsets
roundtrip
roundtrip
GH 4521  divide/multiply by integers
op
astype
reverse op
check that we are getting a TypeError  with 'operate' (from core/ops.py) for the ops that are not  defined
odd numpy behavior with scalar timedeltas
subtraction
without a Series wrapping the NaT, it is ambiguous  whether it is a datetime64 or timedelta64  defaults to interpreting it as timedelta64
addition
multiplication
GH 7500  datetimelike ops need to align
name is reset
GH11339  comparisons vs tuple
test that comparisons work
GH4968  invalid date/int comparisons
rhs is bigger
vs empty
vs non-matching
vs scalars
GH 9016: support bitwise op for integer types
GH 56
really raise this time
rpow does not work with DataFrame
should accept axis=0 or axis='rows'
arithmetic integer ops with an index
GH10483
GH 8215  Series.to_csv() was returning None, inconsistent with  DataFrame.to_csv() which returned string
datetime64
Pass in scalar is disabled
deprecation TimeSeries, 10890
Recognize TimeSeries
Pass in Series
Ensure new index is not created
Mixed type Series
raise on MultiIndex GH4187
the are Index() and RangeIndex() which don't compare type equal  but are just .equals
GH8909
GH 7431  inference on the index
1572
in theory this should be all nulls, but since  we are not specifying a dtype is ambiguous
invalid astypes
invalid dates can be help as object
export
concat
astype
astype - datetime64[ns, tz]
short str
formatting with NaT
long str
GH 12169 : Resample category data with timedelta index  construct Series from dict as data and TimedeltaIndex as index  will result NaN in result Series data
this should work
GH3283
basic
mixed with NaT
improved inference  GH5689
valid astype
invalid casting
this is an invalid casting
leave as object here
empty
with NaNs
with Nones
0 as name
tidy repr
with empty series (4651)
it works (with no Cython exception barf)!
it works!
GH 6863
replace list with a single value
replace list with a single value
replace with different values
replace with different values with 2 lists
replace inplace
malformed
make sure that we aren't just masking a TypeError because bools don't  implement indexing
should NOT upcast to float
MUST upcast to float
casts to float
replace list with a single value
replace with different values
replace with different values with 2 lists
corner case
legacy support
xref 8260  with tz
PeriodIndex
DatetimeIndex
neither specified
both specified
start specified
end specified
corner case, empty series returned
accepts strings
in there
no as of value
testing with timezone, GH 2785
repeat all the above with naive datetimes
also test partial date slicing
testing with timezone, GH 2785
comparison dates with datetime MUST be localized!
testing with timezone, GH 2785
GH 2782
accepts strings
in there
no as of value
Just run the function
int dtype
neg n
0
datetime diff (GH3100)
timedelta diff
Just run the function
Now run it with the lag parameter
GH12800
elementwise-apply
index but no data
not vectorized
input could be a dict
function
All labels should be filled now
not vectorized
GH 5542  should delete the item inplace
empty
missing
None  GH 5652
10711, deprecated
10711, deprecated
10711, deprecated
pass a slice
test slice is a view
list of integers
passing list is OK
invalid because of the boolean indexer  that's empty or not-aligned
getitem
setitem
nans raise exception
don't segfault, GH 495
GH 917
caused bug without test
GH 12533
equivalent of an append
note labels are floats
test return view
caught this bug when writing tests
set item that's not contained
Test for issue 10193
change dtypes  GH 4463
invalid tuples, e.g. self.ts[:, None] vs. self.ts[:, 2]
weird lists. [slice(0, 5)] will work but not two slices
slice with indices
boolean
ask for index value
this is OK
so is this
test alignment
GH 2745
GH 4667  setting with None changes dtype
slice
slice with step
neg slices
list
GH 4548  inplace updating not working with dups
compare with tested results in test_where
set slice with indices
set index value
GH 9280
similiar indexed series
setting
getting
not monotonic
not monotonic
bad axis
empty left
empty right
both empty
empty left
empty right
both empty
these must be the same results (but flipped)
__array_interface__ is not defined for older numpies  and on some pythons
return a copy the same index here
reindex coerces index.dtype to float, loc/iloc doesn't
corner case: pad empty series
pass non-Index
bad fill method
this changes dtype because the ffill happens after
this should work fine
if NaNs introduced
NO NaNs introduced
A series other than float, int, string, or object
this should work fine
if NaNs introduced
NO NaNs introduced
GH 7179
ints
objects
bools
!/usr/bin/python  -*- coding: utf-8 -*-
can't register an already registered option
non-existent keys raise KeyError
gets of non-existent keys fail
gets of non-existent keys fail
we can deprecate non-existent options
make sure callback kicks when using this form of setting
Ensure creating contexts didn't affect the current context.
Ensure the correct value is available inside the context.
Ensure the current context is reset
Unimplemented numeric_only parameter.
ensure propagate to potentially prior-cached items too
Test panel.iteritems(), aka panel.iteritems()  just test that it works
items
major
minor
corner case, empty thing
LongPanel with one item
object dtype
boolean dtype
not contained
not contained
mixed-type yields a copy
all 3 specified
2 specified
only 1
1603
get DataFrame  item
major axis, axis=1
minor axis, axis=2
make sure it's always a view
GH3830, panel assignent by values/frame
Assignment by Value Passes for 'a2'
Assignment by DataFrame Ok w/o loc 'a2'
Assignment by DataFrame Fails for 'a2'
versus same index
versus non-indexed same objs
versus different objs
versus scalar
with BlockManager
no copy
copy
can't cast
GH 411
intersect
a pathological case
corner, blow up
ufunc
transforms
on windows this will be in32
GH10332
items
major
raise exception put both major and major_axis
minor
this ok
with filling
don't necessarily copy
with and without copy full reindexing
1823
it works!
this should also work
duplicate axes
filtered
unfiltered
names
Previously, this was mutating the underlying index and changing its  name
GH 8704  with categorical
items
PeriodIndex
DatetimeIndex
Items
trying to set non-identically indexed panel
careful, mutation
one item
truncate on dates that aren't in there
throw proper exception
test versus Panel version
test versus Panel version
corner case, empty
GH8152
remove the info axis
rename a single axis
relabeling values passed into self.rename
rename a single axis  scalar values
get the numeric data
non-inclusion
GH 7725
empty
these work (though results may be unexpected)
simple boolean
combine_first
add non-like
simple boolean
this is a name matching op
Panel + dims
0-len
bounded
neg index
Check for error when random_state argument invalid.
Giving both frac and N throws error
Check that raises right error for negative lengths
Make sure float values of `n` give error
Weight length must be right
Check won't accept negative weights
Check inf and -inf throw errors:
All zeros raises errors
All missing weights
Check np.nan are replaced by zeros.
Check None are also replaced by zeros.
See gh-12301
get the numeric data
allow single item via bool method
reset
idempotency
not idempotent
allow single item via bool method
GH13104
'all' includes numpy-dtypes + category
merging with override  GH 6923
reset
MultiIndex  GH7846
Invalid level
Test for consistent setattr behavior when an attribute and a column  have the same name (Issue 8994)
idempotency
non-convertible
A few dataframe test with degenerate weights.
Ensure proper error if string given as weight for Series, panel, or  DataFrame with axis = 1.
Check weighting key error
Check that re-normalizes weights that don't sum to one.
Different axis arg types
Check out of range axis values
Test weight length compared to correct axis
Check weights with axis = 1
Weights have index values to be dropped because not in  sampled DataFrame
Weights have empty values to be filed with zeros
No overlap in weight and sampled DataFrame indices
squeezing
don't fail with 0 length dimensions GH11229 & GH8999
calls implementation in pandas/core/base.py
different dtype
different index
different columns
MultiIndex
NaN in index
-*- coding: utf-8 -*-
corner case, empty thing
object dtype
boolean dtype
Panel
GH 8702
versus non-indexed same objs
versus different objs
not contained
not contained
mixed-type
all 4 specified
3 specified
2 specified
only 1
with BlockManager
no copy
copy
can't cast
GH 411
corner, blow up
labels
items
raise exception put both major and major_axis
don't necessarily copy
with filling
don't necessarily copy
this should also work
technically this is allowed
A should not be referenced as a bad column...  will have to rethink regex if you change message!
API change for disallowing these types of nested dicts
GH 10565
in prior versions, we would allow how to be used in the resample  now that its deprecated, we need to handle this in the actual  aggregation functions
GH 12669
valid
see gh-12811
valid
see gh-12811
valid
see gh-12811
not valid: com < 0
not valid: span < 1
not valid: halflife <= 0
not valid: alpha <= 0 or alpha > 1
see gh-12811
other methods not Implemented ATM
ok
bad axis
bad axis
GH 8238
GH 8238
GH 8238
Gh 8238
DataFrame
invalid method
min_periods
GH 8238
GH 8238
GH 8238
GH 8238
GH 8238
suppress warnings about empty slices, as we are deliberately testing  with a 0-length Series
1850
it works!
new API
excluding NaNs correctly
min_periods is working correctly
min_periods=0
check via the API calls if name is provided
check via the moments API
check time_rule works
GH 7925
shifter index
check that ignore_na defaults to False
excluding NaNs correctly
ewmstd, ewmvol, ewmvar (with bias=False) require at least two  values
check series of length 0
pass in ints
data is a tuple(object, is_contant, no_nans)
check that mean equals mock_mean
check that correlation of a series with itself is either 1 or NaN  self.assertTrue(_non_null_values(corr_x_x).issubset(set([1.])))   restore once rolling_cov(x, x) is identically equal to var(x)
check mean of constant series
check correlation of constant series with itself is NaN
check variance debiasing factors
check that var(x) == cov(x, x)
check that var(x) == std(x)^2
check that biased var(x) == mean(x^2) - mean(x)^2
can only easily test two Series with similar  structure
check that cor(x, y) is symmetric
check that cov(x, y) is symmetric
check that cov(x, y) == (var(x+y) - var(x) -  var(y)) / 2
check that corr(x, y) == cov(x, y) / (std(x) *  std(y))
check that biased cov(x, y) == mean(x*y) -  mean(x)*mean(y)
suppress warnings about empty slices, as we are deliberately testing  with empty/0-length Series/DataFrames
test consistency between expanding_xyz() and either (a)  expanding_apply of Series.xyz(), or (b) expanding_apply of  np.nanxyz()
GH 8269
suppress warnings about empty slices, as we are deliberately testing  with empty/0-length Series/DataFrames
test consistency between rolling_xyz() and either (a)  rolling_apply of Series.xyz(), or (b) rolling_apply of  np.nanxyz()
binary moments
test for correct bias correction
GH3155  don't blow the stack
check series of length 0
check series of length 1
scipy needed for rolling_window
scipy needed for rolling_window
suppress warnings about incomparable objects, as we are deliberately  testing with such column labels
yields all NaN (0 variance)
yields all NaN (window too small)
yields all NaN (0 variance)
yields all NaN (window too small)
min_periods is working correctly
min_periods=0
-*- coding: utf-8 -*-
1.5 added PolyCollections to legend handler  so we have twice as many items.
unique and colors length can be differed  depending on slice value
Line2D may contains string color expression
returned as list of np.array
If minor ticks has NullFormatter, rot / fontsize are not  retained
check something drawn on visible axes
check axes coordinates to estimate layout
should be fixed when the returning default is changed
should be fixed when the returning default is changed
GH 6951
GH 9905
Default rot 0
without wedge labels
with less colors than elements
GH 9610
ticks are values, thus ticklabels are blank
check incorrect lengths and types
in mpl 1.5+ this is a TypeError
Make sure plot defaults to rcParams['axes.grid'] setting, GH 9792
multiple colors like mediumaquamarine
single letter colors like k
GH1890
axes[0].figure.savefig("test.png")
GH 6667
GH 9464
GH 5353, 6970, GH 7069
pass different number of axes from required
single column
Rows other than bottom should not be visible
Bottom row should be visible
First column should be visible
(right) is only attached when subplots=False
regular
stacked
regular
stacked
horizontal regular
horizontal stacked
subplots
horizontal subplots
GH 12979
check left-edge of bars
check left-edge of bars
GH 6951
default to Greys
n.b. there appears to be no public method to get the colorbar  label
verify turning off colorbar works
GH 7498  compare margins between lim and bar edges
Check the ticks locates on integer
Check whether the bar locates on center
Check whether the bar's edge starts from the tick
GH3254, GH3298 matplotlib/matplotlib1882, 1892  regressions in 1.2.1
different warning on py3
height of last bin (index 5) must be 1.0
if horizontal, yticklabels are rotated
make color a list if plotting one column frame  handles cases like df.plot(color='DodgerBlue')
Color contains shorthand hex value results in ValueError  Forced show plot
GH 9894
single color char
single color str
Color contains shorthand hex value results in ValueError  Forced show plot
make color a list if plotting one column frame  handles cases like df.plot(color='DodgerBlue')
single character style
Line2D can't have alpha in its linecolor
make color a list if plotting one column frame  handles cases like df.plot(color='DodgerBlue')
Test colormap functionality
string color is applied to all artists except fliers
Color contains invalid key results in ValueError
Default to BuGn
legend labels  NaN's not included in legend with subplots  see https://github.com/pydata/pandas/issues/8390
yerr is iterator
check time-series plots
https://github.com/pydata/pandas/issues/9737 using gridspec, the axis  in fig.get_axis() are sorted differently than pandas expected them,  so make sure that only the right ones are removed
without sharex, no labels should be touched!
https://github.com/pydata/pandas/issues/9737 using gridspec, the axis  in fig.get_axis() are sorted differently than pandas expected them,  so make sure that only the right ones are removed
without sharex, no labels should be touched!
Use a weakref so we can see if the object gets collected without  also preventing it from being collected
have matplotlib delete all the figures  force a garbage collection  check that every plot was collected  need to actually access something to get an error
GH 10657
GH 10819
axis are visible because these are not shared
Regression test for GH8733
check that a scatter plot is effectively plotted: the axes should  contain a PathCollection from the scatter plot (GH11805)
Can't compare generators
numpy_array_equal only accepts np.ndarray
must pass
must success
must success
even less than less precise
GH9744
hack to ensure that SkipTest is *not* raised
-*- coding: utf-8 -*-
GH 8382  Boxplot failures on 1.4 and 1.4.1  Don't need try / except since that's done at class level
change to Axes in future
make sure sharex, sharey is handled
handle figsize arg
check bins argument
scale of y must be 'log'
propagate attr exception from matplotlib.Axes.hist
layout too small for all 4 plots
invalid format for layout
we are plotting multiples on a sub-plot
now works with GH 5610 as gender is excluded
group by a key with single value
scale of y must be 'log'
propagate attr exception from matplotlib.Axes.hist
GH 6970, GH 7069
pass different number of axes from required
without column
GH 6970, GH 7069
pass different number of axes from required
GH4089
share x
don't share y
share y
don't share x
share both x and y
int/positional
we are asserting the code result here  which maps to the -1000 category
categories must be unique
The default should be unordered
Categorical as input
This should result in integer categories, not float!
https://github.com/pydata/pandas/issues/3678
this should result in floats
Catch old style constructor useage: two arrays, codes + categories  We can only catch two cases:   - when the first is an integer dtype and the second is not   - when the resulting codes are all -1/NaN
the next one are from the old docs, but unfortunately these don't  trigger :-(
this is a legitimate constructor
This was raising an Error in isnull(single_val).any() because isnull  returned a scalar for a generator
This uses xrange internally
too few categories
no int codes
no unique categories
too negative
comparisons need to take categories ordering into account
Only categories with same categories can be compared
Only categories with same ordering information can be compared
hack because array_repr changed in numpy > 1.6.x
unicode option should not affect to Categorical, as it doesn't care  the repr width
lengthen
shorten
all "pointers" to '4' must be changed from 3 to 0,...
positions are changed
categories are now in new order
lengthen
shorten
first inplace == False  cat must be the same as before  only res is changed
inplace == True
not all "old" included in "new"
still not all "old" in "new"
all "old" included in "new", but too long
first inplace == False
inplace == True
new is in old categories
first inplace == False
inplace == True
removal is not in categories
Changing categories should also make the replaced category np.nan
Different null values are indistinguishable
Assignments to codes should raise
changes in the codes array should raise  np 1.6.1 raises RuntimeError rather than ValueError
unordered cats are sortable
see gh-12882
GH 9416
shift by zero
sys.getsizeof will call the .memory_usage with  deep=True, and add on some GC overhead
10482
https://github.com/pydata/pandas/issues/9836issuecomment-92123057  and following comparisons with scalars not in categories should raise  for unequal comps, but not for equal/not equal
GH 8453
removing cats
invalid (shape)
ndim > 1
basic sequencing testing
iteration
This method is likely to be confused, so test that it raises an error  on wrong inputs:
sorting
Categoricals should not show up together with numerical columns
GH 9921  Monotonic
Non-monotonic
'order' was deprecated in gh-10726  'sort' was deprecated in gh-12882
unordered cat, but we allow this
sorts 'grade' according to the order of the categories
multi
2:4,: | "j":"k",:
:,"cats" | :,0
"j",: | 2,:
"j","cats | 2,0
iloc  frame
row
col
single value
loc  frame
row
col
single value
ix  frame  res_df = df.ix["j":"k",[0,1]]  doesn't work?
row
col
single value
iat
at
fancy indexing
get_value
i : int, slice, or sequence of integers
iloc    - assign a single value -> exp_single_cats_value
- assign a single value not in the current categories set
- assign a complete row (mixed values) -> exp_single_row
- assign a complete row (mixed values) not in categories set
- assign multiple rows (mixed values) -> exp_multi_row
assign a part of a column with dtype != categorical ->  exp_parts_cats_col
loc    - assign a single value -> exp_single_cats_value
- assign a single value not in the current categories set
- assign a complete row (mixed values) -> exp_single_row
- assign a complete row (mixed values) not in categories set
ix    - assign a single value -> exp_single_cats_value
- assign a single value not in the current categories set
- assign a complete row (mixed values) -> exp_single_row
- assign a complete row (mixed values) not in categories set
assign a part of a column with dtype != categorical ->  exp_parts_cats_col
iat
- assign a single value not in the current categories set
at    - assign a single value -> exp_single_cats_value
- assign a single value not in the current categories set
category c is kept in .categories
set_value
comparisons need to take categories ordering into account
Only categories with same categories can be compared
unequal comparison should raise for unordered cats
https://github.com/pydata/pandas/issues/9836issuecomment-92123057  and following comparisons with scalars not in categories should raise  for unequal comps, but not for equal/not equal
vs list-like
GH 8641  series concat not preserving category dtype
object-object
fillna
object don't sort correctly, so just compare that we have the same  values
array conversion
compare series values  internal .categories can't be compared because it is sorted
numpy ops
numeric ops on a Series
invalid ufunc
https://github.com/pydata/pandas/issues/10661
('tz_localize', ("UTC",), {}),
the series is already localized
See GH 10177
'--with-coverage', '--cover-package=pandas.core']
-*- coding: utf-8 -*-
standard incompatible fill error
no exception o/w
no exception o/w
no exception o/w
axis=0
axis=1
this now accepts a float32!  test with float64 out buffer
-*- coding: utf-8 -*-  pylint: disable-msg=W0612,E1101,W0141
check that the repr is good  make sure that we have a correct sparsified repr  e.g. only 1 header of read
construct single-dtype then sort
setitem then sort
fancy
key error
don't segfault, GH 495  out of bounds access
generator
a df that needs alignment first
getitem
setitem
set with ndarray
it broadcasts
this is a copy in 0.14
setting this will give a SettingWithCopyError  as we are trying to write a view
this is a copy in 0.14
setting this will give a SettingWithCopyError  as we are trying to write a view
raises exception
however this will work
buglet with int typechecking
series
preserve names
inplace
2684 (int64)
it works!
2684 (int32)
can't call with level on regular DataFrame
just check that it works for now
test that ints work
test that int32 work
regular roundtrip
columns unsorted
more than 2 levels in the columns
stack with negative number
it works, 2100
series
Can't use mixture of names and numbers to stack
nlevels == 3
it works!
memory problems with naive impl 2278  Generate Long File & Test Pivot
it works! is sufficient
related to 2278 refactoring
should work
this works because we are modifying the underlying array  really a no-no
but not if it's mixed-type
this will work, but will raise/warn as its chained assignment
skipna=True
for good measure, groupby detail
this works...for now
GH 403
test roundtrip
put it at beginning
it works! 2101
this travels an improper code path
GH 4060
it works!
from datetime combos  GH 7888
-*- coding: utf-8 -*-  pylint: disable-msg=W0612,E1101
return bool indexer
scalar
GH 11485
return label
GH 11485
GH 11485
return location
mixture
GH 11485
return location
-*- coding: utf-8 -*-  pylint: disable-msg=W0612,E1101
check agains values
this is equiv of f[col][row].....  v = f  for a in reversed(i):     v = v.__getitem__(a)  return v
use an artifical conversion to map the key as integers to the labels  so ix can work for comparisions
in case we actually want 0 index slicing
create a tuple accessor
form agglomerates
check agains values
reverse the checks
if we are in fails, the ok, otherwise raise it
check
setitem
GH 7729  make sure we are boxing the returns
GH6296  iloc should allow indexers that exceed the bounds
still raise on a single indexer
GH10779  single positive/negative indexer exceeding Series bounds should raise  an IndexError
slice bounds exceeding is ok
doc example
GH10547 and GH10779  negative integers should be able to reach index 0
check the length 1 Series case highlighted in GH10547
cross-sectional indexing
axis=1
for multiple items  GH 5528
if the expected raises, then compare the exceptions
setitem
edge cases
now work rather than raising KeyError
indexing - boolean
trying to set a single element on a part of a different timezone
GH 12050  indexing on a series with a datetimeindex with tz
getitem
setitem
.loc getitem
.loc setitem
getitem
setitem
.loc getitem
.loc setitem
assign back to self
GH6394  Regression in chained getitem indexing with embedded list-like from  0.12
GH 3053  loc should treat integer slices like label slices
doc examples
expected = df.ix[:,10] (this fails)
raise a KeyError?
inconsistency between .loc[values] and .loc[values,:]  GH 7999
GH 6252  setting with an empty frame
GH 6546  setting with mixed labels
slice
list of integers
neg indicies
dups indicies
with index-like
try with labelled frame
negative indexing
out-of-bounds exception
trying to use a label
slice
list of integers
out-of-bounds exception
trying to use a label
defines ref_locs
for dups
invalid
valid
2nd (last) columns
corner column
this is basically regular indexing
the first row
2nd (last) columns
with a tuple
GH6788  multi-index indexer is None (meaning take all)
GH 7349  loc with a multi-index seems to be doing fallback
test indexing into a multi-index before & past the lexsort depth
test for all partials of this key
covers both unique index and non-unique index
xs
emits a PerformanceWarning, ok
index
columns
ambiguous cases  these can be multiply interpreted (e.g. in this case  as df.loc[slice(None),[1]] as well
this is equivalent of an xs expression
A1 - Get all values under "A0" and "A1"
A2 - Get all values from the start to "A2"
B2 - Get all values in B0, B1 and B2 (similar to what 2 is doing for  the As)
test index maker
not sorted
setitem
axis 1
invalid axis
12411
12045
11594
test index maker
not enough values
raise because these have differing levels
not any values found
empty ok
inconsistent returns for unique/duplicate indices when values are  missing
GH 5835  dups on index and missing values
this works, new column is created correctly
GH 3617
GH8710  multi-element getting with a list
with an object-like  GH 9140
GH 7763  loc and setitem have setting differences
10360  failing with a multi-index
invalid assignments
groupby example
we are actually operating on a copy here  but in this case, that's ok
frame on rhs
GH 1142
GH 6043  ix with a list
ix with an object
remains object dtype even after setting it back
ndarray ok
GH4312 (iloc)
series
iloc/iat raise
iloc/iat raise
in a mixed dtype environment, try to preserve dtypes  by appending
list-like must conform
GH 9516
partial set with new index  Regression from GH4825
loc
raises as nothing in in the index
raises as nothing in in the index
don't allow not string inserts
allow object conversion here
partially set with an empty object  frame
these work as they don't really change  anything but the index  GH5632
no index to start
GH 5756  setting with empty Series
setting via chained assignment  but actually works, since everything is a view
correct setting
this is chained assignment, but will 'work'
3970
Creates a second float block
caches a reference to the 'bb' series
repr machinery triggers consolidation
Assignment to wrong series
GH 5424
ref the cache
set it
a suprious raise as we are setting the entire column here  GH5597
always a copy
should be ok even though it's a copy!
an identical take, so no copy
from SO:  http://stackoverflow.com/questions/24054495/potential-bug-setting-value-for-undefined-column-using-iloc
this should not raise
smoke test for the repr
integer indexes
right hand side; permute the indices and multiplpy by -2
expected `right` result; just multiply by -2
run tests with uniform dtypes
For integer indices, ix and plain getitem are position-based.
GH10645
For objects, we should preserve the None value.
For objects, we should preserve the None value.
value not in the categories
list of labels
element in the categories but not in the values
not all labels in the categories
duplicated slice
GH 10043
since we are actually reindexing with a Categorical  then return a Categorical
passed duplicate indexers are not allowed
CategoricalIndex([1, 1, 2, 1, 3, 2],          categories=[3, 2, 1],          ordered=True,          name=u'B')
CategoricalIndex([1, 1, 2, 1, 3, 2],          categories=[3, 2, 1],          ordered=False,          name=u'B')
tests setitem with non-existing numeric key
tm.assert_series_equal(temp, pd.Series([1, 3, 1, 0]))  self.assertEqual(temp.dtype, np.int64)
int can't hold NaN
bool can't hold NaN
buggy on 32-bit
Expected: do not downcast by replacement
Series
doesn't work in PY3, though ...dict_from_bool works fine
getting
gettitem on a DataFrame is a KeyError as it is indexing  via labels on the columns
label based can be a TypeError or KeyError
contains
setting with a float fails with iloc
setting with an indexer  Value or Type Error
fallsback to position selection, series only
lookup in a pure string index  with an invalid indexer
mixed index so we have label  indexing
integer index
coerce to equal int
coerce to equal int
contains  coerce to equal int
getting
setting
random integer is a KeyError
contains
iloc succeeds with an integer
iloc raises with a float
getitem
setitem
s is an in-range index
getitem
these are all label indexing  except getitem which is positional  empty
positional indexing
getitem out-of-bounds
these are all label indexing  except getitem which is positional  empty
positional indexing
positional indexing
setitem
positional indexing
similar to above, but on the getitem dim (of a DataFrame)
getitem
positional indexing
getitem out-of-bounds
positional indexing
positional indexing
setitem
positional indexing
related 236  scalar/slicing of a float index
exact indexing when found
scalar integers
fancy floats/integers create the correct entry (as nan)  fancy tests
combined test
Assuming AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_S3_HOST  are environment variables
try to decode (if needed on PY3)  Strange. linux py33 doesn't complain, win py33 does
Excel copies into clipboard with \t seperation  inspect no more then the 10 first lines, if they  all contain an equal number (>0) of tabs, infer  that this came from excel and set 'sep' accordingly
str(df) has various unhelpful defaults, like truncation
ExcelFile class
fall through to normal exception handling below
If io is a url, want to keep the data as bytes so can't pass  to get_filepath_or_buffer()  Deal with S3 urls, path objects, etc. Will convert them to  buffer or path string
N.B. xlrd.Book has a read attribute too
Use the xlrd <= 0.9.2 date handling.
GH5394 - Excel 'numbers' are always floats  it's a minimal perf hit and less suprising
xlrd >= 0.9.3 can return datetime objects directly.
Keep sheetname to maintain backwards compatibility.
handle same-type duplicates.
forward fill values for MultiIndex index
GH 12292 : error when read one empty column from excel file
No Data, return an empty DataFrame
trim header row so auto-index inference works  xlrd uses '' , openpyxl None
forward fill blanks entries  from headers if parsing as MultiIndex
no index col specified, trim data for inference path
pop out header name and fill w/ blank
declare external properties you can count on
validate that this engine can handle the extension
Allow use as a contextmanager
Use the openpyxl module as the Excel writer.
Create workbook object with default optimized_write=True.  Openpyxl 1.6.1 adds a dummy sheet. We remove it.
Write the frame cells using openpyxl.
Excel requires that the format of the first cell in a merged  range is repeated in the rest of the merged range.
Ignore first cell. It is already handled.
Write the frame cells using openpyxl.
Excel requires that the format of the first cell in a merged  range is repeated in the rest of the merged range.
Ignore first cell. It is already handled.
>= 2.0.0 < 2.1.0
>= 2.1.0
Write the frame cells using openpyxl.
When cells are merged only the top-left cell is preserved  The behaviour of the other cells in a merged range is  undefined
Ignore first cell. It is already handled.
Use the xlwt module as the Excel writer.
Use the xlsxwriter module as the Excel writer.
If there is no formatting we don't create a format object.
Create a XlsxWriter format object.
Map the cell font to XlsxWriter font properties.
Map the cell borders to XlsxWriter border properties.
see LICENSES directory for copyright and license
Let the gflags module process the command-line arguments.
Set the logging according to the command-line flag.
Prepare credentials, and authorize HTTP object with them.
READ HTML
give class attribute as class_ because class is a python keyword
1. check all descendants for the given pattern and only search tables  2. go up the tree until we find a table
if any table attributes were given build an xpath expression to  search for them
try to parse the input in the simplest way
if the input is a blob of html goop
fill out elements of body that are "ragged"
hack around python 3 deleting the exception variable
pylint: disable-msg=E1101,W0613,W0603
versioning attribute
encoding  PY3 encoding if we don't specify
set the encoding if we need
map object types
axes map
oh the troubles to reduce import time
version requirements
set the file open policy  return the file open policy; this changes as of pytables 3.1  depending on the HDF5 version
grab the scope
if filepath is too long
can't auto open/close if we are using an iterator  so delegate to the iterator
if there is an error, close the store
if we are changing a write mode to read, ok
this would truncate, raise here
close and reopen the handle
trying to read from a non-existant file causes an error which  is not part of IOError, make it one
create the storer and axes
function to call on iteration
create the iterator
collect the tables
axis is the concentation axes
retrieve the objs, _where is always passed as a set of  coordinates here
concat and return
create the iterator
we are actually trying to remove a node (with children)
remove the node
delete from the table
figure out the splitting axis (the non_index_axis)
data_columns
append
compute the val
version requirements
private methods
validate
infer the pt from the passed value
we are actually a table
a storer node
existing node (and must be a table)
if we are a writer, determin the tt
remove the node if we are not appending
we don't want to store a table node at all if are object is 0-len  as there are not dtypes
write the object
set start/stop if they are not set if we are a table
iterate
return the actual iterator
if specified read via coordinates (necessary for multiple selections
directly return the result
values is a recarray
if the output freq is different that what we recorded,  it should be None (see also 'doc example part 2')
frequency/name just warn
reset
set my typ if we need
this is basically a catchall; if say a datetime64 has nans then will  end up here
set as a data block
see if we have a valid string type
we cannot serialize this data, so report an exception on a column  by column basis
itemsize is the maximum length of a string (along any dimension)
check for column in the values conflicts
write the codes; must be in a block shape
write the categories
update the info
convert this column to i8 in UTC, and save the tz
store a converted timezone
values is a recarray
use the meta if needed
convert to the correct dtype
reverse converts
recreate with tz if indicated
we have a categorical
convert nans / decode
indexer helpders
compat: for a short period of time master stored types
length 0 axis
reconstruct a timezone if indicated
write the name
write the labels
Transform needed to interface with pytables row/col notation
get the atom for this datatype
store as UTC  with a zone
hacky - this works for frames, but is reversed for panels
should never get here
ok, apply generally
index columns
values columns
remove the index if the kind/optlevel have changed
validate the version
infer the data kind
create the selection
convert the data
evaluate the passed data_columns, True == use all columns  take only valide axis labels
if min_itemsize is a dict, add the keys (exclude 'values')
return valid columns in the order of our axis
map axes to numbers
currently support on ndim-1 axes
create according to the new data
nan_representation
create axes to index and non_index
we might be able to change the axes on the appending data if  necessary
ahah! -> reindex
the non_index_axes info
set axis positions (based on the axes)
check for column conflicts
reindex by our non_index_axes & compute data_columns
add my values
shape of the data column are the indexable axes
we have a data_column
validate our min_itemsize
validate our metadata
validate the axes if we have an existing table
make a copy to avoid side effects
make sure to include levels if we have them
reorder by any non_index_axes & limit to the select columns
apply the selection filters (but keep in the same order)
see if the field is the name of an axis
if we have a multi-index, then need to include  the levels
this might be the name of a file IN an axis
we need to filter on this dimension
hack until we support reversed dim flags
provided expected rows if its passed
description from the axes & values
validate the version
infer the data kind
validate the version
infer the data kind
find the axes
compute the key
create the objs
the data need to be sorted
create the object
permute if needed
reconstruct
need a better algorithm
create the composite object
apply the selection filters & axis orderings
create the axes
create the table
set the table attributes
create the table
update my info
validate the axes and set the kinds
add the rows
if dropna==True, then drop ALL nan rows
consolidate masks
broadcast the indexes if needed
broadcast to all other indexes except myself
write the chunks
0 len
indexes
values
mask
infer the data kind
create the selection
delete the rows in reverse order
construct groups of consecutive rows
1 group
final element
initial element
return the number of rows removed
if we have a DataIndexableCol, its shape will only be 1 dim
apply the selection filters & axis orderings
remove the default name
the index columns is just a simple index
remove names for 'level_%d'
try not to reindex even if other is provided  if it equals our current index
take a guess for now, hope the values fit
encode if needed
create the sized dtype
guard against a None encoding in PY3 (because of a legacy  where the passed encoding is actually None)
conv = np.frompyfunc(conv, 1, 1)
create the numexpr & the filter
compat pickle
All datetimes should be stored as M8[ns].  When unpickling with  numpy1.6, it will read these as M8[us].  So this ensures all  datetime64 types are read as MS[ns]
Extract some of the arguments (pass chunksize on).
Create the parser.
'engine': 'c',
Column and Index Locations and Names
General Parsing Configuration
NA and Missing Data Handling
Datetime Handling
Iteration
Quoting, Compression, and File Format
Error Handling
Deprecated
Internal
Alias sep -> delimiter.
Compute 'colspecs' from 'widths', if specified.
miscellanea
might mutate self.engine
see gh-12935
C engine not supported yet
really delete this one
Converting values to NA
handle skiprows; this is internally handled by the  c-engine, so only need for python parsers
put stuff back
May alter columns / col_dict
the names are the tuples of the header that are not the index cols  0 is the name of the index, assuming index_col is a list of column  numbers
clean the index_names
extract the columns
clean the column names (if we have an index_col)
see gh-7160 and gh-9424: this helps to provide  immediate alleviation of the duplicate names  issue and appears to be satisfactory to users,  but ultimately, not needing to butcher the names  would be nice!
add names for the index
maybe create a mi on the columns
remove index items from content and columns, don't pop in  loop
remove index items from content and columns, don't pop in  loop
returns data, columns
2442
XXX
we have a multi index in the columns
gh-9755  need to set orig_names here first  so that proper indexing can be done  with _set_noconvert_columns  once names has been filtered, we will  then set orig_names again to names
Done with first read, next time raise StopIteration
implicit index, no index names
rename dict keys
rename dict keys
ugh, mutation
columns as list
maybe create a mi on the columns
Python 2's bz2 module can't take file objects, so have to  run through decompress manually
in Python 3, convert BytesIO or fileobjects passed with an encoding
Set self.data to something that can read lines.
Get columns in two steps: infer from data, then  infer column indices from self.usecols if is is specified.
get popped off for index
Create a set of column ids that are not to be stripped of thousands  operators.
attempt to sniff the delimiter
done with first read, next time raise StopIteration
DataFrame with the right metadata, even though it's length 0
handle new style for names in index
legacy
apply converters
we have a mi columns, so read an extra line
We have an empty file, so check  if columns are provided. That will  serve as the 'line' for parsing
Set _use_cols. We don't store columns because they are  overwritten.
Ignore output but set used columns.
implicitly index_col=0 b/c 1 fewer column names  leave it 0, 2442  Case 1
Update list of original names to include all indices.
Case 1
Case 2
see gh-13320
already fetched some number  we already have the lines in the buffer
need some lines
Check for stop rows. n.b.: self.skiprows is a set.
dict of new name to column list
don't mutate
hack
Convert column indexes to column names.
create float versions of the na_values
we are like 999 here
Note: 'colspecs' is a sequence of half-open intervals.
Support iterators, convert to a list.
@wraps(_urlopen)
cat on the compression to the tuple returned by the function
It is a pathlib.Path/py.path.local or string
ZipFile is not a context manager for <= 2.6  must be tuple index here since 2.6 doesn't use namedtuple for version_info
ignore encoding
Work with a list of indicators
Download
Clean outpu
e.g. "16FEB11:10:07:55"
Start by setting first half of ieee number to first half of IBM  number sans exponent
Get the second half of the ibm number into the second half of  the ieee number
clear the 1 bit to the left of the binary point
Copy to BytesIO, and ensure no encoding
read file header
rest at end gets ignored, so if field is short, pad out  to match struct pattern below
8 byte blank
Get endianness information
Unknown purpose
unknown purpose
Data from Yahoo! Finance
Data from Google Finance
Data from FRED
URL of form:  http://download.finance.yahoo.com/d/quotes.csv?s=@%5EIXIC&f=snxl1d1t1c1ohgv
cannot construct a panel with just 1D nans indicating no data
path of zip files
Items needed for options class
Instantiate object with ticker
Fetch next expiry call data
Can now access aapl.calls instance variable
Fetch next expiry put data
Can now access aapl.puts instance variable
cut down the call data to be 3 below and 3 above the stock price.
Fetch call and put data with expiry from now to 8 months out
check whcih compression libs we have installed
until we can pass this into our conversion functions,  this is pretty hacky
see if we have an actual file
a buffer like
this is platform int, which we need to remap to np.int64  for compat on windows platforms
numpy 1.6.1 compat
return string arrays like they are
convert to a bytes array
return string arrays like they are
convert to a bytes array
ndarray (on original dtype)
Copy the string into a numpy array.
reverse tz conversion
see if we have an actual file
a file-like
if the filepath is too long will raise here  5874
try numpy
don't try to coerce, unless a force conversion
try float
coerce floats to 64
do't coerce 0-len data
coerce ints if we can
coerce ints to 64
coerce floats to 64
no conversion on empty
possibly handle dup columns
our columns to parse
each key gets renamed with prefix
only dicts gets recurse-flattend  only at level>1 do we rename the rest of the keys
A bit of a hackjob
Disastrously inefficient for now
For repeating the metadata later
Data types, a problem
Report all error messages if verbose is set
'allowLargeResults', 'createDisposition',  'preserveNulls', destinationTable, useQueryCache
Only read schema on first page
Loop through each page of data
print basic query stats
see:  http://pandas.pydata.org/pandas-docs/dev/missing_data.html  missing-data-casting-rules-and-indexing  This seems to be buggy without nanosecond indicator
Change the order of columns in the DataFrame based on provided list
Downcast floats to integers and objects to booleans  if there are no NaN's. This is presently due to a  limitation of numpy in handling missing data.
deprecation TimeSeries, 11121
Cast from unsupported types to supported types
Ensure int32
Total length
len
labname
padding - 3 bytes
value_label_table  n - int32
textlen  - int32
off - int32 array (n elements)
val - int32 array (n elements)
txt - Text labels, null terminated
Conversion to long to avoid hash issues on 32 bit platforms 8968
not perfect :-/
Not really a Q, unclear how to handle byteswap
don't know old code for double
Copy to BytesIO, and ensure no encoding
calculate size of a data record
remove format details from %td
Requires version-specific treatment
Get data type information, works for versions 117-118.
Stata 117 data files do not follow the described format.  This is  a work around that uses the previous label, 33 bytes for each  variable, 20 for the closing tag and 17 for the opening tag
necessary data to continue parsing
have bytes not strings, so must decode
Value labels are not supported in version 108 and earlier.
Don't read twice
legacy
Handle empty file or chunk.  If reading incrementally raise  StopIteration.  If reading the whole thing return an empty  data frame.
Setup the dtype.
if necessary, swap the byte order to native here
If index is not specified, use actual row number rather than  restarting at 0 for each chunk.
Decode strings
Explicit call with ordered=True
if 'b' not in fname.mode:
not memory efficient, what else could we  do?
attach nobs, nvars, data, varlist, typlist
Replace missing values with Stata missing value for type
Variable name must not be a reserved word
Variable name may not start with a number
check for duplicates  prepend ascending number to avoid duplicates
need to possibly encode the orig name if its unicode
Check date conversion, and fix key if needed
Ensure column names are strings
Check columns for compatibility with stata, upcast if necessary
Replace NaNs with Stata missing values
Convert categoricals to int data, and strip labels
set the given format for the datetime cols
write 5 zeros for expansion fields
typlist, length nvar, format byte array
varlist names are checked by _check_column_names  varlist, requires null terminated
srtlist, 2*(nvar+1), int array, encoded by byteorder
fmtlist, 49*nvar, char array
1. Convert dates
Legacy default reader configuration
GH 7369, make sure can read a 0-obs dta file
Minimal testing of legacy data method
this is an oddity as really the nan should be float64, but  the casting doesn't fail so need to match stata here
Remove resource warnings
should get warning for each call to read_dta
buggy test because of the NaT comparison on certain platforms  Format 113 test fails since it does not support tc and tC formats  tm.assert_frame_equal(parsed_113, expected)
these are all categoricals
original.index is np.int32, readed index is np.int64
GH 4626, proper encoding handling
should get a warning for that format.
should get a warning for that format.
should get a warning for that format.
read_csv types are the same
See PR 10757
these are all categoricals
Silence warnings
should get a warning for mixed content
Read with and with out categoricals, ensure order is identical
GH12153
Read the whole file
Compare to what we get when reading by chunk
Nuke table
Nuke table just in case
Complex data type should raise error
wrong length of index_label
reading the query in one time
reading the query in chunks with read_sql_query
reading the query in chunks with read_sql_query
GH 11431
test columns argument in read_table
test columns argument in read_table
WIP : GH10846
warns on create table with spaces in names
without providing a connection object (available for backwards comp)
Skip this test if SQLAlchemy not available
to test if connection can be made:
int64 should be converted to BigInteger, GH7433
IMPORTANT - sqlite has no native date type, so shouldn't parse, but  MySQL SHOULD be converted.
check that a column is either datetime64[ns]  or datetime64[ns, UTC]
"2000-01-01 00:00:00-08:00" should convert to  "2000-01-01 08:00:00"
"2000-06-01 00:00:00-07:00" should convert to  "2000-06-01 07:00:00"
"2000-01-01 00:00:00-08:00" should convert to  "2000-01-01 08:00:00"
"2000-06-01 00:00:00-07:00" should convert to  "2000-06-01 07:00:00"
xref 7139  this might or might not be converted depending on the postgres driver
No Parsing
with read_table -> type information from schema used
with read_table -> type information from schema used
write and read again
with read_table
with read_sql
with read_table
NaNs are coming back as None
with read_table
with read_sql
check precision of float64
sqlite3 is built-in
sqlite has no boolean type, so integer type is returned
IMPORTANT - sqlite has no native date type, so shouldn't parse, but
test no warning for BIGINT (to support int64) is raised (GH7433)
MySQL has no real BOOL type (it's an alias for TINYINT)
test delegation to read_sql_query
create a schema
create a schema
sqlite stores Boolean values as INTEGER
For sqlite, these should work fine
Raise error on blank
test connection
Initialize connection again (needed for tearDown)
HACK! Change this once indexes are handled properly.
computing the sum via sql  it should not fail, and gives 3 ( Issue 3628 )
test if invalid value for if_exists raises appropriate error
Try Travis defaults.  No real user should allow root access with a blank password.
Initialize connection again (needed for tearDown)
HACK! Change this once indexes are handled properly.
HACK! Change this once indexes are handled properly.
test if invalid value for if_exists raises appropriate error
The Python 2 C parser can't read bz2 from S3.
The Python 2 C parser can't read bz2 from S3.
Receive a permission error when trying to read a private bucket.  It's irrelevant here that this isn't actually a table.
skip aberration observed on Win64 Python 3.2.2
see gh-1835
skipping lines in the header
no as_recarray
names
usecols
non-numeric index_col
2539
See gh-7773
see gh-9770
Parsers support only length-1 decimals
make sure an error isn't thrown
see gh-10476
See gh-6607
pass list
pass skiprows
2 implicit first cols
fails on some systems
don't segfault pls 2428
somewhat False since the code never sees bytes
it works! and is the right length
2599
Assert that types were coerced.
see gh-3866: if chunks are different types and can't  be coerced using numerical types, then issue warning.
see gh-2601
see gh-10022
With skip_blank_lines = True
gh-10728: WHITESPACE_LINE
gh-10548: EAT_LINE_COMMENT
EAT_CRNL_NOP
EAT_COMMENT
SKIP_LINE
ESCAPED_CHAR
ESCAPE_IN_QUOTED_FIELD
IN_QUOTED_FIELD
make sure that an error is still thrown  when the 'usecols' parameter is not provided
See gh-12493
first, check to see that the response of  parser when faced with no provided columns  throws the correct error, with or without usecols
single newline
test with more than a single newline
see gh-12935
see gh-6607
see gh-9798
None, no index
False, no index
not enough rows
not enough rows
handling of new line at EOF
check with delim_whitespace=True
we expect all object columns, so need to  convert to test for equivalence
invalid dtype
valid but we don't support it
see gh-8002
test numbers between 1 and 2  25 decimal digits of precision
round-trip should match float()
it works!
too many columns, cause segfault if not careful
see gh-11786
read all files in many threads
see gh-11786
see gh-5766
length conflict, passed names and usecols disagree
see gh-2654
see gh-2733
somewhat False since the code never sees bytes
it works!
it works!
test with NaT for the nan_rep  we don't have a method to specif the Datetime na_rep (it defaults  to '')
specify columns out of order!
See gh-1693
asserts that google is minimally working and that it throws  an exception when DataReader can't get a 200 response from  google
asserts that yahoo is minimally working and that it throws  an exception when DataReader can't get a 200 response from  yahoo
as of 7/12/13 the conditional will test false because the link is invalid
Usual culprits, should be around for a while
just test that we succeed
daily interval data
weekly interval data
montly interval data
dividend data
test fail on invalid interval
just test that we succeed
sanity checking
sanity checking
regression test GH6105
-*- coding: utf-8 -*-
excel data is parsed correctly
should read in correctly and infer types
GH8212 - support for converters and missing values
should read in correctly and set types of single cells (not array  dtypes)
GH6403
GH 12870 : pass down column names associated with  keyword argument names
FILE
fails on some systems
GH12655
GH12655
parses okay
Test reading times with and without milliseconds. GH5945.
empty name case current read in as unamed levels,  not Nones
GH 8011
GH 11544
GH 12157
Test with MultiIndex and Hierarchical Rows as merged cells.
test roundtrip
GH 6573
test with convert_float=False comes back as float
take 'A' and 'B' as indexes (same row as cols 'C', 'D')
datetime.date, not sure what to test here exactly
since the reader returns a datetime object for dates, we need  to use df_expected to check the result
Test for Issue 11328. If column indices are integers, make  sure they are handled correctly for either setting of  merge_cells
try multiindex with dates
Initial non-MI frame.
Add a MI.
Write out to Excel without the index.
Read it back in.
Test that it is the same as the initial frame.
this if will be removed once multi column excel writing  is implemented for now fixing 9794
first row taken as columns
This test was failing only for j>1 and header=False,  So I reproduced a simple test.
Test for issue 5235
Test for issue 5427.
10982
Test writing and reading datetimes. For issue 9139. (xref 9185)
GH7074
GH8188
Test that column formats are applied to cells. Test for issue 9167.  Applicable to xlsxwriter only.
Ignore the openpyxl lxml warning.
Get the number format from the cell. This method is backward  compatible with older versions of openpyxl.
Test < 0.13 non-merge behaviour for MultiIndex and Hierarchical Rows.
Test < 0.13 non-merge behaviour for MultiIndex and Hierarchical Rows.
Test < 0.13 non-merge behaviour for MultiIndex and Hierarchical Rows.
some awkward mocking to test out dispatch and such actually works
Compare to this
Read full file
Test incremental read with `read` method.
Test incremental read with `get_chunk` method.
Read full file with `read_sas` method
Compare to this
Read full file
Compare to this
Compare to this
deprecated
If for some reason result actually ever has data, it's cause WB  fixed the issue with this ticker.  Find another bad one.
if it ever gets here, it means WB unretired the indicator.  even if they dropped it completely, it would still get caught above  or the WB API changed somehow in a really unexpected way.
if it ever gets here, it means the country code XXX got used by WB  or the WB API changed somehow in a really unexpected way.
py3 compat when reading py2 pickle  trying to read a py3 pickle in py2
use a specific comparator  if available
GH 9291
Categorical.ordered is changed in < 0.16.0
Categorical.ordered is changed in < 0.16.0
test writing with each pickler
test reading with each unpickler
'--with-coverage', '--cover-package=pandas.core'],
deprecated
infer_types removed in 10892
GH 6114
seems utf-16/32 fail on windows
GH10369
current msgpack cannot distinguish list/tuple
current msgpack cannot distinguish list/tuple
current msgpack cannot distinguish list/tuple
datetime with no freq (GH5506)
run multiple times here
run multiple times here
GH 5947  inferring freq on the datetimeindex
currently these are not implemetned  i_rec = self.encode_decode(obj)  comparator(obj, i_rec, **kwargs)
make sure that we can write to the new frames
make sure that we can write to the new frames even though  we needed to copy the data  mutate the data in some way
check the messages from our warnings
make sure none of our mutations above affected the  original buffers
if this test fails I am sorry because the interpreter is now in a  bad state where b'a' points to 98 == ord(b'b').
GH10581
testing on windows/py3 seems to fault  for using compression
put in the temporary path if we don't have one already
set these parameters so we don't have file sharing
Pytables 3.0.0 deprecates lots of things
Pytables 3.0.0 deprecates lots of things
GH6166  unconversion of long strings was being chopped in earlier  versions of numpy < 1.7.2
GH4584  API issue when to_hdf doesn't acdept append AND format args
File path doesn't exist
default_format option
GH 12221
make a random group in hdf space
storers
GH 2694
write a file and wipe its versioning
this is an error because its table_type is appendable, but no  version info
constructor
context
invalid mode change
create an in memory store
the file should not have actually been written
test attribute access
errors
not stores
not OK, not a table
can't put to a table (use append instead)
overwrite table
can't compress if format='fixed'
can't compress if format='fixed'
non-date, non-string index
cannot use assert_produces_warning here for some reason  a PendingDeprecationWarning is also raised?
basic
select on the values
period index currently broken for table  seee GH7796 FIXME  check('table',tm.makePeriodIndex)
unicode
only support for fixed types (and they have a perf warning)
not sure how to remove latin-1 from code in python 2 and 3
Test to make sure defaults are to not drop.  Corresponding to Issue 9382
this isn't supported
GH 4096; using same frames, but different block orderings
test a different ordering but with more fields (like invalid  combinate)
store additonal fields in different blocks
store multile additonal fields in different blocks
append then change (will take existing schema)
partial selection
apply the wrong field (similar to 1)
test truncation of bigger strings
data column searching
using min_itemsize and a data column
on-disk operations
panel  GH5717 not handling data_columns
try to index a non-table
appending multi-column on existing table (see GH 6167)
validate multi-index names  GH 5527
With a DataFrame
unsuported data types for non-tables
empty frame, GH4273
0 len
store
0 len
store
directy ndarray
series directly
appending an incompatbile table
incompatible dtype
py3 ok for unicode
currently not supported dtypes
this fails because we have a date in the object block......
table
fixed
nonexistence
__delitem__
non-existance
empty where
deleted number (entire table)
with where
upper half
individual row elements
valid for p4d only
check USub node parsing
technically an error, but allow it
GH 454
nose has a deprecation warning in 3.5
put in some random NAs
empty
duplicates on both index and columns
make sparse dataframe
case 1: store uncompressed
case 2: store compressed (works)
set one series to be completely sparse
case 3: store df with completely sparse series uncompressed
case 4: try storing df with completely sparse series compressed  (fails)
put/select ok
non-table ok (where = None)
floats with NaN
test selection with comparison against numpy scalar  GH 11283
single table
GH 8014  using iterator and where clause
no iterator
select w/o iteration and no where clause works
select w/o iterator and where clause, single term, begin  of range, works
select w/o iterator and where clause, single term, end  of range, works
with iterator, full range
select w/iterator and no where clause works
GH 8014  using iterator and where clause
with iterator, non complete range
with iterator, empty where
GH 8014  using iterator and where clause can return many empty  frames.
with iterator, range limited to the first chunk
select w/iterator and where clause, single term, end of range
should be []
GH 3499, losing frequency info on index recreation
invalid terms
invert not implemented in numexpr :(
list like
sccope with list like
sccope with index
not implemented
in theory we could deal with this
GH 2973
test string ==/!=
int ==/!=
error
valid
not a data indexable column
start/stop
all
get coordinates back & test vs frame
pass array/mask as the coordinates
locations
boolean
boolean
start/stop
no tables stored
GH 4858; nan selection bug, only works for pytables >= 3.1
table
out of range
sorted_obj = _test_sort(obj)
multiples
multiples
nested close
double closing
ops on a closed store
fails on win/3.5 oddly
force the frame
old version warning
check keys
check indicies & nrows
check propindixes
new table
make sure the metadata is ok
appending with same categories is ok
appending must have the same categories
GH 9330
GH11773
GH11773
use maybe_get_tz instead of dateutil.tz.gettz to handle the windows  filename issues.
as columns
select with tz aware
as index
GH 4098 example
as columns
select with tz aware
as index
GH 4098 example
check that no tz still works
check utc
double check non-utc
original method
with tz setting
pylint: disable-msg=W0612,E1101
index is not captured in this orientation
index and cols are not captured in this orientation
index and col labels might not be strings
basic
categorical
empty
time series data
JSON deserialisation always creates unicode strings
basic
empty_series has empty index with object dtype  which cannot be revert
frame
frame
force date unit
detect date unit
GH4377 df.to_json segfaults with non-ndarray blocks
GH4377 df.to_json segfaults with non-ndarray blocks
Default behavior assumes encode_html_chars=False.
Make sure explicit encode_html_chars=False works.
Make sure explicit encode_html_chars=True does the encoding.
self.assertEqual(output, json.dumps(input))
will throw typeError  will throw typeError
column indexed
column indexed
nan-based
nan-based
make sure we are < 0.13 compat (in py3)
force our cwd to be the first searched
11121 Deprecation of generate_bq_schema
- PER-TEST FIXTURES -  put here any instruction you want to be run *BEFORE* *EVERY* test is  executed.
- GLOBAL CLASS FIXTURES -  put here any instruction you want to execute only *ONCE* *AFTER*  executing all tests.
- PER-TEST FIXTURES -  put here any instructions you want to be run *AFTER* *EVERY* test is  executed.
- PER-TEST FIXTURES -  put here any instructions you want to be run *AFTER* *EVERY* test is  executed.
Test the default value of if_exists is 'fail'
Test the if_exists parameter with value 'fail'
Initialize table with sample data
Test the if_exists parameter with value 'append'
Try inserting with a different schema, confirm failure
Initialize table with sample data
Test the if_exists parameter with the value 'replace'.
- PER-TEST FIXTURES -  put here any instruction you want to be run *BEFORE* *EVERY* test  is executed.
- PER-TEST FIXTURES -  put here any instructions you want to be run *AFTER* *EVERY* test  is executed.
- PER-TEST FIXTURES -  put here any instruction you want to be run *BEFORE* *EVERY* test  is executed.
- PER-TEST FIXTURES -  put here any instructions you want to be run *AFTER* *EVERY* test  is executed.
For sqlalchemy versions < 0.8.2, the BIGINT type is recognized  for a sqlite engine, which results in a warning when trying to  read/write a DataFrame with int64 values. (GH7433)
parse dates as timestamp
coerce to UTC timezone  GH11216
handle non-list entries for parse_dates gracefully
we want to coerce datetime64_tz dtypes for now  we could in theory do a 'nice' conversion from a FixedOffset tz  GH11216
python 3 compat
We want to initialize based on a dataframe
no data provided, read-only mode
Inserting table into database, add to MetaData object
convert to microsecond resolution so this yields  datetime.datetime
replace NaN with None
for reading: index=(list of) string to specify column to set as index
At this point, attach to new metadata, only attach to self.meta  once table is created.
handle non-list entries for parse_dates gracefully
the type the dataframe column should have
floats support NA, can always convert!
No NA values, can convert ints and bools
Handle date parsing
we have a timezone capable type
Caution: np.datetime64 is also a subclass of np.number.
Avoid casting double-precision floats into decimals
Filter for unquoted identifiers  See http://dev.mysql.com/doc/refman/5.0/en/identifiers.html
SQL enquote and wildcard symbols
Validate and return escaped identifier
GH 8341  register an adapter callable for datetime.time object  this will transform time(12,34,56,789) into '12:34:56.000789'  (this is what sqlalchemy does)
GH11038
return self.df.index.tolist()
PyQt4 gets a QVariant
PySide gets an unicode
Set DataFrame
For state.division and state.region
R observation numbers come through as strings
built-in dataset
Null data
Pandas bug 1282
'--with-coverage', '--cover-package=pandas.core'],
versioning
no setuptools installed
we need to inherit from the versioneer  class as it encodes the version info
args to ignore warnings
some linux distros require it
util  extension for pseudo-safely moving bytes into mutable buffers
s=[x for x in s['matrix']]
Increment custom business month
Sinch vbench.db._reg_rev_results returns an unlabeled dict,  we have to break encapsulation a bit.
all in a good cause...
run_option='eod', start_date=START_DATE,
ARGH. reparse the repo, without discarding any commits,  then overwrite the previous parse results  prprint("Slaughtering kittens...")
print("Disposing of TMP_DIR: %s" % TMP_DIR)
ignore below threshold
just in case
throw away the burn_in
not bullet-proof but enough for us
not bullet-proof but enough for us
move away from the pandas root dir, to avoid possible import  surprises
hack , vbench.git ignores some commits, but we  need to be able to reference any commit.  modified from vbench.git
parse timestamp into datetime object
to UTC for now
even worse, monkey patch vbench
run_option='eod', start_date=START_DATE,
print(k, v)
getting
add
chained comp
add in some nans
run in parallel
Not really a fair test as behaviour has changed!
MUST set name
avoid bmark to be collected as Benchmark object
make the figure
From JSON-like stuff
Have to stuff them in globals() so vbench detects them
as far back as the earliest test currently in the suite
numpy standard doc extensions
Add any paths that contain templates here, relative to this directory.
The suffix of source filenames.
The master toctree document.
General information about the project.
The version info for the project you're documenting, acts as replacement for  |version| and |release|, also used in various other places throughout the  built documents.  The short X.Y version.
version = '%s r%s' % (pandas.__version__, svn_version())
The full version, including alpha/beta/rc tags.
JP: added from sphinxdocs
List of directories, relative to source directory, that shouldn't be searched  for source files.
The name of the Pygments (syntax highlighting) style to use.
The theme to use for HTML and HTML Help pages.  Major themes that come with  Sphinx are currently 'default' and 'sphinxdoc'.
Add any paths that contain custom themes here, relative to this directory.
The name for this set of Sphinx documents.  If None, it defaults to  "<project> v<release> documentation".
Add any paths that contain custom static files (such as style sheets) here,  relative to this directory. They are copied after the builtin static files,  so a file named "default.css" will overwrite the builtin "default.css".
If false, no module index is generated.
Output file base name for HTML help builder.
Grouping the document tree into LaTeX files. List of tuples  (source start file, target name, title, author, documentclass [howto/manual]).
Example configuration for intersphinx: refer to the Python standard library.  intersphinx_mapping = {'http://docs.scipy.org/': None}
didn't add to namespace until later
print("foo")
The Python 2 C parser can't read bz2 from open files.
Skip these benchmarks if `boto` is not installed.
Using the values
GH 12737
Simulate irregular PeriodIndex
didn't add to namespace until later
these dictionaries contain VCS-specific tools
remember shell=False, so use git.cmd on windows, not just git
remember shell=False, so use git.cmd on windows, not just git
parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]  TAG might have hyphens.
look for -dirty suffix
distance: number of commits since tag
commit: short hex revision ID
HEX: no tags
exception 1
exception 1
exception 1
exception 1
exception 1
exception 1
versionfile_source is the relative path from the top of the source  tree (where the .git directory might live) to this file. Invert  this to find the root from __file__.
parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]  TAG might have hyphens.
look for -dirty suffix
distance: number of commits since tag
commit: short hex revision ID
HEX: no tags
exception 1
exception 1
exception 1
exception 1
exception 1
exception 1
see the discussion in cmdclass.py:get_cmdclass()
we add "version" to both distutils and setuptools
now locate _version.py in the new build/ directory and replace  it with an updated value
we override different "sdist" commands for both environments
unless we update this, the command will keep using the old  version
now locate _version.py in the new base_dir directory  (remembering that it may be a hardlink) and replace it with an  updated value
Make VCS-specific changes. For git, this means creating/changing  .gitattributes to mark _version.py for export-time keyword  substitution.
results.columns = ['pandas']
Prepare Database
Create Indices
for _ in range(niter):
dict to hold results
groupby sum
labels = np.tile(groups, N // K)
df2 = DataFrame(np.random.randn(1000000, 5), idx2, columns=range(5, 10))
pandas
larry
Create data
filenames
Delete old files
Time a round trip save and load
elapsed = _timeit(lambda: table.lookup_locations2(values))
groupby1(lat, lon, data)
model = face.ols(y=y, x=x)
corner cases  import pdb; pdb.set_trace()  create a joint index for the axis
arr = get_test_data(ngroups=1000)
wes_timer =  timeit.Timer(stmt='better_unique(arr)',                            setup=setup % sz)
filenames
Delete old files
Delete old files
numpy standard doc extensions
JP: added from sphinxdocs
Add any paths that contain templates here, relative to this directory.
The suffix of source filenames.
The encoding of source files.
The master toctree document.
General information about the project.
The version info for the project you're documenting, acts as replacement for  |version| and |release|, also used in various other places throughout the  built documents.  The short X.Y version.
version = '%s r%s' % (pandas.__version__, svn_version())
The full version, including alpha/beta/rc tags.
List of directories, relative to source directory, that shouldn't be searched  for source files.
The name of the Pygments (syntax highlighting) style to use.
The theme to use for HTML and HTML Help pages.  Major themes that come with  Sphinx are currently 'default' and 'sphinxdoc'.
Add any paths that contain custom themes here, relative to this directory.
Add any paths that contain custom static files (such as style sheets) here,  relative to this directory. They are copied after the builtin static files,  so a file named "default.css" will overwrite the builtin "default.css".
If false, no module index is generated.
Output file base name for HTML help builder.
extlinks alias
This ensures correct rendering on system with console encoding != utf8  (windows). It forces pandas to encode its output reprs using utf8  whereever the docs are built. The docs' target is the browser, not  the console, so this is fine.
lower than MethodDocumenter; otherwise the doc build prints warnings
remove the docstring of the flags attribute (inherited from numpy ndarray)  because these give doc build errors (see GH issue 5331)
int : The first line number in the block. 1-indexed.  int : The last line number. Inclusive!  str : The text block including '' character but not any leading spaces.
Only add if not entirely whitespace.
Start with a dummy.
All of the blocks seen so far.
The index mapping lines of code to their associated comment blocks.
Oops! Trailing comment, not a comment block.
A comment block.
plot:: directive
no argument given, assume used as a flag
Sphinx depends on either Jinja or Jinja2
determine input
ensure that LaTeX includegraphics doesn't choke in foo.bar.pdf filenames
is it in doctest format?
determine output directory name fragment
output_dir: final location in the builder's directory
copy image files to builder's output directory
check if it's valid Python as-is
Redirect stdout
Reset sys.argv
assume that if we have one, we have them all
Clear between runs
Run code
Results
Work out how much of the filepath is shared by start and path.
Work out how much of the filepath is shared by start and path.
Subclasses seemingly do not call this.
string conversion routines
Check if the referenced member can have a docstring or not
Convert signode to a specified format
Call user code to resolve the link  no source
only one link per name, please
PANDAS HACK (to remove the list of methods/attributes for Categorical)
Extra mangling domains
_write each elements, separated by a comma.
Handle the last one without writing comma
self._write(":")     self._dispatch(t.step)
Empty tuple.
_write each elements, separated by a comma.
Handle the last one without writing comma
if t is 0.1, str(t)->'0.1' while repr(t)->'0.1000000000001'  We prefer str here.
-*- encoding:utf-8 -*-
No tests at the moment...
No tests at the moment...
No tests at the moment...
No tests at the moment...
De-indent paragraph
Exclude private traits.
init numpydoc
Sort items so that  - Base classes come before classes inherited from them  - Modules come before their contents
Standard library
Third party
Global constants
Use the 'error' token for output.  We should probably make  our own token, but error is typicaly in a bright color like  red, so it works fine for our output prompts.
Register the extension as a valid pygments lexer
Stdlib
To keep compatibility with various python versions
Third-party
Our own
Globals  for tokenizing blocks
nothing left to parse -- the last line
we're assuming at most one decorator -- may need to  rethink
does this look like an input line?
default to brute utf8 if no encoding succeded
Create config object for IPython
create a profile so instance history isn't saved
Create and initialize global ipython, but don't start its mainloop.  This will persist across different EmbededSphinxShell instances.
io.stdout redirect must be done after instantiating InteractiveShell
For debugging, so we can see normal output, use this:
Store a few parts of IPython we'll need.
Optionally, provide more detailed information to shell.
on the first call to the savefig decorator, we'll import  pyplot as plt so we can make a call to the plt.gcf().savefig
Prepopulate the namespace.
recent ipython 4504
insert relative path to image file in source
set the encodings to be used by DecodingStringIO  to convert the execution output into unicode if  needed. this attrib is set by IpythonDirective.run()  based on the specified block options, defaulting to ['ut
Note: catch_warnings is not thread safe
process the first input line
only submit the line in non-verbatim mode
process a continuation line
the "rest" is the standard output of the  input, which needs to be added in  verbatim mode
context information
Add tabs and join into a single string.
save the image files
Then ipython_matplotlib was set to None but there was a  call to the @figure decorator (and ipython_execlines did  not set a backend).
Always import pyplot into embedded shell.
handle decorators
handle comments
contains sphinx configuration variables
get regex and prompt stuff
Get configuration values.
Repeated calls to use() will not hurt us since `mplbackend`  is the same each time.
Must be called after (potentially) importing matplotlib and  setting its backend since exec_lines might import pylab.
Store IPython directive to enable better error messages
setup bookmark for saving figures directory
delete last bookmark
then we wouldn't have to carry these around
handle pure python code
cleanup
Enable as a proper Sphinx directive
We could just let matplotlib pick whatever is specified as the default  backend in the matplotlibrc file, but this would cause issues if the  backend didn't work in headless environments. For this reason, 'agg'  is a good default backend choice.
If the user sets this config value to `None`, then EmbeddedSphinxShell's  __init__ method will treat it as [].
Simple smoke test, needs to be converted to a proper automatic test.
string methods can be  used to alter the string
use a semicolon to suppress the output
use a semicolon to suppress the output
update the current fig
skip local-file depending first example:
remove stale file
just in case the wonky build box doesn't have zip  don't fail this.
LaTeX format.  Produce pdf.
LaTeX format.  Produce pdf.
clean()
Does this return subsets that need fancy indexing? (i.e. lists  of indices)
Does this class make use of random number generators?
Does it ensure that every batch has the same size?
Does it ensure that every batch has the same size?
Does this return subsets that need fancy indexing? (i.e. lists  of indices)  Needs to be set before initialization. See Examples section in class docs
Does this class make use of random number generators?  Needs to be set before initialization. See Examples section in class docs
base iterator that ForcedEvenIterator class wraps  Needs to be set before initialization. See Examples section in class docs
check if the batch has wrong length, throw it away
find unique lengths in sequences
stop when there are no more sequences left
pick a length from the permuted array of lengths
find the position and the size of the minibatch of sequences  to be returned
get the actual indices for the sequences
update the pointer and counts of sequences in the chosen length
Keep only the needed sources in self._raw_data.  Remember what source they correspond to in self._source
If the dataset is incompatible with the new interface, fall back to  the old one
Python 2.6 don't have usesTime() fct.  So we skip that information for them.
Cache the traceback text to avoid converting it multiple times  (it's constant anyway)
Sometimes filenames have non-ASCII chars, which can lead  to errors when s is Unicode and record.exc_text is str  See issue 8924
Do not propagate messages to the root logger.
Set the log level of our logger, either to DEBUG or INFO.
Get rid of any extant logging handlers that are installed.  This means we can call configure_custom() more than once  and have it be idempotent.
Install our custom-configured handler and formatter.
Propagate log messages upwards.
Restore the log level to its default value, i.e. logging.NOTSET.
Delete any handlers that might be installed on our logger.
key for sorting strings alphabetically with numbers
We add all the arguments to the message, to make sure that this  information isn't lost if this exception is reraised again
Delay import of pylearn2.config.yaml_parse and pylearn2.datasets.control  to avoid circular imports
Groups of Python types that are often used together in `isinstance`
If we don't do that, tests function won't be run.
Return the wrapper so this can be used as a decorator via partial()
Put together all modules with unknown versions.
The first line looks like:    changeset:   1517:a6e634b83d88
If not defined, we default to 0 because this is the default  protocol used by cPickle.dump (and because it results in  maximum portability)
dictionary to convert lush binary matrix magic numbers  to dtypes
Publish environment variables related to file name
this code should never be reached
for loading PY2 pickle in PY3
assert False
Don't attempt to guess the right name if the directory is  huge
end if  end for
don't use *=, we don't want to modify the input array
PIL is too stupid to handle single-channel arrays
Create a temporary file with the suffix '.png'.
The expression below can be re-written in a more C style as  follows :  out_shape    = [0,0]  out_shape[0] = (img_shape[0]+tile_spacing[0])*tile_shape[0] -                 tile_spacing[0]  out_shape[1] = (img_shape[1]+tile_spacing[1])*tile_shape[1] -                 tile_spacing[1]
colors default to 0, alpha defaults to 1 (opaque)
if channel is None, fill it with zeros of the correct  dtype
use a recurrent call to compute the channel and store it  in the output
if we are dealing with only one channel
generate a matrix to store the output
if we should scale values to be between 0 and 1  do this by calling the `scale_to_unit_interval`  function
Standard library imports
Third-party imports
Load as the usual ndarray
Special case for on-the-fly normalization
If there are too much features, outputs kernel matrices
Quantitize data
Store the representations in two temporary files
Reread those files and put them together in a .zip
Sparse datasets are not stored as Theano shared vars.
Prefilter features, if needed.
Valid and test representations
Convert into text info
Concatenate the sets, and give different one hot labels for valid and test
Standard library imports
Local imports
Take the first 3 columns
Examples for which any label is set
Special case for sparse matrices
Compress train and label arrays according to condition
Assumes all values are >0, which is the case for all sparse datasets.
Local shortcuts for array operations
Record external parameters
Upper bounds for each minibatch indexes
Number of rows in the resulting union
Random number generation using a permutation
Use a deterministic seed
Retrieve minibatch from chosen set  Increment the related counter  Return the computed minibatch
message has been "improved"
Right now this file just tests that the utlc module can be imported
Save current properties
Ensure that the logger didn't change
This should be fine, we have enough examples for 4 batches  (with one under-sized batch).
This should be fine, we have enough examples for 4 batches  (with one to spare).
This should fail, since you can't make 5 batches of 3 from 10.
Size of the flattened space
This Space does not contain any data, and should not  be mapped to anything
Space is a simple Space, source should be a simple source
Recursively fill the mapping, and return it
Initialize the flatten returned value with Nones
Fill rval with the auxiliary function
The corresponding space was a NullSpace,  and there is no corresponding value in flat,  we use None as a placeholder
We are at a leaf of the tree
We are at a leaf of the tree
flat is not iterable, this is valid only if spec_mapping  contains only 0's, that is, when self.n_unique_specs == 1
final result shape
unraveling
final result shape
unraveling
Avoid upcast to float64 when floatX==float32 and n_classes is int64
Initialize with threshold set to label all inputs as negative
transfer during test
put the inputs + outputs in shared variables so we don't pay GPU  transfer during test
transfer during test
transfer during test
put the inputs + outputs in shared variables so we don't pay GPU  transfer during test
make sure the test gets good coverage, ie, that it includes many  different activation probs for both detector and pooling layer
plot maps of the estimation error, this is to see if it has  some spatial pattern this is useful for detecting bugs like  not handling the border correctly, etc.
don't really know how tight this should be  but you can try to pose an equivalent problem  and implement it in another way  using a numpy implementation in softmax_acc.py  I got a max error of .17
Do exhaustive checks on just the last sample
make sure the test gets good coverage, ie, that it includes  many different activation probs for both detector and pooling layer
don't really know how tight this should be  but you can try to pose an equivalent problem  and implement it in another way  using a numpy implementation in softmax_acc.py  I got a max error of .17
Do exhaustive checks on just the last sample
make sure the test gets good coverage, ie, that it includes many  different activation probs for both detector and pooling layer
plot maps of the estimation error, this is to see if it has some  spatial pattern this is useful for detecting bugs like not handling  the border correctly, etc.
don't really know how tight this should be  but you can try to pose an equivalent problem  and implement it in another way  using a numpy implementation in softmax_acc.py  I got a max error of .17
Do exhaustive checks on just the last sample
don't really know how tight this should be  but you can try to pose an equivalent problem  and implement it in another way  using a numpy implementation in softmax_acc.py  I got a max error of .17
Do exhaustive checks on just the last sample
use a big value of alpha so mistakes involving alpha show up strong
This call should not raise any error:
This call should not raise any error:
The actual function should do exactly the same arithmetic on  integers so we should get exactly the same floating point values
Note: this is per-example mean across pixels, not the  per-pixel mean across examples. So it is perfectly fine  to subtract this without worrying about whether the current  object is the train, valid, or test set.
ddof=1 simulates MATLAB's var() behaviour, which is what Adam  Coates' code does.
If we don't do this, X.var will return nan.
Don't normalize by anything too small.
Messages that matches the flag value returned by the method
Initialise
Note that Anorm has been initialized by IsOpSym6.
[dlta_k epln_{k+1}] = [cs  sn][dbar_k    0      ]    [gbar_k  dbar_{k+1} ]   [sn -cs][alpha_k beta_{k+1}].
Update x except if it will become too big
Failed to find a suitable step length
print 'armijo'
Make sure variables are tensors otherwise strange things happen
Note: `lazy if` would make more sense, but it is not        implemented in C right now
f(x) = B*(x-a)^2 + C*(x-a) + D
Check new value of a_j
Check new value of a_j
Use <= rather than = so if there are ties  the bigger step size wins  end if obj  end for ind, alpha
end check on alpha_ind
used for statistics gathering
it is initialized to 1 to get all the means started at  data points, but then we turn it into a running average
This works if there is no output,  because the output is an empty list
x is a numpy array  x = pickle.load(open(test_path, 'rb'))
obj is a Dataset
define some common parameters
average MBCE over example rather than sum it
random point to start at
code to get log likelihood from kernel density estimator  this crashed on GPU (out of memory), but works on CPU
average over costs rather than summing
combine costs into GSNCost object  reconstruction on layer 0 with weight 1.0
classification on layer 2 with weight 2.0
turn off corruption
error indices
white block to indicate end of chain
map ids of objects we've fixed before to the fixed version, so we don't clone objects when fixing  can't use object itself as key because not all objects are hashable
ids of objects being fixed right now (we don't support cycles)
Base case: we found a shared variable, must convert it  Sabotage its getstate so if something tries to pickle it, we'll find out
pass args=[] so we can pass options to nosetests on the command line
!/usr/bin/env python
configs on sgd
supervised training
Make the termination criterion really lax so that it is quick
We'll need the serial module to save the dataset
Our raw dataset will be the CIFAR10 image dataset
We'll need the preprocessing module to preprocess the dataset
Our raw training set is 32x32 color images
First we want to pull out small patches of the images, since it's easier  to train an RBM on these
Next we contrast normalize the patches. The default arguments use the  same "regularization" parameters as those used in Adam Coates, Honglak  Lee, and Andrew Ng's paper "An Analysis of Single-Layer Networks in  Unsupervised Feature Learning"
Finally we whiten the data using ZCA. Again, the default parameters to  ZCA are set to the same values as those used in the previously mentioned  paper.
Escape potential backslashes in Windows filenames, since  they will be processed when the YAML parser will read it  as a string
copy data if don't exist
Load train data
prepare preprocessing  without batch_size there is a high chance that you might encounter memory error  or pytables crashes
apply the preprocessings to train
load and preprocess valid
load and preprocess test
The averaging math assumes batches are all same size
Train shortly and prevent saving
Train shortly and prevent saving
Run some checks on the samples, this should help catch any bugs
Now compile the full sampling update
Make shared variables representing the sampling state of the model  Seed the sampling with the data batch
There's as much layers in the DBM as there are bias vectors
Contribution from model B, at temperature beta_k
Contribution from model A, at temperature (1 - beta_k)
There's as much layers in the DBM as there are bias vectors
Contribution of biases
Initialize log-ais weights
Estimate the log-mean of the AIS weights
Perform inference
Copy into negative phase buffers to measure energy
Compute sum of likelihood for current buffer
Perform moving average of negative likelihood  Divide by len(x) and not bufsize, since last buffer might be smaller
There's as much layers in the DBM as there are bias vectors
Top-down input
Bottom-up input
Add a dummy placeholder for visible layer's weights in W_list
Depth of the DBM
For an even number of layers, we marginalize the odd layers  (and vice-versa)
Build function to compute free-energy of p_k(h1).
Possible metrics
replicate the preprocessing described in  Kai Yu's paper Improving LCC with Local Tangents
Maps a label vector to the corresponding index in <values>
shift subplots down to make more room for the text
Returns None if there is no 'blank' category (e.g. if we're using  the small NORB dataset.
Indexes into the first 5 labels, which live on a 5-D grid.
Maps 5-D label vector to a list of row indices for dataset.X, dataset.y  that have those labels.
Indexes into the row index lists returned by label_to_row_indices.
Index into grid_indices currently being edited
Hides axes' tick marks
prepends the current index's line with an arrow.
Shaves off the singleton dimensions  (batch  and channel ), leaving just 's', 0, and 1.
If dataset is big NORB, add one for the image index
increment the image index
increment one of the grid indices
some grid indices have 2 images instead of 3.
Disables left/right key if we're currently showing a blank,  and the current index type is neither 'category' (0) nor  'image number' (5)
defining azimuth, elevation, and depth
find the gradient  (it is two arrays: grad_x and grad_y)  getting the unit incident ray
get MNIST image
extract patch from texture database. Note that texture 14  does not exist.
store output details
generate binary mask for digit outline
copy contents of masked-MNIST image into background texture
this now because the image to emboss
Check if MAT files have been downloaded
equal -> compare val record entries with np.all(x == y)  allclose -> compare val record entries with np.allclose(x, y)
Load the records
Print numerical differences between the channels
Quit scanning channels that we've read all of
Obtain versions of the various Python packages.
Local imports
This will call str() on keys and values, not repr(), so unicode  objects will have the form 'blah', not "u'blah'".
Convert nested DD into nested ydict.
This will be the complete yaml string that should be executed
Instantiate an object from YAML string
print "Executing the model."  This line will call a function defined by the user and pass train_obj  to it.
Standard library imports
Third-party imports
Disable the display for the plot extension to work  An alternative is to create another training script
Local imports
Publish a variable indicating the training phase.
Execute this training phase.
Clean up, in case there's a lot of memory used that's  necessary for the next phase.
run print_monitor_cv.py main
run print_monitor_cv.py main with all=True
cleanup
no unique substring
If there is more than one channel in the monitor ask which ones to  plot
Display the codebook
end for code in codes
plot the requested channels
older saved monitors won't have epoch_record
pdb.set_trace()
'* 1.8' comse from the fact that rows take up about 1.8 times as much  space as columns, due to the title text.
Hides tickmarks
For each pixel, remove mean of 9x9 neighborhood
Scale down norm of 9x9 patch if norm is bigger than 1
Convert the .csv file to numpy
Discard header
discard any zero-padding that was used to give the batches uniform size
Convert the .csv file to numpy
Discard header
discard any zero-padding that was used to give the batches uniform size
Pick whether to iterate over visible or hidden states.
Determine in how many steps to compute Z.
configure base-rate biases to those supplied by user
check that both models have the same number of hidden units  check that both models have the same number of visible units
make sure parameters are in floatX format for GPU support
declare symbolic vars for current sample `v_sample` and temp `beta`
initialize log importance weights
insert key temperatures within
initial sample
whenever we reach a "key" beta value, log log_ais_w and  var(log_ais_w) so we can estimate log_Z_{beta=key_betas[i]} after  the fact.
generate a new sample at temperature beta_{i+1}
estimate the log-mean of the AIS weights
Initialize self._nested_data_specs, self._data_specs_mapping,  and self._flat_data_specs
If the channels have changed at all, we need to recompile the theano  functions used to compute them
need to put d back into self._datasets
If self._flat_data_specs is empty, no channel needs data,  so we do not need to call the iterator in order to average  the monitored values across different batches, we only  have to call them once.
X is a flat (not nested) tuple  end for X
Recompute the data specs, since the channels may have changed.
Get the appropriate kind of theano variable to represent the data  the model acts on
Flatten channel.graph_input and the appropriate part of  nested_theano_args, to iterate jointly over them.
Some channels may not depend on the data, ie, they might just  monitor the model parameters, or some shared variable updated  by the training algorithm, so we need to ignore the unused  input error
Patch old pickled monitors
patch old pkl files
Ask the model for the data_specs needed
Build a nested tuple from ipt, to dispatch the appropriate parts  of the ipt batch to each cost
We need three things: the value itself (raw_channels[name]),  the input variables (cost_ipt), and the data_specs for  these input variables ((spaces[i], sources[i]))
Use the last inputs from nested_ipt for the model  Note: some code used to consider that model_channels[name]  could be a a (channel, prereqs) pair, this is not supported.
Hack to deal with Theano expressions not being serializable.  If this is channel that has been serialized and then  deserialized, the expression is gone, but we should have  stored the doc  Support pickle files that are older than the doc system
Standard library imports
Third-party imports
Do not duplicate the parameters if some are shared  between layers
Build the hidden representation at each layer
Remove this from the top-level namespace.
use the milk implementation of k-means if it's available
print 'iter:',iter,' conv crit:',abs(mmd-prev_mmd)  if numpy.sum(numpy.isnan(mu)) > 0:
computing distances
mean minimum distance:
converged
finding minimum distances
One call to train_all currently trains the model fully,  so return False immediately.
Use version defined in Model, rather than Block (which raises  NotImplementedError).
Third-party imports
Parameters
only for convenience
easy way to turn off corruption (True => corrupt, False => don't)
easy way to turn off sampling
easy way to not use bias (True => use bias, False => don't)
check that autoencoders are the correct sizes by looking at previous  layer. We can't do this for the first ae, so we skip it.
activation for visible layer is aes[0].act_dec
the indices which are being set
intialize steps
main loop
handle splitting of concatenated data
things that require re-compiling everything
everything is cached, return all but state and indices
indices have changed, need to recompile f_init
have no cached function (or incorrect state)
leave out the first time step
set minibatch
Update and corrupt all of the odd layers (which we aren't skipping)
precor is before sampling + postactivation corruption (after preactivation  corruption and activation)
take values from initial
zero out values in activations
using _apply_corruption to apply samplers
Using the activation function from lower autoencoder
ACTIVATION  None implies linear
3d tensor: axis 0 is time step, axis 1 is minibatch item,  axis 2 is softmax output for label (after slicing)
convert argmax's to one-hot format
Standard library imports
taking the mean over each term independently allows for different  mini-batch sizes in the positive and negative phase.
zero mean, std sigma noise
Derived closed to Xavier Glorot's magic formula
THE BETA IS IGNORED DURING TRAINING - FIXED AT MARGINAL DISTRIBUTION
Sometimes, the number of examples in the data set is not a  multiple of self.batch_size.
sample h given v
sample v given (s,h)
Create the stack
Take care of learning rate scales for individual parameters  Base learning rate per example.
A shared variable for storing the iteration number.
A shared variable for storing the annealed base learning rate, used  to lower the learning rate gradually after a certain amount of time.
Update the shared variable for the annealed learning rate.
Calculate the learning rates for each parameter, in the order  they appear in self.params
Add the learning rate/iteration updates
Get the updates from sgd_updates, a PyLearn library function.
Add the things in p_up to ups
Return the updates dictionary.
Space initialization
Look for the right KLIntegrator if it's not specified
Sample from p(z)  Decode theta  Sample from p(x | z)
We express mu in terms of the pre-sigmoid activations. See  `log_conditional` for more details.
`conditional_params` is composed of pre-sigmoid activations; see  `log_conditional` for more details.
Standard library imports
Local imports
This module really has no adjustable parameters -- once train()  is called once, they are frozen, and are not modified via gradient  descent.
Center each feature.
Compute eigen{values,vectors} of the covariance matrix.
Build Theano shared variables  For the moment, I do not use borrow=True because W and v are  subtensors, and I want the original memory to be freed
they were doing  component_cutoff is a shared variable, so updating its value here has  NO EFFECT on the symbolic expression returned by this call (and what  this expression evalutes to can be modified by subsequent calls to  _update_cutoff)
this proprocessing is already done
The resulting components are in *ascending* order of eigenvalue, and  W contains eigenvectors in its *columns*, so we simply reverse both.
Compute feature means.
The resulting components are in *ascending* order of eigenvalue,  and W contains eigenvectors in its *rows*, so we reverse both and  transpose W.
The resulting components are in *ascending* order of eigenvalue, and  W contains eigenvectors in its *columns*, so we simply reverse both.
Can't subtract a sparse vector from a sparse matrix, apparently,  so here I repeat the vector to construct a matrix.
The resulting components are in *ascending* order of eigenvalue, and  W contains eigenvectors in its *columns*, so we simply reverse both.
Update component cutoff, in case min_variance or num_components has  changed (or both).
Total number of observations: to compute the normalizer for the mean  and the covariance.  Index in the current minibatch
Matrix containing on its *rows*:  - the current unnormalized eigen vector estimates  - the observations since the last reevaluation
The discounted sum of the observations.
Holds the unnormalized eigenvectors of the covariance matrix before  they're copied back to Xt.
Add the *non-centered* observation to Xt.
Update the discounted sum of the observations.
To get the mean, we must normalize the sum by:  \gamma^(n_observations-1) + /gamma^(n_observations-2) + ... + 1
Now center the observation.  We will lose the first observation as it is the only one in the mean.
Regularize - not necessary but in case
Convert the n_eigen LAST eigenvectors of the Gram matrix contained in  V into *unnormalized* eigenvectors U of the covariance (unnormalized  wrt the eigen values, not the moving average).
Update Xt, G and minibatch_index
We subtract self.minibatch_index in case this call is not right  after a reevaluate call.
Patch old pickle files
This is not really an unimplemented case.  We actually don't know how to format the weights  in design space. We got the data in topo space  and we don't have access to the dataset
Let the PatchViewer decide how to arrange the units  when they're not pooled  When they are pooled, make each pooling unit have one row
Let the PatchViewer decide how to arrange the units  when they're not pooled  When they are pooled, make each pooling unit have one row
note: I think the desired space thing is actually redundant,  since LinearTransform will also dimshuffle the axes if needed  It's not hurting anything to have it here but we could reduce  code complexity by removing it
Let the PatchViewer decide how to arrange the units  when they're not pooled  When they are pooled, make each pooling unit have one row
Only to be used by the deprecation warning wrapper functions
When applying dropout to a layer's input, use this for masked values.  Usually this will be 0, but certain kinds of layers may want to override  this behaviour.
check if the layer_name is None (the MLP is the outer MLP)
check the case where coeffs is a scalar
check the case where coeffs is a scalar
Patch old pickle files
because of an error in optimization (local_useless_tile)  when tiling with (1, 1)
This is not really an unimplemented case.  We actually don't know how to format the weights  in design space. We got the data in topo space  and we don't have access to the dataset
Let the PatchViewer decide how to arrange the units  when they're not pooled  When they are pooled, make each pooling unit have one row
This is not really an unimplemented case.  We actually don't know how to format the weights  in design space. We got the data in topo space  and we don't have access to the dataset
Original: p = p * (p > 0.) + self.left_slope * p * (p < 0.)  T.switch is faster.  For details, see benchmarks in  pylearn2/scripts/benchmark/time_relu.py
Format the input to be supported by max pooling
Alias the variables for pep8
Check 'not value' to support case of empty list
This is to mimic the behavior of CompositeSpace's restrict  method, which only returns a CompositeSpace when the number  of components is greater than 1
No two layers can contend to scale a parameter  Don't try to scale anything that's not a parameter
update, scaled back onto unit sphere
dot product between scaled update and current W
Standard library imports
Third-party imports
Local imports
why a theano rng? should we remove it?
Compute the input flowing into the hidden units, i.e. the  value before applying the nonlinearity/activation function  Apply the activating nonlinearity.
As long as act_enc is an elementwise operator, the Jacobian  of a act_enc(Wx + b) hidden layer has a Jacobian of the  following form.
Create the stack
Patch old pickle files
Catch classes that try to override the old method.  This check may be removed after 2015-05-13.
Quick check in case __init__ was never called, e.g. by a derived  class.
Validate num_steps
Implement the num_steps > 1 case by repeatedly calling the  num_steps == 1 case
The rest of the function is the num_steps = 1 case  Current code assumes this, though we could certainly relax this  constraint
Validate layer_to_clamp / make sure layer_to_clamp is a fully  populated dictionary
Assemble the return value
Get the sampled state of the layer below so we can condition  on it in our Gibbs update
Compute the Gibbs sampling update  Sample the state of this layer conditioned  on its Markov blanket (the layer above and  layer below)
Sample the odd-numbered layers
Get the sampled state of the layer below so we can condition  on it in our Gibbs update
Compute the Gibbs sampling update  Sample the state of this layer conditioned  on its Markov blanket (the layer above and  layer below)
Check that all layers were updated  Check that we didn't accidentally treat any other object as a layer  Check that clamping worked
Make corrections for if we're also running inference on Y
Last layer before Y does not need its weights doubled  because it already has top down input
Last layer is clamped to Y
end for mf iter  end if recurrent  Run some checks on the output
layer_above = None,
debugging, make sure V didn't get changed in this function
Originally WeightDoubling did not support multi-prediction training,  while a separate class called SuperWeightDoubling did. Now they are  the same class, but we maintain the SuperWeightDoubling class for  backwards compatibility. May be removed on or after 2015-04-20.
Y is observed, specify that it's fully observed
layer_above = None,
debugging, make sure V didn't get changed in this function
Make corrections for if we're also running inference on Y  Last layer is clamped to Y
we only need recurrent inference if there are multiple layers
Run some checks on the output
debugging, make sure V didn't get changed in this function
Make corrections for if we're also running inference on Y  Last layer is clamped to Y
we only need recurrent inference if there are multiple layers
debugging, make sure V didn't get changed in this function
The examples are used to initialize the visible layer's chains
Make sure all DBM have only one hidden layer, except for the last  one, which can have an optional target layer
This condition could be relaxed, but current code assumes it
This condition could be relaxed, but current code assumes it
Patch old pickle files
patch old pickle files
Patch pickle files that predate the freeze_set feature
No two layers can contend to scale a parameter  Don't try to scale anything that's not a parameter
Make a list of all layers
Make a list of all layers
Validate layer_to_clamp / make sure layer_to_clamp is a fully  populated dictionary
Translate update expressions into theano updates
Don't serialize the dataset
Energy function is linear so it doesn't matter if we're averaging  or not
This is not really an unimplemented case.  We actually don't know how to format the weights  in design space. We got the data in topo space  and we don't have access to the dataset
Should probably implement sum pooling for the non-pooled version,  but in reality it's not totally clear what the right answer is
Don't serialize the dataset
data is in [-1, 1], but want biases for a sigmoid  init_bias =
Energy function is linear so it doesn't matter if we're averaging  or not
Don't serialize the dataset
Energy function is linear so it doesn't matter if we're averaging or not
Patch old pickle files
This is not really an unimplemented case.  We actually don't know how to format the weights  in design space. We got the data in topo space  and we don't have access to the dataset
Let the PatchViewer decidew how to arrange the units  when they're not pooled  When they are pooled, make each pooling unit have one row
If the pool size is 1 then pools = detectors  and we should not penalize pools and detectors separately
If the pool size is 1 then pools = detectors  and we should not penalize pools and detectors separately
Should probably implement sum pooling for the non-pooled version,  but in reality it's not totally clear what the right answer is
Patch old pickle files
If you implement this case, also add a unit test for it.  Or at least add a warning that it is not tested.
patch old pickle files
patch old pickle files
we use sum and not mean because this is really one variable per row
To make GaussianVisLayer compatible with any axis ordering
If the pool size is 1 then pools = detectors  and we should not penalize pools and detectors separately
note that, within each group, E[p] is the sum of E[h]
work around theano bug with broadcasted stuff
If the pool size is 1 then pools = detectors  and we should not penalize pools and detectors separately
note that, within each group, E[p] is the sum of E[h]
scale each learning rate by 1 /  times param is reused
work around theano bug with broadcasted stuff
This is not really an unimplemented case.  We actually don't know how to format the weights  in design space. We got the data in topo space  and we don't have access to the dataset
Should probably implement sum pooling for the non-pooled version,  but in reality it's not totally clear what the right answer is
called should cache the compilation results, including those  inside cg
Temporarily change config.floatX to float64, as s3c these  tests currently fail with float32.
Restore previous value of floatX
We also have to change the value of config.floatX in __init__.
begin block
begin block
Create fake data
Construct an equivalent MLP which gives the same output  after flattening both.
Check that the two models give the same output
Check that both costs are not implemented
Skip test if cuda_ndarray is not available.
Compile in debug mode so we don't optimize out the size of the buffer  of zeros
axes for batch, rows, cols, channels, can be given in any order
rng = np.random.RandomState([2012,11,1])
axes for batch, rows, cols, channels, can be given in any order
pool_size=1 is an important corner case
This is just to placate mf_update below
To find the mean of the samples, we use mean field with an input of 0
Make DBM and read out its pieces
Choose which unit we will test
Randomly pick a v, h1[-p_idx], and h2 to condition on  (Random numbers are generated via dbm.rng)
Infer P(h1[i] | h2, v) using mean field
np.asarray(on_probs) doesn't make a numpy vector, so I do it manually
1 is an important corner case  We must also run with a larger number to test the general case
Make DBM and read out its pieces
Choose which unit we will test
Randomly pick a v, h1[-p_idx], and h2 to condition on  (Random numbers are generated via dbm.rng)
Infer P(h1[i] | h2, v) using mean field
np.asarray(on_probs) doesn't make a numpy vector, so I do it manually
1 is the only pool size for which centering is implemented
Make DBM and read out its pieces
Choose which unit we will test
Randomly pick a v, h1[-p_idx], and h2 to condition on  (Random numbers are generated via dbm.rng)
Infer P(h1[i] | h2, v) using mean field
1 is an important corner case  We must also run with a larger number to test the general case
Make DBM
Randomly pick a v to condition on  (Random numbers are generated via dbm.rng)
Infer P(y | v) using mean field
Infer P(y | v) using the energy function
np.asarray(probs) doesn't make a numpy vector, so I do it manually
Make DBM
Randomly pick a v to condition on  (Random numbers are generated via dbm.rng)
Infer P(y | v) using mean field
Infer P(y | v) using the energy function
np.asarray(probs) doesn't make a numpy vector, so I do it manually
Make DBM
Randomly pick a v to condition on  (Random numbers are generated via dbm.rng)
Infer P(y | v) using mean field
copy all the states out into a batch size of num_samples
Tests whether the returned p_sample and h_sample have the right  dimensions
Verifies that VariationalCD works well with make_layer_to_symbolic_state
Verify that get_total_input_dimension works.
Accumulate the sum of output of all masked networks.
Create network with single softmax layer, corresponding to first  layer in the composite network.
Create network with single softmax layer, corresponding to second  layer in the composite network.
Create dataset which we will test our networks against.
Train all models with their respective datasets.
Check that we get same output given the same input on a randomly  generated dataset.
Finally check that calling the internal FlattenerLayer behaves  as we would expect. First, retrieve the FlattenerLayer.
Check that it agrees on the input space.
Check that it agrees on the parameters.
Test linear layer, see the function  compare_flattener_composite_mlp_with_separate_mlps for more details
Create network with single softmax layer, corresponding to first  layer in the composite network.
Now train the three networks on the same dataset
Finally, check that we get same output given the same input on a randomly  generated dataset.
Temporarily change config.floatX to float64, as s3c inference  tests currently fail due to numerical issues for float32.
Restore previous value of floatX
We also have to change the value of config.floatX in __init__.
Second part of the check handles cases where kl is None, etc.
Recurse on the keys too, for backward compatibility.  Is the key instantiation feature ever actually used, by anyone?
In the future it might be good to consider a dict argument that provides  a type->callable mapping for arbitrary transformations like this.
This is apparently here to avoid the odd instance where a file gets  loaded as Unicode instead (see 03f238c6d). It's rare instance where  basestring is not the right call.
We know it's an ImportError, but is it an ImportError related to  this path,  or did the module we're importing have an unrelated ImportError?  and yes, this test can still have false positives, feel free to  improve it
The yaml file is probably to blame.  Report the problem with the full module path from the YAML  file
Add the custom multi-constructor
we could have also just put the corruptor definition here
yaml.load can take a string or a file object  These two things should be the same object
Assert the unsubstituted TEST_VAR is in yaml_src
Try to call theano_expr with a bad label dtype.
Try to call format with a bad label dtype.
Make sure an invalid max_labels raises an error.
Make sure an invalid dtype identifier raises an error.
Make sure an invalid ndim raises an error for theano_expr().
Print all .py files in the library
Work around Python < 2.6 behaviour, which does not generate NL after  a comment which is on a line by itself.
The line could contain multi-byte characters
assert char in '([{'
indent_next tells us whether the next block is indented; assuming  that it is indented by 4 spaces, then we should not allow 4-space  indents on the final continuation line; in turn, some other  indents are allowed to have an extra 4 spaces.
this is the beginning of a continuation line.
record the initial indent.
identify closing bracket
closing bracket for visual indent
closing bracket matches indentation of opening bracket's line
visual indent is broken
hanging indent is verified
visual indent is verified
ignore token lined up with matching one from a previous line
Syntax "class A (B):" is allowed, but avoid it  Allow "return (a.foo for a in range(5))"
ERRORTOKEN is triggered by backticks in Python 3
Found a (probably) needed space
Tolerate the "<>" operator, even if running Python 3  Deal with Python 3's annotated return value "->"
A needed trailing space was not found
Allow keyword args or defaults: foo(bar=None).
Surrounding space is optional, but ensure that  trailing space matches opening space
A needed opening space was not found
The physical line contains only this token.
The comment also ends a physical line
Results
Don't care about expected errors or warnings
The default choice: ignore controversial checks
Ignore all checks which are not explicitly selected
contain a pattern that matches?
First, read the default values
Third, overwrite with the command-line options
Don't read the command line if the module is used as a library.  If parse_argv is True and arglist is None, arguments are  parsed from the command line (sys.argv)
De-indent paragraph
string conversion routines
out += self._str_index()
Skip out-of-library inherited methods
We know scipy.sparse is available
Catches any CompositeSpace batches that were mistakenly hand-constructed  using nested lists rather than nested tuples.
Data-less batches such as None or () are valid numeric and symbolic  batches.  Justification: we'd like  is_symbolic_batch(space.make_theano_batch()) to always be True, even if  space is an empty CompositeSpace.
The subbatch_results must be all true, or all false, not a mix.
Uses the 'CudaNdarray' string to avoid importing  theano.sandbox.cuda when it is not available
theano._asarray is a safer drop-in replacement to numpy.asarray.
Checks if batch belongs to this space
checks if self and space have compatible sizes for formatting.
Checks if batch belongs to this space
When unpickling a Space that was pickled before Spaces had dtypes,  we need to set the _dtype to the default value.
checks that batch isn't a tuple, checks batch.type against self.dtype
Undo subtensor slice
checks that batch isn't a tuple, checks batch.type against self.dtype
Only batch size of 1 is supported
checks that batch isn't a tuple, checks batch.type against self.dtype
Only batch size of 1 is supported
checks that batch isn't a tuple, checks batch.type against self.dtype
Assume pylearn2's get_topological_view format, since this is how  data is currently served up. If we make better iterators change  default to ('b', 'c', 0, 1) for theano conv2d
Converts shape to a tuple, so it can be hashable, and self can be too
Patch old pickle files
checks batch.type against self.dtype
Check for cast
Check to see if axis ordering was changed
NullSpaces don't support validation callbacks, since they only take None  as data batches.
There is no way to know how many examples would actually  have been in the batch, since it is empty. We return 0.
Can't use nose.tools.assert_raises, only introduced in python 2.7. Use  numpy.testing.assert_raises instead
Sparse VectorSpaces throw an exception if batch_size is specified.
Use this when space doesn't specify a dtype
Sparse VectorSpaces throw an exception if batch_size is  specified
simple -> simple
composite -> simple
simple -> composite
no need to make CompositeSpaces with components spanning all possible  dtypes. Just try 2 dtype combos. No need to try different sparsities  either. That will be tested by the non-composite space conversions.
A few composite dtypes to try throwing at CompositeSpace's batch-making  methods.
Tests CompositeSpace's batch-making methods and dtype setter  with composite dtypes
VectorSpace and Conv2DSpace
VectorSpace to Sparse VectorSpace
Start in VectorSpace
VectorSpace and Conv2DSpace
Third-party imports
Shortcuts
for stability
generate random one-hot matrix
pass up a 0 because corruption_level is not relevant here
corruption_level isn't relevant here
pass up the 0 for corruption_level (not relevant here)
This sets to zero all elements where Y == -1
normalize for number of steps
don't include label layer
don't include features
normalize for coefficients on each cost
if there's only 1 cost, then no need to split up the costs
get space for layer i of model
get the spaces for layers that we have costs at
layer_to_chains = model.rao_blackwellize(layer_to_chains)
note: if the Y layer changes to something without linear energy,  we'll need to make the expected energy clamp Y in the positive  phase
note: if the Y layer changes to something without linear energy,        we'll need to make the expected energy clamp Y in the        positive phase
Note that we replace layer_to_chains with a dict mapping to the new  state of the chains
Note that we replace layer_to_chains with a dict mapping to the new  state of the chains
note: if the Y layer changes to something without linear energy,  we'll need to make the expected energy clamp Y in the positive  phase
Note that we replace layer_to_chains with a dict mapping to the new  state of the chains
Note that we replace layer_to_chains with a dict mapping to the new  state of the chains  We first initialize the chain by clamping the visible layer and the  target layer (if it exists)
We then do the required mcmc steps
have been a bug, where costs from lower layers got  applied to higher layers that don't implement the cost
rval['empirical_beta_min'] = empirical_beta.min()  rval['empirical_beta_mean'] = empirical_beta.mean()  rval['empirical_beta_max'] = empirical_beta.max()
ignore Y if some other cost is supervised and has made it get  passed in (can this still happen after the (space, source)  interface change?)
end if include_Y  end if both directions
end for substates  end for layers  end if act penalty
size needs to have a fixed length at compile time or the  theano random number generator will be angry
based on equation 3 of the paper  ours is the negative of theirs because  they maximize it and we minimize it
compute negative phase updates
Compute SML cost
Compute CD cost
X is theano sparse
a random pattern that indicates to reconstruct all the 1s and some of  the 0s in X
L1 penalty on activations
there is a numerical problem when using  tensor.log(1 - model.reconstruct(X, P))  Pascal fixed it.
X is theano sparse
a random pattern that indicates to reconstruct all the 1s and some of  the 0s in X
L1 penalty on activations
Handle explicitly undefined costs
If anybody knows how to add type(self) to the exception message  but still preserve the stack trace, please do so  The current code does neither
Build composite space representing all inputs
This Cost does not depend on any data, and get_data_specs does not  ask for any data, so we should not be provided with some.
Absolute value handles odd-valued p cases
To be compatible with earlier scripts,  try (self.method)_data_specs
We assume aliasing is a bug
In the case the monitor has only one channel, the channel_name can  be omitted and the criterion will examine the only channel  available. However, if the monitor has multiple channels, leaving  the channel_name unspecified will raise an error.
The countdown decreases every time the termination criterion is  called unless the channel value is lower than the best value times  the prop_decrease factor, in which case the countdown is reset to N  and the best value is updated
The optimization continues until the countdown has reached 0,  meaning that N epochs have passed without the model improving  enough.
Defined in setup(). A dict that maps Datasets in self._randomize and  self._randomize_once to zero-padded versions of their topological  views.
Central windowing of auxiliary datasets (e.g. validation sets)
maps each dataset in randomize_now to a zero-padded topological view  of its data.
For each dataset, for each image, extract a randomly positioned and  potentially horizontal-flipped window
If no tag key is provided, use the class name by default.
placeholders
More stuff to be added later. For now, we care about the best cost.
one vs. the rest
Start training an MLP with the LiveMonitoring train extension
Query for first two elements of train_objective data
Query for second element of train_objective data
Close the training process
Test not a list
Test empty list
Test bad start/end combination
4 5x5x2 images (stored in a 2x5x5x4 tensor)
no zero-padding.
Tracks the number of times on_monitor has been called
Determine what type of message was received
Create the desired permission
Set the permission
Initialize the layout
Limit the range of possible layouts
Create the list of color hue
Set the color in HSV format
Put in a matplotlib-friendly format  Convert to RGB
Keep the relevant part
Get the x axis
Put in seconds if needed
Plot the quantities
Set a default freq
fatal
benign
whatever
repeatedly remove the right-most  extension, until none is found
This file is intentionally monolithic.  It also intentionally restricts itself  to standard library modules, with no  extra dependencies.
Global variables for the whole module.
both dictionaries for fast search  (but are semantically lists)
repeatedly remove the right-most  extension, until none is found
deals with non-atomic move
an atomic move is performed  (both files are on the same device,  or the destination doesn't exist)
check if directories exist, and if not,  create them, and then fetch source.lst
not a problem if not found in a given location
file opened
not a problem if not found in a location
read from file and  create a dictionary
then read only root
replace the installed.lst in  a safe way
download to temporary was successful,  let's (try to) perform the atomic replace
file does not exist, just download!
yay! download successful!
open the tarball as read, bz2
ok, it's openable, let's expand it  yay! success!
throws CalledProcessError if return  return code is not zero.
ok, success (or not), let's unstack
readable when invoked with super-powers
remove the package from the list
install location is determined by super-powers  (so a root package can be upgraded locally!)
assign filename to cached package
runs through the .../package_name/scripts/  directory and executes the scripts in a  specific order (which shouldn't display  much unless they fail)
get names only
check what packages are in the list,  and really to be upgraded.
check if there's a date
ok, there's a newer version
no newer version, nothing to update
not installed?
ok, nothing to upgrade,  move along.
ok, nothing to upgrade,  move along.
check if in the installed.lst  then if directory actually exists  then if you have rights to remove it
ok, you may have rights to delete it
ok, nothing to remove, filenames where bad.
does nothing, no cache implemented  yet.
to remove RuntimeWarnings about how  tempfilename is unsafe.
OK, let's construct the environment  needed by dataset-get
maybe not a problem, but  FIXME: print a warning if exists,  but cannot be read (permissions)
read from file and  create a dictionary
PYLEARN2_DATA_PATH may or mayn't be defined
simplest tests
If it's iterable, we're fine. If not, it's a single callback,  so wrap it in a list.
saturate=start, so just jump straight to final momentum
mean_squared_grad := E[g^2]_{t-1}  mean_square_dx := E[(\Delta x)^2]_{t-1}
Accumulate gradient
Accumulate updates
Apply update
sum_square_grad := \sum g^2
Accumulate gradient
Compute update
Apply update
mean_squared_grad := E[g^2]_{t-1}
Store variable in self.mean_square_grads for monitoring.
Accumulate gradient
Apply update
test if force batch size and batch size
Methods of `self.cost` need args to be passed in a format compatible  with data_specs
Concatenate the name of all tensors in theano_args !?
Use standard SGD updates with fixed learning rate.
Make sure none of the parameters have bad values
Make sure none of the parameters have bad values
only the initial monitoring has happened  no learning has happened, so we can't adjust learning rate yet  just do nothing
If we keep on executing the exponentiation on each mini-batch,  we will eventually get an OverflowError. So make sure we  only do the computation until min_lr is reached.
HACK
Get the data specifications needed by the model
model.train_batch and self.train both return False when training  should terminate.
We include a cost other than SumOfParams so that data is actually  queried from the training set, and the expected number of updates  are applied.
We include a cost other than SumOfParams so that data is actually  queried from the training set, and the expected number of updates  are applied.
We include a cost other than SumOfParams so that data is actually  queried from the training set, and the expected number of updates  are applied.
begin adadelta
the value must be positive
We include a cost other than SumOfParams so that data is actually  queried from the training set, and the expected number of updates  are applied.
Implemented only so that DummyCost would work
Make the test fail if algorithm does not  respect get_input_space  Multiplying by P ensures the shape as well  as ndim is correct
Make the test fail if algorithm does not  respect get_input_space  Multiplying by P ensures the shape as well  as ndim is correct
Including a monitoring dataset lets us test that  the monitor works with supervised data
We need to include this so the test actually stops running at some point
Including a monitoring dataset lets us test that  the monitor works with unsupervised data
We need to include this so the test actually stops running at some point
including a monitoring datasets lets us test that  the monitor works with supervised data
We need to include this so the test actually stops running at some point
including this extension for saving learning rate value after each batch
including a monitoring datasets lets us test that  the monitor works with supervised data
We need to include this so the test actually stops running at some point
including this extension for saving learning rate value after each batch
including a monitoring datasets lets us test that  the monitor works with supervised data
We need to include this so the test actually stops running at some point
including a monitoring datasets lets us test that  the monitor works with supervised data
We need to include this so the test actually stops running at some point
including a monitoring datasets lets us test that  the monitor works with supervised data
We need to include this so the test actually stops running at some point
including a monitoring datasets lets us test that  the monitor works with supervised data
We need to include this so the test actually stops running at some point
including a monitoring datasets lets us test that  the monitor works with supervised data
We need to include this so the test actually stops running at some point
We need to include this so the test actually stops running at some point
including a monitoring datasets lets us test that  the monitor works with supervised data
We need to include this so the test actually stops running at some point
including a monitoring datasets lets us test that  the monitor works with supervised data
testing for bad dataset_name input
testing for bad channel_name input
We need to include this so the test actually stops running at some point
including a monitoring datasets lets us test that  the monitor works with supervised data
We need to include this so the test actually stops running at some point
including a monitoring datasets lets us test that  the monitor works with supervised data
including a monitoring datasets lets us test that  the monitor works with supervised data
We need to include this so the test actually stops running at some point
We need to include this so the test actually stops running at some point
We need to include this so the test actually stops running at some point
We need to include this so the test actually stops running at some  point
Must be seeded the same both times run_sgd is called
Synthesize dataset with a linear decision boundary
We include a cost other than SumOfParams so that data is actually  queried from the training set, and the expected number of updates  are applied.
Implemented only so that DummyCost would work
We include a cost other than SumOfParams so that data is actually  queried from the training set, and the expected number of updates  are applied.
without monitoring datasets
with uneven training datasets
with even monitoring datasets
with uneven monitoring datasets
Make the test fail if algorithm does not  respect get_input_space  Multiplying by P ensures the shape as well  as ndim is correct
including a monitoring datasets lets us test that  the monitor works with supervised data
We need to include this so the test actually stops running at some point
Must be seeded the same both times run_bgd is called
Synthesize dataset with a linear decision boundary
Synthesize dataset with a linear decision boundary
Make sure all the right methods were used to compute the updates
Make sure the load_batch callbacks were called the right amount of times
Make sure the gradient updates were run the right amount of times
Methods of `self.cost` need args to be passed in a format compatible  with their data_specs
obj_prereqs has to be a list of function f called with f(*data),  where data is a data tuple coming from the iterator.  this function enables capturing "mapping" and "f", while  enabling the "*data" syntax
msg='Conv2d{%s}'%self._message)
dot(x, A)
fixed parameters
re-arrange these random-images so that the channel data is the minor  dimension: (batch rows cols channels)
symbolic stuff
dot(x, A)  = dot(A.T, x.T).T
dot(x, A.T)  = dot(A, x.T).T
dot(A, x)  = dot(x.T, A.T).T
dot (A.T, x)  = dot(x.T, A).T
OVER-RIDE THIS
split the output rows into pieces  multiply each piece by one transform  sum the results
multiply the input by each transform  join the resuls
t.print_status()
start by computing output dimensions, size, etc
construct indices and index pointers for sparse matrix, which, when multiplied  with input images will generate a stack of image patches
compute output of linear classifier
kern is of shape: nkern x ksize*number_of_input_features  output is thus of shape: bsize*outshp x nkern
start by computing output dimensions, size, etc
construct indices and index pointers for sparse matrix, which, when multiplied  with input images will generate a stack of image patches
compute output of linear classifier
kern is of shape: nkern x ksize*number_of_input_features  output is thus of shape: bsize*outshp x nkern
imgshp contains either 2 entries (height,width) or 3 (nfeatures,h,w)  in the first case, default nfeatures to 1
construct indices and index pointers for sparse matrix, which, when multiplied  with input images will generate a stack of image patches
STALE
inshp contains either 2 entries (height,width) or 3 (nfeatures,h,w)  in the first case, default nfeatures to 1
range of output units over which to iterate
coordinates of image in "fulloutshp" coordinates
sparse matrix specifics...
loop over output image pixels
incremented every time we write something to the sparse matrix  this is used to track the ordering of filter tap coefficient in sparse  column ordering
... ITERATE OVER INPUT UNITS IN RECEPTIVE FIELD
verify if we are still within image boundaries. Equivalent to  zero-padding of the input image
convert to "valid" input space coords  used to determine column index to write to in sparse mat  determine raster-index of input pixel...
convert oy,ox values to output space coordinates
convert to row index of sparse matrix
total number of active taps (used for kmap)
BUG ALERT: scipy0.6 has bug where data and indices are written in reverse column  ordering. Explicit call to ensure_sorted_indices removes this problem
inshp contains either 2 entries (height,width) or 3 (nfeatures,h,w)  in the first case, default nfeatures to 1
construct indices and index pointers for sparse matrix
this corresponds to grad when doing convolution
Skip test if cuda_ndarray is not available.
this needs to work for older versions of theano too
not really a test, but important code to support  Currently exposes error, by e.g.:   CUDA_LAUNCH_BLOCKING=1   THEANO_FLAGS=device=gpu,mode=DEBUG_MODE   nosetests -sd test_localdot.py:TestLocalDotLargeGray.run_autoencoder
import here to fail right away
Use grad_not_implemented for versions of theano that support it
this could be implemented, but GPU case doesn't do it
frows already assigned  fcols already assigned
test only the left so that the right can be a shared variable,  and then TestGpuFilterActs can use a gpu-allocated shared var  instead.
assume default
these are the colors of the activation shells
x coordinate of center of leftmost pixel  x coordinate of center of rightmost pixel
This would be a pretty weird feature to want but I put  the interface here for compatibility with the L2 norm  constraint class.
Column 0 tests the case where a column has zero norm  Column 1 tests the case where a column is smaller than the limit  Column 2 tests the case where a column is on the limit  Column 3 tests the case where a column is too big
Column 0 tests the case where an element has zero norm  Column 1 tests the case where an element is smaller than the limit  Column 2 tests the case where an element is on the limit  Column 3 tests the case where an element is too big
and go
we define here:
load test data
process this data
This needs to come after the prepro so that it doesn't  change the pixel means computed above for toronto_prepro
assumes no preprocessing. need to make preprocessors mark the  new ranges
if the scale is set based on the data, display X oring the  scale determined by orig  assumes no preprocessing. need to make preprocessors mark  the new ranges
Actual image shape may change, e.g. after being preprocessed by  datasets.preprocessing.Downsample
Maps azimuth labels (ints) to their actual values, in degrees.
Maps a label type to its index within a label vector.
Number of labels, for each label type.
Formats data as rows in a matrix, for DenseDesignMatrix
's' is the stereo channel: 0 (left) or 1 (right)
inserts a singleton dimension where the 's' dimesion will be
Local cache seems to be deactivated
Check if a local directory for data has been defined. Otherwise,  do not locally copy the data
Make sure the file to cache exists and really is a file
Determine local path to which the file is to be cached
If the file does not exist locally, consider creating it
Check that there is enough space to cache the file
There is enough space; make a local copy of the file
Obtain a readlock on the downloaded file before releasing the  writelock. This is to prevent having a moment where there is no  lock on this file which could give the impression that it is  unused and therefore safe to delete.
Instead of only looking if there's enough space, we ensure we do not  go over max disk usage level to avoid filling the disk/partition
Register function to release the readlock at the end of the script
Make sure the lock still exists before deleting it
Locally cache the files before reading them
hdf5 handle has no ndim
extract image center, which remains dense
flatten and write to dense output matrix
start by downsampling the periphery of the images  downsample the ring with top-left corner (idx,idx) of width rd  results are written in output[start_idx:]
now restore image center in uncompressed image
now undo downsampling along the periphery  downsample the ring with top-left corner (idx,idx) of width rd  results are written in img[idx:idx+rd, idx:idx+rd]
broadcast along the width and height of the block
determine output shape
perform retina encoding on each channel separately
perform retina encoding on each channel separately
Region 2 -- right inner and left outer, excluding center nut
This will check that dtype is a legitimate dtype string.
Maps column indices of self.y to the label type it contains.  Names taken from http://www.cs.nyu.edu/~ylclab/data/norb-v1.0/
Big NORB has additional label types
Maps label type names to the corresponding column indices of self.y
The size of one side of the image
Needed for pickling / unpickling.  These are set during pickling, by __getstate__()
Get topo view from view converter.
inserts a singleton dimension where the 's' dimesion will be
We don't want to pickle the memmaps; they're already on disk.
Replace memmaps with their constructor arguments
We never want to set mode to w+, even if memmap.mode  is w+. Otherwise we'll overwrite the memmap's contents  when we open it.
This prevents self.X and self.y from being accidentally written to  after the save, thus unexpectedly changing the saved file. If the  user really wants to, they can make the memmaps writeable again  by calling setflags(write=True) on the memmaps.
Converts filename from relative to absolute path.
Reorders the shape vector to match the new axis ordering.
Reads the first two components of the version number as a floating point  number.
Locally cache the files before reading them
Copy ensures that memory is not aliased.
Defaults for iterators
data is organized as data_specs  keep self.data_specs, and convert data
Load data from .npy file
some sanity checkes
self.X_topo_space stores a "default" topological space that  will be used only when self.iterator is called without a  data_specs, and with "topo=True", which is deprecated.
convert to boolean selection
if hdf5 file does not exist make them
The table size for y is being set to [sizes[which_set], 1] since y  contains the labels. If you are using the old one-hot scheme then  this needs to be set to 10.
For consistency between experiments better to make new random stream
The original splits
.mat labels for SVHN are in range [1,10]  So subtract 1 to map labels to range [0,9]  This is consistent with range for MNIST dataset labels
load data
rescale or center if permitted
For consistency between experiments better to make new random stream
The original splits
'packed matrix': 0x1E3D4C52,
what is the rank of the tensor?
Apply transformation on raw_batch, and format it  in the requested Space
If the space is preserved, then raw_batch is already provided  in the requested space
Only one source, return_tuple is False
Apply the transformer only on the first element
0 :  aquatic_mammals  1 :  fish  2 :  flowers  5 :  household_electrical_devices  8 :  large_carnivores  9 :  large_man-made_outdoor_things  10 :  large_natural_outdoor_scenes  13 :  non-insect_invertebrates  14 :  people  15 :  reptiles  16 :  small_mammals  17 :  trees  18 :  vehicles_1  19 :  vehicles_2
limit examples returned to `example_range`
get images and cast to float32  create dense design matrix from topological view
get labels
create view converting for retrieving topological view
init the super class
perform mean and std in float64 to avoid losing  too much numerical precision
Don't import tables initially, since it might not be available  everywhere.
Get the topo_space (usually Conv2DSpace) from the  view_converter
self.X_topo_space stores a "default" topological space that  will be used only when self.iterator is called without a  data_specs, and with "topo=True", which is deprecated.
Defaults for iterators
If there is a view_converter, we have to use it to convert  the stored data for "features" into one that the iterator  can return.
Get the topo_space from the view_converter
self.X_topo_space stores a "default" topological space that  will be used only when self.iterator is called without a  data_specs, and with "topo=True", which is deprecated.
self.X_topo_space stores a "default" topological space that  will be used only when self.iterator is called without a  data_specs, and with "topo=True", which is deprecated.
Update self.data_specs with the updated dimension of self.y
Update self.X_topo_space, which stores the "default"  topological space, which is the topological output space  of the view_converter
For 1D ndarray of int labels, override the atom to integer
Maps the (0, 1, 'c') of self.shape to ('c', 0, 1)
weights view is always for display
Patch old pickle files that don't have the axes attribute.
Same for topo_space
Build a column array for y
get images and cast to floatX
get labels
convert to float for performing regression
retrieve only subset of data
strip off stupid header line they added to only the test set  strip off empty final line
verify  of examples
strip off endlines, separate entries
split data into features and targets
If there is a view_converter, we have to use it to convert  the stored data for "features" into one that the iterator  can return.
the ind of minibatch goes beyond the boundary
This needs to come after the prepro so that it doesn't change  the pixel means computed above
assumes no preprocessing. need to make preprocessors mark the new  ranges
patch old pkl files
if the scale is set based on the data, display X oring the scale  determined  by orig  assumes no preprocessing. need to make preprocessors mark the new  ranges
patch old pkl files
Subclasses that support topological view must implement this to  specify how their data is formatted.
Path substitution done here in order to make the lower-level  mnist_ubyte.py as stand-alone as possible (for reuse in, e.g.,  the Deep Learning Tutorials, or in another package).
Locally cache the files before reading them
test that train/valid/test sets load (when standardize=True).
Force double precision to compute mean and std, otherwise the test  fails because of precision.
instantiate Train object
cleanup
instantiate Train object
cleanup
instantiate Train object
cleanup
instantiate Train object
cleanup
Test loading of transfer data
Test loading of transfer data
Test that the FoveatedNORB class can be instantiated
Get a topological view as two "(b, 0, 1, c)" tensors
Use stop parameter for SmallNORB; otherwise the buildbot uses close  to 10G of RAM.
category
instance
elevation
azimuth
lighting
horizontal shift
vertical shift
lumination change
contrast change
Use of zip rather than safe_zip intentional;  norb.label_to_value_funcs will be shorter than  label_to_value_maps if norb is small NORB.
Load and create ddm from cifar100
without y:
Preparation
Testing mapback()
Testing mapback_for_viewer()
Testing adjust_for_viewer()
Testing has_targets()
Currently this test file does nothing but make sure the module can be  imported.
Test features
the settings of subtract_mean and use_norm are not relevant to  the test  std_bias = 0.0 is the only value for which there should be a risk  of failure occurring
the setting of subtract_mean is not relevant to the test  the test only applies when std_bias = 0.0 and use_std = False
Check some basics of transformation matrix
Check if preprocessed data matrix is white
Keep 3 components
Drop 2 components: result should be similar
sut is an abbreviation for System Under Test
testing whether the eigenvalues are in decreasing order
testing whether the eigenvalues are all ones
Learn PCA preprocessing and apply it to the training set  Now apply the same transformation to the test set
fix lables
fix labels
Return the batch in the original storage space
Uncompress and go through the original view converter
assert below fails when 'whiten' is True or sometimes on test  or validation set when the preprocessor was fit on train set
Analogous to DenseDesignMatrix.design_loc. If not None, the  matrices P_ and inv_P_ will be saved together in <save_path>  (or <save_path>.npz, if the suffix is omitted).
Removes the matrices from the dictionary to be pickled.
Patch old pickle files
puts matrices' items into state, overriding any colliding keys in  state.
Center data
For each pixel, remove mean of 9x9 neighborhood
Scale down norm of 9x9 patch if norm is bigger than 1
put things in pylearn2's DenseDesignMatrix format
this is uint8
Load the class names
The data is stored as uint8  If we leave it as uint8, it will cause the CAE to silently fail  since theano will treat derivatives wrt X as 0
this is uint8 but labels range should be corrected
Load the class names
this is uint8 but labels range should be corrected
this file is stored in HDF format, which transposes everything
x must be formatted as batch index, channel, topo dim 0, topo dim 1  for use with conv2d, so check what the current input space format is
Format the output based on the output space
dot(x, A.T)
Format the output based on the input space
Format the output based on the input space
local_rf_stride specified, make local_rfs on a grid
x must be formatted as channel, topo dim 0, topo dim 1, batch_index  for use with FilterActs
Format the output based on the output space
Format the output based on the input space
dot(x, sq(A).T)
image_shape=self._img_shape,  filter_shape=self._filters_shape,  kernel_stride=self._kernel_stride,  pad = self.pad  )
Format the output based on the input space
Use "self" to refer to layer from now on, so we can pretend we're  just running in the set_input_space method of the layer
Make sure cuda is available
Store the input space
conv_op has to be changed
x must be formatted as batch index, channel, topo dim 0, topo dim 1  for use with conv2d, so check what the current input space format is
Format the output based on the output space
Apply f on the `another_axes`-shaped image  Apply f_def on self.image (b,c,0,1)  transpose output to def
We set up each dataset with a different batch size  check here that we're getting the right one  Each dataset has different content, make sure we  get the right one
Use functools.wraps so that wrapped.func_name matches  fn.func_name. Otherwise nosetests won't recognize the  returned function as a test.
pyflakes gets mad if you set scipy to None here
make sure the decorator gets rid of DEBUG_MODE
make sure the decorator restores DEBUG_MODE when it's done
sum() will call theano.add on the symbolic variables
This cost does not need any data
This cost does not need any data
Model.modify_updates is used by the training algorithm to  enforce constraints after each step of learning. Here we  make sure the constraints are enforced from the start.
First check if the model is already beyond the stop criteria of  training, if so, just return directly.
We catch an exception here instead of relying on return  values for backward compatibility. Lots of extensions  exist that don't return anything, currently.
just keeping these for debugging/examination, not needed
accept less than 1% error
load data to set visible biases to ML solution
Estimate can be off when using the wrong base-rate model.
even_sequences iterator does not support specifying a fixed number  of minibatches.
Note: if the monitor starts supporting variable batch sizes,  take this out. Maybe move it to a new test that the iterator's  uneven property is set accurately  assert X.shape[0] == mon_batch_size
end if  end __call__
check that handling all these datasets did not  result in them getting serialized
mock save
make the dataset part of the model, so it will get  serialized
Unpack the data specs into two tuples
Add mask source for targets  ('targets') -> ('targets', 'targets_mask')
Initialize the parameters
W is the input-to-hidden matrix
U is the hidden-to-hidden transition matrix
b is the bias
Later we will add a noise function
Only update the state for non-masked data, otherwise  just carry on the previous state until the end
Initialize the parameters
W is the input-to-hidden matrix
b is the bias
Only update the state for non-masked data, otherwise  just carry on the previous state until the end
Initialize the parameters
W is the input-to-hidden matrix
b is the bias
These can't be wrapped
Newly added part  end of newly added part
Set a number which is not equal to batch_size for comfort debugging
Data here is [time, batch, data]
Set a number which is not equal to batch_size for comfort debugging  creating masks that have different zero values.  Non-zeros is 15 out of 20
This effectively makes this class a subclass  of the class instance that was passed in the constructor
The MLP defines f(x) = (x W)^2, with df/dW = 2 W x^2
df/dW = df/db = 20 for W = 10, x = 1, so the norm is 20 * sqrt(2)  and the gradients should be clipped to 20 / sqrt(2)
check that all of them are equal
Load data into self._data (defined in PennTreebank)
This should be fixed to allow iteration with default data_specs  i.e. add a mask automatically maybe?
The amount of braces that must be closed at the end
The amount of braces that must be closed at the end
bench.py should always be run in gpu mode so we should not need a gpu_from_host here
bench.py should always be run in gpu mode so we should not need a gpu_from_host here
The amount of braces that must be closed at the end
The amount of braces that must be closed at the end
Note : `get_scalar_constant_value` returns a ndarray not a  int
The amount of braces that must be closed at the end
Not strictly necessary I don't think
The amount of braces that must be closed at the end
The amount of braces that must be closed at the end
Computing whether the rows and columns are broadcastable requires doing  arithmetic on quantities that are known only at runtime, like the specific  shape of the image and kernel
nb mul and add by output pixed  for all outputs imagesn_stack==self.imshp[0]
The amount of braces that must be closed at the end
Must delay import to avoid circular import problem
Computing whether the rows and columns are broadcastable requires doing  arithmetic on quantities that are known only at runtime, like the specific  shape of the image and kernel
The amount of braces that must be closed at the end
We don't know anything about filter_rows or filter_cols at compile  time, so we assume they're not broadcastable.
The partial sum is just a way to specify how to compute  stuff inside the op.  It don't change the number of flops.  nb mul and add by output pixed  for all outputs imagesn_stack==self.imshp[0]
For some reason, the function called in the C code (_weightActs)  is not defined in cudaconv2.cuh, so I defined it in weight_acts.cuh
The amount of braces that must be closed at the end
gpu op
theano graph
gpu op
theano graph
Tests that running FilterActs with a non-square  kernel is an error
Tests that running FilterActs with a  of filters per  group that is not 16 is an error
Don't call verify_grad. There was problem with  the test and we already assert that 2 version  are equals.  Also, it will be slower to verify  like that then the comparison.
Proper random projection, like verify_grad does.
Proper random projection, like verify_grad does.
XXX: use verify_grad
put the inputs + outputs in shared variables so we don't pay GPU  transfer during test
put the inputs + outputs in shared variables so we don't pay GPU  transfer during test
Make sure the cuda_convnet library is compiled and up-to-date
This is needed as otherwise DebugMode will consider that  BaseActs.make_thunk do something else then the default code, and  would duplicate verification.
If already compiled, OK
If there was an error, do not try again
Else, we need CUDA
Try to actually compile
Initialize variables in convnet_available
Check if the compilation has already been done by another process  while we were waiting for the lock
Raise an error if libcuda_convnet_so is still not available
Add cuda_convnet to the list of places that are hard-coded into  compiled modules' runtime library search list.
Cast is for compatibility with default bit depth of T.iscalar  (wtf, theano?)
stack the k-th block from each layer
_folds contains a StackedBlocks instance for each CV fold
setup monitoring datasets
extensions
Train extensions
TrainCV extensions
correct proportion to correspond to a subset of the training set
correct proportion to correspond to a subset of the training set
data_subsets is an OrderedDict to maintain label order
clean up
one label
multiple labels
improper label (iterator only returns 'train' and 'test' subsets)
bogus label (not in approved list)
train the first hidden layer (unsupervised)  (test for TrainCV)
train the second hidden layer (unsupervised)  (test for TransformerDatasetCV)
train the third hidden layer (unsupervised)  (test for StackedBlocksCV)
train the full model (supervised)  (test for PretrainedLayerCV)
clean up
--test: build the docs with warnings=errors to test them (exclusive)
Make sure the appropriate 'theano' directory is in the PYTHONPATH
If your extensions are in another directory, add it here. If the directory  is relative to the documentation root, use os.path.abspath to make it  absolute, like shown here.
Add any Sphinx extension module names here, as strings. They can be extensions  coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
Add any paths that contain templates here, relative to this directory.
The suffix of source filenames.
The master toctree document.
General substitutions.
The default replacements for |version| and |release|, also used in various  other places throughout the built documents.  The short X.Y version.  The full version, including alpha/beta/rc tags.
There are two options for replacing |today|: either, you set today to some  non-false value, then it is used:  Else, today_fmt is used as the format for a strftime call.
List of documents that shouldn't be included in the build.
List of directories, relative to source directories, that shouldn't be searched  for source files.
The reST default role (used for this markup: `text`) to use for all documents.
If true, '()' will be appended to :func: etc. cross-reference text.
If true, the current module name will be prepended to all description  unit titles (such as .. function::).
If true, sectionauthor and moduleauthor directives will be shown in the  output. They are ignored by default.
The name of the Pygments (syntax highlighting) style to use.
The style sheet to use for HTML and HTML Help pages. A file of that name  must exist either in Sphinx' static/ path, or in one of the custom paths  given in html_static_path.
The name for this set of Sphinx documents.  If None, it defaults to  "<project> v<release> documentation".
A shorter title for the navigation bar.  Default is the same as html_title.
The name of an image file (within the static path) to place at the top of  the sidebar.
The name of an image file (within the static path) to use as favicon of the  docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32  pixels large.
If not '', a 'Last updated on:' timestamp is inserted at every page bottom,  using the given strftime format.
If true, SmartyPants will be used to convert quotes and dashes to  typographically correct entities.
Custom sidebar templates, maps document names to template names.
Additional templates that should be rendered to pages, maps page names to  template names.
If false, no module index is generated.
If false, no index is generated.
If true, the index is split into individual pages for each letter.
If true, the reST sources are included in the HTML build as _sources/<name>.
If true, an OpenSearch description file will be output, and all pages will  contain a <link> tag referring to it.  The value of this option must be the  base URL from which the finished HTML is served.
If nonempty, this is the file name suffix for HTML files (e.g. ".xhtml").
Output file base name for HTML help builder.
The paper size ('letter' or 'a4').
The font size ('10pt', '11pt' or '12pt').
Grouping the document tree into LaTeX files. List of tuples  (source start file, target name, title, author, document class [howto/manual]).
The name of an image file (relative to this directory) to place at the top of  the title page.
For "manual" documents, if this is true, then toplevel headings are parts,  not chapters.
Additional stuff for the LaTeX preamble.
Documents to append as an appendix to all manuals.
If false, no module index is generated.
Import python libs
Import third party libs  pylint: disable=import-error  pylint: enable=import-error
Import salt libs
Define the module's virtual name
repo.branches() returns a list of 3-tuples consisting of  (branch name, rev , nodeid)  Example: [('default', 4, '7c96229269fa')]
mountpoint not specified
Only init if the directory is empty.
Somehow this path is a directory. Should never happen  unless some wiseguy manually creates a directory at this  path, but just in case, handle it.
remote was non-string, try again
remote was non-string, try again
Path exists and is a file, remove it and retry
Path exists and is a file, remove it and retry
Branch or tag not found in repo, try the next
Shouldn't get here, but if we do, this prevents a TypeError
Don't add files outside the hgfs_root
Cannot have empty dirs in hg
Import python libs
Auth support (auth params can be global or per-remote, too)
Import salt libs
Define the module's virtual name
Initialization of the GitFS object did not fail, so we know we have  valid configuration syntax and that a valid provider was detected.
Cannot have empty dirs in git
Import python libs
Import salt libs
An invalid index was passed
An invalid index option was passed
Hash file won't exist if no files have yet been served up
data to send on event
generate the new map
compare the maps, set changed to the return value
if the file doesn't exist, we can't get a hash
set the hash_type as it is determined by config-- so mechanism won't change that
Delete the file since its incomplete (either corrupted or incomplete)
Catch msgpack error in salt-ssh
Shouldn't get here, but if we do, this prevents a TypeError
Import python libs
Import salt libs
Import 3rd-party libs
The dest is not here, sleep for a bit, if the dest is not here yet  kill the lockfile and start the write
wait for a filelist lock for max 15min
calculate filelist age is possible
if filelist does not exists yet, mark it as expired
Set the w_lock and go
skip dangling symlinks
check if the mtimes are the same
we made it, that means we have no changes
don't attempt to find URL query arguements in the path
Invalid option, skip it
Import python libs
Import salt libs
Define the module's virtual name
Hash file won't exist if no files have yet been served up
if the file doesn't exist, we can't get a hash
set the hash_type as it is determined by config-- so mechanism won't change that
There should be no emptydirs
Import python libs
Import salt libs
Import 3rd-party libs  pylint: disable=import-error,no-name-in-module,redefined-builtin  pylint: enable=import-error,no-name-in-module,redefined-builtin
update and grab the envs from the metadata keys
load the file from S3 if it's not in the cache or it's old
jit load the file from S3 if it's not in the cache or it's old
get the saltenv/path file from the cache
grab all the dirs from the buckets cache file  trim env and trailing slash  remove empty string left by the base env dir in single bucket mode
check mtime of the buckets files cache
bucket files cache expired or does not exist
Or is that making too many assumptions?
make sure bucket and saltenv directories exist
helper s3 query function
Single environment per bucket
s3 query returned nothing
grab only the files/dirs
Multiple environments per buckets
s3 query returned nothing
pull out the environment dirs (e.g. the root dirs)
pull out the files for the environment  grab only files/dirs that match this saltenv
write the metadata to disk
filter out the dirs
Get rid of quotes surrounding md5
check the local cache...
hashes match we have a cache hit
... or get the file from S3
Import python libs
Import third party libs  pylint: disable=import-error  pylint: enable=import-error
Import salt libs
Define the module's virtual name
mountpoint not specified
Somehow this path is a directory. Should never happen  unless some wiseguy manually creates a directory at this  path, but just in case, handle it.
remote was non-string, try again
remote was non-string, try again
There were problems getting the revision ID
Add base as the env for trunk
Environment not found, try the next repo
If the file doesn't exist, we can't get a hash
Set the hash_type as it is determined by config
Environment not found, try the next repo
svnfs root (global or per-remote) does not exist in env
Convert all compiled sets to lists  Shouldn't get here, but if we do, this prevents a TypeError
Import python libs
fcntl is not available on windows
Import salt libs
An invalid index was passed
An invalid index option was passed
Hash file won't exist if no files have yet been served up
Pull in any files that have changed
Write out any new files to disk
Make sure the directory exists first
Write out the file
if the file doesn't exist, we can't get a hash
set the hash_type as it is determined by config  -- so mechanism won't change that
-*- coding: utf-8 -*-
check if the argument is pointing to a file on disk
check if the argument is pointing to a file on disk
check if the argument is pointing to a file on disk
Setup file logging!
Import python libs
All salt related deprecation warnings should be shown once each!
While we are supporting Python2.6, hide nested with-statements warnings
Filter the backports package UserWarning about being re-imported
Import salt libs  We import log ASAP because we NEED to make sure that any logger instance salt  instantiates is using salt.log.setup.SaltLoggingClass
the try block below bypasses an issue at build time so that modules don't  cause the build to fail
Let's instantiate log using salt.log.setup.logging.getLogger() so pylint  leaves us alone and stops complaining about an un-used import
Logfile is not using Syslog, verify  Clear out syndics from cachedir
Late import so logging works correctly
Add a udp port check here
Logfile is not using Syslog, verify
Bail out if we find a process running and it matches out pidfile
Proxies get their ID from the command line.  This may need to change in  the future.
Logfile is not using Syslog, verify
Logfile is not using Syslog, verify
Import python libs
Don't die on missing transport libs since only one transport is required
Import 3rd-party libs
Custom exceptions
Default to ZeroMQ for now
Handle this here so other deeper code which might  be imported as part of the salt api doesn't do  a  nasty sys.exit() and tick off our developer users
Don't require msgpack with local
close raet channel here
spin up and fork minion here  wait here until '/var/run/salt/minion/alpha_caller.manor.uxd' exists
Import python libs
Import salt libs
Import 3rd-party libs  pylint: disable=import-error,no-name-in-module,redefined-builtin  pylint: enable=import-error,no-name-in-module,redefined-builtin
wait the specified time before decide a job is actually done
check if the tracker contains the iterator
add all minions that belong to this iterator and  that have not responded to parts{} with an empty response
When failhard is passed, we need to return all data to include  the retcode to use in salt/cli/salt.py later. See issue 24996.
Import Python libs
Import Salt libs
Logfile is not using Syslog, verify
-*- coding: utf-8 -*-
Setup file logging!
-*- coding: utf-8 -*-
Import Python libs
Import Salt libs
Import python libs
Import salt libs
Setup file logging!
Import python libs
Import 3rd-party libs
Setup file logging!
We don't need to bail on config file permission errors  if the CLI  process is run with the -a flag
NOTE: Return code is set here based on if all minions  returned 'ok' with a retcode of 0.  This is the final point before the 'salt' cmd returns,  which is why we set the retcode here.
We will print errors to the console further down the stack  Printing the output is already taken care of in run() itself
Import Python libs
Import salt libs
Import 3rd-party libs
Use os.open() to obtain filehandle so that we can force an  exception if the file already exists. Concept found here:  http://stackoverflow.com/a/10979569
Write the lock file  Lock successfully acquired  Transfer control back to the code inside the with block  Exit the loop
Import python libs
Import salt libs
-*- coding: utf-8 -*-
Import python libs
Import third party libs  pylint: disable=import-error,no-name-in-module  pylint: enable=import-error,no-name-in-module
Import salt libs
Import python libs
Import third party libs
Import salt libs
checks for relative '..' paths
there is no file under current path  pylint: enable=cell-var-from-loop
there is no template file within searchpaths
keeps quotes around strings
let default output
Raw string formatter required here because this is a repr  function.
doc1.sls
doc2.sls
Import Python Libs
Import Salt Libs
proc files may be removed at any time during this process by  the minion process that is executing the JID in question, so  we must ignore ENOENT during this process
Proc file is empty, remove
Invalid serial object
The process is no longer running, clear out the file and  continue
Import python libs
Only yield a value if the slice was not an empty string,  because if it is then we've reached the end. This keeps  us from yielding an extra blank value at the end.
Import salt libs
Public attributes
Support iteration
Support with statements
Import python libs
Import 3rd-party libs
pylint: disable=E0611,minimum-python-version
pylint: disable=import-error,no-name-in-module  pylint: enable=import-error,no-name-in-module
Setting a new item creates a new link which goes at the end of the linked  list, and the inherited dictionary is updated with the new key/value pair.
Deleting an existing item uses self.__map to find the link which is  then removed by updating the links in the predecessor and successor nodes.
-*- coding: utf-8 -*-  Import Python libs
Import salt libs
Do the same as the parent but also persist
test code for the CacheCli
Import Python libs
Import 3rd-party libs
Import Salt libs
Try grabbing the credentials from the EC2 instance IAM metadata if available
On error the S3 API response should contain error message
This can be used to return a binary object wholesale
Import python libs
Import third party libs
Optional per-remote params that can only be used on a per-remote basis, and  thus do not have defaults in salt/config.py.
Auth support (auth params can be global or per-remote, too)
pylint: disable=import-error
on a production system (!),  but cffi might be absent as well!  Therefore just a generic Exception class.
Minimum versions for backend providers  dulwich.__version__ is a versioninfotuple so we can compare tuples  instead of using distutils.version.LooseVersion
Separate the per-remote-only (non-global) parameters
Winrepo doesn't support the 'root' option, but it still must be part  of the GitProvider object because other code depends on it. Add it as  an empty string.
Set all repo config params as attributes
For providers which do not use a mountpoint, assume the  filesystem is mounted at the root of the fileserver.
No lock file present
Somehow this path is a directory. Should never happen  unless some wiseguy manually creates a directory at this  path, but just in case, handle it.
Run provider-specific fetch code
Write the lock file and close the filehandle
Lock file is empty, set pid to 0 so it evaluates as  False.
Should only happen the first time we are checking out, since  we fetch first before ever checking anything out.
Re-raise with a different strerror containing a  more meaningful error message for the calling  function.
Work around GitPython bug affecting removal of tags  https://github.com/gitpython-developers/GitPython/issues/260
Repo cachedir is empty, initialize a new repo there
This exception occurs when two processes are trying to write  to the git config at once, go ahead and pass over it since  this is the only write. This should place a lock down.
Not found, return empty objects
Branch/tag/SHA not found
File not found or repo_path points to a directory
Branch or tag not matched, check if 'tgt_env' is a commit
Shouldn't happen, but just in case a future pygit2 API change  breaks things, avoid a traceback and log an error.
Checkout the local branch corresponding to the  remote ref.
Re-raise with a different strerror containing a  more meaningful error message for the calling  function.
Get commit id for the remote ref  No local branch for this remote, so create one and point  it at the commit id of the remote ref
Return the relative root, if present
If no AttributeError raised, this is an annotated tag
Shouldn't happen, but could if a future pygit2  API change breaks things.
Return the relative root, if present
Rename heads to match the remote ref names from  pygit2.Repository.listall_references()
Local head, ignore it
Repo cachedir is empty, initialize a new repo there
This exception occurs when two processes are trying to write  to the git config at once, go ahead and pass over it since  this is the only write. This should place a lock down.
Entry is a submodule, skip it
pygit2.Remote.fetch() returns a dict in pygit2 < 0.21.0
pygit2.Remote.fetch() returns a class instance in  pygit2 >= 0.21.0
Entry is a submodule, skip it
Not found, return empty objects
This might need to be changed to account for a root that  spans more than one directory
Branch/tag/SHA not found in repo
Path is a symlink. The blob data corresponding to this  path's object ID will be the target of the symlink. Follow  the symlink and set path to the location indicated  in the blob data.
Branch or tag not matched, check if 'tgt_env' is a commit  Not a valid commit, likely not a commit SHA
If the URL is an absolute file path, there is no authentication.
Auth information not configured for this remote
Assume scp-like SSH syntax (user@domain.tld:relative/path.git)
These transports do not use auth
No auth params were passed, assuming this is unauthenticated  http(s).
Entry is a submodule, skip it
Empty repository
Update local refs  Prune stale refs
Entry is a submodule, skip it
Branch/tag/SHA not found
Branch/tag/SHA not found in repo
Branch or tag not matched, check if 'tgt_env' is a commit. This is more  difficult with Dulwich because of its inability to deal with shortened  SHA-1 hashes.  Not hexidecimal, likely just a non-matching environment
No matching SHA
No matching sha_commit object was created. Unable to find SHA.
Dulwich will throw an error if 'ssh://' is used, so make the URL  use git+ssh:// as dulwich expects
Repo cachedir is empty, initialize a new repo there
Walk down the tree to get to the file  Directory not found, or tree passed into function is not a Tree  object. Either way, desired path does not exist.
Specific remote URL/pattern was passed, ensure that the URL  matches or else skip this one  remote was non-string, try again
Specific remote URL/pattern was passed, ensure that the URL  matches or else skip this one  remote was non-string, try again
data for the fileserver event
pylint: disable=no-member
Path exists and is a file, remove it and retry
Path exists and is a file, remove it and retry
No matching file was found in tgt_env. Return a dict with empty paths  so the calling function knows the file could not be found.
Cannot have empty dirs in git
Dulwich has no function to check out a branch/tag, so this will be  limited to GitPython and Pygit2 for the forseeable future.
Dulwich has no function to check out a branch/tag, so this will be  limited to GitPython and Pygit2 for the forseeable future.
Set language
Set keyboard
Set selinux
Get package data together
Import python libs
Let's import pwd and catch the ImportError. We'll raise it if this is not  Windows  We can't use salt.utils.is_windows() from the import a little down  because that will cause issues under windows at install time.
Import salt cloud libs
Import third party libs
Get logging started
Specified renderer was not found
The user provided an absolute path to the deploy script, let's use it
The user provided an absolute path to the deploy script, although no  extension was provided. Let's use it anyway.
No deploy script was found, return an empty string
Mandate that keys are at least 2048 in size
Don't start with a copy of the default minion opts; they're not always  what we need. Some default options are Null, let's set a reasonable default
Get ANY defined grains settings, merging data, in the following order  1. VM config  2. Profile config  3. Global configuration
Let's get a copy of the salt master default options  Some default options are Null, let's set a reasonable default
Get ANY defined master setting, merging data, in the following order  1. VM config  2. Profile config  3. Global configuration
If we haven't generated any keys yet, do so now.
forward any info about possible ssh gateway to deploy script  as some providers need also a 'gateway' configuration
get rid of None's or empty names  Keep a copy of the usernames the user might have provided
Assign test ports because if a gateway is defined  we first want to test the gateway before the host.
Stop any remaining reads/writes on the socket  Close it!
Let the user know that his gateway is good!
minion_pub, minion_pem
Let's not just fail regarding this change, specially  since we can handle it
Let's not just fail regarding this change, specially  since we can handle it
Copy pre-seed minion keys
https://github.com/zeromq/pyzmq/issues/173issuecomment-4037083  Assertion failed: get_load () == 0 (poller_base.cpp:32)
Signal an error
Don't add new hosts to the host key database  Set hosts key database path to /dev/null, i.e., non-existing  Don't re-use the SSH connection. Less failures.
Don't add new hosts to the host key database  Set hosts key database path to /dev/null, i.e., non-existing  Don't re-use the SSH connection. Less failures.
There should never be both a password and an ssh key passed in, so  tell SSH to skip password authentication  Make sure public key authentication is enabled  do only use the provided identity file  No Keyboard interaction!  Also, specify the location of the key file
Signal an error
Use double `-t` on the `ssh` command, it's necessary when `sudo` has  `requiretty` enforced.
Don't add new hosts to the host key database  Set hosts key database path to /dev/null, i.e., non-existing  Don't re-use the SSH connection. Less failures.
ipv6  ipv6 link local
10.0.0.0/24
192.168.0.0/16
172.16.0.0/12
in last case, assuming we got a script content
Get the path to the built-in deploy scripts directory
Compute the search path from the current loaded opts conf_file  value
Compute the search path using the install time defined  syspaths.CONF_DIR
Get a copy of any defined search paths, flagging them not to  create parent  We won't write the updated script to the built-in deploy  directory
Allow parent directories to be made
In case the user is not using defaults and the computed  'cloud.deploy.d' from conf_file and syspaths is not included, add  them
This handles duplicate entries, which are likely to appear
Check to see if any nodes in the cache are not in the new list
Convert non-breaking space to space  Convert en dash to dash
There's nothing else we can do, raise the exception
WE should raise something else here to be able to use this  as/from an API
Import python libs
Import 3rd-party libs
Because YAML loads empty strings as None, we return the original string  >>> import yaml  >>> yaml.load('') is None  True  >>> yaml.load('      ') is None  True
dicts must be wrapped in curly braces
we don't support this type
In case anything goes wrong...
only required in 2015.2
Import Python libs
Import third party libs  pylint: disable=import-error  pylint: disable=import-error  pylint: enable=import-error  pylint: enable=import-error
Import Python Libs
Import Salt Libs
Import Third Party Libs
Get Logging Started
Set default port and protocol if none are provided.
Then we are connecting directly to an ESXi server,  'host' points at that server, and esxi_host is a reference to the  ESXi instance we are manipulating
Start at the rootFolder if container starting point not specified
Create an object view
Create traversal spec to determine the path for collection
Create property spec to determine properties to be retrieved
Create object spec to navigate content
Create a filter spec and specify object, property spec in it
Retrieve the contents
Destroy the object view
Get list of all managed object references with specified property
Get all the content
Import Python libs
the secret key must be bytes and not unicode to work   properly with hmac.new (see http://bugs.python.org/issue5285)
JSON failed to decode
JSON failed to decode, so just pass no credentials back
-*- coding: utf-8 -*-  Import python libs
Import third party libs
If value was all zeros, node.value would have been reduced to  an empty string. Change it to '0'.
Here we need to discard any duplicate entries based on key_node
Import python libs
Import salt libs
Import 3rd-party libs
Only return list form if a nodegroup was expanded. Otherwise return  the original string to conserve backwards compat
Target is an address?
Target is a network?
Nodegroups should already be expanded/resolved to other engines
If an unknown engine is called at any time, fail out
Add a closing ')' for each item left in unmatched
Take the auth list and get all the minion names inside it
'minions' here are all the names of minions matched by the target  if we take out all the allowed minions, and there are any left, then  the target includes minions that are not allowed by eauth  so we can give up here.
If the non-exact match gets more minions than the exact match  then pillar globbing or PCRE is being used, and we have a  problem  compound commands will come in a list so treat everything as a list
Allowed for all minions
Invalid argument
Invalid argument  whitelist args, kwargs
Import python libs
urlparse will split on valid filename chars such as '?' and '&'
Import python libs
pylint: disable=F0401,W0611  pylint: enable=F0401,W0611
Import salt libs
This can happen if two threads create a new Terminal instance  It's harmless that it was already removed, so ignore.  < Cleanup Running Instances
Terminal Size
Logging options
sys.stdXYZ streaming options
Let's avoid Zombies!!!
Internally Set Attributes >
status returned by os.waitpid  < Internally Set Attributes
Spawn our terminal >
Common Internal API >  PTY's always return \r\n as the line feeds  < Common Internal API
Context Manager Methods >
Wait for the process to terminate, to avoid zombies.  < Context Manager Methods
Platform Specific Methods >   Windows Methods >
ERROR_ACCESS_DENIED (winerror 5) is received when the  process already died.
< Windows Methods    Linux Methods >   Internal API >
Child
Set the terminal size
Do not allow child to inherit open file descriptors from  parent
Parent
Child.  Close parent FDs
Make STDOUT the controlling PTY >  Disconnect from controlling tty. Harmless if not already  connected  which exception, shouldn't we catch explicitly .. ?
Already disconnected. This happens if running inside cron
New session!
Good! We are disconnected from a controlling tty.
Duplicate Descriptors >  < Duplicate Descriptors
Parent. Close Child PTY's
Store FD Flags >  < Store FD Flags
Check for any incoming data >  < Check for any incoming data
Process STDERR >
Process STDOUT >
< Process STDOUT
Return a default value of 24x80  < Internal API
This is for Linux, which requires the blocking form  of waitpid to get status of a defunct process.  This is super-lame. The flag_eof_* would have been set  in recv(), so this should be safe.
I think there are kernel timing issues that sometimes cause  this to happen. I think isalive() reports True, but the  process is dead to the kernel.  Make one last attempt to see if the kernel is up to date.
We didn't get to successfully create a child process.
In case the child hasn't been waited on, check if it's done.  Child is still running, keep us alive until we can wait on it.  < Cleanup!!!   < Platform Specific Methods
Import 3rd-party libs
Import salt libs
Set up logger
this is a passthrough object, continue
order criteria so that least expensive checks are done first
-*- coding: utf-8 -*-
Import python libs
We need __setstate__ and __getstate__ to avoid pickling errors since  'self.rend' (from salt.state.Compiler) contains a function reference  which is not picklable.  These methods are only used when pickling so will not be used on  non-Windows platforms.
for 20841, inject the sls name here since verify_high()  assumes it exists in case there are any errors
class-wide cache of clients
Import Python libs
Import 3rd-party libs
Import python libs
The path exists and is writeable
The path exists and is not writeable
We're not allowed to check the parent directory of the provided path
Lets get the parent directory of the provided path
Parent directory does not exit
Finally, return if we're allowed to write in the parent directory of the  provided path
The path exists and is readable
The path does not exist
Import python libs
Import salt libs
Import third party libs
Verify that IP address is valid
Verify that mask is valid
Import python libs
-*- coding: utf-8 -*-
Import Python Libs
If the requested function isn't available, lets say why
we are zero if dict is empty and loaded is true
we are zero if dict is empty and loaded is true
create a dict to store loaded values in
have we already loded everything?
if not loaded,
Import python libs
Import python libs
Import 3rd-party libs  pylint: disable=import-error
pylint: disable=redefined-builtin  pylint: enable=import-error,redefined-builtin
Import 3rd-party libs
fcntl is not available on windows
grp is not available on windows
pwd is not available on windows
Non-existent file or permission denied to the parent dir
Try to set all of the colors to the passed color  except for color reset
in test, a single line template would return a crazy line number like,  357.  do this sanity check and if the given line is obviously wrong, just  return the entire template
warning: jinja content may contain unicode strings  instead of utf-8.
Make the system account easier to identify.
exit first parent
decouple from parent environment  noinspection PyArgumentList
check for os.X_OK doesn't suffice because directory may executable
executable in cwd or fullpath
If logging is not configured it also means that either  the master or minion instance calling this hasn't even  started running
accumulator_dir is not present, create it
':' is an illegal filesystem path character on Windows
Normalize path converting any os.sep as needed
Is odd
Since we WILL be changing the data dictionary, let's change a copy of it
Let's leave the default value in place
The function accepts **kwargs, any non expected extra keyword  arguments will made available.
No need to check for extra keyword arguments since they are all  **kwargs now. Return
Did not return yet? Lets gather any remaining and unexpected keyword  arguments
We'll be showing errors to the users until Salt Carbon comes out, after  which, errors will be raised instead.  Let's not show the deprecation warning on the console, there's no  need.
Lets pack the current extra kwargs as template context
This wasn't an open filehandle, so treat it as a file path and try to  open the file  Unable to open file, bail out and return false
Files with null bytes are binary
An empty file is considered a valid text file
modify the file descriptor on systems with fcntl  unix and unix-like systems only
We are just checking that the key exists
We might want to search for a key
Encountered a non-indexable value in the middle of traversing
Index was not numeric, lets look at any embedded dicts  No embedded dicts matched, return the default
This is a hack.  If a proxy minion is started by other  means, e.g. a custom script that creates the minion objects  then this will fail.
Now apply include/exclude conditions
only override return value if we are not already failed  return as soon as we got a failure
First, try int/float conversion
Is the error an access error ?
st_ino is always 0 on some filesystems (FAT, NTFS); ignore them
read the file in in chunks, not the entire file
Do nothing on CTRL_LOGOFF_EVENT
fuzzy date  py3: yes, timelib.strtodatetime wants bytes, not str :/
not parsed yet, obviously a timestamp?
Attribute the warning to the calling function, not to warn_until()
Attribute the warning to the calling function,  not to kwargs_warn_until() or warn_until()
Check if integer/long
Gracefully handle cmp_result not in (-1, 0, 1).
New key
Key removed
Key modified
this happens if not callable
Not json, raise an error
If more than 30% non-text characters, then  this is considered a binary file
We don't work on platforms that don't have grp and pwd  Just return an empty list
Try pysss.getgrouplist
Historically, saltstack code for getting group lists did not  include the default group. Some things may only want  supplemental groups, so include_default=False omits the users  default group.  If for some reason the user does not have a default group
We don't work on platforms that don't have grp and pwd  Just return an empty dict
We don't work on platforms that don't have grp and pwd  Just return an empty list
The below code block is based on posixpath.relpath from Python 2.7,  which has the fix for this bug.
work out how much of the filepath is shared by start and path.
Import python libs
Import azure libs
Import salt libs
Import Python libs
Import Salt libs
Import 3rd-party libs
pylint: disable=import-error,redefined-builtin,no-name-in-module  pylint: enable=import-error,redefined-builtin,no-name-in-module
Declare globals
Check to see if we have cache credentials that are still good  Current timestamp less than expiration fo cached credentials  We don't have any cached credentials, or they are expired, get them
Retrieve access credentials from meta-data, or use provided
Add in security token if we have one
current time in epoch seconds
Retrieve access credentials from meta-data, or use provided
Create payload hash (hash of the request body content). For GET  requests, the payload is an empty string ('').
Combine elements to create create canonical request
Create the signing key using the function defined above.
Sign the string_to_sign using the signing_key
Add signing information to the request
Add in security token if we have one
Cached region
Connections to instance meta-data must fail fast and never be proxied
Do not try again
Import python libs
Import third party libs
pylint: disable=import-error,no-name-in-module
Older jinja does not need markupsafe
Older python where the backport from pypi is installed
Other older python we use our bundled copy
Import salt libs
For openSUSE, which apparently doesn't include the whole stdlib
Bail! Though, how did we reached this far in the first place.
This is likely a compressed python .egg
top is a single file module
Bail! Though, how did we reached this far in the first place.
This is likely a compressed python .egg
top is a single file module
Import python libs
Import salt's Libs
asking for a password, and we can't seem to send it
Auth success!  We now have a prompt
saw_prompt = False
import python libs
Import Salt Libs
Import 3rd Party Libs
'win32net.NetUserGetLocalGroups' will fail if you pass in 'SYSTEM'.
Make the system account easier to identify.
Return owner
No profile name
One or more profiles defined
Import python libs
Import salt libs
Import third party libs
Set up logging
Don't shadow built-ins
Import Python libs
If this XML tree has an xmlns attribute, then etree will add it  to the beginning of the tag, like: "{http://path}tag". This  aggression will not stand, man.
If a tag appears more than once in the same place, convert it to  a list. This may require that the caller watch for such a thing  to happen, and behave accordingly.
Import python libs
Import 3rd-party libs
This attribute here won't actually do anything. But, if you need to  specify an order or a dependency within the mix-ins, please define the  attribute on your own MixIn
Mark process_<opt> functions with the base priority for sorting
Function already has the attribute set, don't override it
see https://github.com/python/cpython/blob/master/Lib/optparse.pyL786
Private attributes
Setup multiprocessing logging queue listener
Let's get some proper sys.stderr logging as soon as possible!!!  This logging handler will be removed once the proper console or  logfile logging is setup.
Gather and run the process_<option> functions in the proper order
Retain the standard behavior of optparse to return options and args
Stop the logging queue listener process
Add an additional function that will merge the shell options with  the config options and if needed override them
Merge parser options  --version does not have dest attribute set for example.  All options defined by us, even if not explicitly(by kwarg),  will have the dest attribute set
Get the passed value from shell. If empty get the default one
There's no value in the configuration file  There's an actual value, add it to the config
Only set the value in the config file IF it was explicitly  specified by the user, this makes it possible to tweak settings  on the configuration files bypassing the shell option flags'  defaults
Let's update the option value with the one from the  configuration file. This allows the parsers to make use of  the updated value by using self.options.<option>
No one passed a Saltfile as an option, environment variable!?
There's still no valid Saltfile? No need to continue...
Make sure we have an absolute path
Make sure we let the user know that we will be loading a Saltfile
No configuration was loaded from the Saltfile
There's no configuration specific to the CLI tool. Stop!
We just want our own configuration
If there are any options, who's names match any key from the loaded  Saltfile, we need to update its default value  --version does not have dest attribute set for example.
If we don't have anything in Saltfile for this option, let's  continue processing right now
Get the passed value from shell. If empty get the default one  The user passed an argument, we won't override it with the  one from Saltfile, if any
We reached this far! Set the Saltfile value on the option
Let's also search for options referred in any option groups
If we don't have anything in Saltfile for this option,  let's continue processing right now
Get the passed value from shell. If empty get the default one  The user passed an argument, we won't override it with  the one from Saltfile, if any
Any left over value in the saltfile can now be safely added
No logging is configured yet
Make sure we have an absolute path
This is an attribute available for programmers, so, raise a  RuntimeError to let them know about the proper usage.
Setup extended logging right before the last step  Setup the multiprocessing log queue listener if enabled  Setup the console as the last _mixin_after_parsed_func to run
There's a configuration setting defining this log file path,  i.e., `key_log_file` if the cli tool is `salt-key`
Is the regular log file setting set?
Nothing is set on the configuration? Let's use the cli tool  defined default
There's a configuration setting defining this log file  logging level, i.e., `key_log_file_level` if the cli tool is  `salt-key`
Is the regular log file level setting set?
Nothing is set on the configuration? Let's use the cli tool  defined default
Remove it from config so it inherits from log_level
From the config setting  From the console setting
Remove it from config so it inherits from log_level_logfile
Remove it from config so it inherits from log_file
First from the config cli setting  From the config setting  From the default setting
Remove it from config so it inherits from log_fmt_logfile
Remove it from config so it inherits from log_fmt_console
Remove it from config so it inherits from log_datefmt_logfile
Remove it from config so it inherits from log_datefmt_console
Remove it from config so it inherits from log_datefmt
If daemon is set force console logger to quiet
Remove it from config so it inherits from log_datefmt_console
Remove it from config so it inherits from log_datefmt
We've loaded and merged options into the configuration, it's safe  to query about the pidfile
Stop the logging queue listener for the current process  We'll restart it once forked
Late import so logging works correctly
Setup the multiprocessing log queue listener if enabled
There is no os.getppid method for windows
Common methods for scripts which can daemonize
The option was already added by the DaemonMixin
Make this a zero length filename instead of removing  it. This way we keep the file permissions.
Include description here as a string
ConfigDirMixIn config filename attribute  LogLevelMixIn attributes
ConfigDirMixIn config filename attribute  LogLevelMixIn attributes
ConfigDirMixIn config filename attribute  LogLevelMixIn attributes
ConfigDirMixIn config filename attribute  LogLevelMixIn attributes
ConfigDirMixIn config filename attribute
We get an argument that Python's optparser just can't deal  with. Perhaps stdout was redirected, or a file glob was  passed in. Regardless, we're in an unknown state here.
ConfigDirMixIn config filename attribute
LogLevelMixIn attributes
salt-cp needs arguments
ConfigDirMixIn config filename attribute
LogLevelMixIn attributes
Don't change its mixin priority!
Since we're generating the keys, some defaults can be assumed  or tweaked
It was decided to always set this to info, since it really all is  info or error.
ConfigDirMixIn config filename attribute
LogLevelMixIn attributes
ConfigDirMixIn config filename attribute
LogLevelMixIn attributes
ConfigDirMixIn config filename attribute
LogLevelMixIn attributes
ConfigDirMixIn attributes
LogLevelMixIn attributes
Late import in order not to break setup
ConfigDirMixIn config filename attribute  LogLevelMixIn attributes
ConfigDirMixIn config filename attribute  LogLevelMixIn attributes
Python equivalent of an enum
Import python libs
Import third party libs
clear_pillar and clear_grains are both True or both False.  This means we don't deal with pillar/grains caches at all.
Unless both clear_pillar and clear_grains are True, we need  to read in the pillar/grains data since they are both stored  in the same file, 'data.p'
Not saving pillar or grains, so just delete the cache file
Delete the whole mine file
the socket for outgoing timer events
__setstate__ and __getstate__ are only used on Windows.  We do this so that __init__ will be invoked on Windows in the child  process so that a register_after_fork() equivalent will work on Windows.
the possible settings for the cache
the actual cached minion ids
the timer provides 1-second intervals to the loop in run()  to make the cache system most responsive, we do not use a loop-  delay which makes it hard to get 1-second intervals without a timer
__setstate__ and __getstate__ are only used on Windows.  We do this so that __init__ will be invoked on Windows in the child  process so that a register_after_fork() equivalent will work on Windows.
avoid getting called twice
the socket for incoming cache requests
our serializer
register a signal handler
secure the sockets from the world
requests to the minion list are send as str's  Send reply back to client
check for next cache-update from workers  tell the worker to exit
check if the returned data is usable
check for next timer-event to start new jobs
update the list every 30 seconds
test code for the ConCache class
Import python libs
Import salt libs
EPOCHNUM can't be used until RHEL5 is EOL as it is not present
Handle unpack errors (should never happen with the queryformat we are  using, but can't hurt to be careful).
Import 3rd-party libs  pylint: disable=import-error,no-name-in-module,redefined-builtin  pylint: enable=import-error,no-name-in-module
Import python libs
Import third party libs
to be overridden
to be overridden
maybe py2exe went by
Import python libs
Import python libs
Import salt libs
Import 3rd-party libs
make sure nobody creates another Null value
Mark the instance as a configuration document/section
This configuration block is to be treated as a part of the  configuration for which it was defined as an attribute, not as  it's own sub configuration
The configuration block only accepts the configuration items  which are defined on the class. On additional items, validation  with jsonschema will fail
Define some class level attributes to make PyLint happier
Store it as a configuration section
Handle the configuration items defined in the class instance
If it's a required item, add it to the required list
Store the order of the item
Only include required if not empty
Only include ordering if not empty
Define some class level attributes to make PyLint happier
This is handled elsewhere
None values are not meant to be included in the  serialization, since this is not None...
This is either a Schema or a Basetem, return it in it's  serialized form
Custom Preconfigured Configs >
Import Python Libraries
Import Third Party Libs
Import Salt Libs
Set up logging
ctypes definitions
https://msdn.microsoft.com/en-us/library/ms687032
https://msdn.microsoft.com/en-us/library/ms683231
https://msdn.microsoft.com/en-us/library/ms724211
https://msdn.microsoft.com/en-us/library/ms724935
https://msdn.microsoft.com/en-us/library/ms724251
https://msdn.microsoft.com/en-us/library/ms683179
https://msdn.microsoft.com/en-us/library/ms683189
https://msdn.microsoft.com/en-us/library/aa365152
https://msdn.microsoft.com/en-us/library/ms682431
Get User Token
Get Security Attributes
Create a pipe to set as stdout in the child. The write handle needs to be  inheritable.
Get startup info structure
Get User Environment
Build command
Run command and return process info structure
Initialize ret and set first element
Get Return Code
Close handle to process
This only works when not running under the system account  Debug mode for example
Create a pipe to set as stdout in the child. The write handle needs to be  inheritable.
Create inheritable copy of the stdin
Get startup info structure
Build command
Run command and return process info structure
Initialize ret and set first element
Close handle to process
Import python libs
Subprocess cleanup (best effort)
return the correct data structure for the requisite
our requisites should all be lists, but when you only have a  single item it's more convenient to provide it without  wrapping it in a list. transform them into a list
handle our requisites  rebuild the requisite list transforming any of the actual  StateRequisite objects into their representative dict
build our attrs from kwargs. we sort the kwargs by key so that we  have consistent ordering for tests
find all of our filters
only process classes
which grain are we filtering on
does the object pointed to have a __match__ attribute?  if so use it, otherwise use the name of the object  this is so that you can match complex values, which the python  class name syntax does not allow
-*- coding: utf-8 -*-
Import python libs
new in Vista and Windows Server 2008  pylint: disable=C0103  pylint: enable=C0103
the updated config is more valid--report that we are using it
new config is a dict, other isn't--new one wins
the updated config is more valid--report that we are using it
new config is a list, other isn't--new one wins
Import python libs
Import 3rd-party libs
Attempt to import wmi
Import salt libs
should never have a space in hostname  favor hostnames w/o spaces
punish localhost list
punish ipv6
favor hosts with more dots
favor longest fqdn
strip spaces and ignore empty strings
remove duplicates
include public and private ipaddresses
if no minion id
XXX What is the prefixlen!?
XXX seen used for tunnel adaptors  might be useful
['root', 'python2.7', '1456', '37', 'tcp4',   '127.0.0.1:4505-', '127.0.0.1:55703']  print chunks
['root', 'python2.7', '1456', '37', 'tcp4',   '127.0.0.1.4505-', '127.0.0.1.55703']  print chunks
Lsof return 1 if any error was detected, including the failure  to locate Internet addresses, and it is not an error in this case.
Import python libs
Import salt libs
Import 3rd-party libs
kind -> Dependency -> list of things that depend on it
if not, unload dependent_set
if we don't have this module loaded, skip it!
we already did???
Import Python libs
Import Salt libs
if you have a job_cache, or an ext_job_cache, don't write to  the regular master cache
Import python libs
Import Salt libs
Get logging started
Import Swift client libs
-*- coding: utf-8 -*-
Import Python libs
pylint: disable=import-error
Import Python libs
Import pyrax (SDK for Rackspace cloud) third party libs
Import salt classes
First if not exists() -> exit
First if not exists() -> exit  If exist, search the queue to return the Queue Object
Import Python libs
Import salt libs
Get logging started
Version added to novaclient.client.Client function
kwargs has to be an object instead of a dictionary for the __post_parse_arg__
used in novaclient extensions to see if they are rackspace or not, to know if it needs to load  the hooks for that extension or not.  This is cleaned up by sanatize_novaclient
Requires novaclient version >= 2.6.1
Import python libs
Import third party libs  pylint: disable=import-error
Import salt libs
Get logging started
Import python libs
Half a megabyte in memory is more than enough to start writing to  a temporary file.
self._stdin_logger_name_.format(pid=self.pid)
Import python libs
Import salt libs
Import python libs
Import Salt libs
Import 3rd-party libs
deep merging is more or less a.update(obj_b)
introspection on obj_b keys only
Import python libs
Import third party libs
The SUB_EVENT set is for functions that require events fired based on  component executions, like the state system
dict map of namespaced base tag prefixes for salt events
Only connect to the publisher at initialization time if  we know we want to listen. If we connect to the publisher  and don't read out events from the buffer on an on-going basis,  the buffer will grow resulting in big memory usage.
This is in the class namespace, to minimize cache memory  usage and maximize cache hits  The prepend='^' is to reduce differences in behavior between  the default 'startswith' and the optional 'regex' match_type
For the async case, the connect will be defered to when  set_event_handler() is invoked.
For the async case, the connect will be defered to when  fire_event() is invoked.
If no_block is False and wait is 0, that  means an infinite timeout.
Trigger that at least a single iteration has gone through
tornado.ioloop.IOLoop.run_sync() timeouts are in seconds.  IPCMessageSubscriber.read_sync() uses this type of timeout.
Since the pack / unpack logic here is for local events only,  it is safe to change the wire protocol. The mechanism  that sends events from minion to master is outside this  file.
This will handle reconnects
skip exceptions in destroy-- since destroy() doesn't cover interpreter  shutdown-- where globals start going missing
Let's try to create the directory defined on the configuration  file  Let's not fail yet and try using the default path  We're already trying the default system path, stop now!
Let's stop at this stage
Add an extra fallback in case a forked process leeks through
__setstate__ and __getstate__ are only used on Windows.  We do this so that __init__ will be invoked on Windows in the child  process so that a register_after_fork() equivalent will work on Windows.
Make sure the IO loop and respective sockets are closed and  destroyed
Add an extra fallback in case a forked process leeks through
__setstate__ and __getstate__ are only used on Windows.  We do this so that __init__ will be invoked on Windows in the child  process so that a register_after_fork() equivalent will work on Windows.
Flush and terminate
don't waste processing power unnecessarily on converting a  potentially huge dataset to a string
Import python libs
Import Python libs
Import salt libs
Import 3rd-party libs
Cleanup empty dirs
Import python libs
Import 3rd-party libs
pylint: disable=import-error
Keep track of the lowest loop interval needed in this variable
ensure job exists, then delete it
remove from self.intervals
if enabled is not included in the job,  assume job is enabled.
Reset current signals before starting the process in  order not to inherit the current signal handlers
Remove all jobs from self.intervals
Fire the complete event back along with the list of schedule
Since function references can't be pickled and pickling  is required when spawning new processes on Windows, regenerate  the functions and returners.
Reconfigure multiprocessing logging after daemonizing
write this to /var/cache/salt/minion/proc
runners do not provide retcode
EEXIST and ENOENT are OK because the file is gone and that's what  we wanted
Otherwise, failing to delete this file is not something  we can cleanly handle.
Let's make sure we exit the process!
Used for quick lookups when detecting invalid option combinations.
Grab the first element  which is the next run time
If we're switching to the next run in a list  ensure the job can run
scheduled time is in the past
Backup the run time
A new 'when' ensure _when_run is True
scheduled time is in the past
Backup the run time
A new 'when' ensure _when_run is True
If run_on_start is True, the job will run when the Salt  minion start.  If the value is False will run at the next  scheduled run.  Default is True.
Temporarily stash our function references.  You can't pickle function references, and pickling is  required when spawning new processes on Windows.
Reset current signals before starting the process in  order not to inherit the current signal handlers
Restore our function references.
Windows cannot delete an open file
Windows cannot delete an open file  Maybe the file is already gone
Import Python libs
Import Salt libs
Import 3rd-party libs
Import python libs
Import third party libs
Import salt libs
Return True for local mode
The last matched group can be None if the version  is something like 3.1 and that will work properly
point very well could be None
If starting the process as root, chown the new dirs
Run the extra verification checks
We could just reset the whole environment but let's just override  the variables we can get from pwuser
paths with trailing separators will return an empty string
loop until head is the same two consecutive times
Make the error message more intelligent based on how  the user invokes salt-call or whatever other script.
We don't need to bail on config file permission errors  if the CLI  process is run with the -a flag  Propagate this exception up so there isn't a sys.exit()  in the middle of code that could be imported elsewhere.
Check the Windows API for more detail on this  http://msdn.microsoft.com/en-us/library/xt874334(v=vs.71).aspx  and the python binding http://timgolden.me.uk/pywin32-docs/win32file.html
We check for the soft value of max open files here because that's the  value the user chose to raise to.  The number of accepted keys multiplied by four(4) is lower than the  soft value, everything should be OK
This should never occur, it might have already crashed
This is way too low, CRITICAL
The accepted count is more than 3 time, WARN
Get the root path directory where salt is installed
Create the root path directory if missing
Run the extra verification checks
Import python libs
Import salt libs
There's not even a pki directory, don't bother migrating
only required in 2015.2
Import Python libs
Import third party libs  pylint: disable=import-error  pylint: disable=import-error
pylint: enable=import-error
Import python libs
Import python libraries
Import 3rd-party libs
Windows does not have the crypt module
Import salt libs
Import python libs
Import third party libs
Alias cmd.run to cmd.shell to make python_shell=True the default for  templated calls
We want explicit context to overwrite the **kws
Template is a bin file, return the raw file
Show full traceback if debug logging is enabled
Write out with Windows newlines
Note: If nothing is replaced or added by the rendering        function, then the contents of the output file will        be exactly the same as the input.
http://jinja.pocoo.org/docs/api/unicode
i.e., the template is from a file outside the state tree  XXX: FileSystemLoader is not being properly instantiated here is  it? At least it ain't according to:    http://jinja.pocoo.org/docs/api/jinja2.FileSystemLoader
Don't include the line number, since it is misreported  https://github.com/mitsuhiko/jinja2/issues/276
Workaround a bug in Jinja that removes the final newline  (https://github.com/mitsuhiko/jinja2/issues/75)
i.e., the template is from a file outside the state tree
Import python libs
Import python libs
Import 3rd-party libs
this mapping is not a dict
Import python libs
Import salt libs
Import python libs
Import salt libs
Import third party libs
Import python libs
recognize methods
Get a reference to the function globals dictionary  Save the current function globals dictionary state values for the  overridden objects
Override the function globals with what's passed in the above overrides
The context is now ready to be used
Restore the overwritten function globals
Remove any entry injected in the function globals
state should be thread local, so this object can be threadsafe  variable for the overriden data
merge self.global_data into self._data
Import Python libs
Import 3rd-party libs
-*- coding: utf-8 -*-
Time moves forward so we might not reenter the loop if we step too long
Don't allow cases of over-stepping the timeout
import python libs
Import Salt libs
Import python libs
Don't need a try/except block, since Salt depends on tornado
Some libraries don't support separation of url and GET parameters  Don't need a try/except block, since Salt depends on tornado
Make sure no secret fields show up in logs
Tornado
'stream' was called 'prefetch' before 1.0, with flipped meaning
fake a HTTP response header  fake streaming the content
Tornado
Debian has paths that often exist on other distros  RedHat is also very common  RedHat's link for Debian compatibility  SUSE has an unusual path  OpenBSD has an unusual path
Set the last cookie that was processed
cookielib.Cookie() requires an epoch
-*- coding: utf-8 -*-
Import python libs
Import salt libs
Import 3rd-party libs
pylint: disable=import-error
Daemon was not started by systemd
The current user already owns the pidfile. Return!
if no count passed, default to number of CPUs
create a task queue of queue_size
create worker threads
1s timeout so that if the parent dies this thread will die within 1s
During shutdown, `queue` may not have an `Empty` atttribute. Thusly,  we have to catch a possible exception from our exception handler in  order to avoid an unclean shutdown. Le sigh.
pid -> {tgt: foo, Process: object, args: args, kwargs: kwargs}
store some pointers for the SIGTERM handler
Need to ensure that 'log_queue' is correctly transfered to  processes that inherit from 'MultiprocessingProcess'.
don't block, the process is already dead
If it's not a "No such process" error, raise it  Otherwise, it's a dead process, remove it from the process map
in case someone died while we were waiting...
os.wait() is not supported on Windows.  OSError is raised if a signal handler is called (SIGTERM) during os.wait
IOError with errno of EINTR (4) may be raised  when using time.sleep() on Windows.
The process is no longer alive, remove it from the process map dictionary  This is a race condition if a signal was passed to all children
Patch the run method at runtime because decorating the run method  with a function with a similar behavior would be ignored once this  class'es run method is overridden.
Set the logging queue so that it can be retrieved later with  salt.log.setup.get_multiprocessing_logging_queue().
Call __init__ from 'multiprocessing.Process' only after removing  'log_queue' from kwargs.
__setstate__ and __getstate__ are only used on Windows.  We do this so that __init__ will be invoked on Windows in the child  process so that a register_after_fork() equivalent will work on Windows.  This will invoke __init__ of the most derived class.
Remove the version of these in the parent process since  they are no longer needed.
These are handled by multiprocessing.Process._bootstrap()
Re-raise the exception. multiprocessing.Process will write it to  sys.stderr and set the proper exitcode and we have already logged  it above.
On Windows, no need to call register_after_fork().  register_after_fork() would only work on Windows if called  from the child process anyway. Since we know this is the  child process, call __setup_signals() directly.
Do whatever is needed with the reset signals
Restore signals
Import Python Libraries
Import Salt Libs
Set up logging
python libs
salt libs
3rd-party libs
smartos does not have libraries in std location
openssl/rsa.h:define RSA_X931_PADDING 5
Import python libs
Import salt libs
Import 3rd-party libs
adds a leading dot to make use of stateconf's namespace feature.
A list is an error
track the position of the auto-added require for easy  removal if run at compile time.
No profile name
One or more profiles defined
Import python libs
Import salt libs
Import third party libs
Set up logging
This gets raised when we can't contact etcd at all
Same issue as ReadTimeoutError.  When it 'works', python-etcd  throws EtcdConnectionFailed, so we'll do that for it.
EtcdValueError inherits from ValueError, so we don't want to accidentally  catch this below on ValueError and give a bogus error message
python-etcd doesn't fully support python 2.6 and ends up throwing this for *any* exception because  it uses the newer {} format syntax
this method would reguarly return a future
Overload the ioloop for the func call-- since it might call .current()
Certain things such as streams should be closed before  their associated io_loop is closed to allow for proper  cleanup.
Other things should be deallocated after the io_loop closes.  See Issue 26889.
Import python libs
Import salt libs
Import Python libs
Import Salt libs
Import 3rd-party libs
Always [device, mime, charset]. See (file --help)
Import python libs
Import python libs
Import virtualbox libs
Reloading the API extends sys.paths for subprocesses of multiprocessing, since they seem to share contexts
Time tracking
Keep trying to start a machine
We already waited for stuff, don't push it
The session state should best be unlocked otherwise subsequent calls might cause problems
Replaced keys
Import python libs
Import 3rd-party libs
Invalid event, how did this get here?
Not what we are looking for, throw it away
Invalid event, how did this get here?
Set language  ( This looks like it maps to something else )
Set keyboard  ( This looks like it maps to something else )
Import python libs
Import salt libs
Also canonical BSD-way of printing progress is SIGINFO  which on BSD-derivatives can be sent via Ctrl+T
No exception handling, as we want ImportError if psutil doesn't exist
Alias new module functions
Deprecated in 1.0.1, but not mentioned in blog post
Reimplement overloaded getters/setters
Adding our two functions to the socket library
Ported from python3-syntax:  leftmost, *remainder = dn.split(r'.')
Issue 17980: avoid denials of service by refusing more  than one wildcard per fragment.  A survey of established  policy among SSL implementations showed it to be a  reasonable choice.
speed up common case w/o wildcards
add the remaining fragments, ignore any wildcards
Useful for very coarse version differentiation.
Jython always uses 32 bits.
This is a bit ugly, but it avoids running this again by  removing this descriptor.
Subclasses should override this
in case of a reload
This is about 2x faster than the implementation above on 3.2+
Use iterator versions of map and range:
Except that xrange only supports machine integers, not longs, so...
This backport uses bytearray instead of bytes, as bytes is the same  as str in Python 2.7.
When checking for instances of int, also allow Python 2's long.
Python 2.6 has no int.bit_length()
All bits of interest were zero, even if there are more in the number
sort and dedup
Parse the netmask/hostmask like an IP address.
Try matching a netmask (this would be /1*0*/ as a bitwise regexp).  Note that the two ambiguous cases (all-ones and all-zeroes) are  treated as netmasks.
Invert the bits, and try matching a /0+1+/ hostmask instead.
Shorthand for Integer addition and subtraction. This is not  meant to ever support addition/subtraction of addresses.
Returning bare address objects (rather than interfaces) allows for  more consistent behaviour across the network address, broadcast  address and individual host addresses.
Make sure we're comparing the network of other.
If we got here, there's a bug somewhere.
If we got here, there's a bug somewhere.
Equivalent to 255.255.255.255 or 32 bits of 1's.
the valid octets for host and netmasks. only useful for IPv4.
Found something that isn't an integer or isn't valid
Efficient constructor from integer.
Constructing from a packed address
Assume input argument to be string or any object representation  which converts into a formatted IP string.
An interface with an associated network is NOT the  same as an unassociated address. That's why the hash  takes the extra info into account.
We *do* allow addresses and interfaces to be sorted. The  unassociated address is considered less than all interfaces.
Class to use when creating address objects
Constructing from a packed address
Assume input argument to be string or any object representation  which converts into a formatted IP prefix string.
Check for a netmask in prefix length form
Check for a netmask or hostmask in dotted-quad form.  This may raise NetmaskValueError.
An IPv6 address needs at least 2 colons (3 parts).
An IPv6 address can't have more than 8 colons (9 parts).  The extra colon comes from using the "::" notation for a single  leading or trailing zero part.
Start of a sequence of zeros.
This is the longest sequence of zeros so far.
For zeros at the end of the address.  For zeros at the beginning of the address.
Efficient constructor from integer.
Constructing from a packed address
Assume input argument to be string or any object representation  which converts into a formatted IP string.
An interface with an associated network is NOT the  same as an unassociated address. That's why the hash  takes the extra info into account.
We *do* allow addresses and interfaces to be sorted. The  unassociated address is considered less than all interfaces.
Class to use when creating address objects
Efficient constructor from integer.
Constructing from a packed address
Assume input argument to be string or any object representation  which converts into a formatted IP prefix string.
This may raise NetmaskValueError
Import python libs
Import Salt Libs  pylint: disable=import-error,no-name-in-module,redefined-builtin  pylint: enable=no-name-in-module,redefined-builtin
e.g. master: mysaltmaster
e.g. master: localhost:1234  e.g. master: 127.0.0.1:1234  e.g. master: ::1:1234
proc_dir is not present, create it with mode settings
only on unix/unix like systems
Don't append the version that was just derived from parse_cli  above, that would result in a 2nd call to  salt.utils.cli.yamlify_arg(), which could mangle the input.
Function supports **kwargs or is a positional argument to  the function.
**kwargs not in argspec and parsed argument name not in  list of positional arguments. This keyword argument is  invalid.
this function accepts **kwargs, pack in the publish data
Check if scheduler requires lower loop interval than  the loop_interval setting
if we have a list of masters, loop through them and be  happy with the first one that allows us to connect  shuffle the masters and then loop through them
on first run, update self.opts with the whole master list  to enable a minion to re-use old masters if they get fixed
Exhausted all attempts. Return exception.  If the code reaches this point, 'last_exc'  should already be set.
Late setup of the opts grains, so we can log from the grains module
timeout for one of the minions to auth with a master
Fire off all the minion coroutines
serve forever!
this means that the parent class doesn't know *which* master we connect to
Warn if ZMQ < 3.2  PyZMQ <= 2.1.9 does not have zmq_version_info, fall back to  using zmq.zmq_version() and build a version info tuple.
Late setup the of the opts grains, so we can log from the grains  module.  If this is a proxy, however, we need to init the proxymodule  before we can get the grains.  We do this for proxies in the  post_master_init
We don't have the proxy setup yet, so we can't start engines  Engines need to be able to access __proxy__
Install the SIGINT/SIGTERM handlers if not done so far  No custom signal handling was added, install our own
No custom signal handling was added, install our own
escalate the signals to the process manager  kill any remaining processes
This might be a proxy minion
we're done, reset the limits!
Reset current signals before starting the process in  order not to inherit the current signal handlers
Shutdown the multiprocessing before daemonizing
Reconfigure multiprocessing logging after daemonizing
The file is gone already
Local job cache has been enabled
Do not exit if a pillar refresh fails.
if eval_master finds a new master for us, self.connected  will be True again on successful master authentication
Add an extra fallback in case a forked process leaks through
start up the event publisher, so we can see events during startup
Make sure to gracefully handle SIGUSR1
Make sure to gracefully handle CTRL_LOGOFF_EVENT
start all the other callbacks
Verify that the publication is valid  Verify that the publication applies to this minion
Set up default tgt_type
add handler to subscriber
register the event sub to the poller
forward events every syndic_event_forward_timeout
Send an event to the master that the minion is live
Make sure to gracefully handle SIGUSR1
Instantiate the local client
add handler to subscriber
Not a job return
__'s to make sure it doesn't print out on the master cli
Add generic event aggregation here
if eval_master finds a new master for us, self.connected  will be True again on successful master authentication
We borrowed the local clients poller so give it back before  it's destroyed. Reset the local poller reference.
time to connect to upstream master
sync (old behavior), cluster (only returns and publishes)
set up the syndic to handle publishes (specifically not event forwarding)
if its connected, mark it dead
Targeted master previous send not done yet, call again later
Fallback master is busy, try the next one
Loop done and didn't exit: wasn't sent, try again later
Instantiate the local client
register the event sub to the poller
forward events every syndic_event_forward_timeout
Make sure to gracefully handle SIGUSR1
Not a job return
__'s to make sure it doesn't print out on the master cli
The value is not defined
We are matching a single component to a single list member
Target is an address?
Target is a network?
Nodegroups should already be expanded/resolved to other engines
If an unknown engine is called at any time, fail out
The match is not explicitly defined, evaluate it as a glob
Need to load the modules so they get all the dunder variables
we can then sync any proxymodules down from the master  we do a sync_all here in case proxy code was installed by  SPM or was manually placed in /srv/salt/_modules etc.
Then load the proxy module
Check config 'add_proxymodule_to_opts'  Remove this in Carbon.
Start engines here instead of in the Minion superclass __init__  This is because we need to inject the __proxy__ variable but  it is not setup until now.
Sync the grains here so the proxy can communicate them to the master
Import python libs
Import salt libs
otherwise run it in the main process
Import python libs
Import Salt Libs
Configuration for load beacon should be a list of dicts
Import Python libs
Import python libs
Import salt libs
Configuration for sh beacon should be a list of dicts
Import Python libs
Salt libs
Import third party libs
Import Python libs
Import Salt libs
remove 'enabled' item before processing the beacon
Backwards compatibility
Import python libs
Import Python Libs
Configuration for service beacon should be a list of dicts
If no options is given to the service, we fall back to the defaults  assign a False value to oncleanshutdown and onchangeonly. Those  key:values are then added to the service dictionary.
Import python libs
Import Python libs
Import salt libs
Read in existing events
Find the matching path in config
Get paths currently being watched
Return event data
Import Python libs
Import salt libs
Import third party libs
get to the end of the journal
Match!
Import Python libs
Import Salt libs
Import Third Party Libs
Configuration for memusage beacon should be a list of dicts
Import Python libs
Import salt libs
Configuration for wtmp beacon should be a list of dicts
Import Python libs
Salt libs
Import Python libs
Import salt libs
Important: If used with salt-proxy  this is required for the beacon to load!!!
Import Python Libs
Import third party libs  pylint: disable=import-error  pylint: enable=import-error
Configuration for ps beacon should be a list of dicts
Import Python libs
Import third party libs  pylint: disable=import-error  pylint: enable=import-error
Import Python libs
Import Third Party Libs
Configuration for diskusage beacon should be a list of dicts
Ensure a valid mount point
Import Python libs
Import 3rd Party libs
Configuration for twilio_txt_msg beacon should be a list of dicts
Import Python libs
Import Salt libs
Import Py3 compat
Default config if not present
Import Python libs
Import 3rd-party libs
Copy kwargs keys so we can iterate over and pop the pub data
pull out pub_data if you have it
if you were passed kwarg, add it to arglist
Inject some useful globals to *all* the function's global  namespace only once per module-- not per func
throw a warning for the badly formed low data if we found  kwargs using the old mechanism
Initialize a context for executing the method.
if we fired an event, make sure to delete the event object.  This will ensure that we call destroy, which will do the 0MQ linger
Shutdown the multiprocessing before daemonizing
Reconfigure multiprocessing logging after daemonizing
pack a few things into low
Reset current signals before starting the process in  order not to inherit the current signal handlers
if we are "quiet", don't print
some suffixes we don't want to print
Import third party libs  pylint: disable=import-error
Try to import range from https://github.com/ytoolshed/range  pylint: enable=import-error
Late import to prevent circular import
The username may contain '\' if it is in Windows  'DOMAIN\username' format. Fix this for the keyfile path.
Make sure all key parent directories are accessible
Fall back to eauth
Looks like the timeout is invalid, use config
Re-raise error with specific message
Convert to generic client error and pass along message
make sure the minions is a set (since we do set operations on it)
start this before the cache lookup-- in case new stuff comes in
get the info from the cache
if you have all the returns, stop
timeouts per minion, id_ -> timeout time
if we have a new minion in the list, make sure it has a timeout
check for minions that are running the job still  if there are no more events, lets stop waiting for the jinfo
Keep track of the jid events to unsubscribe from later
if the job isn't running there anymore... don't count
don't spin
If there are any remaining open events, clean them up.
create the iterator-- since we want to get anyone in the middle
if we have all the minion returns, lets just return
are we done yet?
otherwise we hit the timeout, return what we have
lazy load the connected minions
Convert a range expression to a list of nodes and change expression  form to list
if kwargs are passed, pack them.
If we're a syndication master, pass the timeout
Ensure that the event subscriber is connected.  If not, we won't get a response, so error out
The master key could have changed out from under us! Regen  and try again if the key has changed
We have the payload, let's get rid of the channel fast(GC'ed faster)
Ensure that the event subscriber is connected.  If not, we won't get a response, so error out
We have the payload, let's get rid of the channel fast(GC'ed faster)
This IS really necessary!  When running tests, if self.events is not destroyed, we leak 2  threads per test case which uses self.client  The call below will take care of calling 'self.event.destroy()'
Import python libs
Import Salt libs
Import Python libs
Import Salt libs
check for wheel or runner prefix to fun name to use wheel or runner client
Import python libs
Import 3rd-party libs
The directory where salt thin is deployed
The regex to find RSTR in output - Must be on an output line by itself  NOTE - must use non-grouping match groups or output splitting will fail.
On esky builds we only have the .pyc file
we have ssh-copy-id, use it!
Sleep when limit or all threads started
Save the invocation information
save load to the master job cache
Save the invocation information
Error on host
Use the ID defined in the roster file
If we found mine_args, replace our command's args
Write the shim to a temporary file in the default temp directory
Copy shim to target system, under $HOME/.<randomized name>
Remove our shim file
Execute shim
Remove shim from target system
If RSTR is not seen in both stdout and stderr then there  was a thin deployment problem.
FIXME: this discards output from ssh_shim if the shim succeeds.  It should  always save the shim output regardless of shim success or failure.
Found RSTR in stderr which means SHIM completed and only  and remaining output is only from salt.
RSTR was found in stdout which means that the shim  functioned without *errors* . . . but there may be shim  commands, unless the only thing we found is RSTR
Undefined state
SHIM was not even reached
Import python libs
Import salt libs
Allow pillar.data to also be used to return pillar data
Import python libs
Import salt libs
Create the SSH object to handle the actual call
Run salt-ssh to get the minion returns
Create and run the runner
Import python libs
Import salt libs
Import 3rd-party libs
Seed the grains dict so cython will build
Import python libs
Import salt libs
Import 3rd-party libs
We're in an inner FunctionWrapper as created by the code block  above. Reconstruct the original cmd in the form 'cmd.run' and  then evaluate as normal
Form of salt.cmd.run in Jinja -- it's expecting a subdictionary  containing only 'cmd' module calls, in that case. We don't  support assigning directly to prefixes in this way
We're in an inner FunctionWrapper as created by the first code  block in __getitem__. Reconstruct the original cmd in the form  'cmd.run' and then evaluate as normal
Here was assume `value` is a `caller` function from __getitem__.  We save it as an alias and then can return it when referenced  later in __getitem__
Import python libs
Import salt libs
Import 3rd-party libs
Import python libs
Import salt libs
Create the SSH object to handle the actual call
Run salt-ssh to get the minion returns
Import python libs
Clean up our tar
If for some reason the json load fails, return the stdout
Clean up our tar
If for some reason the json load fails, return the stdout
Clean up our tar
If for some reason the json load fails, return the stdout
Clean up our tar
If for some reason the json load fails, return the stdout
Clean up our tar
If for some reason the json load fails, return the stdout
Verify that the high data is structurally sound
state.fun -> [state, fun]
Create the low chunk, using kwargs as a base
Set test mode
Get the override pillar data
Create the State environment
Verify the low chunk
Must be a list of low-chunks
Create the tar containing the state pkg and relevant files.
Create a hash so we can verify the tar on the target system
We use state.pkg to execute the "state package"
Create a salt-ssh Single object to actually do the ssh work
Copy the tar down
Run the state.pkg command on the target
Clean up our tar
If for some reason the json load fails, return the stdout
Import salt libs
render the path as a template using path_template_engine as the engine
Keep these in sync with salt/defaults/exitcodes.py
The below line is where OPTIONS can be redefined with internal options  (rather than cli arguments) when the shim is bundled by  client.ssh.Single._cmd_str()  pylint: disable=block-comment-should-start-with-cardinal-space
Delimiter emitted on stdout *only* to indicate shim message to master.
read the file in in chunks, not the entire file
Fix parameter passing issue
Import Python libs
Import Salt libs
Import python libs
Import salt libs
cwd may not exist if it was removed but salt was run from it
Import python libs
Import salt libs
Keep these in sync with ./__init__.py
We're getting results back, don't try to send passwords
asking for a password, and we can't seem to send it
Import python libs
Import salt-api libs
Install the SIGINT/SIGTERM handlers if not done so far  No custom signal handling was added, install our own
No custom signal handling was added, install our own
escalate the signals to the process manager  kill any remaining processes
Import python libs
Solve the Chicken and egg problem where grains need to run before any  of the modules are loaded and are generally available for any usage.
Import 3rd-party libs
Will be set to pyximport module at runtime if cython is enabled in config.
if we don't have the module, return an empty dict
if we have an attribute named that, lets return it.
map of suffix to description for imp
Prefer packages (directories) over modules (single files)!
add to suffix_map so file_mapping will pick it up
create mapping of filename (without suffix) to (path, suffix)  The files are added in order of priority, so order *must* be retained.
make sure it is a suffix we support
if its a directory, lets allow us to load that  is there something __init__?
Made it this far - add it
if we have been loaded before, lets clear the file mapping since  we obviously want a re-do
do we have an exact match?
do we have a partial match?
anyone else? Bueller?
reload only custom "sub"modules  it is a submodule if the name is in a namespace under mod
pack whatever other globals we were asked to
if process_virtual returned a non-True value then we are  supposed to not process this module  If a module has information about why it could not be loaded, record it
If we had another module by the same virtual name, we should put any  new functions under the existing dictionary.
private functions are skipped
Save many references for lookups  Careful not to overwrite existing (higher priority) functions
if we got what we wanted, we are done
At this point, __virtual__ did not return a  boolean value, let's check for deprecated usage  or module renames  The module is renaming itself. Updating the module name  with the new name
If the __virtual__ function returns True and __virtualname__  is set then use it
Key errors come out of the virtual function when passing  in incomplete grains sets, these can be safely ignored  and logged to debug, still, it includes the traceback to  help debugging.
If the module throws an exception during __virtual__()  then log the information and continue to the next.
Import python libs
Import Salt libs
Import 3rd-party libs
Import salt libs
Import python libs
Import salt libs
Import python libs
For non-dictionary data, just use print
Import Python libs
Import 3rd-party libs
Import python libs
Import salt libs
Look up the outputter
Only raise if it's NOT a broken pipe
new --out option, but don't choke when using --out=highstate at CLI  See Issue 29796 for more information.
Since the grains outputter was removed we don't need to fire this  error when old minions are asking for it
Import third party libs
Import salt libs
Define the module's virtual name
default indentation
custom indent
Define the module's virtual name
Import Python libs
Import Salt libs
Import salt libs
Import python libs
Define the module's virtual name
Import python libs
Import salt libs
Import 3rd-party libs
Data in this format is from saltmod.function,  so it is always a 'change'
Skip this state if it was successful & diff output was requested
Skip this state if state_verbose is False, the result is True and  there were no changes made
Print this chunk in a terse way and continue in the  loop
Print terse unless it failed
Print terse if no error and no changes, otherwise, be  verbose
Comment isn't a list either, just convert to string  If there is a data attribute, append it to the comment
This nukes any trailing \n and indents the others.
Import python libs
Define the module's virtual name
Return valid JSON for unserializable objects
Import salt libs
Import python libs
Import 3rd-party libs
Import python libs
Import 3rd-party libs
Import Python libs
Import Salt libs
Import python libs
Import Salt libs
Import python libs
Import 3rd-party libs
Import Salt libs
Try to import range from https://github.com/ytoolshed/range  pylint: enable=import-error
Currently we only support giving a raw range entry, no target filtering supported other than what range returns :S  'glob': target_glob,
Import salt libs
Import python libs
Comma-separate list of integers
Import python libs
Import salt libs
Comma-separate list of integers
Import python libs
Try to import range from https://github.com/ytoolshed/range  pylint: enable=import-error
Import Salt libs
Import Python libs
Import Salt libs
Import 3rd-party libs
Import python libs
Import third party libs
Set up logging
Import python libs
Import third party libs
Set up logging
Import python libs
Import third party libs
Set up logging
put the minion's ID in the path if necessary
Import python libs
Import third party libs
Set up logging
put the minion's ID in the path if necessary
if value is empty in Consul then it's None here - skip it
If value is not None then it's a string  Use YAML to parse the data  YAML strips whitespaces unless they're surrounded by quotes
sort in reverse based on pattern length  but use alphabetic order within groups of patterns of same length
Import python libs
Import 3rd-party libs  pylint: disable=import-error,no-name-in-module,redefined-builtin  pylint: enable=import-error,no-name-in-module,redefined-builtin
Import salt libs
Set up logging
Avoid recursively re-adding this same pillar
check if cache_file exists and its mtime  file does not exists then set mtime to 0 (aka epoch)
make sure bucket and saltenv directories exist
grab only the files/dirs in the bucket
Single environment per bucket
s3 query returned something
Multiple environments per buckets
s3 query returned data
pull out the files for the environment  grab only files/dirs that match this saltenv.
write the metadata to disk
grab the paths from the metadata  filter out the dirs
hashes match we have a cache hit
... or get the file from S3
Import python libs
Import third party libs
Set up logging
Do the regex string replacement on the minion id
Converting _id to a string  will avoid the most common serialization error cases, but DBRefs  and whatnot will still cause problems.
If we can't find the minion the database it's not necessarily an  error.
Import python libs
Import salt libs
Set up logging
merge-first is same as merge-last but the other way round  so let's switch stack[k] and v
Import python libs
Import salt libs
Set up logging
Import python libs
Import Salt libs
Set up logging
This ext_pillar is abstract and cannot be used directory
First, this is the query buffer.  Contains lists of [base,sql]
Add on the non-keywords...
And then the keywords...  They aren't in definition order, but they can't conflict each other.
There is no collision protection on root name isolation
If this test is true, the penultimate field is the key
Import python libs
Import 3rd-party libs
raise an exception! Pillar isn't empty, we can't sync it!
Go ahead and assign these because they may be needed later
Determine caching backend
We haven't seen this minion yet in the cache. Store it.
pylint: disable=cell-var-from-loop  pylint: enable=cell-var-from-loop
return state, mods, errors
Restore the actual file_roots path. Issue 5449
Import python libs
Import AWS Boto libs
Set up logging
Get the Master's instance info, primarily the region
Import python libs
Import salt libs
Import 3rd-party libs
Define the module's virtual name
each pillar entry is a single-key hash of name -> options
If reclass is installed, __virtual__ put it onto the search path, so we  don't need to protect against ImportError:
the source path we used above isn't something reclass needs to care  about, so filter it:
if no inventory_base_uri was specified, initialize it to the first  file_roots of class 'base' (if that exists):
I purposely do not pass any of __opts__ or __salt__ or __grains__  to reclass, as I consider those to be Salt-internal and reclass  should not make any assumptions about it.
import python libs
Import python libs
Import Salt libs
Set up logging
Import third party libs
Import python libs
Set up logging
Declare virtualname
Foreman API version 1 is currently not supported
Import python libs
Set up logging
Import python libs
Import third party libs
Set up logging
Import python libs
Set up logging
Define the module's virtual name
Import python libs
Import salt libs
Set up logging
Create dicts for subdirectories
Not used
No data for host with this ID
Import python libs
Set up logging
-*- coding: utf-8 -*-
Import Python Libs
Import Salt Libs
Import Third Party Libs
The default option values
Import python libs
Import Salt libs
Set up logging
new configuration
Import python libs
Import salt libs
Import third party libs
Set up logging
Import python libs
Call specified function to pull redis data
Return nothing for non-existent keys
Return as requested
Import futures
Import Salt libs
Import 3rd-party libs
Import python libs
Import salt libs
No keys have been generated
Import python libs
Import salt libs
Import third party libs
Set up logging
Import python libs
Set up logging
Import Python Libs
Import Salt Libs
Set up logging
Import python libs
Import third party libs
Import salt libs
Set up logging
Define the module's virtual name
split the branch, repo name and optional extra (key=val) parameters.
environment is "different" from the branch
Optional: If your project is not using the system python,  add your virtualenv path below.
Required: the app that is included in INSTALLED_APPS
Required: the model name
Required: model field to use as the key in the rendered  Pillar. Must be unique; must also be included in the  ``fields`` list below.
Optional:  See Django's QuerySet documentation for how to use .filter()
Required: a list of field names  List items will be used as arguments to the .values() method.  See Django's QuerySet documentation for how to use .values()
load the virtualenv first
load the django project
Check that the human-friendly name given is valid (will  be able to pick up a value from the query) and unique  (since we're using it as the key in a dictionary)
Import python libs
Set up logging
Use a different delimiter for nested dictionaries, defaults to '..' since some keys may use '.' in the name
Supply Grains for Pepa, this should **ONLY** be used for testing or validation   environment: dev
Supply Pillar for Pepa, this should **ONLY** be used for testing or validation   saltversion: 0.17.4
Enable debug for Pepa, and keep Salt on warning
Import futures
Import python libs
Import 3rd-party libs
Import Salt libs
Only used when called from a terminal
Options
Default input
Load templates
Get configuration
Get grains
Get pillars
Validate or not
Import python libs
Import Salt libs
Set up logging
Import third party libs
Import python libs
Import salt libs
Import third party libs  pylint: disable=import-error  pylint: enable=import-error
Set up logging
Define the module's virtual name
No git external pillars were configured
Verification of new git pillar configuration  Initialization of the GitPillar object did not fail, so we  know we have valid configuration syntax and that a valid  provider was detected.
If masterless, fetch the remotes. We'll need to remove this once  we make the minion daemon able to run standalone.
Ensure that the current pillar_dir is first in the list, so that  the pillar top.sls is sourced from the correct location.
Git directory we are working on  Should be the same as self.repo.working_dir
This exception occurs when two processes are  trying to write to the git config at once, go  ahead and pass over it since this is the only  write.  This should place a lock down.
split the branch, repo name and optional extra (key=val) parameters.
environment is "different" from the branch
normpath is needed to remove appended '/' if root is empty string.
Don't recurse forever-- the Pillar object will re-call the ext_pillar  function
Import python libs
Don't rely on external packages in this module since it's used at install time  pylint: disable=invalid-name,redefined-builtin  pylint: enable=invalid-name,redefined-builtin
Higher than 0.17, lower than first date based
Both have rc information, regular compare is ok
RC's are always lower versions than non RC's
Dynamic/Runtime Salt Version Information >  This might be a 'python setup.py develop' installation type. Let's  discover the version information at runtime.
This is not a Salt git checkout!!! Don't even try to parse...
This is not a Salt git checkout!!! Don't even try to parse...
Let's not import `salt.utils` for the above check
We only define the parsed SHA and set NOC as ??? (unknown)
If the errno is not 2(The system cannot find the file  specified), raise the exception so it can be catch by the  developers
Get additional version information if available  This function has executed once, we're done with it. Delete it!  < Dynamic/Runtime Salt Version Information
Common version related attributes - NO NEED TO CHANGE >  < Common version related attributes - NO NEED TO CHANGE
Import python libs
Import salt libs
check pid alive (Unix only trick!)
forcibly exit, regular sys.exit raises an exception-- which  isn't sufficient in a thread
escalate signal
if multiprocessing does not work
Process exited or was terminated. Since we're going to try to restart  it, we MUST, reset signal handling to the previous handlers
ontop of the random_reauth_delay already preformed  delay extra to reduce flooding and free resources  NOTE: values are static but should be fine.  need to reset logging because new minion objects  cause extra log handlers to accumulate
check pid alive (Unix only trick!)
forcibly exit, regular sys.exit raises an exception-- which  isn't sufficient in a thread
preform delay after minion resources have been cleaned
No salt cloud on Windows
Import Python libs
All salt related deprecation warnings should be shown once each!
While we are supporting Python2.6, hide nested with-statements warnings
Filter the backports package UserWarning about being re-imported
This is now garbage collectable  This is most likely ascii which is not the best but we were  unable to find a better encoding. If this fails, we fall all  the way back to ascii
We can't use six.moves.builtins because these builtins get deleted sooner  than expected. See:     https://github.com/saltstack/salt/issues/21036
Define the detected encoding as a built-in variable for ease of use
This is now garbage collectable
This is now garbage collectable
Import python libs
Import 3rd-party libs
Let's define these custom logging levels before importing the salt.log.mixins  since they will be used there
Import salt libs
Make a list of log level names sorted by log level
Store an instance of the current logging logger class
Store a reference to the temporary queue logging handler
Store a reference to the temporary console logger
Store a reference to the "storing" logging handler
pylint: disable=E1321  pylint: enable=E1321
Not matched. Release handler and return.
No digits group. Release handler and return.
No valid digits. Release handler and return.
There are no registered loggers yet
Override the python's logging logger class as soon as this module is imported
No configuration to the logging system has been done so far.  Set the root logger at the lowest level possible
Add a Null logging handler until logging is configured(will be  removed at a later stage) so we stop getting:    No handlers could be found for logger 'foo'
Add the queue logging handler so we can later sync all message records  with the additional logging handlers
Not a stream handler, continue
There's already a logging handler outputting to sys.stderr
Remove the temporary null logging handler
Remove the temporary logging handler
Not a stream handler, continue
There's already a logging handler outputting to sys.stderr
Set the default console formatter config
Remove the temporary logging handler
The user has set a syslog facility, let's update the path to  the logging socket
This python syslog version does not know about the user provided  facility name
There's not socktype support on python versions lower than 2.7
Et voilá! Finally our syslog handler instance
Logfile logging is UTF-8 on purpose.  Since salt uses YAML and YAML uses either UTF-8 or UTF-16, if a  user is not using plain ASCII, their system should be ready to  handle UTF-8.
Do not proceed with any more configuration since it will fail, we  have the console logging already setup and the user should see  the error.
Don't re-configure external loggers
Explicit late import of salt's loader
Let's keep a reference to the current logging handlers
Load any additional logging handlers
Let's keep track of the new logging handlers so we can sync the stored  log records with them
Keep a reference to the logging handlers count before getting the  possible additional ones.
Make sure we have an iterable
Remove the temporary queue logging handler
Remove the temporary null logging handler (if it exists)
We're not in the MainProcess, return! No Queue shall be instantiated
We're not in the MainProcess, return! No logging listener setup shall happen
We're in the MainProcess, return! No multiprocessing logging setup shall happen
Let's set it to true as fast as possible
The temp null and temp queue logging handlers will store messages.  Since noone will process them, memory usage will grow. If they  exist, remove them.
We're in the MainProcess, return! No multiprocessing logging shutdown shall happen
Let's remove the queue handler from the logging root handlers
We're in the MainProcess and we're not daemonizing, return!  No multiprocessing logging listener shutdown shall happen
We were unable to deliver the sentinel to the queue  carry on...
Process is still alive!?
On Windows, creating a new process doesn't fork (copy the parent  process image). Due to this, we need to setup extended logging  inside this process.
A sentinel to stop processing the queue  Just log everything, filtering will happen on the main process  logging handlers
Already removed
Redefine the null handler to None so it can be garbage collected
Already removed
Redefine the null handler to None so it can be garbage collected
In this case, the temporary logging handler has been removed, return!
This should already be done, but...
Redefine the null handler to None so it can be garbage collected
Python versions >= 2.7 allow warnings to be redirected to the logging  system now that it's configured. Let's enable it.
Do not log the exception or display the traceback on Keyboard Interrupt  Stop the logging queue listener thread
Set our own exception handler as the one to use
Import python libs
If we reached this far it means the log record was created with exc_info_on_loglevel  If this specific handler is enabled for that record, then we should format it to  include the exc_info details  This handler is not enabled for the desired exc_info_on_loglevel, don't include exc_info
If we reached this far it means we should include exc_info  This should actually never occur
Let's format the record to include exc_info just like python's logging formatted does
Import severals classes/functions from salt.log.setup for backwards  compatibility
Import python libs
Import salt libs
Import third party libs
Import python libs
Import salt libs
Import Third party libs
Attempt to import msgpack  There is a serialization issue on ARM and potentially other platforms  for some msgpack bindings, check for it
Define the module's virtual name
Not set? Get the main salt log_level setting on the  configuration file  Also not set?! Default to 'error'
will be retried in emit()
buffering
reconnect if possible
send message
send finished
Import python libs
Import salt libs
Loose the initial log records
If the handler's level is higher than the log record one,  it should not handle the log record
Import python libs
Import salt libs
Import Third party libs
Define the module's virtual name
Not set? Get the main salt log_level setting on the  configuration file  Also not set?! Default to 'error'
Not set? Get the main salt log_level setting on the  configuration file  Also not set?! Default to 'error'
Import python libs
Import salt libs
Import 3rd party libs
Define the module's virtual name
site: An optional, arbitrary string to identify this client installation.  site: An optional, arbitrary string to identify this client  installation
name: This will override the server_name value for this installation.  Defaults to socket.gethostname()
exclude_paths: Extending this allow you to ignore module prefixes  when sentry attempts to discover which function an error comes from
include_paths: For example, in Django this defaults to your list of  INSTALLED_APPS, and is used for drilling down where an exception is  located
list_max_length: The maximum number of items a list-like container  should store.
string_max_length: The maximum characters of a string that should be  stored.
auto_log_stacks: Should Raven automatically log frame stacks  (including locals) all calls as it would for exceptions.
timeout: If supported, the timeout value for sending messages to  remote.
processors: A list of processors to apply to events before sending  them to the Sentry server. Useful for sending additional global state  data or sanitizing data that you want to keep off of the server.
dsn: Ensure the DSN is passed into the client
Import Salt Libs
Don't die on missing transport libs since only one transport is required
Import third party libs
for backwards compatibility  Default to ZeroMQ for now
Import Python libs
Import Tornado libs
Import Salt libs
'tornado.concurrent.Future' doesn't support  remove_done_callback() which we would have called  in the timeout case. Due to this, we have this  callback function outside of FutureWithTimeout.
Reusing a future that has previously been used.  Due to this, no need to call add_done_callback()  because we did that before.
'tornado.concurrent.Future' doesn't support  remove_done_callback(). So we set an attribute  inside the future itself to track what happens  when it completes.
Placeholders for attributes to be populated by method calls
Create singleton map between two sockets
FIXME
FIXME
Handled by singleton __new__
Import Tornado libs
Import Salt libs
Connect to the server
Import Tornado libs
Import Salt libs
Bind to the socket and prepare to run
Start the server
This callback is run whenever a message is received
Placeholders for attributes to be populated by method calls
Import Tornado libs
Import Salt libs
Create a new IO Loop.  We know that this new IO Loop is not currently running.
Connect to the server  Use the associated IO Loop that isn't running.
Remove the timeout once we get some data or an exception  occurs. We will assume that the rest of the data is already  there or is coming soon if an exception doesn't occur.
We read at least one piece of data
In the timeout case, just return None.  Keep 'self._read_stream_future' alive.
Stop the IO Loop so that self.io_loop.start() will return in  read_sync().
This will prevent this message from showing up:  '[ERROR   ] Future exception was never retrieved:  StreamClosedError'
Import python libs
Import Python Libs
Default to ZeroMQ for now
Default to ZeroMQ for now
switch on available ttypes
Import Python Libs
Import Salt Libs
end of buffer
Import Python Libs
Import Tornado Libs
Import third party libs
This class is only a singleton per minion/master pair  mapping of io_loop -> {key -> channel}
has to remain empty for singletons, since __init__ will *always* be called
an init for the singleton instance to call
crypt defaults to 'aes'
we don't need to worry about auth as a kwarg, since its a singleton
Return controle back to the caller, continue when authentication succeeds  Return control to the caller. When send() completes, resume by populating ret with the Future.result
Reauth in the case our key is deleted on the master side.
Return control back to the caller, resume when authentication succeeds
We did not get data back the first time. Retry.
If auth error, return control back to the caller, continue when authentication succeeds
IPv6 sockets work for both IPv6 and IPv4 addresses
Socket monitor shall be used the only for debug  purposes so using threading doesn't look too bad here
always attempt to return an error to the minion
Start the minion command publisher
add some targeting stuff for lists only (for now)
Send list of miions thru so zmq can target them
wire up sockets
mapping of message -> future
socket options
Timedout
Hasn't been already timedout
if a future wasn't passed in, we need to serialize the message
Add this future to the mapping
Import Python Libs
Import Tornado Libs
pylint: disable=import-error,no-name-in-module  pylint: enable=import-error,no-name-in-module
Import third party libs
__setstate__ and __getstate__ are only used on Windows.  We do this so that __init__ will be invoked on Windows in the child  process so that a register_after_fork() equivalent will work on  Windows.
Wait for a connection to occur since the socket is  blocking.  Wait for a free slot to be available to put  the connection into.  Sockets are picklable on Windows in Python 3.
ECONNABORTED indicates that there was a connection  but it was closed while still in the accept queue.  (observed on FreeBSD).
This class is only a singleton per minion/master pair  mapping of io_loop -> {key -> channel}
we need to make a local variable for this, as we are going to store  it in a WeakValueDictionary-- which will remove the item if no one  references it-- this forces a reference while we return to the caller
has to remain empty for singletons, since __init__ will *always* be called
an init for the singleton instance to call
crypt defaults to 'aes'
Convert to 'SaltClientError' so that clients can handle this  exception more appropriately.
always attempt to return an error to the minion
'self.io_loop' initialized in super class  'tornado.tcpserver.TCPServer'.  'self._handle_connection' defined in same super class.
Add the callback only when a new future is created
if the last connect finished, then we need to make a new one
if the last connect finished, then we need to make a new one
this shouldn't ever happen, but just in case
Add this future to the mapping
This will prevent this message from showing up:  '[ERROR   ] Future exception was never retrieved:  StreamClosedError'  This happens because the logic is always waiting to read  the next message and the associated read future is marked  'StreamClosedError' when the stream is closed.
Only when the transport is TCP only, the presence events will  be handled here. Otherwise, it will be handled in the  'Maintenance' process.
This is possible if _remove_client_present() is invoked  before the minion's id is validated.
Since _remove_client_present() is potentially called from  _stream_read() and/or publish_payload(), it is possible for  it to be called twice, in which case we will get here.  This is not an abnormal case, so no logging is required.
We only accept 'aes' encoded messages for 'id'
Write the packed str
Check if io_loop was set outside
run forever
add some targeting stuff for lists only (for now)  Send it over IPC!
Import Python Libs
Import Salt Libs
All Sync interfaces are just wrappers around the Async ones
Resolver is used by Tornado TCPClient.  This static field is shared between  AsyncReqChannel and AsyncPubChannel.  This will check to make sure the Resolver  is configured before first use.
Default to ZeroMQ for now
Default to ZeroMQ for now
switch on available ttypes
FIXME for now, just UXD  Obviously, this makes the factory approach pointless, but we'll extend later
Import Python Libs
Import Salt Libs
Import Third Party Libs
Check if key is configured to be auto-rejected/signed
open mode is turned on, nuts to checks and overwrite whatever  is there
The key is being automatically accepted, don't do anything  here and let the auto accept logic below handle it.
the con_cache is enabled, send the minion id to the cache
Token failed to decrypt, send back the salty bacon to  support older minions
Token failed to decrypt, send back the salty bacon to  support older minions
Import python libs
Import third party libs  pylint: disable=import-error,no-name-in-module,redefined-builtin  pylint: enable=import-error,no-name-in-module,redefined-builtin
resource is not available on windows
Import halite libs
How often do we perform the maintenance tasks  Track key rotation intervals
__setstate__ and __getstate__ are only used on Windows.  We do this so that __init__ will be invoked on Windows in the child  process so that a register_after_fork() equivalent will work on Windows.
For a TCP only transport, the presence events will be  handled in the transport code.
init things that need to be done after the process is forked
Make Start Times  Clean out the fileserver backend cache  Clean out pub auth
Ping all minions to get them to pick up the new key
Check if scheduler requires lower loop interval than  the loop_interval setting
On the first run it may need more time for the EventPublisher  to come up and be ready. Set the timeout to account for this.
Run init() for all backends which support the function, to  double-check configuration
Init any values needed by the git ext pillar
run_reqserver cannot be defined within a class method in order for it  to be picklable.
Reset signals to default ones before adding processes to the process  manager. We don't want the processes being started to inherit those  signal handlers
must be after channels
No need to call this one under default_signals because that's invoked when  actually starting the ReqServer
Install the SIGINT/SIGTERM handlers if not done so far  No custom signal handling was added, install our own
No custom signal handling was added, install our own
escalate the signals to the process manager  kill any remaining processes
__setstate__ and __getstate__ are only used on Windows.  We do this so that __init__ will be invoked on Windows in the child  process so that a register_after_fork() equivalent will work on Windows.
Prepare the AES key
Cannot delete read-only files on Windows.
Also stop the workers
using ZMQIOLoop since we *might* need zmq in there
The minion is not who it says it is!  We don't want to listen to it!
Can overwrite master files!!
The minion is not who it says it is!  We don't want to listen to it!
Route to master event bus  Process locally
The eauth system is not enabled, fail
Bail if the token is empty or if the eauth type specified is not allowed
Accept find_job so the CLI will function cleanly
The eauth system is not enabled, fail
First we need to know if the user is allowed to proceed via any of their group memberships.  If a group_auth_match is set it means only that we have a  user which matches at least one or more of the groups defined  in the configuration file.
A group def is defined and the user is a member  Auth successful, but no matching user found in config
Perform the actual authentication. If we fail here, do not  continue.
Accept find_job so the CLI will function cleanly
Send it!
the jid in clear_load can be None, '', or something else. this is an  attempt to clean up the value before passing to plugins
Announce the job on the event bus
Properly handle EINTR from SIGUSR1
Import Python Libs
Import Salt Libs
This must be present or the Salt loader won't load this module.
Variables are scoped to this module so we can have persistent data  across calls to fns in here.
Set up logging
Define the module's virtual name
This is a collection of the configuration of all running devices under NSO
pylint: disable=import-error,no-name-in-module,redefined-builtin
Import python libs
Here blink them
This is no-op method, which is required but makes nothing at this point.
Invert the current state
Import python libs
Import Salt's libs
This must be present or the Salt loader won't load this module
Want logging!
Send the command to execute
"scrape" the output and return the right fields as a dict
Send the command to execute
"scrape" the output and return the right fields as a dict
Send the command to execute
"scrape" the output and return the right fields as a dict
Send the command to execute
"scrape" the output and return the right fields as a dict
Send the command to execute
"scrape" the output and return the right fields as a dict
Send the command to execute
"scrape" the output and return the right fields as a dict
Send the command to execute
"scrape" the output and return the right fields as a dict
Send the command to execute
"scrape" the output and return the right fields as a dict
Import python libs
This must be present or the Salt loader won't load this module
Variables are scoped to this module so we can have persistent data  across calls to fns in here.
Want logging!
Import Python Libs
Import Salt Libs
This must be present or the Salt loader won't load this module.
Variables are scoped to this module so we can have persistent data  across calls to fns in here.
Set up logging
Define the module's virtual name
Get the correct login details
Try to authenticate with the given user/password combination
If we can't authenticate, continue on to try the next password.  If we have data returned from above, we've successfully authenticated.
We've reached the end of the list without successfully authenticating.
Import python lib
Import third party lib
no exception raised here, means connection established
Import python libs
Import 3rd-party libs
Define the module's virtual name
NXOS does not like non alphanumeric characters.  Using the random module from pycrypto  can lead to having non alphanumeric characters in the salt for the hashed password.
Import python libs
This must be present or the Salt loader won't load this module
Variables are scoped to this module so we can have persistent data  across calls to fns in here.
Want logging!
Save the REST URL
Make sure the REST URL ends with a '/'
Import python libs
Import third party libs
No need for crypt in local mode
Between first checking and the generation another process has made  a key! Use the winner's key
The specified user was not found, allow the backup systems to  report the error
set names for the signing key-pairs
We need __setstate__ and __getstate__ to avoid pickling errors since  some of the member variables correspond to Cython objects which are  not picklable.  These methods are only used when pickling so will not be used on  non-Windows platforms.
This class is only a singleton per minion/master pair  mapping of io_loop -> {key -> auth}
mapping of key -> creds
do we have any mapping for this io_loop
we need to make a local variable for this, as we are going to store  it in a WeakValueDictionary-- which will remove the item if no one  references it-- this forces a reference while we return to the caller
has to remain empty for singletons, since __init__ will *always* be called
The io_loop has a thread Lock which will fail to be deep  copied. Skip it because it will just be recreated on the  new copy.
Make sure all key parent directories are accessible
master and minion sign and verify  master and minion do NOT sign and do NOT verify
This is not the last master we connected to
This class is only a singleton per minion/master pair
has to remain empty for singletons, since __init__ will *always* be called
simple integrity check to verify that we got meaningful data
Import python libs
Import salt libs
Because salt's configuration is very permissive with additioal  configuration settings, let's allow them in the schema or validation  would fail
Import Python libs
Import Salt libs
Pretty naive pattern matching
Import Pythosn libs
Import salt libs
Import python libs
Import third party libs
pylint: disable=import-error,no-name-in-module  pylint: enable=import-error,no-name-in-module
Since an 'ipc_mode' of 'ipc' will never work on Windows due to lack of  support in ZeroMQ, we want the default to be something that has a  chance of working.
The address of the salt master. May be specified as IP address or hostname
The TCP/UDP port of the master to connect to in order to listen to publications
Specify the format in which the master address will be specified. Can  specify 'default' or 'ip_only'. If 'ip_only' is specified, then the  master address will not be split into IP and PORT.
The fingerprint of the master key may be specified to increase security. Generate  a master fingerprint with `salt-key -F master`
Selects a random master when starting a minion up in multi-master mode
When in multi-master mode, temporarily remove a master from the list if a conenction  is interrupted and try another master in the list.
When in multi-master failover mode, fail back to the first master in the list if it's back  online.
When in multi-master mode, and master_failback is enabled ping the top master with this  interval.
The name of the signing key-pair
Sign the master auth-replies with a cryptographic signature of the masters public key.
Enables verification of the master-public-signature returned by the master in auth-replies.  Must also set master_sign_pubkey for this to work
If verify_master_pubkey_sign is enabled, the signature is only verified, if the public-key of the master changes.  If the signature should always be verified, this can be set to True.
The name of the file in the masters pki-directory that holds the pre-calculated signature of the masters public-key.
Instead of computing the signature for each auth-reply, use a pre-calculated signature.  The master_pubkey_signature must also be set for this.
The key fingerprint of the higher-level master for the syndic to verify it is talking to the intended  master
The user under which the daemon should run
The root directory prepended to these options: pki_dir, cachedir,  sock_dir, log_file, autosign_file, autoreject_file, extension_modules,  key_logfile, pidfile:
The directory used to store public key data
A unique identifier for this daemon
The directory to store all cache files.
Flag to cache jobs locally.
The path to the salt configuration file
The directory containing unix sockets for things like the event bus
Specifies how the file server should backup files, if enabled. The backups  live in the cache dir.
A default renderer for all operations on this host
Renderer whitelist. The only renderers from this list are allowed.
Rendrerer blacklist. Renderers from this list are disalloed even if specified in whitelist.
A flag indicating that a highstate run should immediately cease if a failure occurs.
A flag to indicate that highstate runs should force refresh the modules prior to execution
Force the minion into a single environment when it fetches files from the master
Force the minion into a single pillar root when it fetches pillar data from the master
Allows a user to provide an alternate name for top.sls
States to run when a minion starts up
List of startup states
A top file to execute if startup_states == 'top'
Location of the files a minion should look for. Set to 'local' to never ask the master.
When using a local file_client, this parameter is used to allow the client to connect to  a master for remote execution.
A map of saltenvs and fileserver backend locations
A map of saltenvs and fileserver backend locations
The type of hashing algorithm to use when doing file comparisons
FIXME Does not appear to be implemented
FIXME Does not appear to be implemented
Tell the loader to only load modules in this list
A list of additional directories to search for salt modules in
A list of additional directories to search for salt returners in
A list of additional directories to search for salt states in
A list of additional directories to search for salt grains in
A list of additional directories to search for salt renderers in
A list of additional directories to search for salt outputters in
A list of additional directories to search for salt utilities in. (Used by the loader  to populate __utils__)
salt cloud providers
First remove all modules during any sync operation
A flag indicating that a master should accept any minion connection without any authentication
Whether or not processes should be forked when needed. The alternative is to use threading.
Whether or not the salt minion should run scheduled mine updates
Whether or not scheduled mine updates should be accompanied by a job return for the job cache
Schedule a mine update every n number of seconds
The ipc strategy. (i.e., sockets versus tcp, etc)
Enable ipv6 support for daemons
The chunk size to use when streaming files with the file server
The TCP port on which minion events should be published if ipc_mode is TCP
The TCP port on which minion events should be pulled if ipc_mode is TCP
The TCP port on which events for the master should be pulled if ipc_mode is TCP
The TCP port on which events for the master should be pulled if ipc_mode is TCP
The TCP port on which events for the master should pulled and then republished onto  the event bus on the master
The TCP port for mworkers to connect to on the master
The file to send logging data to
The level of verbosity at which to log
The log level to log to a given file
The format to construct dates in log files
The dateformat for a given logfile
The format for console logs
The format for a given log file
A dictionary of logging levels
If an event is above this size, it will be trimmed before putting it on the event bus
Always execute states with test=True if this flag is set
Tell the loader to attempt to import *.pyx cython files if cython is available
Tell the loader to attempt to import *.zip archives
Tell the client to show minions that have timed out
Tell the client to display the jid when a job is published
Tells the highstate outputter to show successful states. False will omit successes.
Specify the format for state outputs. See highstate outputter for additional details.
Tells the highstate outputter to only report diffs of states that changed
When true, states run in the order defined in an SLS file, unless requisites re-order them
Fire events as state chunks are processed by the state compiler
The number of seconds a minion should wait before retry when attempting authentication
The number of seconds a minion should wait before giving up during authentication
Retry a connection attempt if the master rejects a minion's public key
The interval in which a daemon's main loop should attempt to perform all necessary tasks  for normal operation
Perform pre-flight verification steps before daemon startup, such as checking configuration  files and certain directories.
The grains dictionary for a minion, containing specific "facts" about the minion
Allow a daemon to function even if the key directories are not secured
The path to a directory to pull in configuration file includes
If a minion is running an esky build of salt, upgrades can be performed using the url  defined here. See saltutil.update() for additional information
If using update_url with saltutil.update(), provide a list of services to be restarted  post-install
The number of seconds to sleep between retrying an attempt to resolve the hostname of a  salt master
set the zeromq_reconnect_ivl option on the minion.  http://lists.zeromq.org/pipermail/zeromq-dev/2011-January/008845.html
If recon_randomize is set, this specifies the lower bound for the randomized period
Tells the minion to choose a bounded, random interval to have zeromq attempt to reconnect  in the event of a disconnect event
Specify a returner in which all events will be sent to. Requires that the returner in question  have an event_return(event) function!
The number of events to queue up in memory before pushing them down the pipe to an event returner  specified by 'event_return'
Only forward events to an event returner if it matches one of the tags in this list
Events matching a tag in this list should never be sent to an event returner.
default match type for filtering events tags: startswith, endswith, find, regex, fnmatch
This pidfile to write out to when a daemon starts
Used with the SECO range master tops system
The tcp keepalive interval to set on TCP ports. This setting can be used to tune salt connectivity  issues in messy network environments with misbehaving firewalls
Sets zeromq TCP keepalive idle. May be used to tune issues with minion disconnects
Sets zeromq TCP keepalive count. May be used to tune issues with minion disconnects
Sets zeromq TCP keepalive interval. May be used to tune issues with minion disconnects.
The network interface for a daemon to bind to
The port for a salt master to broadcast publications on. This will also be the port minions  connect to to listen for publications.
Set the zeromq high water mark on the publisher interface.  http://api.zeromq.org/3-2:zmq-setsockopt
ZMQ HWM for SaltEvent pub socket  ZMQ HWM for EventPublisher pub socket
The number of MWorker processes for a master to startup. This number needs to scale up as  the number of connected minions increases.
The port for the master to listen to returns on. The minion needs to connect to this port  to send returns.
The number of hours to keep jobs around in the job cache on the master
A master-only copy of the file_roots dictionary, used by the state compiler
Specify a list of external pillar systems to use
Reserved for future use to version the pillar structure
Whether or not a copy of the master opts dict should be rendered into minion pillars
Cache the master pillar to disk to avoid having to pass through the rendering system
Pillar cache TTL, in seconds. Has no effect unless `pillar_cache` is True
Pillar cache backend. Defaults to `disk` which stores caches in the master cache
When creating a pillar, there are several strategies to choose from when  encountering duplicate values
Recursively merge lists by aggregating them instead of replacing them.
How to merge multiple top files from multiple salt environments  (saltenvs); can be 'merge' or 'same'
The ordering for salt environment merging, when top_file_merging_strategy  is set to 'same'
The salt environment which provides the default top file when  top_file_merging_strategy is set to 'same'; defaults to 'base'
The number of open files a daemon is allowed to have open. Frequently needs to be increased  higher than the system default in order to account for the way zeromq consumes file handles.
Automatically accept any key provided to the master. Implies that the key will be preserved  so that subsequent connections will be authenticated even if this option has later been  turned off.
A mapping of external systems that can be used to generate topfile data.
A flag that should be set on a top-level master when it is ordering around subordinate masters  via the use of a salt syndic
Whether or not to cache jobs so that they can be examined later on
Define a returner to be used as an external job caching storage backend
Specify a returner for the master to use as a backend storage system to cache jobs returns  that it receives
Specify whether the master should store end times for jobs as returns come in
The number of seconds between AES key rotations on the master
Defines a salt reactor. See http://docs.saltstack.com/en/latest/topics/reactor/
The TTL for the cache of the reactor configuration
The number of workers for the runner/wheel in the reactor
The queue size for workers in the reactor
Defines engines. See https://docs.saltstack.com/en/latest/topics/engines/
The update interval, in seconds, for the master maintenance process to update the search  index
A compound target definition. See: http://docs.saltstack.com/en/latest/topics/targeting/nodegroups.html
List-only nodegroups for salt-ssh. Each group must be formed as either a  comma-separated list, or a YAML list.
The logfile location for salt-key
The source location for the winrepo sls files  (used by win_pkg.py, minion only)
Set a hard limit for the amount of memory modules can consume on a minion.
The number of minutes between the minion refreshing its cache of grains
Use lspci to gather system data for grains on a minion
The number of seconds for the salt client to wait for additional syndics to  check in with their lists of expected minions before giving up
If this is set to True leading spaces and tabs are stripped from the start  of a line to a block.
If this is set to True the first newline after a Jinja block is removed
Cache minion ID to file
If set, the master will sign all publications before they are sent out
The size of key that should be generated when creating new keys
The transport system for this daemon. (i.e. zeromq, raet, etc)
The number of seconds to wait when the client is requesting information about running jobs
The number of seconds to wait before timing out an authentication request
The number of attempts to authenticate to a master before giving up
Never give up when trying to authenticate to a master
An upper bound for the amount of time for a minion to sleep before attempting to  reauth after a restart.
The number of seconds for a syndic to poll for new messages that need to be forwarded
The number of seconds for the syndic to spend polling the event bus
The length that the syndic event queue must hit before events are popped off and forwarded
Enable ioflo verbose logging. Warning! Very verbose!
Set ioflo to realtime. Useful only for testing/debugging to simulate many ioflo periods very quickly.
Location for ioflo logs
Instructs the minion to ping its master(s) ever n number of seconds. Used  primarily as a mitigation technique against minion disconnects.
Instructs the salt CLI to print a summary of a minion responses before returning
The maximum number of minion connections allowed by the master. Can have performance  implications in large setups.
Use zmq.SUSCRIBE to limit listening sockets to only process messages bound for them
Connection caching. Can greatly speed up salt performance.
Cache ZeroMQ connections. Can greatly improve salt performance.
Can be set to override the python_shell=False default in the cmd module
Used strictly for performance testing in RAET.
Used by salt-api for master requests timeout
If set, all minion exec module actions will be rerouted through sudo as this user
HTTP request timeout in seconds. Applied for tornado http fetch functions like cp.get_url should be greater than  overall download time.
HTTP request max file content size.
Delay in seconds before executing bootstrap (salt cloud)
Command to use to restart salt-minion
Whether or not a minion should send the results of a command back to the master  Useful when a returner is the source of truth for a job result
ZMQ HWM for SaltEvent pub socket - different for minion vs. master  ZMQ HWM for EventPublisher pub socket - different for minion vs. master
Salt master settings overridden by Salt-API >  < Salt master settings overridden by Salt-API
Bare type name won't have a length, return the name of the type  passed.
When the passed path is None, we just want the configuration  defaults, not actually loading the whole configuration.
Default to the environment variable path, if it exists  If non-default path from `-c`, use that over the env variable
When the passed path is None, we just want the configuration  defaults, not actually loading the whole configuration.
The user has explicitly defined the syndic master port  No syndic_master_port, grab master_port from opts  No master_opts, grab from the provided minion defaults  Not on the provided minion defaults, load from the  static minion defaults
Convert relative to absolute paths if necessary
The configuration setting is being specified in the main cloud  configuration file
Convert relative to absolute paths if necessary
The configuration setting is being specified in the main cloud  configuration file
Convert relative to absolute paths if necessary
Prepare the deploy scripts search path
Path exists, let's update the entry (its path might have been  made absolute)
It's not a directory? Remove it from the search path
Let's make the search path a tuple and add it to the overrides.
Override master configuration with the salt cloud(current overrides)  We now set the overridden master_config as the overrides
use the value from the cloud config file  if not found, use the default path
use the value from the cloud config file  if not found, use the default path
Apply the salt-cloud configuration
No exception was raised? It's the old configuration alone
Load from configuration file, even if that files does not exist since  it will be populated with defaults.
Let's assign back the computed providers configuration
4th - Include VM profiles config  Load profiles configuration from the provided file
recurse opts for sdb configs
Return the final options
Weird, but...
Weird, but...
Migrate old configuration
Since using "provider: <provider-engine>" is deprecated, alias provider  to use driver: "driver: <provider-engine>"
provider alias  provider alias, provider driver
Update the profile's entry with the extended data
Is the user still using the old format in the new configuration file?!
Let's help out and migrate the data
old_to_new will migrate the old data into the 'providers' key of  the config dictionary. Let's map it correctly
Since using "provider: <provider-engine>" is deprecated,  alias provider to use driver: "driver: <provider-engine>"
Set a holder for the defined profiles
change provider details '-only-extendable-' to extended  provider name
We're still not aware of what we're trying to extend  from. Let's try on next iteration
Merge provided extends
Extends resolved or non existing, continue!
Since there's a nested extends, resolve this one in the  next iteration
As a last resort, return the default
The setting name exists in the cloud(global) configuration
Since using "provider: <provider-engine>" is deprecated, alias provider  to use driver: "driver: <provider-engine>"
If we reached this far, the provider included all required keys
If we reached this point, the provider is not configured.
Standard dict keys required by all drivers.
Most drivers need an image to be specified, but some do not.
If cloning on Linode, size and image are not necessary.  They are obtained from the to-be-cloned VM.
If cloning on VMware, specifying image is not necessary.
Check if required fields are supplied in the provider config. If they  are present, remove it from the required_keys list.
If a vm_ dict was passed in, use that information to get any other configs  that we might have missed thus far, such as a option provided in a map file.
Check for cached minion ID
No ID provided. Will getfqdn save us?
it does not make sense to append a domain to an IP based id
Enabling open mode requires that the value be set to True, and  nothing else!
Set up the utils_dirs location from the extension_modules location
Insert all 'utils_dirs' directories to the system path
Prepend root_dir to other paths
These can be set to syslog, so, not actual paths on the system
if there is no beacons option yet, add an empty beacons dict
if there is no schedule option yet, add an empty scheduler
Make sure hash_type is lowercase
it does not make sense to append a domain to an IP based id
These can be set to syslog, so, not actual paths on the system
Enabling open mode requires that the value be set to True, and  nothing else!
If file_ignore_regex was given, make sure it's wrapped in a list.  Only keep valid regex entries for improved performance later on.
Can't store compiled regex itself in opts (breaks  serialization)
If file_ignore_glob was given, make sure it's wrapped in a list.
Make sure hash_type is lowercase
Return the client options
Let's grab a copy of salt's master default opts  Let's override them with salt-api's required defaults
Let's grab a copy of salt's master default opts  Let's override them with spm's required defaults
Prepend root_dir to other paths
These can be set to syslog, so, not actual paths on the system
Import Python Libs
Using bitmask to emulate rollover behavior of C unsigned 32 bit int
With default splaytime
Import python libs
Import salt libs
Import python libs
Let's find out the path of this module  This is from the exec() call in Salt's setup.py
The installation time value was not provided, let's define the default
Import python libs
Import salt libs
if any error occurs, we return an empty dictionary
data input to the first render function in the pipe  Template is nothing but whitespace
Get the list of render funcs in the render pipe line.
The file is empty or is being written elsewhere
ret is not a StringIO, which means it was rendered using  yaml, mako, or another engine which renders to a data  structure. We don't want to log this, so ignore this  exception.
pull out the shebang data
A dict of combined renderer (i.e., rend1_rend2_...) to  render-pipe (i.e., rend1|rend2|...)
Import python libs
Import salt libs
init.sls
redis.sls
lib.sls
redis.conf
The following two function calls are equivalent.
Import python libs
Import salt libs
Import 3rd-party libs
create an empty object that we can add attributes to
Import python libs
Import salt libs
Import 3rd party libs
Import salt libs
Import python libs
here also
-*- coding: utf-8 -*-
Import python libs
Import salt libs
Import python libs
Import salt libs
Import 3rd party libs
Import salt libs
Import python libs
Import salt libs
some-other-package is defined in some other state file
Import Python Libs
Import Salt Libs
these hold the scope that our sls file will be executed with
add our include and extend functions
add our map class
the "dunder" formats are still available for direct use
if salt_data is not True then we just return the global scope we've  built instead of returning salt data from the registry
this will be used to fetch any import files
if we don't have a third group in the matches object it means  that we're importing everything
process the template that triggered the render
re-enable the registry
now exec our template using our created scopes
Import python libs
Import salt libs
you can call requisites on function declaration
or you can call requisites on state declaration.  this actually creates an anonymous function declaration object  to add the requisites.
we still need to set the name of the function declaration.
including multiple sls returns a list.
including a single sls returns a single object
myfunc is a function that calls state(...) to create more states.
configure it
ensure that states in xxx run BEFORE states in this file.
ensure that states in yyy run AFTER states in this file.
to workaround state.py's use of copy.deepcopy(chunk)
Import python libs
Import salt libs
Import 3rd-party libs
-*- coding: utf-8 -*-
Import third party libs
Import salt libs
Import python libs
Import salt libs
Import 3rd-party libs
make a copy so that the original, un-preprocessed highstate data  structure can be used later for error checking if anything goes  wrong during the preprocessing.
We must extract no matter what so extending a stateconf sls file  works!
first pass to extract the state configuration
if some config has been extracted then remove the sls-name prefix  of the keys in the extracted stateconf.set context to make them easier  to use in the salt file.
do a second pass that provides the extracted conf as template context
To avoid cycles among states when each state requires the one before it:    explicit require/watch can only contain states before it    explicit require_in/watch_in can only contain states after it
add a (- state: sid) item, at the beginning of the require of this  state if there's a state before this one.
Then id declaration must have been included from a  rendered sls. Currently, this is only possible with  pydsl's high state output.
Quick and dirty way to get attribute access for dictionary keys.  So, we can do: ${apache.port} instead of ${apache['port']} when possible.
Import 3rd party libs
Import salt libs
Import python libs
Import salt libs
Define the module's virtual name
Import python libs
Import salt libs
Import 3rd-party libs
Import python libs
Import salt libs
Import third party libs
Define the module's virtual name
Import python libs
Import 3rd-party libs
Python >2.5
Python >2.5
normal cElementTree install
normal ElementTree install
True if we are running on Python 3.
import python libs
Import python libs  import salt libs
Import python libs
Import Salt libs
import python libs
Import salt libs
Import python libs  import salt libs
import python libs
Import salt libs
import python libs
Import Python libs
Import Salt libs
Function aliases
Define the module's virtual name
extract header
prepare faults
extract header
parse entries
prepare component
keying
we hit a divider  we have data, store it and reset
we don't have all data, colelct more
if we do not have the header, store it
if we have the header but no data, store the data and parse it
we have data, store it and reset
Import salt libs
Define the module's virtual name
Remove the User Profile directory
Load information for the current name
Look for an existing user with the new name
Rename the user account  Connect to WMI
Get the user object
Rename the user
Import python libs
Import Salt libs
Support old-style grains/pillar  config as well as new.
prepare authentication
Make the HTTP request
Trying tomcat >= 7 url
Trying tomcat6 url
Deploy
Cleanup
Import Python libs
Import salt libs
Set up logging
Define the module's virtual name
enabled service should have runlevels  in any other case service is disabled
Import python libs
Import salt libs
Import 3rd-party libs
permits deleting elements rather than using slices
match against all known/supported subcmds  apply subcommand requires a manifest file to execute
no arguments are required
finally do this after subcmd has been matched for all remaining args
Loop over the facter output and  properly  parse it into a nice dictionary for using  elsewhere
Import python libs
Import salt libs
Define a function alias in order not to shadow built-in's
Define the module's virtual name
Import salt libs
Import python libs
Import salt libs
Function alias to not shadow built-ins.
which dir sv works with
available service directory(ies)
Define the module's virtual name
service does not exist
service available but not enabled
default: return available services
exhaustive check instead of (only) os.path.exists(_service_path(name))
return True for a non-existent service
non-existent service
if service is aliased, refuse to enable it
down_file: file that disables sv autostart
enable the service
(attempt to) remove temp down_file anyway
if an error happened, revert our changes
non-existent as registrered service
down_file: file that prevent sv autostart
Import python libs
Import salt libs
Import salt libs
Import python libs
Non-string passed
make sure all entries are IPs
make sure all entries are IPs
empty string is successful query, but nothing to return. So, try TXT  record.
Run a lookup on the part after 'redirect=' (9 chars)
Regex was not matched
Let lowercase work, since that is the convention for Salt functions
only valid in proxy minions for now
Import python libs  SmartOS joyent_20130322T181205Z does not have spwd
Import salt libs
Define the module's virtual name
For example:    root      NL  We have all fields:    buildbot L 05/09/2013 0 99999 7
Import python libs
Import salt libs
Set up logging
Define the module's virtual name
Putting quotes around the parameter protects against command injection
Putting quotes around the parameter protects against command injection
Make sure the path exists
If the name of the config isn't passed, make it the name of the .ps1
Run the script and see if the compile command is in the script
Script compiled, return results
Script compiled, return results
Make sure the path exists
Execute Config to create the .mof
Import python libs
Import 3rd-party libs
Import salt libs
Define the module's virtual name
Remove the prefix from the name.
Some fields are formatted like '{data}'. Salt tries to convert these to dicts  automatically on input, so convert them back to the proper format.
Get the settings post-change so that we can verify tht all properties  were modified successfully. Track the ones that weren't.
Since IIsSmtpServerSetting stores the log type as an id, we need  to get the mapping from IISLogModuleSetting and extract the name.
It's okay to accept an empty list for set_connection_ip_list,  since an empty list may be desirable.
Convert addresses to the 'ip_address, subnet' format used by  IIsIPSecuritySetting.
Order is not important, so compare to the current addresses as unordered sets.
First we should check GrantByDefault, and change it if necessary.
Import Python Libs
Import salt libs
Import salt libs
keep lint from choking on _get_conn and _cache_id
Import Python libs
Import Salt libs
Import python libs
Import salt libs
Define the module's virtual name
Define a function alias in order not to shadow built-in's
Older parted (2.x) doesn't show disk flags in the 'print'  output, and will return a 7-column output for the info  line. In these cases we just leave this field out of the  return dict.
Import python libs
Import salt libs
Set up logging
Define the module's virtual name
Follow symbolic links of files in _launchd_paths  ignore broken symlinks
This assumes most of the plist files  will be already in XML format
Match on label
Match on full path
Match on basename
The service is not loaded, further, it might not even exist  in either case we didn't get XML to parse, so return an empty  dict
Import python libs
Import salt libs
Import Python libs
Import Salt libs
Import third party libs
Don't shadow built-ins.
We have to replace the minion/master directories
We have to differentiate between RaetKey._check_minions_directories  and Zeromq-Keys. Raet-Keys only have three states while ZeroMQ-keys  have an additional 'denied' state.
key dir kind is not created yet, just skip
The retcode status comes from the first kill signal
Import Python libs
Import Salt libs
Define the module's virtual name
keep lint from choking on _get_conn and _cache_id
Import Python libs
Import third party libs  pylint: disable=unused-import  pylint: enable=unused-import
Import python libs
Import salt libs
Import 3rd-party libs
Import python libs
Only available on POSIX systems, nonfatal on windows
Define the module's virtual name
Set up logging
Default to python_shell=True when run directly from remote execution  system. Cross-module calls won't have a jid.  Override-switch for python_shell
render the path as a template using path_template_engine as the engine
Set the default working directory to the home directory of the user  salt-minion is running as. Defaults to home directory of user under which  the minion is running.
make sure we can access the cwd  when run from sudo or another environment where the euid is  changed ~ will expand to the home of the original uid and  the euid might not have access to it. See issue 1844
Handle edge cases where numeric/other input is entered, and would be  yaml-ified into non-string types
If we were called by script(), then fakeout the Windows  shell to run a Powershell script.  Else just run a Powershell command.
munge the cmd and cwd through the template
On Windows set the codepage to US English.
close_fds is not supported on Windows platforms if you redirect  stdin/stdout/stderr
ok return code for timeouts?
Will happen if redirect_stderr is True, since stderr was sent to  stdout.
Ignore the context error during grain generation
Rewrite file
Avoids errors from msgpack not being loaded in salt-ssh
Execute chroot routine
No known method of determining available shells
Append PowerShell Object formatting
ignore_retcode=ignore_retcode,  password=kwargs.get('password', None),
Import python libs
Import salt libs
Function alias to make sure not to shadow built-in's
If package is already installed, Bower will emit empty dict to STDOUT
If package is not installed, Bower will emit empty dict to STDOUT
Define the module's virtual name
pylint: disable=import-error,no-name-in-module  pylint: enable=import-error,no-name-in-module
Import python libs
Import salt libs
Define the module's virtual name
Import python libs
Import salt libs
Search $PATH for the newalises command
Import python libs
Import salt libs
Added args to file
Config file must be on system to create a poudriere jail
Check if the jail is there
Make jail pkgng aware
Make sure the jail was created
Make sure jail is gone
Could not find jail.
Import python libs
Import salt libs
Import python libs
Import salt libs
Import Python libs
Import Salt libs
turn off all profiles
Import python libs
Import salt libs
Define the module's virtual name
Refresh before looking for the latest version available
Return a string if only one package name passed
available_version is being deprecated
Missing return value check due to opkg issue 160
not yet implemented or not applicable
do not store duplicated uri's
explicit refresh after a repo is deleted
Import Python libs
Import Salt libs
Import Python Futures
Import Python libs
Import Salt libs
Import 3rd-party libs  pylint: disable=import-error
Define the module's virtual name
make sure we override default timeout of docker-py  only if defined by user.
Let docker-py auto detect docker version incase  it's not defined by user.
Check if the DOCKER_HOST environment variable has been set
Optionally for each container get more granular information from them  by inspecting the container
Do stuff with byte.
no need to check if container is running  because some signals might not stop the container.
Valid statest when pulling an image from the docker registry
Import salt libs
Import python libs
Import python libs
Import 3rd-party libs
Fix included in py2-ipaddress for 32bit architectures  Except that xrange only supports machine integers, not longs, so...
Import salt libs
Set up logging
Check for required arguments
Family only valid for certain set types
Using -exist to ensure entries are updated if the comment changes
Entry doesn't exist in set return false
Set doesn't exist return false
Set doesn't exist return false
Only split if item has a colon
Import Python libs
Import Salt libs
Import 3rd-party libs
Don't shadow built-in's.
These are needed during building of the configuration tree
must be a list
return first 3 characters
The format of the first line in the output is:  syslog-ng 3.6.0alpha0
Import python libs
Import third party libs
Import salt libs
Preserve backwards compatibility
Allow pillar.data to also be used to return pillar data
Provide a jinja function call compatible get aliased as fetch
Import Python libs
Import Salt libs
Function aliases
Define the module's virtual name
Import python libs
Import salt libs
Import python libs
Define the module's virtual name
Still not implemented on windows
Need to search for a way to figure it out ...
Need to search for a way to figure it out ...
Import python libs
Import salt libs
Import python libs
Import salt libs
Import 3rd-party libs
get the first: [alias][driver][vm_name]
Import python libs
Import third party libs
Import salt libs
Cache package from remote source (salt master, HTTP, FTP) and  append the cached path.
Return a string if no globbing is used, and there is one item in the  return dict
Import python libs
Set up logger
Cleanup the failed installation so it doesn't list as installed
Import python libs
Import salt libs
This may be unnecessary, but I am paranoid
Import Python Libs
Import python libs
Python < 2.7 does not have importlib
Import salt libs
Define the module's virtual name
xend local UNIX socket
VM.add_to_VCPUs_params_live() implementation in xend 4.1+ has  a bug which makes the client call fail.  That code is accurate for all others XenAPI implementations, but  for that particular one, fallback to xm / xl instead.
virtual_subtype isn't set everywhere.
there must be a smarter way...
Divide by vcpus to always return a number between 0 and 100
Import python libs
Import salt libs
need to update rpm with public key
have the prompt for inputting the passphrase
0.125 is really too fast on some systems
Disable INFO level logs from requests/urllib3
We need to save the has for later operations
Import Python Libs
Import Salt Libs
Import python libs
Import 3rd-party libs
Import salt libs
Seed the grains dict so cython will build
Change the default outputter to make it more readable
http://stackoverflow.com/a/12414913/127816
Get val type
Provide a jinja function call compatible get aliased as fetch
Import python libs
Import salt libs
remove any escape characters
Return False if could not find the interface
Return true if configured
If configured in the wrong order delete it
remove any escape characters
Import python libs
Import salt libs
Import Python libs
channel must start with a hash
Slack wants the body on POST to be urlencoded.
salt-call -l debug dockercompose.create /tmp/toto '
Import Python libs
If it's not a section, it may already exist as a  commented-out key/value pair
Import python libs
Import salt libs
Import 3rd-party libs
Define the module's virtual name
Import python libs
Import salt libs
Define the module's virtual name
Import python libs
Import salt libs
Import 3rd-party libs
Define the module's virtual name
Dunno why it would, but...
Refresh before looking for the latest version available
These are explanation comments
available_version is being deprecated
not yet implemented or not applicable
Some versions of pkgin check isatty unfortunately  this results in cases where a ' ' or ';' can be used
Support old "repo" argument
There is not easy way to upgrade packages with old package system
Import python libs
Import salt libs
Set up logging
Define the module's virtual name
Import python libs
Import salt libs
ecdsa defaults to ecdsa-sha2-nistp256  otherwise enc string is actual encoding string
Commented Line
get "{options} key"  not an auth ssh key, perhaps a blank line
Not a valid line
It has options, grab them
Due to the ability for a single file to have multiple keys, it's  possible for a single call to this function to have both "replace"  and "new" as possible valid returns. I ordered the following as I  thought best.
Remove the key
Return something sensible if the file doesn't exist
Read every line in the file to find the right ssh key  and then write out the correct one. Open the file once  Commented Line
get "{options} key"  not an auth ssh key, perhaps a blank line
Not a valid line
This is the key we are "deleting", so don't put  it in the list of keys to be re-added back
Let the context manager do the right thing here and then  re-open the file in write mode to save the changes out.
If SELINUX is available run a restorecon on the file
The following list of OSes have an old version of openssh-clients  and thus require the '-t' option for ssh-keyscan
remove everything we had in the config so far  set up new value
ensure ~/.ssh exists
set proper ownership/permissions
only one so convert to list
no home directory, skip
Add the default public keys
Add the default private keys
if not full path, assume key is in .ssh  in user's home directory
clean up any empty items
Raise a CommandExecutionError
Import python libs
Import salt libs
Import 3rd-party libs  pylint: disable=import-error,no-name-in-module
Set up logging
Don't shadow built-in's.
clone_from should default to None if not available
Overlay the kwargs to override matched profile data
coming from elsewhere
if we have a profile name, get the profile and load the network settings  this will obviously by default  look for a profile called "eth0"  or by what is defined in nic_opts  and complete each nic settings by sane defaults
only one network gateway ;)
be sure to reset harmful settings
2 step rendering to be sure not to open/wipe the config  before as_string succeeds.
this might look like the function name is shadowing the  module, but it's not since the method belongs to the class
let kwarg overrides be the preferred choice
remove the above three variables from kwargs, if they exist, to avoid  duplicates if create() is invoked below.
Changes is a pointer to changes_dict['init']. This method is used so that  we can have a list of changes as they are made, providing an ordered list  of things that were changed.
let kwarg overrides be the preferred choice
Required params for 'download' template
Return the profile match if the the kwarg match was None, as the  lxc.present state will pass these kwargs set to None by default.
some backends won't support some parameters
please do not merge extra conflicting stuff  inside those two line (ret =, return)
let kwarg overrides be the preferred choice
please do not merge extra conflicting stuff  inside those two line (ret =, return)
This will be a no-op but running the function will give us a pretty  return dict.
Gracefully stopping a frozen container is slower than unfreezing and  then stopping it (at least in my testing), so if we're not  force-stopping the container, unfreeze it first.
Compatibility between LXC and nspawn
container may be just created but we did cached earlier the  lxc-ls results
The size is the 2nd column of the last line
Ensure an empty changes dict if nothing was modified
only cache result if we got a known exit code
Bad input, but assume since a value was passed that  a delay was desired, and sleep for 5 seconds
mark seeded upon successful install
Set context var to make cmd.run_chroot run cmd.run instead of  cmd.run_all.
Destination file does not exist or could not be accessed
let kwarg overrides be the preferred choice
Import Python libs
Import Salt libs
Import 3rd-party libs
Import salt libs
Build the search string
Initialize the PyCom system
Create a session with the Windows Update Agent
Create a searcher object
If no categories were passed, return all categories  Set categoryMatch to True  Loop through each category found in the update  If the update category exists in the list of categories  passed, then set categoryMatch to True
If no severities were passed, return all categories  Set severity_match to True  If the severity exists in the list of severities passed, then set  severity_match to True
Count the total number of updates available
Updates available for download
Updates downloaded awaiting install
Updates installed
Add Categories and increment total for each one  The sum will be more than the total because each update can have  multiple categories
Add Severity Summary
Severity of the Update  Can be: Critical, Important, Low, Moderate, <unspecified or empty>
This value could easily be confused with the Reboot Behavior value  This is stating whether or not the INSTALLED update is awaiting  reboot
Interpret the RebootBehavior value  This value is referencing an update that has NOT been installed
Add categories (nested list)
Recommended Usage using GUID without braces  Use this to find the status of a specific update
Using a KB number (could possibly return multiple results)  Not all updates have an associated KB
Using part or all of the name of the update  Could possibly return multiple results  Not all updates have an associated KB
Initialize the PyCom system
Create a session with the Windows Update Agent
Create the searcher
Create the found update collection
Try searching for the GUID first
Populate wua_found  Found using GUID so there should only be one  Add it to the collection  Not found using GUID  Try searching the title for the Name or KB
Normal Usage (list all software updates)
List all updates with categories of Critical Updates and Drivers
List all Critical Security Updates
List all updates with a severity of Critical
A summary of all available updates
A summary of all Feature Packs and Windows 8.1 Updates
Get the list of updates
Filter the list of updates
If the list is empty after filtering, return a message
Check for empty GUID
Initialize the PyCom system
Create a session with the Windows Update Agent
Create the Searcher, Downloader, Installer, and Collections
Check the download list  Not necessarily a failure, perhaps the update has been downloaded
Download the updates
Check for empty GUID
Initialize the PyCom system
Create a session with the Windows Update Agent
Download the updates  Not necessarily a failure, perhaps the update has been downloaded  but not installed  Otherwise, download the update
Install the updates  Make sure the update has actually been downloaded
There are not updates to install  This would only happen if there was a problem with the download  If this happens often, perhaps some error checking for the download
Try to run the installer
See if we know the problem, if not return the full error
Initialize the PyCom system
Create an AutoUpdate object
Create an AutoUpdate Settings Object
Microsoft Update requires special handling  First load the MS Update Service Manager
Give it a bogus name
msupdate is true, so add it to the services
Consider checking for -2147024891 (0x80070005) Access Denied
msupdate is false, so remove it from the services  check to see if the update is there or the RemoveService function  will fail  Service found, remove the service
Consider checking for the following  -2147024891 (0x80070005) Access Denied  -2145091564 (0x80248014) Service Not Found (shouldn't get  this with the check for _get_msupdate_status above
Initialize the PyCom system
Create an AutoUpdate object
Create an AutoUpdate Settings Object
To get the status of Microsoft Update we actually have to check the  Microsoft Update Service Manager  Create a ServiceManager Object
Return a collection of loaded Services
Loop through the collection to find the Microsoft Udpate Service  If it exists return True otherwise False
Initialize the PyCom system
Create an AutoUpdate object
Import python libs
Import salt libs
Import python libs
Import Salt libs
Import python libs
Import salt libs
Define the module's virtual name
Defaults for Ubuntu, Debian, Arch, and others
Import python libs
Import salt libs
get http data
parse the data
return the good stuff
Import python libs
Import salt libs
Set up logging
compatibility for api change
Import python libs
Import salt libs
Import third party libs
Import python libs
Import salt libs
Import 3rd-party libs
Set up logger
Define the module's virtual name
Disable on Windows, a specific file module exists:
preserve data format
Note: not clear why we always return 'True'  --just copying previous behavior at this point...
Fix the opts type if it is a list
generate entry and criteria objects, handle invalid keys in match_on
parse file, use ret to cache status
Note: If ret isn't None here,  we've matched multiple lines
add line if not present or changed
The line was changed, commit it!
Update automount
Fix the opts type if it is a list
Commented
Blank line
Invalid entry
The line was changed, commit it!
The right entry is already here
The entry is new, add it to the end of the fstab
The line was changed, commit it!
Commented
Blank line
Invalid entry
Darwin doesn't expect defaults when mounting without other options
No point in running ldd on a command that doesn't exist
Import python libs
Define the module's virtual name
Import third party libs
Import Python LIbs
inspector_allowed overrides inspector_ignored.  I.e. if allowed is not empty, then inspector_ignored is completely omitted.
Import Python Libs
Import Salt Libs
Get list of all available packages
Dpkg meta data is unreliable. Check every package  and remove which actually does not have config files.
Just silencing os.kill exception if no such PID, therefore try/pass.
Double-fork stuff
Import Python Libs
Import Salt Libs
System does not supports all accounts descriptions, just skipping.
Get patterns
Get packages
Get repositories
Import salt libs
We're on a node status line
Import salt libs
Define the module's virtual name
We found what we wanted, let's break out of the loop
We found what we wanted, let's break out of the loop
/etc/usermgmt.conf not present: defaults will be used
Command executed with no errors
There's a known bug in Debian based distributions, at least, that  makes the command exit with 12, see:   https://bugs.launchpad.net/ubuntu/+source/shadow/+bug/1023509
We've hit the bug, let's log it and not fail
Put GECOS info into a list  Make sure our list has at least four elements
Import python libs
Import salt libs
Define a function alias in order not to shadow built-in's
This is the new config line that will be set
Import python libs
Import salt libs
Define the module's virtual name
keep lint from choking on _get_conn and _cache_id
Import Python libs
Import Salt libs
IdentityPoolName and AllowUnauthenticatedIdentities are required for the call to update_identity_pool
we can only set DeveloperProviderName one time per AWS.
Import python libs
Define the module's virtual name
provided list is '': users previously deleted from group
Import Python libs
Effectively a no-op, since we can't really return without an event system
Effectively a no-op, since we can't really return without an event system
Effectively a no-op, since we can't really return without an event system
Effectively a no-op, since we can't really return without an event system
Effectively a no-op, since we can't really return without an event system
Effectively a no-op, since we can't really return without an event system
Effectively a no-op, since we can't really return without an event system
Effectively a no-op, since we can't really return without an event system
Import python libs
Import salt libs
Define the module's virtual name
Gracefully continue on invalid lines
Import salt libs
get list of available modules
Make sure the session is still valid
Make sure that the request hasn't been canceled
Get a list of the current potential lock holders. If they change,  notify our wake_event object. This is used to unblock a blocking  self._inner_acquire call.
If there are leases available, acquire one
Check if our acquisition was successful or not. Update our state.
Return current state
forcibly get the lock regardless of max_concurrency
if someone passed in zk_hosts, and the path isn't in SEMAPHORE_MAP, lets  see if we can find it
Import python libs
Import salt libs
Import third party libs
Import python libs
Import salt libs
Define the module's virtual name
Common options to virtualenv and pyvenv
Finally the virtualenv path
Let's create the virtualenv  Something went wrong. Let's bail out now!
Install setuptools
clear up the distribute archive which gets downloaded
Something went wrong. Let's bail out now!
Import python libs
Import 3rd Party Libraries
Define the module's virtual name
Define Constants  TASK_ACTION_TYPE
TASK_COMPATIBILITY
TASK_CREATION
TASK_INSTANCES_POLICY
TASK_LOGON_TYPE
TASK_RUNLEVEL_TYPE
TASK_STATE_TYPE
TASK_TRIGGER_TYPE
Create the task service object
Get the folder to list tasks from
Create the task service object
Get the folder to list folders from
Create the task service object
Get the folder to list folders from
Create the task service object
Get the folder to list folders from
Check for existing task  Connect to an existing task definition
connect to the task scheduler
Create a new task definition
Modify task settings
Add Action
Add Trigger
get the folder to create the task in
Save the task
Verify task was created
Check for existing task  Connect to an existing task definition
Create the task service object
Load xml from file, overrides xml_text  Need to figure out how to load contents of xml
Get the folder to list folders from
Save the task
Verify creation
Check for existing folder  Connect to an existing task definition
Create the task service object
Get the folder to list folders from
Verify creation
Check for passed task_definition  If not passed, open a task definition for an existing task
Make sure a name was passed
Make sure task exists to modify
Connect to the task scheduler
get the folder to create the task in
Connect to an existing task definition
Not found and create_new not set, return not found
General Information
Settings  https://msdn.microsoft.com/en-us/library/windows/desktop/aa383480(v=vs.85).aspx  Settings: General Tab
Settings: Conditions Tab (Idle)  https://msdn.microsoft.com/en-us/library/windows/desktop/aa380669(v=vs.85).aspx
Settings: Conditions Tab (Power)
Save the task  Save the Changes
Check for existing task
connect to the task scheduler
get the folder to delete the task from
Verify deletion
Check for existing folder
connect to the task scheduler
get the folder to delete the folder from
Delete the folder
Verify deletion
Check for existing folder
connect to the task scheduler
get the folder to delete the folder from
Check for existing folder
connect to the task scheduler
get the folder to delete the folder from
Is the task already running
Check for existing folder
connect to the task scheduler
get the folder to delete the folder from
Check for existing folder
connect to the task scheduler
get the folder to delete the folder from
Check for existing folder
connect to the task scheduler
get the folder to delete the folder from
Make sure a name was passed
Make sure task exists
Connect to the task scheduler
get the folder to create the task in
Connect to an existing task definition
Not found and create_new not set, return not found
Required Parameters
Save the task  Save the Changes
Create the task service object
Get the actions from the task
Save the Changes
Format Date Parameters
Make sure a name was passed
Make sure task exists
Connect to the task scheduler
get the folder to create the task in
Connect to an existing task definition
Not found and create_new not set, return not found
Create a New Trigger
Daily Trigger Parameters
On Idle Trigger Parameters
On Task Creation Trigger Parameters
On Boot Trigger Parameters
On Logon Trigger Parameters
Save the task  Save the Changes
Check for existing task
Create the task service object
Get the triggers from the task
Save the Changes
keep lint from choking on _get_conn and _cache_id
Import Python libs
Returns an object if stack exists else an exception
Returns an object if json is validated and an exception if its not
write the proxy file if necessary
Import python libs
Import salt libs
Define the module's virtual name
Make tempfile to hold the adminfile contents.
not yet implemented or not applicable
Return a string if only one package name passed
available_version is being deprecated
Installing a data stream package only in the global zone:
Overriding the 'instance' adminfile option when calling the module directly
Only makes sense in a global zone but works fine in non-globals.
Install the package{s}
Remove the temp adminfile
Make tempfile to hold the adminfile contents.
Remove the package
Remove the temp adminfile
Import python libs
Import salt libs
Allow switch from '*' or not present to 'random'
It's a commented cron job
load the identifier if any  skip leading :
For consistency's sake
Failed to commit, return the error
This catches both '2' and '*'
Failed to commit, return the error
Failed to commit
No date/time params were specified
Failed to commit, return the error
Failed to commit, return the error
Failed to commit, return the error
Import python libs
Import salt libs
Import python libs
Import salt libs
Function alias to not shadow built-ins.
Ensure that daemontools is installed properly.
-*- coding: utf-8 -*-
Skip empty string returned by cp.fileclient.cache_files.
Determine formula namespace from query
Fetch and load defaults formula files from states.
Fetch value
Import python libs
Import python libs
Import 3rd party libs
Define the module's virtual name
Test if the service already exists
Input validation
Connect to Service Control Manager
Create the service
You can only set delayed start for services that are set to auto start  Start type 2 is Auto
Import salt libs
Import python libs
Import salt libs
Define the module's virtual name
sorted_types is sorted from greatest to least bitmask.
Filter services for unique items, and sort them for comparison purposes.
Calculate the total value. Produces 0 if an empty list was provided,  corresponding to the None _SERVICE_TYPES value.
Get the fields post-change so that we can verify tht all values  were modified successfully. Track the ones that weren't.
The communities are stored as the community name with a numeric permission value. Convert  the numeric value to the text equivalent, as present in the Windows SNMP service GUI.
Create any new communities.
Get the fields post-change so that we can verify tht all values  were modified successfully. Track the ones that weren't.
Import python libs
Import salt libs  Gated for salt-ssh (salt.utils.cloud imports msgpack)
Import 3rd-party libs
Make it a string in case it's not  Strip any quotes and initial 0, though zero-pad it up to 4
Always include a leading zero
Import Python Libs
Import salt libs
Import python libs
Import salt libs
Import Python Libs
Import Salt Libs
Import python libs
Import salt libs
Import python libs
Import salt libs
Import 3rd-party libs
msgpack does not like OrderedDict's
Look for the op  ip not found
support removing a host entry by providing an empty string
make sure there is a newline
No aliases exist for the line, make it empty
Only an alias was removed
/etc/hosts needs to end with EOL so that some utils that read  it do not break
Import salt libs
Import Python libs
Import Salt libs
Function aliases
Define the module's virtual name
Define the module's virtual name
Import python libs
Import salt libs
Import third party libs
Don't shadow built-ins
Import python libs
Import 3rd-party libs  pylint: disable=import-error  pylint: enable=import-error
Import salt libs
Define the module's virtual name
Import python lib
will try to import NAPALM  https://github.com/napalm-automation/napalm  pylint: disable=W0611  pylint: enable=W0611
Import python libs
Import Salt Libs
Define the module's virtual name
check if gid is already in use
provided list is '': users previously deleted from group
getgrnam seems to cache weirdly, so don't use it
Import python libs
Import salt libs
Import salt libs
If a custom gem is given, use that and don't check for rvm/rbenv. User  knows best!
Import python libs
Import salt libs
Check if Chocolatey is already present in the path
It took until .NET v4.0 for Microsoft got the hang of making installers,  this should work under any version of Windows
Check if PowerShell is installed. This should be the case for every  Windows release following Server 2008.
Import python libs
Import salt libs
We only want numbers
Import python libs
Import 3rd-party libs  pylint: disable=import-error,no-name-in-module,redefined-builtin  pylint: enable=import-error,no-name-in-module,redefined-builtin
Import salt libs
Define the module's virtual name
signal the decorator or our return
Import salt libs
Is this os x?
This is so it looks right in shadow.info
Import python libs
Don't shadow built-in's.
keep lint from choking on _get_conn and _cache_id
Import Python libs
this presenter magic makes yaml.safe_dump  work with the objects returned from  boto.describe_alarms()
Import python libs
Import third party libs
Import python libs
Import Salt libs
Import 3rd-party libs
Define the module's virtual name
Import python libs
Import third party libs
Import salt libs
Don't shadow built-in's.
uglyness of splunk lib
yaml magic to output OrderedDict
Import python libs
Import third party libs
Import salt libs
Set up logging
Set up template environment
Define the module's virtual name
Check for supported parameters
without newlines character. http://stackoverflow.com/questions/12330522/reading-a-file-without-newlines
Slave devices are controlled by the master.
Slave devices are controlled by the master.
Read current configuration and store default values
Write settings
Import python libs
Import salt libs
Import third party libs  pylint: disable=import-error
Try to import MySQLdb
MySQLdb import failed, try to import PyMySQL
No MySQL Connector installed, return False
identifiers cannot be used as values
identifiers cannot be used as values
identifiers cannot be used as values
If a default file is explicitly passed to kwargs, don't grab the  opts/pillar settings, as it can override info in the defaults file
the shlex splitter may have split on special database characters `  Read-ahead
Read-ahead
Server is not a slave if master is not defined.  Return empty tuple  in this case.  Could probably check to see if Slave_IO_Running and  Slave_SQL_Running are both set to 'Yes' as well to be really really  sure that it is a slave.
Replication is broken if you get here.
check if db exists
check if db exists
Grants  MySQL normalizes ALL to ALL PRIVILEGES, we do the same so that  grant_exists and grant_add ALL work correctly
Like most other "salt dsl" YAML structures, ssl_option is a list of single-element dicts
SSL option parameters (cipher, issuer, subject) are pasted directly to SQL so  we need to sanitize for single quotes...  omit if falsey
_ and % are authorized on GRANT queries and should get escaped  on the db name, but only if not requesting a table level grant
_ and % are authorized on GRANT queries and should get escaped  on the db name, but only if not requesting a table level grant
add revoke for *.*  before the modification query send to mysql will looks like  REVOKE SELECT ON `*`.* FROM %(user)s@%(host)s
check for if this minion is not a master
check for if this minion is not a slave
Import python libs
Import salt libs
We can only use wildcards in python_shell which is  sent by the macpackage state
Create temp directory
Extract the PackageInfo files
Find our identifiers
Clean up
List all of the .pkg files
We can only use wildcards in python_shell which is  sent by the macpackage state
Import salt libs
Import python libs
Import salt libs
Import Python libs
Import Salt libs
Import Python libs
Import salt libs
the list of categories that the user wants to be searched for.
the list of categories that are present in the updates found.  careful not to get those two confused.
list of updates that are applicable by current settings.
list of updates to be installed.
the object responsible for fetching the actual downloads.
the object responsible for the installing of the updates.
the results of the download process
the results of the installation process
search results from CreateUpdateSearcher()
step through the list of the updates to ensure that the updates match the  features desired.  this skipps an update if UI updates are not desired.
if this update is already downloaded, it doesn't need to be in  the download_collection. so skipping it unless the user mandates re-download.
gets the categories of the updates available in this collection of updates
if there is no type, the is nothing to search.
if the blugger is empty, the results are nil.
this gets the result from install_results, but the title comes from the update  collection install_collection.
translates the list of update results into a library that salt expects.
Return list of titles.
bragging rights.
Normal Usage
Specific Fields
List all critical updates list in verbose detail
Normal Usage
Download critical updates only
this is where we be seeking the things! yar!
this is where we get all the things! i.e. download updates.
Normal Usage
Install all critical updates
this is where we be seeking the things! yar!
this is where we get all the things! i.e. download updates.
this is where we put things in their place!
Import Python Libs
Import Salt Libs
Define the module's virtual name
Pull in the information from WMIC
Find the location of LoadPercentage
Get the end of the number.
Return pull it out of the informatin and cast it to an int
Open up a subprocess to get information from WMIC
Get the line that has when the computer started in it:  use second line from output
Extract the time string from the line and parse  Get string, just use the leading 14 characters  Convert to time struct  Convert to datetime object
Subtract startup time from current time to get the uptime of the system
Access Denied for System Idle Process and System
the default publishing port
Check if we have FQDN/hostname defined as master  address and try resolving it first. _remote_port_tcp  only works with IP-addresses.
useSSL = True will be enforced
Import python libs
Import salt libs
Import 3rd-party libs
Define the module's virtual name
Refresh before looking for the latest version available
Initialize the dict with empty strings
Only add to return dict if package is in the list of packages  passed, otherwise dependencies will make their way into the  return data.
Return a string if only one package name passed
available_version is being deprecated
not yet implemented or not applicable
Allow 'version' to work for single package target
If no prefix characters were supplied, use '='
Import python libs
Import salt libs
Define the module's virtual name
Dunno why it would, but...
not yet implemented or not applicable
xbps-query -l output sample:  ii desktop-file-utils-0.22_4  Utilities to ...  XXX handle package status (like 'ii') ?
Refresh repo index before checking for latest version available
Refresh repo index before checking for latest version available
Initialize the dict with empty strings
Return a string if only one package name passed
available_version is being deprecated
keep only installed packages
Matches a line where first printing is "repository" and there is an equals  sign before the repo, an optional forwardslash at the end of the repo name,  and it's possible for there to be a comment after repository=repo
Import python libs
Import salt libs
Import python libs
Import python libs
Import salt libs
Set up logging
Jumps should appear last, except for any arguments that are passed to  jumps, which of course need to follow.
Strip trailing spaces off rule
Write out any changes
Specify one, rqeuire all
nftables rules can only be deleted using the handle  if we don't have it, find it.
Import python libs
Import salt libs
Import Python Futures
Import Python libs  Remove unused-import from disabled pylint checks when we uncomment the logic  in _get_exec_driver() which checks the docker version
Import Salt libs
Import 3rd-party libs
pylint: disable=import-error
Set up logging
Minimum supported versions
Default timeout as of docker-py 1.0.0
Timeout for stopping the container, before a kill is invoked
Define the module's virtual name
Container doesn't exist anymore
Check if the DOCKER_HOST environment variable has been set
Let docker-py auto detect docker version incase  it's not defined by user.
Set a new timeout if one was passed
Destination file does not exist or could not be accessed
LooseVersion is less preferable, but OK as a fallback.
This if block can be removed once we make docker-exec a default  option, as it is part of the logic in the commented block above.
Would happen if some wiseguy requests a tag ending in a colon  (e.g. 'somerepo:')
Generic handling of Docker API errors
Allow API errors to be caught further up the stack
Generic handling of Docker API errors
Allow API errors to be caught further up the stack
Status is an image ID
Layer already exists
Pushed a new layer
No need to validate
Translate command into a list of strings
No need to validate
Either an int or a string int will work when creating the  container, so just force this to be a string.
Don't perform validation again, we already did this
no need to validate
No need to validate
No need to validate
Translate entrypoint into a list of strings
No need to validate
No need to validate
No need to validate
No need to validate
No need to validate
No need to validate
Normalize container name to make comparisons simpler
No need to validate
No need to validate
Handle cases where a container's name is numeric
No need to validate
Ensure that the user didn't just pass 'container:', because  that would be invalid.
just a name assume it is a network
No need to validate
No need to validate
Passed as None and None is the default. Skip validation. This  catches cases where user explicitly passes a value of None.
User explicitly passed None for an option that cannot be  None, don't let them do this.
Look for custom validation function
Run validation function
Clear any context variables created during validation process
Resolve tag or short-SHA to full SHA
If verbose info was requested, go get it
docker.client.Client.port() doesn't do what we need, so just inspect the  container and get the information from there. It's what they're already  doing (poorly) anyway.
If verbose info was requested, go get it
Read in column names
Build return dict
Try to inspect the image, if it fails then we know we need to pull it  first.
Docker cp gets a file from the container, alias this to copy_from
gzip doesn't use a Compressor object, it uses a .open() method to  open the filehandle. If not using gzip, we need to open the  filehandle here.
Flush any remaining data out of the compressor
Process push
gzip doesn't use a Compressor object, it uses a .open()  method to open the filehandle. If not using gzip, we need  to open the filehandle here.
Flush any remaining data out of the compressor
Clean up temp file
Process push
Only non-error return case is a True return, so just return the response
Only non-error return case is a True return, so just return the response
Only non-error return case is a True return, so just return the response
Only non-error return case is a True return, so just return the response
Only non-error return case is a True return, so just return the response
Only non-error return case is a True return, so just return the response
Only non-error return case is a True return, so just return the response
Only non-error return case is a True return, so just return the response
Only non-error return case is a True return, so just return the response
Only non-error return case is a True return, so just return the response
Only non-error return case is a True return, so just return the response
Container doesn't exist anymore
Container doesn't exist anymore
Import python libs
Import third party libs
Import salt libs
Set up logging
Set up template environment
Define the module's virtual name
ipv4 static & manual
Add this later.
Identify the clauses by the first word of each line.  Go to the next line if the current line is a comment  or all spaces.  Parse the iface clause
Create item in dict, if not already there
Create item in dict, if not already there
Parse the detail clauses.
return a list of sorted keys to ensure consistent order
iface_data.append({})
default to 'static' if proto is 'none'
replace dashes with underscores for jinja
Return as a array so the difflib works
Read /etc/network/interfaces into a dict  Apply supplied settings over on-disk settings
Return as a array so the difflib works
Return as a array so the difflib works
Load kernel module
install ifenslave-2.6
ensure lines in list end with newline, so difflib works
Slave devices are controlled by the master.  Source 'interfaces' aren't brought down.
ensure lines in list end with newline, so difflib works
Slave devices are controlled by the master.  Source 'interfaces' aren't brought up.
Read current configuration and store default values
Build settings
Write settings
A domain line didn't exist so we'll add one in  with the new domainname
Write /etc/resolv.conf
Import python libs
Import salt libs
parts.repo will be empty if there is no repo part
Next sort is just aesthetic, can be commented for a small performance  boost
Next sort is just aesthetic, can be commented for a small performance  boost
wildcards are valid in confs
aux_get might return dupes, so run them through set() to remove them
reset cpv filter
Import python libs
Import salt libs
keep lint from choking on _get_conn and _cache_id
Import Python libs
Import third party libs  pylint: disable=unused-import  pylint: enable=unused-import
Setup the logger
we failed to discover, lets use k8s default address
Get salt minion ID  Try to get kubernetes master
Get data
Get salt minion ID  Try to get kubernetes master
Get all labels
This is a old label and it has already the wanted value
Get salt minion ID  Try to get kubernetes master
Get salt minion ID  Try to get kubernetes master
Try to get kubernetes master
This is a new namespace
Try to get kubernetes master
Get data
Try to get kubernetes master
Get data
Try to get kubernetes master
Import python lib
salt libs
will try to import NAPALM  https://github.com/napalm-automation/napalm  pylint: disable=W0611  pylint: enable=W0611
if negative, leave the output unchanged
Import Python libs
Import Salt libs
Somehow the path already exists
Import python libs
Import Salt libs
Import 3rd-party libs
Set up logger
from python:salt.utils.shlex_split: passing None for s will read  the string to split from standard input.
Cleanup the failed installation so it doesn't list as installed
This is a positional argument so this should never happen, but this  will handle cases where someone explicitly passes a false value for  cmdline.
This is a positional argument so this should never happen, but this  will handle cases where someone explicitly passes a false value for  cmdline.
Import python libs
Import salt libs
Define the module's virtual name
import python libs
Import salt libs
build salt equivalents from scratch
dscs should only contain salt orig and debian tarballs and dsc file
preset passphase and interaction with gpg-agent
Import python libs
Import salt libs
Import python libs
Import salt libs
Import python libs
Import salt libs
Define the module's virtual name
available_version is being deprecated
not yet implemented or not applicable
Support pkg.delete to remove packages to more closely match pkg_delete  No equivalent to purge packages, use remove instead
Import python libs
Import salt libs
Function alias to make sure not to shadow built-in's
Validate Composer is there
Don't need a dir for the 'selfupdate' action; all other actions do need a dir
Base Settings
If php is set, prepend it
Add Working Dir
Other Settings
Import salt libs
Define the module's virtual name
Import python libs
Import third party libs
Import 3rd-party libs
Import salt libs
Setup the logger
Define the module's virtual name
sometimes the bigip just likes a plain ol list of items  other times it's picky and likes key value pairs...
for states
for execution
Import python libs
Import Salt libs  pylint: disable=import-error,no-name-in-module,redefined-builtin
salt my-minion vsphere.list_hosts <vcenter-ip> <vcenter-user> <vcenter-password>
salt my-minion vsphere.get_coredump_network_config <vcenter-ip> <vcenter-user> \
Import Python Libs
Import Salt Libs
Import Third Party Libs
Used for ESXi host connection information
Used for ESXi host connection information
Used for connecting to a vCenter Server
format the response stdout into something useful
Used for ESXi host connection information
Used for ESXi host connection information
Update the cmd.run_all dictionary for each particular host.
Used for ESXi host connection information
format the response stdout into something useful
Used for ESXi host connection information
Handles a single host or a vCenter connection when no esxi_hosts are provided.
Used for ESXi host connection information
Handles a single host or a vCenter connection when no esxi_hosts are provided.
Used for ESXi host connection information
First, enable the syslog firewall ruleset, for each host, if needed.
Set the config value on each esxi_host, if provided.
Ensure we don't overwrite any dictionary data already set  By updating the esxi_host directly.
Used for ESXi host connection information
format the response stdout into something useful
Handles a single host or a vCenter connection when no esxi_hosts are provided.  format the response stdout into something useful
Used for ESXi host connection information
Handles a single host or a vCenter connection when no esxi_hosts are provided.
Used for single ESXi host connection information
Used for single ESXi host connection information
Used for single ESXi host connection information
Check if the service_name provided is a valid one.  If we don't have a valid service, return. The service will be invalid for all hosts.
Don't require users to know that VMware lists the ssh service as TSM-SSH
Used for single ESXi host connection information
Check if the service_name provided is a valid one.  If we don't have a valid service, return. The service will be invalid for all hosts.
Don't require users to know that VMware lists the ssh service as TSM-SSH
Used for single ESXi host connection information
Used for single ESXi host connection information
Used for single ESXi host connection information
Used for single ESXi host connection information
Used for single ESXi host connection information
Used for single ESXi host connection information
Get NTP Config Object from ntp_servers
Get DateTimeConfig object from ntp_config
Used for single ESXi host connection information
Don't require users to know that VMware lists the ssh service as TSM-SSH
Check if the service_name provided is a valid one.  If we don't have a valid service, return. The service will be invalid for all hosts.
Used for single ESXi host connection information
Don't require users to know that VMware lists the ssh service as TSM-SSH
Check if the service_name provided is a valid one.  If we don't have a valid service, return. The service will be invalid for all hosts.
Used for single ESXi host connection information
Don't require users to know that VMware lists the ssh service as TSM-SSH
Check if the service_name provided is a valid one.  If we don't have a valid service, return. The service will be invalid for all hosts.
Used for single ESXi host connection information
Check if the service_name provided is a valid one.  If we don't have a valid service, return. The service will be invalid for all hosts.
Services are stored in a general list - we need loop through the list and find  service key that matches our service name.
Find the service key based on the given service_name
Used for single ESXi host connection information
Get LocalAccountManager object
Create user account specification object and assign id and password attributes
Used for single ESXi host connection information
Used for single ESXi host connection information
Used for single ESXi host connection information
We need to return ONLY the disk names, otherwise Message Pack can't deserialize the disk objects.
If we have a string type in the eligible value, we don't  have any VSAN-eligible disks. Pull the message through.
If we hit an error, populate the Error return dict for state functions.
If we made it this far, we somehow have eligible disks, but they didn't  match the disk list and just got an empty list of matching disks.
Used for single ESXi host connection information
Create a VSAN Configuration Object and set the enabled attribute to True
Used for single ESXi host connection information
Create a VSAN Configuration Object and set the enabled attribute to True
First, try to find the host reference by DNS Name.
If we couldn't find the host by DNS Name, then try the IP Address.
Get all VSAN suitable disks for this host.
Get disks for host and combine into one list of Disk Objects
Get disks that are in both the disks list and suitable_disks lists.
try:
Import salt libs
Cache the output of running which('ipvsadm')
A non-zero return code means fail
A non-zero return code means fail
A non-zero return code means fail
A non-zero return code means fail
A non-zero return code means fail
A non-zero return code means fail
A non-zero return code means fail
A non-zero return code means fail
A non-zero return code means fail
Exact match
Exact match
keep lint from choking on _get_conn and _cache_id
Import Python libs
Import Salt libs
keep lint from choking on _get_conn and _cache_id
Import Python libs
Import Salt libs
Import python libs
Import salt libs
Import 3rd-party libs
Define the module's virtual name
allow both "sys" and "sys." to match sys, without also matching  sysctl
allow both "sys" and "sys." to match sys, without also matching  sysctl
allow both "sys" and "sys." to match sys, without also matching  sysctl
allow both "sys" and "sys." to match sys, without also matching  sysctl
We're being asked for all functions
allow both "sys" and "sys." to match sys, without also matching  sysctl
This is handled inside the minion.py file, the function is caught before  it ever gets here
We're being asked for all functions
allow both "sys" and "sys." to match sys, without also matching  sysctl
We're being asked for all functions
allow both "sys" and "sys." to match sys, without also matching  sysctl
We're being asked for all functions
allow both "sys" and "sys." to match sys, without also matching  sysctl
S.ArrayItem(title=i, description=i, required=True),  S.DictItem(title=i, description=i, required=True),
S.ArrayItem(title=i, description=i, default=j),  S.DictItem(title=i, description=i, default=j),
Import python libs
Import salt libs
Define the module's virtual name
Support pkg.info get version info, since this is the CLI usage
Support pkg.update to refresh the db, since this is the CLI usage
Return a string if only one package name passed
available_version is being deprecated
not yet implemented or not applicable
add change return later
pkg add has smaller set of options (i.e. no -y or -n), filter below
Only use the 'version' param if 'name' was not specified as a  comma-separated list
pkg add doesn't have a dryrun mode, so echo out what will be run
FreeBSD pkg supports `openjdk` and `java/openjdk7` package names
Support pkg.delete to remove packages, since this is the CLI usage  No equivalent to purge packages, use remove instead
Import python libs
Import Salt libs
Import python libs
Import salt libs
Don't shadow built-in's.
Import Python libs
Import 3rd-party libs
Change endpoint if there are params to filter by:
Convert all ints to strings:
Import Python libs
Define the module's virtual name
Don't let this work on Solaris 9 since SMF doesn't exist on it.
Return code 3 means there was a problem with the service  A common case is being in the 'maintenance' state  Attempt a clear and try one more time
calling restart doesn't clear maintenance  or tell us that the service is in the 'online' state
calling reload doesn't clear maintenance  or tell us that the service is in the 'online' state
Note that this returns the full FMRI
Note that this returns the full FMRI
Import python libs
Import salt libs
The long version, added for consistency
The long version, added for consistency
Import python libs
Import salt libs
Define the module's virtual name
Make sure home directory exists
dscl buffers changes, sleep before setting group membership
force is added for compatibility with user.absent state function
remove home directory from filesystem
Remove from any groups other than primary group. Needs to be done since  group membership is managed separately from users and an entry for the  user will persist even after the user is removed.
dscl buffers changes, sleep 1 second before checking if new value  matches desired value
dscl buffers changes, sleep 1 second before checking if new value  matches desired value
dscl buffers changes, sleep 1 second before checking if new value  matches desired value
dscl buffers changes, sleep 1 second before checking if new value  matches desired value
use a 'create' command, because a 'change' command would fail if  current fullname is an empty string. The 'create' will just overwrite  this field.
dscl buffers changes, sleep 1 second before checking if new value  matches desired value
dscl buffers changes, sleep 1 second before checking if new value  matches desired value
Import third party libs
Only load if imports exist.
This helper is used to procure the channel ids  used to notify when the alarm threshold is violated
Non 200 response, sent back the error response'
Delete all the alarms associated with this deployment
Import python libs
Import Salt libs
Import 3rd-party libs
Import python libs
Import salt libs
Define the module's virtual name
Verify that the target binding exists.
Check to see if the certificate is already assigned.
Verify that the certificate exists.
Verify that the binding exists for the site, and that the target  certificate is assigned to the binding.
Validate the setting names that were passed in.
Treat all values as strings for the purpose of comparing them to existing values.
If the value is numeric, don't treat it as a string in PowerShell.
Get the fields post-change so that we can verify tht all values  were modified successfully. Track the ones that weren't.
The target physical path must exist.
The target physical path must exist.
Import salt libs
Import 3rd party libraries
Define the module's virtual name
We're in the last block listing addresses
Alias hwaddr to preserve backward compat
Import python libs
Import Salt's libs
Define the module's virtual name
" is a valid character in SSIDs, but iwlist likes to wrap SSIDs in them
Slave devices are controlled by the master.
Slave devices are controlled by the master.
Import python libs
or
or
or
Import python libs
Import salt libs
Get logging started
Import Python libs
Import 3rd-party libs  pylint: disable=import-error,no-name-in-module,redefined-builtin  pylint: enable=import-error,no-name-in-module
keep lint from choking on _get_conn and _cache_id
Import Python libs
Import Salt libs
Import third party libs  pylint: disable=unused-import  pylint: enable=unused-import
Import python libs
Import salt libs
Define the module's virtual name
awk is in core-os package so we can use it without checking
Get a list of the packages before install so we can diff after to see  what got installed.
Install or upgrade the package  If package is already installed
not yet implemented or not applicable
column 1 is full FMRI name in form pkg://publisher/class/name@version
available_version is being deprecated
already full fmri
already full fmri
error = nothing found  no error, processing pkg listing  column 1 is full FMRI name in form pkg://publisher/pkg/name@version
Get a list of the packages before install so we can diff after to see  what got installed.
Install or upgrade the package  If package is already installed
Get a list of the packages again, including newly installed ones.
No error occurred
Get a list of the currently installed pkgs.
Remove the package(s)
Get a list of the packages after the uninstall
Import python libs
Define the module's virtual name
keep lint from choking on _get_conn and _cache_id
Import Python libs
Import Salt libs
exponential backoff
exponential backoff
keep lint from choking on _get_conn and _cache_id
Import Python libs
Import third party libs  pylint: disable=unused-import  pylint: enable=unused-import
Import Python libs
Import Salt libs
Import Python libs
Define the module's virtual name
influxdb 0.9+
influxdb 0.8 and older
Import python libs
Import 3rd-party libs
Import salt libs
Don't choke on empty lines
h_len will become smallest number of fields in stat lines
The header was longer than the smallest number of fields  Therefore the sys stats are hidden in there
Import Python libs
Import third party libs
Import salt libs
is there not SaltStackVersion.current() to get  the version of the salt running this code??
pylint: disable=import-error
Workaround, as the Glance API v2 requires you to  already have a keystone session token
The trailing 'v2' causes URLs like thise one:  http://127.0.0.1:9292/v2/v1/images
If we had a password we could just  ignore the admin-token and move on...
'name': item.name,     }
Import salt libs
Import python libs
Import salt libs
Define the module's virtual name
Note: pw exits with code 65 if group is unknown
Note: pw exits with code 65 if group is unknown
Import python libs
Import salt libs
Define the module's virtual name
unknown will be returned while inside a kickstart environment, since  this is usually a server deployment it should be safe to assume runlevel  3.  If not all service related states will throw an out of range  exception here which will cause other functions to fail.
Look for user-execute bit in file mode.
Try chkconfig first.
Return only the services that have an initscript present
Import python libs
Import salt libs
This is the one line that we're changing
No changes to this line, but it still needs to be  formatted properly
This line is a comment or is empty
This config value does not exist, so append it to the end
(yes)   (yes)   (yes)   (never) (100)
Return both sets of data, they compliment each other elsewhere
discard in-queue wrapper
Import python libs
Import salt libs
Set up logging
Define the module's virtual name
Putting quotes around the parameter protects against command injection
Putting quotes around the parameter protects against command injection
Putting quotes around the parameter protects against command injection
Putting quotes around the parameter protects against command injection
Putting quotes around the parameter protects against command injection
Import python libs
Import third party libs
The user is in splunk group, just return
update
Clean up the temp file
Import Python libs
Import third party libs
Import python libs
Import salt libs
SuSE does not seem to use separate files for IPv4 and IPv6
Ignore name and state for this function
Jumps should appear last, except for any arguments that are passed to  jumps, which of course need to follow.  All jump arguments as extracted from man iptables-extensions, man iptables,  man xtables-addons and http://www.iptables.info/en/iptables-targets-and-jumps.html
WEB
MATCH EXTENSIONS
Import python libs
Import salt libs
Import python libs
Import Python libs
Import 3rd-party libs  pylint: disable=import-error,no-name-in-module,redefined-builtin  pylint: enable=import-error,no-name-in-module,redefined-builtin
Import salt libs
Import python libs
Import salt libs
Import salt libs
Define the module's virtual name
Import Python libs
Import Salt libs
Import 3rd-party libs
Post-migration procedures
Import python libs
Import salt libs
offset file does not exist
Import python libs
Import salt libs
Import 3rd-party libs
Disable on Windows, a specific file module exists:
Line either doesn't have the right number of columns, or the  regex which looks for address information did not match. Either  way, ignore this line and continue on to the next one.
IPv6 addresses have the address part enclosed in brackets (if the  address part is not a wildcard) to distinguish the address from  the port number. Remove them.
Parse version of traceroute  Linux traceroute version looks like:    Modern traceroute for Linux, version 2.0.19, Dec 10 2012  Darwin and FreeBSD traceroute version looks like: Version 1.4a12+[FreeBSD|Darwin]
Alias hwaddr to preserve backward compat
Modify the /etc/hosts file to replace the old hostname with the  new hostname
also set a copy of the hostname
Modify the /etc/sysconfig/network configuration file to set the  new hostname
remove shell cmd prefix from msg
Import salt libs
Import python libs
Import salt libs
Import 3rd party libs
Define the module's virtual name
Follow symbolic links of files in _launchd_paths  ignore broken symlinks
This assumes most of the plist files  will be already in XML format
Match on label
Match on full path
Match on basename
Could not find service
Get return type
Construct command
Run command
Get service information and label
Collect information on service: will raise an error if it fails
Collect information on all services: will raise an error if it fails
Get service information and label
Enable the service: will raise an error if it fails
Get service information and label
disable the service: will raise an error if it fails
Get service information and file path
Load the service: will raise an error if it fails
Get service information and file path
Disable the Launch Daemon: will raise an error if it fails
Restart the service: will raise an error if it fails
Find service with ps
Try to list the service.  If it can't be listed, it's not enabled
A service is disabled if it is not enabled
Get list of enabled services
Get list of all services
Return composite list
Collect list of enabled services
Construct list of enabled services  Skip header line
keep lint from choking on _get_conn and _cache_id
Import Python libs
Import salt libs
Python Libs
Third party libs
Import salt libs
Settings
Trim ending backslash
Will add to the beginning of the path
validate index boundaries
Check if we are in the system path at the right location
Broadcast WM_SETTINGCHANGE to Windows
Import python libs
Import salt libs
remove first and last line: Listing ... - ...done
Generate a random, temporary password. RabbitMQ requires one.
Now, Clear the random password from the account, if necessary
Clearing the password failed. We should try to cleanup  and rerun and error.
Import python libs
Import salt libs
Define the module's virtual name
Get the missing information from the /var/lib/dpkg/available, if it is there.  However, this file is operated by dselect which has to be installed.
Import python libs
Define the module's virtual name
Import salt libs
Import python libs
Import python libs
Import salt libs
Import 3rd-party lib
Import python libs
Import Salt libs
Import python libs
Import salt libs
Import third party libs
Define the module's virtual name
Import python libs
Import salt libs
Import 3rd-party libs
Set up logging
Import python libs
Import salt libs
Import python libs
Import salt libs
Import 3rd-party libs
'removed', 'purge_desired' not yet implemented or not applicable
available_version is being deprecated
Handle version kwarg for a single package target
Import python libs
Import salt libs
Define the module's virtual name
Import python libs
Import salt libs
Function alias to make sure not to shadow built-in's
npm >1.2.21 is putting the output to stderr even though retcode is 0  Not JSON! Try to coax the json out of it!
Strip all lines until JSON output starts
Still no JSON!! Return the stdout as a string
Protect against injection
Protect against injection
npm will return error code 1 for both no packages found and an actual  error. The only difference between the two cases are if stderr is empty
keep lint from choking on _get_conn and _cache_id
Import Python libs
subnet_ids=subnets can accept either a string or a list
Import Python Libs
Import Salt Libs
pylint: disable=import-error,no-name-in-module  pylint: enable=import-error,no-name-in-module
Salt won't return dictionaries with odd types like uuid.UUID  Must support Cassandra collection types.  Namely, Cassandras set, list, and map collections.
Add the strategy, replication_factor, etc.
Import python libs
Import salt libs
Import python libs
Import 3rd-party libs  pylint: disable=import-error  pylint: enable=import-error
Import salt libs
Define the module's virtual name
Remove README.  If it's an enabled service, it will be added back in.
Import python libs
Import 3rd-party libs
Import salt libs
We can't decide at load time whether supervisorctl is present. The  function _get_supervisorctl_bin does a much more thorough job and can  only be accurate at call time.
pylint: disable=maybe-no-member  pylint: enable=maybe-no-member
Import python libs
Import salt libs
Import third party libs  pylint: disable=import-error
TOTAL_PHYMEM is deprecated but with older psutil versions this is  needed as a fallback.
NUM_CPUS is deprecated but with older psutil versions this is needed  as a fallback.
get_boot_time() has been removed in newer psutil versions, and has  been replaced by boot_time() which provides the same information.
get_users is only present in psutil > v0.5.0  try utmp
keep lint from choking on _get_conn and _cache_id
Import Python libs
Import salt libs
Import third party libs  pylint: disable=unused-import
Boto weirdly returns an exception here if an instance profile doesn't  exist.
This call returns an instance profile if successful and an exception  if not. It's annoying.
I _hate_ you for not giving me an object boto.  Policy is url encoded
The get_user call returns an user ARN:     arn:aws:iam::027050522557:user/salt-test
this presenter magic makes yaml.safe_dump  work with the objects returned from  boto.export_users()
Import python libs
Import salt libs
take the only directory in /sys/fs/bcache and return it's basename
basename of the /sys/block/{dev}/bcache/cache symlink target
pylint: disable=too-many-return-statements
You might want to override, we pick the cache set's as sane default
pylint: disable=too-many-return-statements   Preflight checks
Still here, start doing destructive stuff   Wipe the current cache device as well,  forever ruining any chance of it accidentally popping up again
Can't do enough wiping
if wipe was incomplete & part layout remains the same,  this is one condition set where udev would make it accidentally popup again
Finally, create a cache
Actually bucket_size should always have a value, but for testing 0 is possible as well
filter out 'hidden' kwargs added by our favourite orchestration system
it's a backing dev
It's the cache itself
Parse through both the blockdev & the FS
It's a backing device
Superblock
name = re.sub(r'^([a-z]+)(?<!(bcache|md|dm))([0-9]+)', r'\1/\1\2', name)
Parse through the interfaces list
Actions, we ignore
Fetch SysFS vals
Parse through well known bools
standarization yay
Some stuff (<cough>GPT</cough>) writes stuff at the end of a dev as well
Import python libs
Import salt libs
Define the module's virtual name
Import python libs
Import salt libs
Define the module's virtual name
this block only applies to Debian without systemd
this block only applies to Debian without systemd
FIXME: why are we writing to a file that is dynamically generated?
if the charmap has not been supplied, normalize by appening it
Import python libraries  Juniper interface libraries  https://github.com/Juniper/py-junos-eznc
Set up logging
Define the module's virtual name
Import python libs
Import salt libs
Import Python Libs
Import python libs
Import salt libs
Import python libs
Import salt libs
Define the module's virtual name
Import salt libs
Import python libs
Import 3rd-party libs
Unset any keys not defined in 'environ' dict supplied by user
Import python libs
Import 3rd Party Libs
Import salt libs
Set up logging
Define the module's virtual name
Make sure the system exists  Return an object containing current information array for the computer
If desc is passed, decode it for unicode
remove any escape characters
Get date/time object from newtime
Set time using set_system_date_time()
Create the time tuple to be passed to SetLocalTime, including day_of_week
Get date/time object from newdate
Set time using set_system_date_time()
Order the checks for reboot pending in most to least likely.
Import salt libs
awscli is installed, load the module
Import python libs
Import salt libs
Import Python libs
Import Salt libs
Don't shadow built-in's.
all the args
all the kwargs
Import Python libs
Import Salt Libs
Import third party libs  pylint: disable=import-error  pylint: enable=import-error
look in connection_args first, then default to config file
'insecure' keyword not supported by all v2.0 keystone clients    this ensures it's only passed in when defined
'id': item.id,         'name': item.name,         }
Import salt libs
Cache the output of running which('nginx') so this module  doesn't needlessly walk $PATH looking for the same binary  for nginx over and over and over for each function herein
Import Python Libs
Import Salt Libs
It's permanent because the 4 concerned functions need the permanent option, it's wrong without
Import Python libs
Import salt libs
We can't send an event if we're in masterless mode
If preload is specified, we must send a raw event (this is  slower because it has to independently authenticate)
Allow values in the ``data`` arg to override any of the above values.
Import python libs
Define the module's virtual name
Import python libs
Import Python libs
Import Salt libs
Import 3rd-party libs
Import Salt libs
Import salt libs
Define the module's virtual name
only valid in proxy minions for now
Import python libs
Import salt libs
pylint: disable=import-error,no-name-in-module,redefined-builtin  pylint: enable=import-error,no-name-in-module,redefined-builtin
win_file takes care of windows
Get the cachedir from the minion config
This is not an integer, maybe it's already the group name?
Don't even bother to feed it to grp
If group is not present, fall back to the gid.
If user is not present, fall back to the uid.
Broken symlinks will return false, but still need to be chowned
Largely inspired by Fabric's contrib.files.sed()  XXX:dc: Do we really want to always force escaping?
Largely inspired by Fabric's contrib.files.contains()
Largely inspired by Fabric's contrib.files.sed()  XXX:dc: Do we really want to always force escaping?  Mandate that before and after are strings
The pattern to replace with does not need to be escaped!!!
Load the real path to the file
Make sure the file exists
Make sure it is a text file
We've searched the whole file. If we didn't find anything, return False
Create a copy to read from and to use as a backup later
Return a diff using the two dictionaries
Before/after has privilege. If nothing defined, match is used by content.
No dupes or append, if "after" is the last line
Search the file; track if any changes have been made for the return val
Avoid TypeErrors by forcing repl to be a string
found anything? (even if no change)  Identity check the potential change
Search for content, to avoid pre/appending the  content if it was pre/appended in a previous run.  Content was found, so set found.
Write the replacement text in this block.  Create a copy to read from and to use as a backup later
append_if_not_found  Make sure we have a newline at the end of the file
Create a copy to read from and for later use as a backup
write new content in the file while avoiding partial reads
managed block start found, start recording
end of block detected
Check for multi-line '\n' terminated content as split will  introduce an unwanted additional new line.
push new block content in file
remove old content, but keep a trace  else: we are not in the marked block, keep saving things
unterminated block => bad, always fail
add the markers and content at the beginning of file
changes detected  backup old content
write new content in the file while avoiding partial reads
this may have overwritten file attrs
This function wraps file.replace on purpose in order to enforce  consistent usage, compatible regex's, expected behavior, *and* bugs. :)  Any enhancements or fixes to one should affect the other.
this argument prevents interactive prompts when the patch fails to apply.  the exit code will still be greater than 0 if that is the case.
by default, patch will write rejected patch files to <filename>.rej.  this option prevents that.
Broken symlinks will return False for os.path.exists(), but still  have a uid and gid
Not a broken symlink, just a nonexistent path
None of the list items matched
Copy the file to the minion and templatize it
Only test the checksums on files with managed contents
No changes actually made
Create the file, user rw-only if mode will be set to prevent  a small security race problem before the permissions are set
If a caller such as managed() is invoked  with makedirs=True, make  sure that any created dirs are created with the same user and group  to follow the principal of least surprise method.
walk up the directory structure until we find the first existing  directory
create parent directories from the topmost to the most deeply nested one  all directories have the user, group and mode set!!
be happy if someone already created the path
If the character device does not exist in the first place
be happy it is already there....however, if you are trying to change the  major/minor, you will need to unlink it first as os.mknod will not overwrite  quick pass at verifying the permissions of the newly created character device
If the block device does not exist in the first place
be happy it is already there....however, if you are trying to change the  major/minor, you will need to unlink it first as os.mknod will not overwrite  quick pass at verifying the permissions of the newly created block device
If the fifo does not exist in the first place
be happy it is already there  quick pass at verifying the permissions of the newly created fifo
':' is an illegal filesystem path character on Windows
Figure out full path of location of backup file in minion cache
':' is an illegal filesystem path character on Windows
File didn't match the strp format string, so it's not a backup  for this file. Move on to the next one.
Figure out full path of location of backup folder in minion cache
Folder didn't match the strp format string, so it's not a backup  for this folder. Move on to the next one.
First we collect valid PIDs  Not a valid PID, move on
Then we look at the open files for each PID
Collect the names of all of the file descriptors
fd_.append(os.path.realpath('{0}/task/{1}exe'.format(ppath, tid)))     pass
Loop through file descriptors and return useful data for each file  Sometimes PIDs and TIDs disappear before we can query them  Running stat on the file cuts out all of the sockets and  deleted files from the list
We still want to know which PIDs are using each file
Import python libs
Import salt libs
Import 3rd-party libs
Just return the first line
Base query to create a database
Execute the command
Execute the command
will return empty string if return_password = False
check if user exists
user exists, proceed
run the generated sqlfile in the db
schema exists, proceed
Import python libs
Import 3rd-party libs
Don't shadow built-in's.
dict that return a function that does the right thing per platform
dict that returns a function that does the right thing per platform
dict that return a function that does the right thing per platform
select all filesystems
select path
select fstype
dict that returns a function that does the right thing per platform
dict that returns a function that does the right thing per platform
dict that returns a function that does the right thing per platform
dict that returns a function that does the right thing per platform
the default publishing port
Check if we have FQDN/hostname defined as master  address and try resolving it first. _remote_port_tcp  only works with IP-addresses.
Import python libs
Import Salt libs
Import 3rd-party libs
Create user accountvfirst
Configure users permissions
Configure users password
Enable users admin
When users don't provide a user ID we need to search for this
Generate privilege bit mask
Import python libs
Import 3rd party libs
Import salt libs
Define the module's virtual name
Import Python libs
Import salt libs
Import 3rd-party libs
Show all jobs including hidden internal jobs
Hide disabled jobs from list of jobs
Default jobs added by salt begin with __  by default hide them unless show_all is True.
if enabled is not included in the job,  assume job is enabled.
if the job is disabled and show_disabled is False, skip job
if _seconds is greater than zero  then include the original back in seconds.  otherwise remove seconds from the listing as the  original item didn't include it.
remove _seconds from the listing
Effectively a no-op, since we can't really return without an event system
Effectively a no-op, since we can't really return without an event system
if enabled is not included in the job,  assume job is enabled.
Effectively a no-op, since we can't really return without an event system
Effectively a no-op, since we can't really return without an event system
Effectively a no-op, since we can't really return without an event system
Effectively a no-op, since we can't really return without an event system
Effectively a no-op, since we can't really return without an event system
Effectively a no-op, since we can't really return without an event system
Get errors and list of affeced minions
Get errors and list of affeced minions
Import python libs
pylint: disable=C0103
Import python libs
Import salt libs
Import 3rd party libs
Match any GUID
Construct command
Execute command and return output
Construct argument list
Execute command and return output
Construct argument list
Execute command and return output
Construct argument list
Execute command and return output
Get the snapshot information of the snapshot having the requested ID
Parallels desktop returned no information for snap_id
Validate VM and snapshot names
Get a multiline string containing all the snapshot GUIDs
Get a set of all snapshot GUIDs in the string
Try to match the snapshot name to an ID
Try to convert snapshot name to an ID without {}
Validate VM and snapshot names
Construct argument list
Execute command
Construct ID, name pairs  Find all GUIDs in the result
Return information directly from parallels desktop
Validate VM and snapshot names
Construct argument list
Execute command and return output
Validate VM and snapshot names
Construct argument list
Execute command and return output
Validate VM and snapshot names
Construct argument list
Execute command and return output
Import python libs
Import salt libs
Define the module's virtual name
extend params value by optional zabbix API parameters
User groups
merge results
try to locate the resource by any of the identifier_fields that are specified in data
flush the resource_cache, because we're modifying a resource  create
Import python libs
Import salt libs
Set up logging
Don't shadow built-in's.
The directory already exists
Import python libs
Import salt libs
Setup the hasher
Salt the password hash
keep lint from choking on _get_conn and _cache_id
Import Python libs
Import Salt libs
Import python libs
Import salt libs
pylint: disable=import-error
Source format for urllib fallback on PPA handling
Define the module's virtual name
Export these puppies so they persist
Initialize the dict with empty strings
Refresh before looking for the latest version available
Virtual package is a candidate for installation if and only  if it is not currently installed.
If there are no installed versions that are greater than or equal  to the install candidate, then the candidate is an upgrade, so  add it to the return dict
Return a string if only one package name passed
available_version is being deprecated
Strip filesize from end of line
If the versions don't match, refresh is True, otherwise no need  to refresh
If the versions don't match, refresh is True, otherwise  no need to refresh
No version specified, so refresh should be True
Support old "repo" argument
Only use the 'version' param if 'name' was not specified as a  comma-separated list
If we are installing a package file and not one from the repo,  and version_num is not None, then we can assume that pkgname is  not None, since the only way version_num is not None is if DEB  metadata parsing was successful.
Downgrading requires --force-yes. Insert this before 'install'
rexp parses lines that look like the following:  Conf libxfont1 (1:1.4.5-1 Debian:testing [i386])
the apt_pkg module needs to be manually initialized
Try to use shell version in case of errors w/python bindings
PPAs are special and can add deb-src where expand_ppa_line  doesn't always reflect this.  Lets just cleanup here for good  measure
explicit refresh after a repo is deleted
Import python libs
pylint: disable=import-error,redefined-builtin  Import 3rd-party libs
Import salt libs
Define the module's virtual name
Suppport packages with no 'Release' parameter
Installed packages show a '@' at the beginning
We're done with this package, create the pkginfo namedtuple  Clear the dict for the next package  Yield the namedtuple
Support old 'repo' argument
in case of any non-fatal failures, these defaults will be used
fall back to parsing the config ourselves  Look for the config the same order yum does
these options are expected to be lists
if we are passed a string (for backward compatibility), convert to a list
nothing specified, so use the reposdir option as the default
Refresh before looking for the latest version available
no need to check another match, if there was one
Return a string if only one package name passed
available_version is being deprecated
not yet implemented or not applicable
Search in all enabled repos
Can't concatenate because args is a tuple, using list.extend()
Sort versions newest to oldest
Preserve expected CLI usage (yum list updates)
Translate dpkg-specific keys to a common structure
Allow "version" to work for single package target
Yum's versionlock plugin doesn't support passing just the package name  when removing a lock, so we need to get the full list and then use  fnmatch below to find the match.
Ignore any administrative comments (plugin info, repo info, etc.)
We've reached a new section, break from loop
We've reached the targeted section
Don't shadow the pkgtype variable from the outer loop
Don't install packages that are already installed, install() isn't smart  enough to make this distinction.
Find out what file the repo lives in
Return just one repo
this is so we know which dirs are searched for our error messages below
Find out what file the repo lives in
See if the repo is the only one in the file
If this is the only repo in the file, delete the file itself
Filter out '__pub' arguments, as well as saltenv
Build a list of keys to be deleted
convert disabled to enabled respectively from pkgrepo state
Add baseurl or mirrorlist to the 'todelete' list if the other was  specified in the repo_opts
Fail if the user tried to delete the name
Give the user the ability to change the basedir
The repo does exist, open its file
Delete anything in the todelete list
Import python libs
Import salt libs
Define the module's virtual name
Not all versions of SUSE use zypper, check that it is available
Call config
Reset after the call
Prevent the use of "refreshable" together with "nolock".
Provide a list_updates function for those used to using zypper list-updates
Refresh db before extracting the latest package
Return a string if only one package name passed
available_version is being deprecated
not yet implemented or not applicable
Is there already such repo under different alias?
Complete user URL, in case it is not
Add new repo
Modify added or existing repo according to the options
If repo nor added neither modified, error should be thrown
Allow "version" to work for single package target
Import python libs
Import Salt libs  pylint: disable=import-error,redefined-builtin
pylint: enable=import-error,redefined-builtin
Define the module's virtual name
Can't concatenate a tuple, must do a list.extend()
If there is no stdout and the retcode is 0, then verification  succeeded, but if the retcode is nonzero, then the command failed.
If verification has an output, then it means it failed  and the return code will be 1. We are interested in any bigger  than 1 code.
Can't concatenate a tuple, must do a list.extend()
Import python libs
Import 3rd-party libs
Import Salt libs
Define the module's virtual name
The -f flag, used to force a script to run even if disabled,  was added after the 5.0 release.  the rcctl(8) command is the preferred way to manage services.
now read the system startup script /etc/rc  to know what are the system enabled daemons
the matched line is a call to start_daemon()  we remove the function name  we retrieve each daemon name from the parameters of start_daemon()  mark it as enabled
this will remove rc.subr and all non executable files
Import python libs
Import salt libs
Import python libs
Import python libs
Import salt libs
All the "unused" imports here are needed for the imported winrepo runner code  pylint: disable=unused-import
Define the module's virtual name
Get the location of the local repo
Add the sls file name to the path
Check for the sls file by name
Maybe it's a directory with an init.sls
It's neither, return
Load the renderer
Run the file through the renderer
Import python libs
Import salt libs
Function alias to make sure not to shadow built-in's
Import python libs
Import Salt libs
MD5 here is temporary. Change to SHA256 when retired.
MD5 here is temporary. Change to SHA256 when retired.
Import Python libs
Import Salt libs
Function aliases
Define the module's virtual name
we skip if we are searching but don't have a match
output: Deleted image d5b3865c-0804-11e5-be21-dbc4ce844ddc
Import python libs
Import salt libs
Define a function alias in order not to shadow built-in's
Import python libs
Import external libs
Import salt libs
Import salt libs
Don't shadow built-in's.
Change the egid first, as changing it after the euid will fail  if the runas user is non-privileged.
Restore the euid/egid  Wait to raise the exception until euid/egid are restored to avoid  permission errors in writing to minion log.
Change the egid first, as changing it after the euid will fail  if the runas user is non-privileged.
Define cleaned_files here so that an exception will not prevent this  variable from being defined and cause a NameError in the return  statement at the end of the function.
Check if zipped file is a symbolic link
Restore the euid/egid  Wait to raise the exception until euid/egid are restored to avoid  permission errors in writing to minion log.
render the path as a template using path_template_engine as the engine
Trim the file list for output
Other BSD-like derivatives that use ifconfig may work too
get rid of first line  get rid of ^\n's
bridge name bridge id       STP enabled interfaces  br0       8000.e4115bac8ddc   no      eth0                                        foo0  br1       8000.e4115bac8ddc   no      eth1
Import python libs
import salt libs
rexp parses lines that look like the following:     * Safari6.1.2MountainLion-6.1.2          Safari (6.1.2), 51679K [recommended]     - iCal-1.0.2          iCal, 1.0.2, 6520K
rexp parses lines that look like the following:     * Safari6.1.2MountainLion-6.1.2          Safari (6.1.2), 51679K [recommended]
rexp parses lines that look like the following:     * Safari6.1.2MountainLion-6.1.2          Safari (6.1.2), 51679K [recommended] [restart]
remove everything after and including the '-' in the updates name.
rep parses lines that look like the following:      "Safari6.1.2MountainLion-6.1.2",  or:      Safari6.1.2MountainLion-6.1.2
This command always returns an error code, though it completes  successfully. Success will be determined by making sure get_catalog  returns the passed url
This command always returns an error code, though it completes  successfully. Success will be determined by making sure get_catalog  returns 'Default'
Import python libs
Import salt libs
Define the module's virtual name
Example:      sysctl -w net.ipv4.tcp_rmem="4096 87380 16777216"     net.ipv4.tcp_rmem = 4096 87380 16777216
If the sysctl.conf is not present, add it
Strip trailing whitespace and split the k,v
On Linux procfs, files such as /proc/sys/net/ipv4/tcp_rmem or any  other sysctl with whitespace in it consistently uses 1 tab.  Lets  allow our users to put a space or tab between multi-value sysctls  and have salt not try to set it every single time.
Do the same thing for the value 'just in case'
Import python libs
Import salt libs
Define the module's virtual name
Import python libs
Import Salt libs
Make sure we stop the container if necessary, even if an exception  was raised.
Bad input, but assume since a value was passed that  a delay was desired, and sleep for 5 seconds
mark seeded upon successful install
'machinectl list' shows only running containers, so allow this to work as an  alias to nspawn.list_running
'machinectl reboot' will fail on a stopped container
Compatibility between LXC and nspawn
No need to validate the index URL, machinectl will take care of this  for us.
Import python libs
Import Salt libs
Import 3rd party libs
for released versions the suffix for the file is same as version
for released versions the suffix for the file is same as version
for released versions the suffix for the file is same as version
Import python libs
Import salt libs
Don't shadow built-ins.
If the module is not installed, we'll be short a line
This must not be a real package
Import python libs
Make sure augeas python interface is installed
Import salt libs
Define the module's virtual name
first part up to space is always the  command name (i.e.: set, move)
optional: some functions require a repo_name, which  can be set in the config file, or passed in at the CLI.
Import python libs
Import Salt Libs
Import third party libs
User is in the org, no need for additional Data
Pull requests are included in the issue list from GitHub  Let's not include those in the return.
If only querying for one item, such as a single issue  The GitHub API returns a single dictionary, instead of  A list of dictionaries. In that case, we can return.
Only one page of data was returned; exit the loop.
Get the 'next' page number from the Link header.
Last page already processed; break the loop.
Import python libs
Import salt libs
Import Python libs
Import Salt libs
Import Python Libs
Import 3rd-party libs  pylint: disable=import-error,no-name-in-module,redefined-builtin  pylint: enable=import-error,no-name-in-module
Import salt libs
Don't shadow built-ins.
No key so recurse and show all values
Import Python libs
Import Salt libs
Import 3rd-party libs
No need to know sub-elements here
Preserve ordering
No inventory yet
keep lint from choking on _get_conn and _cache_id
Import Python libs
Import Salt libs
convert instances to list type, enabling consistent use of instances  variable throughout the register_instances method
convert instances to list type, enabling consistent use of instances  variable throughout the deregister_instances method
Import python libs
Import salt libs
Define the module's virtual name
If there were blocks and emerge could not resolve it.
Refresh before looking for the latest version available
Return a string if only one package name passed
available_version is being deprecated
not yet implemented or not applicable
GPG sign verify is supported only for "webrsync"  We prefer 'delta-webrsync' to 'webrsync'
We fall back to "webrsync" if "rsync" fails for some reason  We prefer 'delta-webrsync' to 'webrsync'
Import Python Libs
Import Salt Libs
Import Python libs
Import 3rd-party libs
Define the module's virtual name
Don't shadow built-in's.
Import python Libs
Import salt libs
if it's a string, and it's not empty check for none  return None
if there was an error return the entire response so the  alterer can get what it wants
make sure that it's a master minion
Import python libs
Import salt libs
Define the module's virtual name
When we ask to varnishadm for a specific param, it gives full  info on what that parameter is, so we just process the first  line and we get out of the loop
keep lint from choking on _get_conn and _cache_id
Import Python libs
Import Salt libs
Import Python Libs
Import Salt Libs
Import salt libs
Import python libs
Import python libs
Import salt libs
Import 3rd Party Libs
get the first certificate  Split based on defined headers
Remove all whitespace from body
Generate correctly formatted pem
Strip newlines to make passing through as cli functions easier
Remove system entries in kwargs  Including listen_in and preqreuired because they are not included in STATE_INTERNAL_KEYWORDS  for salt 2014.7.2
Overwrite any arguments in kwargs with signing_policy
X509 Version 3 has a value of 2 in the field.  Version 2 has a value of 1.  https://tools.ietf.org/html/rfc5280section-4.1.2.1
If neither public_key or csr are included, this cert is self-signed
Use explicitly set values first, fall back to CSR values.
Import python libs
Import salt libs
Destination file does not exist or could not be accessed
Get the appropriate functions
Import Python libs
Import third party libs
Get connection args from keywords if set
Return false if unable to ping server
This should fail now if the server is shutdown, which we want
Import python libs
Import salt libs
Import 3rd-party libs
We need to pause here to allow for the decoupled nature of  events time to allow the mine to propagate
If we don't have any mine functions configured, then we should just bail out
Safe error, arg may be in kwargs
Get docker info
Process docker info
Import python libs
Import salt libs
If single brick given as a string, accept it
Error for block devices with multiple bricks
Get volume status  Most probably non-existing volume, the error output is logged  Tiis return value is easy to test and intuitive
Stop volume if requested to and it is running
Fail if volume is running if stop is not requested
Import _future_ python libs first & before any other code  Import python libs
Import third party libs
Import salt libs
Define the module's virtual name
unicode to windows utf8
Assume its byte str or not a str/unicode
We should be able to leave it alone if the user has passed binary data in yaml with  binary !!  In python < 3 this should have type str and in python 3+ this should be a byte array
https://msdn.microsoft.com/en-us/library/windows/desktop/ms644952(v=vs.85).aspx
Instantiate the registry object
Get a reverse list of registry keys to be deleted  Add the top level key last, all subkeys must be deleted first
Import python libs
Import Salt libs
If var is in file
If var in file, trim value from its value
Remove any escaping that was needed to past through salt
Import python libs
Import salt libs
Import 3rd-party libs
Set default retcode to 0
Make sure cache file isn't read-only
Make sure cache file isn't read-only
Work around Windows multiprocessing bug, set __opts__['test'] back to  value from before this function was run.
Modification to __opts__ lost after this if-else
Ensure desired environment
Make sure cache file isn't read-only
Work around Windows multiprocessing bug, set __opts__['test'] back to  value from before this function was run.
Can't serialize pydsl
Work around Windows multiprocessing bug, set __opts__['test'] back to  value from before this function was run.
Work around Windows multiprocessing bug, set __opts__['test'] back to  value from before this function was run.
Work around Windows multiprocessing bug, set __opts__['test'] back to  value from before this function was run.
Work around Windows multiprocessing bug, set __opts__['test'] back to  value from before this function was run.
refresh the grains
refresh the grains
Import python libs
Import salt libs
Define the module's virtual name
Import python libs
Making sure the result is JSON serializable
Trying to look like the output of cur.fetchall()
We should get one, and only one row
We should get one, and only one row
We should get one, and only one row
'database' argument is mandatory
We should get one, and only one row
'database' argument is mandatory
Import python libs
Import 3rd-party libs
Define the module's virtual name
Set context key to avoid repeating this check
The runlevel is unknown, return the default
Add in any sysvinit services that are enabled
Add in any sysvinit services that are disabled
sysvinit services cannot be static
Import python libs
Import salt libs
Define the module's virtual name
Verify pvcreate was successful
All specified devices are already LVM volumes
Verify pvcreate was successful
Verify pvcremove was successful
Import python lib
will try to import NAPALM  https://github.com/napalm-automation/napalm  pylint: disable=W0611  pylint: enable=W0611
Import python libs
Import salt libs
/proc/PID/fd
Check kernel versions
scripts that does a single job and then exit
Import python libs
Import salt libs  import salt.log
Solve the Chicken and egg problem where grains need to run before any  of the modules are loaded and are generally available for any usage.
Sometimes dmidecode delivers comments in strings.  Don't.
The first line of a handle is a description of the type
empty record
log.debug('{0} record contains {1}'.format(record, dmi_raw))
Installable Languages: 1         en-US  Characteristics:         PCI is supported         PNP is supported  log.debug('DMI key {0} gained list item {1}'.format(key, val))
pylint: disable=bare-except
log.debug('DMI {0} value {1} seems invalid or empty'.format(key, val))
AssetTag0. Manufacturer04. Begone.
Import Python Libs
PyLint wrongly reports an error when calling super, hence the above  disable call
Remove: [WARNING ] Use of send mask waiters is deprecated.
Import Python libs
Import Salt libs
Function alias to set mapping.
Get the path to the zfs binary.
get man location
not using salt.utils.which('zfs') to keep compatible with others
if zpool properties specified, then  create "-o property=value" pairs
append name
Create filesystem
Check and see if the dataset is available
fix up conflicting parameters
filter on type
recursively list
add properties
add name if set
if zpool properties specified, then  create "-o property=value" pairs
abort if we do not have feature flags
verify snapshots
fallback in case we hit a weird error
verify snapshots
fallback in case we hit a weird error
verify snapshots
if zpool properties specified, then  create "-o property=value" pairs
fallback in case we hit a weird error
verify snapshots
for better error handling we don't do one big set command
recursively get
fields
filter on type
filter on type
properties
datasets
Import python libs
Import 3rd-party libs
Remove all the spaces, the key must not have any space
Check if is a dict to get the arguments  in command if not set the arguments to empty string
Check if execute bit
Import salt libs
Import python libs
Import 3rd-party libs
Optional
Import python libs
Import salt libs
Get logging started
Function alias to not shadow built-ins
Define the module's virtual name
Import python libs
Import salt libs
Don't shadow built-in's.
Requirements file does not exist in the given saltenv.
It's not cached, let's cache it.  Check if the master version has changed.
Catch floating point input, exception will be caught in  exception class below.
Check the locally installed pip version
From pip v1.4 the --pre flag is available
Is the editable local?
Missing egg=theEggName
Catch floating point input, exception will be caught in  exception class below.
ignore -f line as it contains --find-links directory  ignore comment lines
ignore hg + not trust problem
will try to import NAPALM  https://github.com/napalm-automation/napalm  pylint: disable=W0611  pylint: enable=W0611
Import python libs
Import salt libs
Define the module's virtual name
If the sysctl.conf is not present, add it
If apply_change=True, apply edits to system
Import python libs
Import salt libs
Get logging started
Function alias to not shadow built-ins
Import python libs
Import Salt libs
Import 3rd-party libs
This is a local call
This is a local call
Create user account first
Configure users permissions
Configure users password
Enable users admin
When users don't provide a user ID we need to search for this
Generate privilege bit mask
The keys for this dictionary are strings, not integers, so convert the  argument to a string
Import salt libs
Define the module's virtual name
What type of rule is this?
If a user is not specified, use the owner of the file
Import python libs
Import Salt libs
Define the module's virtual name
until a hash gets published that we can verify the newest setup against  just go ahead and download a new one.
If we want to install packages  but we don't have cygwin installed yet  install just the base system
Can't update something that isn't installed
Import Python Libs
override timeout
Import Python libs
Import 3rd-party libs  pylint: disable=import-error,no-name-in-module,redefined-builtin
Import salt libs
keep lint from choking on _get_conn and _cache_id
Import Python libs
Import third party libs
Table creation can take several seconds to propagate.  We will check MAX_ATTEMPTS times.
Table deletion can take several seconds to propagate.  We will retry MAX_ATTEMPTS times.
Import python libs
Import salt libs
Should this faile on the wrong type.
salt-call tls.create_ca my_little \
salt-call tls.create_csr my_little CN=www.example.com
salt-call tls.create_ca_signed_cert my_little CN=www.example.com
salt-call tls.create_csr my_little CN=DBReplica_No.1 cert_type=client
salt-call tls.create_ca_signed_cert my_little CN=DBReplica_No.1
salt-call tls.create_csr my_little CN=MasterDBReplica_No.2  \
salt-call tls.create_ca_signed_cert my_little CN=MasterDBReplica_No.2
salt-call tls.create_csr my_little CN=MasterDBReplica_No.2 \
salt-call tls.create_csr my_little CN=MasterDBReplica_No.2 \
salt-call tls.create_ca_signed_cert my_little CN=MasterDBReplica_No.2
salt-call tls.create_ca_signed_cert my_little CN=MasterDBReplica_No.2 \
salt-call tls.create_csr my_little CN=www.anothersometh.ing \
salt-call tls_create_ca_signed_cert my_little CN=www.anothersometh.ing \
Import python libs
NOTE: Not having configured a cert path should not prevent this  module from loading as it provides methods to configure the path.
gotta prepend a /
try to determine the key bits
create certificate
format that OpenSSL returns dates in
Import python libs
Import salt libs
Define the module's virtual name
'removed' not yet implemented or not applicable
Initialize the dict with empty strings
Refresh before looking for the latest version available
Remove revision for version comparison
Return a string if only one package name passed
available_version is being deprecated
Ignore 'sources' argument
Import python libs
Import salt libs
Import 3rd-party libs
Set up logging
Define the module's virtual name
Get the path to the gpg binary.
Delete the secret key
This version does not allow us to encrypt using the  file stream  have to read in the contents and encrypt.
Import salt libs
Import python libs
Import salt libs
Define the module's virtual name
Disable on these platforms, specific service modules exist:
probably will never reached
Import third party libs
Define the module's virtual name
findPackages requires one arg, but does nothing with it.  So we will just pass None in for the required arg
Import Salt libs
Don't shadow built-in's.
Import python libs
Import salt libs
Import third party libs
Define the module's virtual name
These config items must be set in the minion config
Import python libs
Import salt libs
Define the module's virtual name
No change in package list, but the make install was successful.  Assume that the installation was a recompile with new options, and  set return dict so that changes are detected by the ports.installed  state.
Get top-level key for later reference
Import python libs
Import salt libs
Use Install-WindowsFeature on Windows 8 (osversion 6.2) and later minions. Includes Windows 2012+.  Default to Add-WindowsFeature for earlier releases of Windows.  The newer command makes management tools optional so add them for partity with old behavior.
Import python libs
Import third party libs
Define the module's virtual name
Import python libs
Import salt libs
Import 3rd-party libs
render the path as a template using path_template_engine as the engine
Backwards compatibility
Backwards compatibility
Backwards compatibility
Backwards compatibility
Backwards compatibility
Backwards compatibility
Cache was successful, store the result in __context__ to prevent  multiple caches (see above).
Backwards compatibility
Backwards compatibility
Backwards compatibility
If the file has already been cached, return the path
The file hasn't been cached or has changed; cache it
Backwards compatibility
Backwards compatibility
Backwards compatibility
Backwards compatibility
Backwards compatibility
Backwards compatibility
Backwards compatibility
Import python libs
Import 3rd-party libs  pylint: disable=import-error  pylint: disable=no-name-in-module  pylint: enable=import-error,no-name-in-module
Fix a nasty bug with Win32 Python not supporting all of the standard signals
If we're going to block, first setup a listener
Wait for the finish event to fire  Blocks until we hear this event or until the timeout expires
salt my-minion saltutil.find_job 20160503150049487736
msgpack error in salt-ssh
Invalid serial object
Some OS's (Win32) don't have SIGKILL, so use salt_SIGKILL which is set to  an appropriate value for the operating system this is running on.
Some OS's (Win32) don't have SIGKILL, so use salt_SIGKILL which is set to  an appropriate value for the operating system this is running on.
fcn_ret can be empty, so we cannot len the whole return dict  do not wait for timeout when explicit list matching  and all results are there
this assignment is so that the rest of fxns called by salt still  have minion context
this assignment is so that fxns called by mminion have minion  context
Import python libs
Import Salt libs
Define the module's virtual name
not yet implemented or not applicable
Initialize the dict with empty strings
Return a string if only one package name passed
available_version is being deprecated
Import python libs
Import salt libs
Set up logger
Define a function alias in order not to shadow built-in's
Define the module's virtual name
Lets make sure the device exists before running mdadm
Remove entry from config file:
Devices may have been written with a blob:
Import python libs  pylint: disable=W0611
Import third party libs
Import salt libs
Define the module's virtual name
check if the privilege is already in the requested state
if user does not exist...  1332 = No mapping between account names and security IDs was carried  out.
if user does not exist...  1332 = No mapping between account names and security IDs was carried  out.
get SID object for user
get SID object for group
set owner and group
set owner only
get SID object for group
set up dictionary for attribute values
Get cumulative int value of attributes
Does the file/folder exists
Remove ReadOnly Attribute  Get current file attributes
A file and a symlinked file are removed the same way
If it's a symlink directory, use the rmdir command
If it's a normal directory, recurse to remove it's contents
rmdir will work now because the directory is empty
Reset attributes to the original if delete fails.
When Python 3.2 or later becomes the minimum version, this function can be  replaced with the built-in os.symlink function, which supports Windows.
ensure paths are using the right slashes
check that it is a symlink reparse point (in case it is something else,  like a mount point)
sanity check - this should not happen  not a reparse point
parse the structure header to work out which type of reparse point this is  http://msdn.microsoft.com/en-us/library/windows/desktop/aa365511.aspx
ensure paths are using the right slashes
parse as a symlink reparse point structure (the structure for other  reparse points is different)
comes out in 8.3 form; convert it to LFN to make it look nicer
if file is not found (i.e. bad symlink), return it anyway like on *nix
keep lint from choking on _get_conn and _cache_id
Import Python libs
Import salt libs
if rate limit, retry:
if rate limit, retry:
if rate limit, retry:
add_record requires a ttl value, annoyingly.
if rate limit, retry:
if rate limit, retry:
if rate limit, retry:
Import python libs
Import salt libs
Function alias to make sure not to shadow built-in's
The monit binary exists, let the module load
Import python libs
Import third party libs
Import salt libs
Set up template environment
support old location
old style dicts (top-level dicts)  virt:     nic:         eth0:             bridge: br0         eth1:             network: test_net
we want to discard the original key
When using a disk profile extract the sole dict key of the first  array element as the filename for disk
Apply umask and remove exec bit
This domain already exists
libvirt has a funny bitwise system for the flags in that the flag  to affect the "current" setting is 0, which means that to set the  current setting we have to call it a second time with just 0 set
return True if both calls succeeded
see notes in setmem
Take off just enough to sustain the hypervisor
reset takes a flag, like reboot, but it is not yet used  so we just pass in 0  see: http://libvirt.org/html/libvirt-libvirt.htmlvirDomainReset
return False if state is set to something other then on or off
This is thrown if the machine is already shut down
No /proc/modules? Are we on Windows? Or Solaris?
virtual_subtype isn't set everywhere.
No /proc/modules? Are we on Windows? Or Solaris?
Divide by vcpus to always return a number between 0 and 100
Can not run function blockStats on inactive VMs
This one is only in 1.2.14+
This one is only in 1.1.3+
Try do it by ourselves  Find the models in cpu_map.xml and iterate over them for as long as entries have submodels
Import python libs
Import salt libs
Ignore value_regex
Users should never be numeric but if we don't account for this then  we're going to get a traceback if someone passes this invalid input.
if the statefile provides multiple identities, they need to be tried  (but also allow a string instead of a list)  force it into a list
--local added in 1.7.10.2
For earlier versions, need to specify the path to the git config file
Sanitize kwargs and make sure that no invalid ones were passed. This  allows us to accept 'format' as an argument to this function without  shadowing the format() global, while also not allowing unwanted arguments  to be passed.
No output (unless --verbose is used, and we don't want all files listed  in the output in case there are thousands), so just return True
Checkout message goes to stderr
Add the '--' to terminate CLI args, but only if it wasn't already  passed in opts string.
Sanitize kwargs and make sure that no invalid ones were passed. This  allows us to accept 'all' as an argument to this function without  shadowing all(), while also not allowing unwanted arguments to be passed.
git config --get exits with retcode of 1 when key does not exist  Should never happen but I'm paranoid and don't like tracebacks
ref is a new tag/branch
ref is a branch update
ref is an updated tag
Using lower here because booleans would be capitalized when  converted to a string.
Stale worktrees are not desired, skip this one
Only stale worktrees are desired, skip this one
Initialize the dict where we're storing the tracked data points
Check to see if HEAD points at a tag
Ignore other lines, if they exist (which they  shouldn't)
Raise a CommandExecutionError
Emulate what 'git worktree list' does under-the-hood, and  that is using the toplevel directory. It will still give  inaccurate results, but will avoid a traceback.
Check to see if HEAD points at a tag
No numeric actions but this will prevent a traceback when the git  command is run.
Update submodule and ensure it is initialized (before 2015.8.0)  Update submodule and ensure it is initialized (2015.8.0 and later)
Rebase submodule (2015.8.0 and later)
Add submodule (2015.8.0 and later)
Somehow git --version returned no stdout while not raising an  error. Should never happen but we should still account for this  possible edge case.
Checkout message goes to stderr
Import salt libs
Define the module's virtual name
Test data
Initializing the test variables
Test data
Initializing the test variables
Initializing the test variables
We test memory read / write against global / local scope of memory  Test data
Initializing the test variables
Test data
Initializing the required variables
Test begins!
Prepare phase
Test phase
Clean up phase
Import Python Libs
Import python libs
Import third party libs
Set up logging
Define a function alias in order not to shadow built-in's
Import python libs
Import salt libs
time must be double quoted '"17:46"'
Import python libs
Import python libs
Import salt libs
Define the module's virtual name
not yet implemented or not applicable
Perhaps this will need an option to pick devel by default
available_version is being deprecated
Ensure we've tapped the repo if necessary  Feels like there is a better way to allow for tap being  specified as both a string and a list
Import python libs  pylint: disable=import-error,no-name-in-module
Import third party libs  pylint: disable=import-error,no-name-in-module  pylint: disable=import-error  pylint: enable=import-error
Import salt libs
Define the module's virtual name
Initialize the return dict with empty strings
Refresh before looking for the latest version available
iterate over all requested package names
available_version is being deprecated
not yet implemented or not applicable
Look up version from winrepo
some MS Office updates don't register a product name which means  their information is useless
Cache repo-ng locally
The source_hash is a file on a server
The source_hash is a hash string
Make sure name or pkgs is passed
Ignore pkg_type from parse_targets, Windows does not support the  "sources" argument
Only use the 'version' param if 'name' was not specified as a  comma-separated list
Get a list of currently installed software for comparison at the end
Loop through each package
Load package information for the package
Make sure pkginfo was found
Get the version number passed or the latest available
Check if the version is already installed  Desired version number already installed
If version number not installed, is the version available?
Get the installer settings from winrepo.p
Is there an installer configured?
Is the installer in a location that requires caching
Check to see if the cache_file is cached... if passed
Check to see if the file is cached
Check if the cache_file was cached successfully
Check to see if the installer is cached  It's not cached. Cache it, mate.
Check if the installer was cached successfully
Run the installer directly (not hosted on salt:, https:, etc.)
Fix non-windows slashes
Install the software  Check Use Scheduler Option
Get a new list of installed software
For installers that have no specific version (ie: chrome)  The software definition file will have a version of 'latest'  In that case there's no way to know which version has been installed  Just return the current installed version
Check for changes in the registry
Compare the software list before and after  Add the difference to ret
if salt.utils.is_true(refresh):     refresh_db()
Make sure name or pkgs is passed
Get package parameters
Get a list of currently installed software for comparison at the end
Loop through each package
Load package information for the package
Make sure pkginfo was found
Get latest version if no version passed, else use passed version
Get the uninstaller
If no uninstaller found, use the installer
If still no uninstaller found, fail
Where is the uninstaller
Check to see if the uninstaller is cached  It's not cached. Cache it, mate.
Check if the uninstaller was cached successfully
Run the uninstaller directly (not hosted on salt:, https:, etc.)
Fix non-windows slashes
Get parameters for cmd
Uninstall the software  Check Use Scheduler Option
Get a new list of installed software
Compare the software list before and after  Add the difference to ret
Import Python libs
Import salt libs
Don't shadow built-in's.
Define the module's virtual name
keep lint from choking on _get_conn and _cache_id
Import Python libs
Import third party libs
Import python libs
Import salt libs
Define the module's virtual name
This is a kernel module, return it without the .ko extension
It's compiled into the kernel
Import python libs
Import salt libs
Set up logging
Allow switch from '*' or not present to 'random'
Appears to be a standard incron line
For consistency's sake
Scrub the types
Check for valid mask types
Failed to commit, return the error
Scrub the types
Check for valid mask types
Failed to commit, return the error
keep lint from choking on _get_conn and _cache_id
Import third party libs
Import salt libs
constraint command follows a different order
constraint command only shows id, when using '--full'-parameter
constraint command follows a different order
constraint command needs item_id in format 'id=<id' after all params  constraint command follows a different order
constraint command needs item_id in format 'id=<id' after all params
Import python libs
Import Python libs
Import Salt libs
Function aliases
Define the module's virtual name
return uuid
prepare vmcfg
prepare vmcfg
Import python libs
Import salt libs
Define the module's virtual name
Import Python libs
Import Salt libs
Import 3rd-party libs
Import python libs
Import salt libs
Define the module's virtual name
Import Python libs
Import Salt libs
get man location
ignore header
parse zpool list data
parse zpool list data
Check if the pool_name is already being used
Create storage pool
Check and see if the pools is available
check for pool
check for pool
check for pool
Make sure pool is there
check devices
Get file names to create  check if file is present if not add it
Makesure the files are there
Check if the pool_name exists
get expand option
Check if the pool_name exists
Import python libs
Import salt libs
Import Python libs
Import Salt libs
raise CommandExecutionError(sysrcs)
Import python libs
Import 3rd-party libs  pylint: disable=import-error,redefined-builtin  pylint: enable=import-error,redefined-builtin
Import salt libs
Tested on OpenBSD 5.0
Shim to produce output similar to what __virtual__() should do  but __salt__ isn't available in __virtual__()  Tested on CentOS 5.8
No jobs so return
Split each job into a dictionary and handle  pulling out tags or only listing jobs with a certain  tag
Shim to produce output similar to what __virtual__() should do  but __salt__ isn't available in __virtual__()
Shim to produce output similar to what __virtual__() should do  but __salt__ isn't available in __virtual__()
Shim to produce output similar to what __virtual__() should do  but __salt__ isn't available in __virtual__()
Import Python Libs
Import salt libs
Filter out types
Import python libs
Import salt libs
Earlier versions of traffic_ctl do not support  "server stop", so we prefer traffic_line here.
Earlier versions of traffic_ctl do not support  "server start", so we prefer traffic_line here.
Import python libs
Import salt libs
Define the module's virtual name
These are the likely locations for the file on Ubuntu
The runlevel is unknown, return the default
Import salt libs
Define the module's virtual name
Import python libs
Import third party libs
Import salt libs
Provides:    shell
Import salt libs
Import python libs
Import salt libs
Solve the Chicken and egg problem where grains need to run before any  of the modules are loaded and are generally available for any usage.
Import Python Libs
Import Salt Libs
Try to authenticate with the given user/password combination
If we can't authenticate, continue on to try the next password.  If we have data returned from above, we've successfully authenticated.
We've reached the end of the list without successfully authenticating.
Import Python Libs
Import Salt Libs
Import python libs
Import salt libs
Import python libs
Import salt libs
Solve the Chicken and egg problem where grains need to run before any  of the modules are loaded and are generally available for any usage.
Import 3rd-party libs
attempt to import the python wmi module  the Windows minion uses WMI for some of its grains
dominant gpu vendors to search for (MUST be lowercase for matching below)
Provides:    cpuarch    num_cpus    cpu_model    cpu_flags
Provides:    cpuarch    num_cpus    cpu_model    cpu_flags
Provides:    mem_total
get the Total Physical memory as reported by msinfo32  return memory info in gigabytes
Provides:    virtual    virtual_subtype
FIXME: Make this detect between kvm or qemu
This is going to be a monster, if you are running a vm you can test this  grain with please submit patches!  Provides:    virtual    virtual_subtype
Skip the below loop on platforms which have none of the desired cmds  This is a temporary measure until we can write proper virtual hardware  detection.
Add additional last resort commands
systemd-detect-virt always returns > 0 on non-virtualized  systems  prtdiag only works in the global zone, skip if it fails
Break out of the loop so the next log message is not issued
if 'virt-what' returns nothing, it's either an undetected platform  so we default just as virt-what to 'physical', otherwise use the  platform detected/returned by virt-what
a posteriori, it's expected for these to have failed:  Provide additional detection for OpenVZ
Tested on CentOS 5.3 / 2.6.18-194.26.1.el5xen  Tested on CentOS 5.4 / 2.6.18-164.15.1.el5xen
Requires dmidecode!
Tested on CentOS 5.5 / 2.6.18-194.3.1.el5xen
Shouldn't get to this, but just in case  Tested on Fedora 10 / 2.6.27.30-170.2.82 with xen  Tested on Fedora 15 / 2.6.41.4-1 without running xen
An actual DomU will have several drivers  whereas a paravirt ops kernel will  not.  If a Dom0 or DomU was detected, obviously this is xen
the name of the OS comes with a bunch of other data about the install  location. For example:  'Microsoft Windows Server 2008 R2 Standard |C:\\Windows|\\Device\\Harddisk0\\Partition2'
Shell special characters ("$", quotes, backslash, backtick)  are escaped with backslashes
pylint: disable=unpacking-non-sequence  pylint: enable=unpacking-non-sequence
Add lsb grains on any distro with lsb-release
Set a blank osrelease grain and fallback to 'Solaris'  as the 'os' grain.
freebsd-version was introduced in 10.0.  derive osrelease from kernelversion prior to that
this assigns family names based on the os name  family defaults to the os name if not found
Return osarch in priority order (higher to lower)
Get the hardware and bios data
Load the virtual machine info
Load additional OS family grains
locale.getdefaultlocale can ValueError!! Catch anything else it  might do, per 2205
This is going to need some work  Provides:    fqdn    host    localhost    domain
Provides:    hwaddr_interfaces
Provides:    path
Provides:    pythonversion
Provides:    pythonpath
Provides:    pythonexecutable
Provides:    saltpath
Provides:    saltversion
Provides:    zmqversion
Provides:    saltversioninfo
sysinfo derived smbios grains
Provides:    master
Import python libs
remove destdir if it is a regular file to avoid an OSError when  running os.makedirs below
We want to make sure files start with this *directory*, use  '/' explicitly because the master (that's generating the  list of files) only runs on POSIX
Use shallow copy so we don't disturb the memory used by os.walk. Otherwise this breaks!
Local filesystem
Use list here to make it writable inside the on_header callback. Simple bool doesn't  work here: on_header creates a new local variable instead. This could be avoided in  Py3 with 'nonlocal' statement. There is no Py2 alternative for this.
Not the first line, do nothing
Failed to render the template
No destination passed, set the dest as an extrn_files cache  If Salt generated the dest name, create any required dirs
The path arguments are escaped
Hash compare local copy with master and skip download  if no difference found.
If a directory was formerly cached at this path, then  remove it to avoid a traceback trying to write the file
The generated api key to use  The apikey's shared secret
Import python libs
Import salt cloud libs
Get logging started
Check for required profile parameters before sending any API calls.
Since using "provider: <provider-engine>" is deprecated, alias provider  to use driver: "driver: <provider-engine>"
Show the traceback if the debug logging level is enabled
Be default, service_url is set to amazonaws.com. If you are using this  driver for something other than Amazon EC2, change it here:
The endpoint that is ultimately used is usually formed using the region  and the service_url. If you would like to override that entirely, you  can explicitly define the endpoint:
Defaults to None  Required
Defaults to port 22  Optional
Defaults to root  Optional
Private key defaults to None
Password defaults to None
Pass userdata to the instance to be created
Import python libs
Import libs for talking to the EC2 API
Import Salt Libs
Import salt.cloud libs
Import 3rd-Party Libs  Try to import PyCrypto, which may not be installed on a RAET-based system  PKCS1_v1_5 was added in PyCrypto 2.5
Get logging started
Retrieve access credentials from meta-data, or use provided
Create a date for headers and the credential string
result.getcode()
Check to see if a SSH Gateway will be used.
ssh_gateway
ssh_gateway_port
ssh_gateway_username
ssh_gateway_private_key
ssh_gateway_password
Validate user-specified AZ
check specified AZ is available
Since using "provider: <provider-engine>" is deprecated, alias provider  to use driver: "driver: <provider-engine>"
Assume id of EIP as value
convert boolean True/False to 'true'/'false'
Technically this function may be called other ways too, but it  definitely cannot be called with --function.
All of the necessary launch parameters for a VM when using  spot instances are the same except for the prefix below  being tacked on.
Normal instances should have no prefix.
Show the traceback if the debug logging level is enabled
make sure we have a response
Device already listed, just grab the index
Add the device name in since it wasn't already there
Set the termination value
Use default volume type if not specified
Show the traceback if the debug logging level is enabled
if we're using spot instances, we need to wait for the spot request  to become active before we continue
Trigger a failure in the wait for spot instance method
Trigger a failure in the wait for spot instance method
Still waiting for an active state
Request will never be active, fail
Cancel the existing spot instance request
Technically this function may be called other ways too, but it  definitely cannot be called with --function.
Just a little delay between attempts...
Just a little delay between attempts...
Trigger a failure in the wait for IP function
Trigger a failure in the wait for IP function
It might be already up, let's destroy it!
Technically this function may be called other ways too, but it  definitely cannot be called with --function.
This wait is so high, because the password is unlikely to  be generated for at least 4 minutes
SMB used whether winexe or winrm
If not using winrm keep same winexe behavior
If using winrm
Default HTTPS port can be changed in cloud configuration
Wait for winrm port to be available
Check for required profile parameters before sending any API calls.
Since using "provider: <provider-engine>" is deprecated, alias provider  to use driver: "driver: <provider-engine>"
wait_for_instance requires private_key
Get SSH Gateway config early to verify the private_key,  if used, exists or not. We don't want to deploy an instance  and not be able to access it via the gateway.
Put together all of the information required to request the instance,  and then fire off the request for it
If data is a str, it's an error
Multiple instances may have been spun up, get all their IDs
Multiple instances were spun up, get one now, and queue the rest
Wait for vital information, such as IP addresses, to be available  for the new instance
At this point, the node is created and tagged, and now needs to be  bootstrapped, once the necessary port is available.
The instance is booted and accessible, let's Salt it!
Ensure that the latest node data is returned
Update the delvol parameter for this volume
This second check is a safety, in case the above still failed to produce  a usable ID
We were not setting this tag
This is a correctly set tag with no value
Just a little delay between attempts...
Just a little delay between attempts...
If there aren't any profiles defined for EC2, check  the provider config file, or use the default location.
Since using "provider: <provider-engine>" is deprecated, alias provider  to use driver: "driver: <provider-engine>"
This number represents GiB
You can't set `encrypted` if you pass a snapshot
Waits till volume is available
Waits till volume is available
The AWS correct way is to use non-plurals like snapshot_id INSTEAD of snapshot_ids.
pylint: disable=wrong-import-position,wrong-import-order
pylint: disable=invalid-name  pylint: enable=invalid-name
Get logging started
Find under which cloud service the name is listed, if any
There is no public IP on this interface
https://{storage_account}.blob.core.windows.net/{path}/{vhd}
Check for required profile parameters before sending any API calls.
Since using "provider: <provider-engine>" is deprecated, alias provider  to use driver: "driver: <provider-engine>"
Import Python Libs
Import salt libs
Import pyrax libraries  This is typically against SaltStack coding styles,  it should be 'import salt.utils.openstack.pyrax as suop'.  Something  in the loader is creating a name clash and making that form fail
aliyun Access Key ID  aliyun Access Key Secret
Import python libs
Import salt cloud libs
Import Third Party Libs
Get logging started
Optional parameters  'DataDisk.n.Size', 'DataDisk.n.Category', 'DataDisk.n.SnapshotId'
invoke web call
Check for required profile parameters before sending any API calls.
Since using "provider: <provider-engine>" is deprecated, alias provider  to use driver: "driver: <provider-engine>"
Show the traceback if the debug logging level is enabled
Trigger an error in the wait_for_ip function
It might be already up, let's destroy it!
The instance is booted and accessible, let's Salt it!
All aliyun API only support GET method
include action or function parameters
Calculate the string for Signature
Just a little delay between attempts...
DescribeImages so far support input multi-image. And  if not found certain image, the response will include  blank image list other than 'not found' error message
Import python libs
Import salt libs
Import salt cloud libs
Attempt to import pyVim and pyVmomi libs
Disable InsecureRequestWarning generated on python > 2.6
Salt version <= 2014.7.0
Get logging started
If type not specified or does not match, don't change adapter type
Virtual disks can be shared between virtual machines on the same server
Virtual disks can be shared between virtual machines on any server
Virtual disks cannot be shared between virtual machines
this is an IDE controller to add new cd drives to
create the network adapter
create the SCSI controller
create the IDE controller
Make sure IP has four octets
convert octet from string to int
couldn't convert octet to an integer
map variables to elements of octets list
Check first_octet meets conditions
Check 169.254.X.X condition
Check 2nd - 4th octets  Passed all of the checks
Check if child snapshots exist
Exit if template
Exit if VMware tools is already up to date
Exit if VM is not powered on
Exit if VMware tools is either not running or not installed
Get the service instance object
Get the inventory
Show the traceback if the debug logging level is enabled
Show the traceback if the debug logging level is enabled
Show the traceback if the debug logging level is enabled
Show the traceback if the debug logging level is enabled
Show the traceback if the debug logging level is enabled
Show the traceback if the debug logging level is enabled
Check for required profile parameters before sending any API calls.
Since using "provider: <provider-engine>" is deprecated, alias provider  to use driver: "driver: <provider-engine>"
If datacenter is specified, set the container reference to start search from it instead
Create the relocation specs
Create the config specs
Create the clone specs
datastore cluster has been specified so apply Storage DRS recomendations
get si instance to refer to the content
get recommended datastores
clone the VM/template
Show the traceback if the debug logging level is enabled
Find how to power on in CreateVM_Task (if possible), for now this will do
Check if datacenter already exists
Get the service instance
Check if cluster already exists
Show the traceback if the debug logging level is enabled
Show the traceback if the debug logging level is enabled
Show the traceback if the debug logging level is enabled
Get the service instance object
Either memdump or quiesce should be set to True
Show the traceback if the debug logging level is enabled
Show the traceback if the debug logging level is enabled
Show the traceback if the debug logging level is enabled
Show the traceback if the debug logging level is enabled
Show the traceback if the debug logging level is enabled
This is a host system that is part of a Cluster
This is a standalone host system
Show the traceback if the debug logging level is enabled
Show the traceback if the debug logging level is enabled
Show the traceback if the debug logging level is enabled
Show the traceback if the debug logging level is enabled
Check if datastore cluster already exists
Show the traceback if the debug logging level is enabled
Proxmox account information
Import python libs
Import salt libs
Import salt cloud libs
Import Third Party Libs
Get logging started
Requested to include the detailed configuration of a VM
could also use the get_resources_nodes but speed is ~the same
Figure out which is which to put it in the right column
Check for required profile parameters before sending any API calls.
Since using "provider: <provider-engine>" is deprecated, alias provider  to use driver: "driver: <provider-engine>"
Show the traceback if the debug logging level is enabled
Determine which IP to use in order of preference:
wait until the vm has been created so we can start it
VM has been created. Starting..
Wrong VM type given
Use globally configured/default location
No location given for the profile
Required by both OpenVZ and Qemu (KVM)
OpenVZ related settings, using non-default names:
optional VZ settings
LXC related settings, using non-default names:
inform user the "disk" option is not supported for LXC hosts
gateway is optional and does not assume a default
optional Qemu settings
The node is ready. Lets request it to be added
If we reached this point, we have all the information we need
stop the vm
wait until stopped
required to wait a bit here, otherwise the VM is sometimes  still locked and destroy fails.
Import python libs
Import salt libs
Import salt cloud libs
Import 3rd-party libs
Get logging started
opts = _master_opts()  opts['output'] = 'quiet'
try to has some low timeouts for very basic commands
we cant use profile as a configuration key as it conflicts  with salt cloud internals
Since using "provider: <provider-engine>" is deprecated, alias provider  to use driver: "driver: <provider-engine>"
Import python libs
Import salt libs
Import salt cloud libs
Get logging started
SoftLayer account api key
Import python libs
Import salt cloud libs
Attempt to import softlayer lib
Get logging started
Check for required profile parameters before sending any API calls.
Since using "provider: <provider-engine>" is deprecated, alias provider  to use driver: "driver: <provider-engine>"
Show the traceback if the debug logging level is enabled
If the VM was created with use_fqdn, the short hostname will be used instead.
Import python libs
Import salt libs
Get logging started
Find under which cloud service the name is listed, if any
Check for required profile parameters before sending any API calls.
Since using "provider: <provider-engine>" is deprecated, alias provider  to use driver: "driver: <provider-engine>"
Domain and WinRM configuration not yet supported by Salt Cloud
Services can't be cleaned up unless disks are too
To reflect the Azure API
To reflect the Azure API
For consistency with Azure SDK
For consistency with Azure SDK
For consistency with Azure SDK
For consistency with Azure SDK
For consistency with Azure SDK
For consistency with Azure SDK
For consistency with Azure SDK
For consistency with Azure SDK
For consistency with Azure SDK
For consistency with Azure SDK
For consistency with Azure SDK
Import Python Libs
Import Salt Libs
Import Salt Cloud Libs
Import Third Party Libs
Get Logging Started
Check for required profile parameters before sending any API calls.
Since using "provider: <provider-engine>" is deprecated, alias provider  to use driver: "driver: <provider-engine>"
Show the traceback if the debug logging level is enabled
Show the traceback if the debug logging level is enabled
Trigger an error in the wait_for_ip function
It might be already up, let's destroy it!
Just a little delay between attempts...
The OpenStack password  The OpenStack API key
The OpenStack password is stored in keyring  don't forget to set the password by running something like:  salt-cloud --set-password=myuser my-openstack-keyring-config
Ignore IP addresses on this network for bootstrap
Import python libs
Import libcloud
Import salt libs
Import salt.cloud libs
Import netaddr IP matching
Get logging started
If we cannot check, assume all is ok
Technically this function may be called other ways too, but it  definitely cannot be called with --function.
Note: This currently requires libcloud trunk
Check for required profile parameters before sending any API calls.
Since using "provider: <provider-engine>" is deprecated, alias provider  to use driver: "driver: <provider-engine>"
Put together all of the information required to request the instance,  and then fire off the request for it
Pull the instance ID, valid for both spot and normal instances
Show the traceback if the debug logging level is enabled
Trigger a failure in the wait for IP function
Still not running, trigger another iteration
Note(pabelanger): Because we loop, we only want to attach the  floating IP address one. So, expect failures if the IP is  already attached.
It might be already up, let's destroy it!
SoftLayer account api key
Import python libs
Import salt cloud libs
Attempt to import softlayer lib
Get logging started
Check for required profile parameters before sending any API calls.
Since using "provider: <provider-engine>" is deprecated, alias provider  to use driver: "driver: <provider-engine>"
Default is 273 (100 Mbps Public & Private Networks)
Default is 1800 (0 GB Bandwidth)
Leaving the following line in, commented, for easy debugging
Show the traceback if the debug logging level is enabled
15 minutes
If the VM was created with use_fqdn, the short hostname will be used instead.
Import Python Libs
Import Salt-Cloud Libs
Get logging started
The epoch of the last time a query was made
Boot the VM and get the JobID from Linode
Check for required profile parameters before sending any API calls.
Since using "provider: <provider-engine>" is deprecated, alias provider  to use driver: "driver: <provider-engine>"
Linode's default datacenter is Dallas, but we still have to set one to  use the create function from Linode's API. Dallas's datacenter id is 2.
Update the Linode's Label to reflect the given VM name
Add private IP address if requested
Define which ssh_interface to use
If ssh_interface is set to use private_ips, but assign_private_ip  wasn't set to True, let's help out and create a private ip.
Create a ConfigID using disk ids
Boot the Linode
Pass the correct IP address to the bootstrap ssh_host key
If a password wasn't supplied in the profile or provider config, set it now.
Bootstrap!
138 appears to always be the latest 64-bit kernel for Linux
Scaleway organization and token
Import Python Libs
Import Salt Libs
Import Third Party Libs
For each server, iterate on its parameters.
Check for required profile parameters before sending any API calls.
Since using "provider: <provider-engine>" is deprecated, alias provider  to use driver: "driver: <provider-engine>"
Show the traceback if the debug logging level is enabled
It might be already up, let's destroy it!
success without data
Just a little delay between attempts...
if this fails install using
Import python libs
Import salt libs
Import salt cloud libs
Attempt to import pysphere lib
Get logging started
Check for required profile parameters before sending any API calls.
Since using "provider: <provider-engine>" is deprecated, alias provider  to use driver: "driver: <provider-engine>"
Show the traceback if the debug logging level is enabled
new_instance = conn.get_vm_by_name(vm_['name'])  ret = new_instance.get_properties()
Store what was used to the deploy the VM
The ProfitBricks login username  The ProfitBricks login password  The ProfitBricks virtual datacenter UUID  SSH private key filename  SSH public key filename
Import python libs
Import salt libs
Import salt.cloud libs
Get logging started
Show the traceback if the debug logging level is enabled
Check for required profile parameters before sending any API calls.
Apply component overrides to the size from the cloud profile config.
Retrieve list of SSH public keys
Construct server
Show the trackback if the debug logging level is enabled
Trigger a failure in the wait for IP function
Still not running, trigger another iteration
It might be already up, let's destroy it!
Import python libs
Import salt libs
Get logging started
Check for required profile parameters before sending any API calls.
Since using "provider: <provider-engine>" is deprecated, alias provider  to use driver: "driver: <provider-engine>"
Show the traceback if the debug logging level is enabled
Trigger a failure in the wait for IP function
Still not running, trigger another iteration
It might be already up, let's destroy it!
Show the traceback if the debug logging level is enabled
Import python libs
Import salt cloud libs
CloudStackNetwork will be needed during creation of a new node  pylint: disable=import-error  See https://github.com/saltstack/salt/issues/32743
Get logging started
with versions <0.15 of libcloud this is causing an AttributeError.
Check for required profile parameters before sending any API calls.
Since using "provider: <provider-engine>" is deprecated, alias provider  to use driver: "driver: <provider-engine>"
Show the traceback if the debug logging level is enabled
Show the traceback if the debug logging level is enabled
Show the traceback if the debug logging level is enabled
The Joyent login user  The Joyent user's password  The location of the ssh private key that can log into the new VM  The name of the private key
Import python libs
Import salt libs
Get logging started
joyent no longer reports on all data centers, so setting this value to true  causes the list_nodes function to get information on machines from all  data centers
Technically this function may be called other ways too, but it  definitely cannot be called with --function.
Trigger a failure in the wait for IP function
Trigger a failure in the wait for IP function
It might be already up, let's destroy it!
Check for required profile parameters before sending any API calls.
Since using "provider: <provider-engine>" is deprecated, alias provider  to use driver: "driver: <provider-engine>"
Show the traceback if the debug logging level is enabled
Show the traceback if the debug logging level is enabled
added for consistency with old code
add any undefined desired keys
remove all the extra key value pairs to provide a brief listing
post form data
Import python libs
Import salt libs
Get logging started
custom UA
Finally, add the images in this current project last so that it overrides  any image that also exists in any public project.
Consider warning the user that the tags in the cloud profile  could not be interpreted, bad formatting?
Consider warning the user that the metadata in the cloud profile  could not be interpreted, bad formatting?
Check for required profile parameters before sending any API calls.
Since using "provider: <provider-engine>" is deprecated, alias provider  to use driver: "driver: <provider-engine>"
node_data is a libcloud Node which is unsubscriptable
Parallels account information
Import python libs
Import salt cloud libs
Get logging started
Start the tree
Name of the instance
Description, defaults to name
How many megabytes of RAM
Bandwidth available, in kbps
How many public IPs will be assigned to this instance
Check for required profile parameters before sending any API calls.
Since using "provider: <provider-engine>" is deprecated, alias provider  to use driver: "driver: <provider-engine>"
Show the traceback if the debug logging level is enabled
Trigger another iteration
It might be already up, let's destroy it!
The ID of the minion that will execute the salt nova functions  The name of the configuration profile to use on said minion
Ignore IP addresses on this network for bootstrap
create the block storage device
with the volume already created
create the volume from a snapshot
create the create an extra ephemeral disk
create the create an extra ephemeral disk
Import python libs
Import Salt Libs
Get logging started
Some of the libcloud functions need to be in the same namespace as the  functions defined in the module, so we create new function objects inside  this module namespace
Technically this function may be called other ways too, but it  definitely cannot be called with --function.
Check for required profile parameters before sending any API calls.
Since using "provider: <provider-engine>" is deprecated, alias provider  to use driver: "driver: <provider-engine>"
Put together all of the information required to request the instance,  and then fire off the request for it
Pull the instance ID, valid for both spot and normal instances
Show the traceback if the debug logging level is enabled
Trigger a failure in the wait for IP function
Still not running, trigger another iteration
It might be already up, let's destroy it!
If the server is deleted while looking it up, skip
Command parity with EC2 and Azure
Command parity with EC2 and Azure
Command parity with EC2 and Azure
Import python libs
Import Third Party Libs
Get logging started
Calculate the string for Signature
Check for required profile parameters before sending any API calls.
Since using "provider: <provider-engine>" is deprecated, alias provider  to use driver: "driver: <provider-engine>"
It might be already up, let's destroy it!
The instance is booted and accessible, let's Salt it!
Import python libs
Import salt libs
Import Third Party Libs  See https://github.com/saltstack/salt/issues/32743
Get logging started
Check for required profile parameters before sending any API calls.
Since using "provider: <provider-engine>" is deprecated, alias provider  to use driver: "driver: <provider-engine>"
Show the traceback if the debug logging level is enabled
Show the traceback if the debug logging level is enabled
Trigger a failure in the wait for IP function
Still not running, trigger another iteration
It might be already up, let's destroy it!
Vultr account api key
Import python libs
Import salt cloud libs
Get logging started
Find under which cloud service the name is listed, if any
Show the traceback if the debug logging level is enabled
Bootstrap
Import python libs
If the name of the driver used does not match the filename,   then that name should be returned instead of True.  return __virtualname__
Check for required profile parameters before sending any API calls.
to create the virtual machine.
Booting and deploying if needed
ssh or smb using ip and install salt only if deploy is True
Passwords should be included in this object!!
Import Python Libs
Import Third Party Libs
Get logging started
Check for required profile parameters before sending any API calls.
Since using "provider: <provider-engine>" is deprecated, alias provider  to use driver: "driver: <provider-engine>"
backwards compat
Show the traceback if the debug logging level is enabled
Trigger an error in the wait_for_ip function
It might be already up, let's destroy it!
request.read()
success without data
Just a little delay between attempts...
Import python libs
Import salt.cloud libs
Import third party libs
Get logging started
Since using "provider: <provider-engine>" is deprecated, alias provider  to use driver: "driver: <provider-engine>"
Since using "provider: <provider-engine>" is deprecated, alias provider  to use driver: "driver: <provider-engine>"
This should not be called without either an instance or a  provider. If both an instance/list of names and a provider  are given, then we also need to exit. We can only have one  or the other.
Show the traceback if the debug logging level is  enabled
Failed to communicate with the provider, don't list any  nodes
There's no providers details?! Skip it!
The capability to gather locations is not supported by this  cloud module
Show the traceback if the debug logging level is enabled
The capability to gather images is not supported by this  cloud module
Show the traceback if the debug logging level is enabled
The capability to gather sizes is not supported by this  cloud module
Show the traceback if the debug logging level is enabled
kick off the parallel destroy
massage the multiprocessing output a bit
now the processed data structure contains the output from either  the parallel or non-parallel destroy and we should finish up  with removing minion keys if necessary
There's no such key file!? It might have been renamed
Single key entry. Remove it!
These machines were asked to be destroyed but could not be found
Since using "provider: <provider-engine>" is deprecated, alias provider  to use driver: "driver: <provider-engine>"
Note(pabelanger): We still reference pub_key and priv_key when  deploy is disabled.
Accept the key on the local master
a small pause makes the sync work reliably
Build the dictionary of invalid functions with their associated VMs.
Find the VMs that are in names, but not in set of invalid functions.
Don't return missing VM information for invalid functions until after we've had a  Chance to return successful actions. If a function is valid for one driver, but  Not another, we want to make sure the successful action is returned properly.
If we reach this point, the Not Actioned and Not Found lists will be the same,  But we want to list both for clarity/consistency with the invalid functions lists.
If it's not there, then our job is already done
Convert the dictionary mapping to a list of dictionaries  Foo:   bar1:     grains:       foo: bar   bar2:     grains:       foo: bar
If it's a single string entry, let's make iterable because of  the next step
Get associated provider data, in case something like size  or image is specified in the provider file. See issue 32510.
Update the provider details information with profile data  Profile data should override provider data, if defined.  This keeps map file data definitions consistent with -p usage.
Add the computed information to the return data  Add the node name to the defined set
The machine is set to be created. Does it already exist?
A machine by the same name exists  Machine already removed
Hard maps are enabled, Look for the items to delete.
Generate the minion keys to pre-seed the master:
Store the minion's public key in order to be pre-seeded in  the master
The minion is explicitly defining a master and it's  explicitly saying it's the local one
Get the needed data  Strip the deploy_kwargs from the returned data since we don't  want it shown in the console.
Force display_ssh_output to be False since the console will  need to be reset afterwards
Already deployed, it's the master's minion
The minion is explicitly defining a master and it's  explicitly saying it's the local one
Show the traceback if the debug logging level is enabled
Show the traceback if the debug logging level is enabled
Show the traceback if the debug logging level is  enabled
Failed to communicate with the provider, don't list any nodes
for pickle and multiprocessing, we can't use directly decorators
Import salt libs
Import python libs
Import salt libs
Import salt.cloud libs
Parse shell arguments
Logfile is not using Syslog, verify
Setup log file logging
This is obviously not a machine name, treat it as a kwarg
nothing to create or destroy & nothing exists
nothing to create or destroy, print existing
Show the traceback if the debug logging level is  enabled
Import python libs
Import salt libs
Import salt cloud libs
Get logging started
Import python libs
Import third party libs
Set up logging
Do the regex string replacement on the minion id
If we can't find the minion the database it's not necessarily an  error.
Define the module's virtual name
If reclass is installed, __virtual__ put it onto the search path, so we  don't need to protect against ImportError:
Salt's top interface is inconsistent with ext_pillar (see 5786) and  one is expected to extract the arguments to the master_tops plugin  by parsing the configuration file data. I therefore use this adapter  to hide this internality.
the source path we used above isn't something reclass needs to care  about, so filter it:
if no inventory_base_uri was specified, initialise it to the first  file_roots of class 'base' (if that exists):
Salt expects the top data to be filtered by minion_id, so we better  let it know which minion it is dealing with. Unfortunately, we must  extract these data (see 6930):
I purposely do not pass any of __opts__ or __salt__ or __grains__  to reclass, as I consider those to be Salt-internal and reclass  should not make any assumptions about it. Reclass only needs to know  how it's configured, so:
Import python libs
Set up logging
Define the module's virtual name
Import python libs
Import third party libs
Import python libs
Set up logging
Import python libs
Import salt libs
Import 3rd-party libs
Import salt libs
Import python libs
Import third party libs
Import salt libs
Import salt libs
Import python libs
Import salt libs
Import 3rd-party libs
Import python libs
Import salt libs
Import python libs
Import 3rd-party libs
pwd is not available on windows
New git_pillar code
The username may contain '\' if it is in Windows  'DOMAIN\username' format. Fix this for the keyfile path.
Cannot delete read-only files on Windows.
don't allow others to write to the file
check group flags
check if writable by group or other
If anything happens in the top generation, log it and move on
Can overwrite master files!!
save the load, since we don't have it
This is the list of funcs/modules!
The minion is not who it says it is!  We don't want to listen to it!  Prepare the runner object
The eauth system is not enabled, fail
to make sure we don't step on anyone else's toes
Import Python Libs
Import Salt Libs
now parse each hostname string for host and optional port
Import ioflo libs
Make life easier
pylint: skip-file  pylint: disable=C0103
should not join since role same but keys different
should join since same role and keys
should join since open mode
Import Ioflo Deeds
Bootstrap
Check
Bootstrap
Check
Bootstrap
Check
Bootstrap
Check
Bootstrap
Check
Bootstrap
Bootstrap
Check
Bootstrap
Check
Bootstrap
Check
Bootstrap
Check
Bootstrap
Check
Bootstrap
Check
Import Python Libs
Import 3rd-party libs
pylint: skip-file  pylint: disable=C0103
pylint: skip-file  pylint: disable=C0103
Import Ioflo Deeds
Check
Check
Check
Check
Check
Check
Check
Check
Check
Check
pylint: skip-file
IPv6 sockets work for both IPv6 and IPv4 addresses
Import python libs
Import ioflo libs
add remote for the manor yard
SaltRaetNixJobber is not picklable. Pickling is necessary  when spawning a process in Windows. Since the process will  be spawned and joined on non-Windows platforms, instead of  this, just run the function directly and absorb any thrown  exceptions.
set up return destination from source
Import python libs
Import ioflo libs
Import salt libs
Import salt libs  Import ioflo libs
Import Python libs
Import modules
Import salt libs
Import 3rd-party libs
Import python libs
Import salt libs
Import python libs
Import salt libs
Import ioflo libs
Import python libs
Import Third Party Libs  pylint: disable=import-error
pylint: disable=no-name-in-module,redefined-builtin  pylint: enable=import-error,no-name-in-module,redefined-builtin
we're done, reset the limits!
Yes, this class is identical to RX, this is because we still need to  separate out rx and tx in raet itself
Yes, this class is identical to RX, this is because we still need to  separate out rx and tx in raet itself
Meant for another yard, send it off!
No queue destination!
Refuse local commands over the wire
Send it to a remote worker
Forward to the correct estate
Meant for another yard, send it off!
No queue destination!
Meant for another yard, send it off!
No queue destination!
Forward to the correct estate
Meant for another yard, send it off!
No queue destination!
Import Python libs
Import ioflo libs
kludge to persist the keys since no way to write
Import python libs
Import Salt libs
Explicit late import to avoid circular import
NOTE: exc_str will be passed to the parent class' constructor and  become self.strerror.
Import python libs
Import salt libs
Import third party libs  pylint: disable=import-error,no-name-in-module,redefined-builtin  pylint: enable=import-error,no-name-in-module,redefined-builtin
default to master.pub
default to master_sign.pem
We have to differentiate between RaetKey._check_minions_directories  and Zeromq-Keys. Raet-Keys only have three states while ZeroMQ-keys  havd an additional 'denied' state.
key dir kind is not created yet, just skip
open mode is turned on, force accept the key
Import salt libs
Import python libs
Import salt libs
Import 3rd-party libs
Import python libs
Import salt libs
Import third party libs
Import Salt libs
Import salt libs
Import python libs
Import Salt libs
Get logging started
-*- coding: utf-8 -*-
This will hang if the master daemon is not running.
Import Python Libs
Import Salt Libs
Import python libs
Import Salt libs
Import 3rd-party lib
Don't shadow built-in's.
marking ping status as True only and only if we have at  least provisioned one container
if no lxc detected as touched (either inited or verified)  we result to False
Import python libs
Import python libs
Import salt libs
Import python libs
Import Salt libs
Import python libs
Import salt libs
Import 3rd-party libs
Import python libs
Import third party libs
Salt version <= 2014.7.0
Import python libs
Import salt libs
The long version, added for consistency
The long version, added for consistency
Import python libs
Import third party libs
Import salt libs
Iterate over possible reverse zones
Import Python libs
Import salt libs
Import 3rd-party libs
Import python libs
Import salt libs
Import python libs
Import salt libs
Import salt libs
Import python libs
Import python libs
Import 3rd-party libs
Import salt libs
Import Python libs
Import Salt libs  See https://docs.saltstack.com/en/latest/topics/tutorials/http.html
Import python libs
Import third party libs
Import salt libs
Global parameters which can be overridden on a per-remote basis
Import python libs
Import third party libs  Salt version <= 2014.7.0
Import Python Libs
Import salt libs
Import pytohn libs
Import salt libs
Aliases for orchestrate runner
Reboot a minion and run highstate when it comes back online
Reboot multiple minions and run highstate when all are back online
Watch the event bus forever in a shell while-loop.
Import Python libs
Import salt libs
Import python libs
Import salt libs
Import python libs
Import Salt libs
Import 3rd-party libs
Fall through to ret error handling below
Import python libs
Import 3rd-party libs
Add version of Master to output
Import python libs
Import salt libs
Get logging started
Module files are distributed via _modules, _states, etc
Pillars are automatically put in the pillar_path
Configuration files go into /etc/salt/
Reactor files go into /srv/reactor/
Module files are distributed via _modules, _states, etc
Pillars are automatically put in the pillar_path
Configuration files go into /etc/salt/
Reactor files go into /srv/reactor/
This ensures that double directories (i.e., apache/apache/) don't  get created
Import Python libs
Get logging started
Kick off the install
First pass: check for files that already exist
We've decided to install
Second pass: install the files
Don't try to resolve the same package more than once
Look at local repo index
Get logging started
Import python libraries
If this is a regular command, it is a single function  If this is a compound function
Import python libs
Import third party libs  pylint: disable=import-error,no-name-in-module,redefined-builtin  pylint: enable=import-error,no-name-in-module,redefined-builtin
catch unhandled data
Merge the comps  Salt doesn't support state files such as:  /etc/redis/redis.conf:    file.managed:      - user: redis      - group: redis      - mode: 644    file.comment:      - regex: ^requirepass
The exclude statement is a string, assume it is an sls
Pillar data must be gathered before the modules are loaded, since  it will be packed into each loaded function. Thus, we will not  have access to the functions and must past an empty dict here.
ensure that the module is loaded
No reason to stop, return ret
User explicitly requests a reload
Don't pass down a state override
Invalid name, fall back to ID
The exclude statement is a string, assume it is an sls
User is using a deprecated env setting which was parsed by  format_call.  We check for a string type since module functions which  allow setting the OS environ also make use of the "env"  keyword argument, which is not a string
The user is passing an alternative environment using __env__  which is also not the appropriate choice, still, handle it
Let's use the default environment
If format_call got any warnings, let's show them to the user
Allow requisite tracking of entire sls files
Is this is a short state, it needs to be padded  '__sls__': template,  '__env__': None,
No further checks will work, bail out
This is a sls module
Show the traceback if the debug logging level is enabled
Resolve inc_sls in the specified environment
Resolve inc_sls in the subset of environment matches
PyDSL OrderedDict?
Include's or excludes as lists?
Bad syntax, let the verify seq pick it up later on
quite certainly a syntax error, managed elsewhere
Is this is a short state, it needs to be padded
if we did not found any sls in the fileserver listing, this  may be because the sls was generated or added later, we can  try to directly execute it, and if it fails, anyway it will  return the former error
If there is extension data reconcile it
Verify that the high data is structurally sound
Compile and verify the raw chunks
a stack of active HighState objects during a state.highstate run
tracks all pydsl state declarations globally across sls files
a stack of current rendering Sls objects, maintained and used by the pydsl renderer.
Nuclear option  Blow away the entire stack. Used primarily by the test runner but also  useful in custom wrappers of the HighState class, to reset the stack  to a fresh state.
self.auth = salt.crypt.SAuth(opts)
Import python libs
Define the module's virtual name
All python servers should have sqlite3 and so be able to use  this default sqlite queue system
we need a list of one item tuples here
Import python libs
Define the module's virtual name
we need a list of one item tuples here
Too many situations use "exit 1" - try not to use it when something  else is more appropriate.
Salt SSH "Thin" deployment failures
One of a collection failed
keepalive exit code is a hint that the process should be restarted
SALT_BUILD_FAIL is used when salt fails to build something, like a container
Default delimiter for multi-level traversal in targeting
import salt libs
Prepare
Test
Deploy
Prepare
Test
setup return structure
get existing config if app is present
std lib
third party libs
if not a valid IP Address  try to see if it is a valid NS  no a valid DNS entry either
Import python libs
Import salt libs
Import 3rd-party libs
Import salt libs
Some fields are formatted like '{data}'. Salt/Python converts these to dicts  automatically on input, so convert them back to the proper format.
Fix if we were passed None as a string.
Import python libs
Import Python Libs
Import Salt Libs
there's an update for this key
Using a profile from pillars
The mode needs to change...
Import salt libs
Import 3rd-party libs
If given an explicit version check the installed version matches.
Import python libs
Import salt libs
Import Python libs
Import Salt libs
create data-structure to return with default value
get the current state of tuned-adm
check valid profiles, and return error if profile name is not valid
if current state is same as requested state, return without doing much
return None when testing
we come to this stage if current state is different that requested state  we there have to set the new state request
create the comment data structure
fill in the ret data structure
return with the dictionary data structure
create data-structure to return with default value
check the current state of tuned
if profile is already off, then don't do anything
return None when testing
Import salt libs
Enable proper logging
Define the module's virtual name
backward compatibility: name could be already tagged
bare dict style
list of dicts style:
If volumes as a whole is a dict, then there's no way to specify a non-bound volume  so we exit early and assume the dict is properly formed.
Needs to refresh the image
if container exists but is not started, try to start it
if container is known by name,  and the container is based on expected image,  then assume it already exists.
Import Python libs
Import salt libs
check if extension exists
Check if grant exists, and if so, remove it
Import python libs
Import salt libs
check if user exists with a different password
fallback
Import python libs
Import 3rd-party libs
Import Salt Libs
for some reason loader overwrites __opts__['test'] with default  value of False, thus store and then load it again after action
Import python libs
Import Python Libs
Import python libs
Import salt libs
If we've reached this far before returning, we have changes.
Alias join to preserve backward compat
Import python libs
Import salt libs
If we've reached this far before returning, we have changes.
If we've reached this far before returning, we have changes.
Import python libs
Import python libs
Import Salt Libs
Validate syntax
Import python libs
Import salt libs
unpack the options from the top-level return dict
Port is installed as desired
Intersection of loaded modules and persistent modules
Complement of proposed modules and already loaded modules
Union of loaded modules and persistent modules
Import salt libs
Check to see if we have a designated version
Modify the name to include the version and proceed.
Import salt libs
Container exists and we're not managing whether or not it's  running. Set the result back to True and return
Make sure we know the final state of the container before we return
No need to restart since container is not running
Import python libs
Import Salt libs
fallback
fallback
Import python libs
Import salt libs
Defaults is not a valid option on Mac OS
Make sure that opts is correct, it can be a list or a comma delimited  string
remove possible trailing slash
options which are provided as key=value (e.g. password=Zohp5ohb)
Some filesystems have options which should not force a remount.
Some options are translated once mounted
Failed to (re)mount, the state has failed!
(Re)mount worked!
Override default for Mac OS
fallback
set name to host as required by the module
Import Python libs
check if schema exists
The schema is not present, make it!
Import Python Libs
we will never change the IdentityPoolName from the state module
Now update the Auth/Unauth Roles
Import Python libs
Define a function alias in order not to shadow built-in's
Import python libs
Import salt libs
Define the module's virtual name
Populate the venv via a requirements file
Import Python libs
Import 3rd party libs
create a pagerduty service using cloudwatch integration
service_key on create must 'foo' but the GET will return 'foo@bar.pagerduty.com'
Import python libs
Import salt libs
Initial set up
Avoid variable naming confusion in below module calls, since ID  declaration for this state will be a source URI.
If the source is a list then find which file exists
Gather the source file from the server
Import salt libs
Import Python libs
Import Salt libs
is service available?
Service needs to be enabled
Service needs to be disabled
Check for common error: using enabled option instead of enable
Check if the service is available
Run the tests
Check for common error: using enabled option instead of enable
Store the current date in a file
!/bin/bash
Import python libs
Import salt libs
if stdout is the state output in JSON, don't show it.  otherwise it contains the one line name=value pairs, strip it.
won't be shown as the function output. (though, they will be shown        inside INFO logs).
never use VT for onlyif/unless executions because this will lead  to quote problems
No reason to stop, return True
Ignoring our arguments is intentional.
Alias "cmd.watch" to "cmd.wait", as this is a common misconfiguration
Ignoring our arguments is intentional.
Wow, we passed the test, run this sucker!
Change the source to be the name arg if it is not specified
If script args present split from name and define args
Ignoring our arguments is intentional.
fallback
Import python libs
Filter services for unique items, and sort them for comparison purposes.
Note any existing communities that should be removed.
Import python libs
Import salt libs
Import 3rd-party libs  pylint: disable=import-error
Remove references to the loaded pip module above so reloading works
Define the module's virtual name
This is most likely an url and there's no way to know what will  be installed before actually installing it.
result: None means the command failed to run  result: True means the package is installed  result: False means the package is not installed
No requirements case.  Check pre-existence of the requested packages.
If _check_if_installed result is None, something went wrong with  the command running. This way we keep stateful output.
The package is not present. Add it to the pkgs to install.  Replace commas (used for version ranges) with semicolons  (which are not supported) in name so it does not treat  them as multiple packages.
The package is already present and will not be reinstalled.  Append comment stating its presence
The command pip.list failed. Abort.
Construct the string that will get passed to the install call
Check that the packages set to be installed were installed.  Create comments reporting success and failures
Case for packages that are not an URL
Import python libs
Import salt libs
Import python libs
Import salt libs
Import salt libs
Import Python libs
Import salt libs
Install the package
Uninstall the package
Import python libs
Import salt libs
Define the module's virtual name
Import python libs
Import 3rd-party libs
pylint: disable=invalid-name
The following imports are used by the namespaced win_pkg funcs  and need to be included in their globals.  pylint: disable=import-error,unused-import
A comparison operator of "=" is redundant, but possible.  Change it to "==" so that the version comparison works
Badly-formatted SLS
Find out which packages will be targeted in the call to pkg.remove  Check current versions against specified versions  FreeBSD pkg supports `openjdk` and `java/openjdk7` package names
FreeBSD pkg supports `openjdk` and `java/openjdk7` package names
FreeBSD pkg supports `openjdk` and `java/openjdk7` package names
salt myminion pkg.latest_version vim-enhanced
salt myminion pkg.list_repo_pkgs httpd
Actual vim-enhanced version: 2:7.4.160-1.el7
If version is empty, it means the latest version is installed  so we grab that version to avoid passing an empty string
Get information for state return from the exception.
If there was nothing unpurged, just set the changes dict to the contents  of changes['installed'].
Remove the rtag if it exists, ensuring only one refresh per salt run  (unless overridden with refresh=True)
Repack the cur/avail data if only a single package is being checked
Find up-to-date packages  There couldn't have been any up-to-date packages if this state  only targeted a single package and is being allowed to proceed to  the install step.
Build updated list of pkgs to exclude non-targeted ones
No need to refresh, if a refresh was necessary it would have been  performed above when pkg.latest_version was run.
Actual vim-enhanced version: 2:7.4.160-1.el7
Get information for state return from the exception.
Actual vim-enhanced version: 2:7.4.160-1.el7
Get information for state return from the exception.
Get information for state return from the exception.
Get information for state return from the exception.
Already ran the pkg state, skip aggregation
Import python libs
Import salt libs
Define the module's virtual name
Test mode with changes is the only case where result should ever be none
Define a function alias in order not to shadow built-in's
Import Python libs
Import 3rd-party libs
Python Libs
Import 3rd-party libs
Import python libs
Import salt libs
Check onlyif, unless first
Check if we have already installed this  Run with python shell due to the wildcard
Unmount to be kind
No reason to stop, return True
Import salt libs
Import python libs
Import salt libs
Import Python libs
Import salt libs
pylint: disable=invalid-name
list of updates that are applicable by current settings.
list of updates to be installed.
the object responsible for fetching the actual downloads.
the object responsible for the installing of the updates.
the results of the download process
the results of the installation process
if there is no type, the is nothing to search.
this is where we be seeking the things! yar!
this is where we get all the things! i.e. download updates.
this is where we put things in their place!
this is where we be seeking the things! yar!
this is where we get all the things! i.e. download updates.
Import python libs
Import salt libs
Import salt libs  pylint: disable=no-name-in-module,import-error  pylint: enable=no-name-in-module,import-error
Define the module's virtual name
Volumes declared in docker file should be part of desired_volumes.
Sometimes docker daemon returns `None` and  sometimes `[]`. We have to deal with it.
Sometimes docker daemon returns `None` and  sometimes `[]`. We have to deal with it.
Generic comparison, works on strings, numeric types, and  booleans
Ensure that we have repo:tag notation
Image was pulled again (because of force) but was also  already there. No new image was available on the registry.
Only add to the changes dict if layers were pulled
This shouldn't happen, failure to pull should be caught above
Don't stomp on images with unicode characters in Python 2,  only force image to be a str if it wasn't already (which is  very unlikely).
Pull image
Strip __pub kwargs and divide the remaining arguments into the ones for  container creation and the ones for starting containers.
No need to check the container config if force=True, or the image was  updated in the block above.
Removal was successful, add the list of removed IDs to the  changes dict.
Pull image
Create new container  Already validated input
Creation of new container was successful, add the return data to the  changes dict.
Start container
Change was applied successfully
No necessary changes detected on post-container-replacement  check. The diffs will be the original changeset detected in  pre-flight check.
Container was not replaced, no necessary changes detected  in pre-flight check, and no signal sent to container
map containers to container's Ids.
Force image to be updated
Use salt k8s module to set label
Use salt k8s module to set label
Use salt k8s module to set label
Import python libs
Add pillar keys for default configuration
Define the module's virtual name
Check that debconf was loaded
fallback
Import salt libs
The state of the system does need to be changed. Check if we're running  in ``test=true`` mode.
Import Python libs
Import 3rd-party libs
Return ``None`` when running with ``test=true``.
if it exists
if it doesn't exist
else something else was returned
if it exists by name
not found, attempt to create it
were we able to create it?  try modification
roll it back
unable to create it  an error occurred
if it exists by name
not found, attempt to create it  an error occurred
if it exists by name
something bad happened
if it exists
if it doesn't exist
else something else was returned
if it exists
if it doesn't exist
unable to create it
else something else was returned
if it exists
if it doesn't exist  else something else was returned
if it exists by name
something bad happened
if it exists
member wasn't added
if it exists by name
does this virtual exist?
else something else was returned
does this virtual exist?
does this virtual exist?
else something else was returned
if it exists by name
if it exists
if it doesn't exist
else something else was returned
if it exists
if it doesn't exist
else something else was returned
if it exists
if it doesn't exist  else something else was returned
if it exists by name
if it exists
if it doesn't exist
else something else was returned
if it exists
if it doesn't exist
else something else was returned
if it exists
if it doesn't exist  else something else was returned
if it exists by name
!/bin/bash
Using a profile from pillars.
Import Python Libs
No API provides this info, so the result will never  match, and we will always update. Result is still  idempotent  'DisplayName': ???,
No API provides this info, so the result will never  match, and we will always update. Result is still  idempotent  'DisplayName': ???,  'ID': ???
Once versioning has been enabled, it can't completely go away, it can  only be suspended
Substitute full ARN into desired state for comparison
versioning must be turned on before replication can be on, thus replication  must be turned off before versioning can be off  replication will be on, must deal with versioning first  replication will be off, deal with it first
Import Python Libs
Import Salt Libs
policy exists, ensure config matches
policy exists, ensure config matches
cmd = 'diff /usr/local/etc/pkg.conf /usr/local/etc/pkg.conf.bak'     res = __salt__['cmd.run'](cmd)     ret['changes'] = res
Import python libs
fallback
fallback
Import python libs
Import 3rd-party libs
Anything different from 2 will fallback into the v1.
override salt_params with given params
Import python libs
Import Salt libs
Import 3rd-party libs
some changes to the datasource may have already been made, therefore we don't quit here
Import python libs
Define the module's virtual name
Import python libs
Import salt libs
Set up logging
Define the module's virtual name
Validate default gateway
If we're currently set to 'dhcp' but moving to 'static', specify the changes.
Import Python Libs
Import salt libs
I /believe/ this situation is impossible but let's hedge our bets...
error detected
Import python libs
Import salt libs
Import Python Libs
Import Salt Libs
No way to judge whether the item in the s3 bucket is current without  downloading it. Cheaper to just request an update every time, and still  idempotent
Using a profile from pillars
Passing in a profile
Import Python libs
Import salt libs
We need to alter what Boto returns if no ports are specified  so that we can compare rules fairly.  Boto returns None for from_port and to_port where we're required  to pass in "-1" instead.
Import Python Libs
Import Salt Libs
try to open the swagger file and basic validation
retrieve stage variables
VENDOR SPECIFIC FIELD PATTERNS
JSON_SCHEMA_REF
This string should not be modified, every API created by this state will carry the description  below.
only check for response code from 400-599
check for Required Swagger fields by Saltstack boto apigateway state
no matching stage_name/deployment found
need to walk each property object
remove the model from other depednencies before returning
add in a few attributes into the model schema that AWS expects  _schema = schema.copy()
check to see if model already exists, aws has 2 default models [Empty, Error]  which may need upate with data from swagger file
try look up in the same region as the apigateway as well if previous lookup failed
Import salt libs
Monitoring state, no changes will be made so no test interface needed
Import python libs
Import OpenStack libs
Just pop states until we reach the  first acceptable one:
If we've created a new image also return its last status:
Refresh our info about the image
Import python libs
Import salt libs
Need the check for None here, if env is not provided then it falls back  to None and it is assumed that the environment is not being overridden.
found a user... updating
no update
Import salt libs
Already ran the iptables state, skip aggregation
Check for the same function
Import Python Libs
Import Salt Libs
Get Logging Started
Allow users to disable core dump, but then return since  nothing else can be set if core dump is disabled.
If current_enabled and enabled match, but are both False,  We must return before configuring anything. This isn't a  failure as core dump may be disabled intentionally.
Check that the first two list items of clean key lists are equal.
If current_ssh_key is None, but we're setting a new key with  either ssh_key or ssh_key_file, then we need to flag the change.
The output of get_syslog_config has different keys than the keys  Used to set syslog_config values. We need to look them up first.
fallback
Import python libs
Import salt libs
Import salt libs
Alias module.watch to module.wait
Set idrac_passwords for blades.  racadm needs them to be called 'server-x'
Import python libs
Import salt libs
Set up metadata
Check the time zone
Import python libs
Set up logging
set ranged status
Debian based system can have a type of source  in the interfaces file, we don't ifup or ifdown it
Apply interface routes
Apply global network settings
Python Libs
Import python libs
Import salt libs
Import 3rd-party libs
If given an explicit version check the installed version matches.
The first package is always the cache path
Import Python Libs
Import Salt Libs
Look up subnet ids
Don't remove the main route table association
deal with PD API bug: when you submit member_order=N, you get back member_order=N+1
Import python libs
check if any processes in this group are stopped
check if any processes in this group are stopped
process name doesn't exist
Always restart on watch
Import Python Libs
Import Salt Libs
Import 3rd party libs
Import python libs
Import salt libs
Init logger
Repeat lsblk check up to 10 times with 3s sleeping between each  to avoid lsblk failing although mkfs has succeeded  see https://github.com/saltstack/salt/issues/25775
Import Python Libs
Import Salt Libs
Import python libs
Import salt libs
The key does not exist in environment  This key will be added with value ''
Import python libs
Import salt libs
Define the module's virtual name
Just in case someone decides to enter a numeric description
Just in case someone decides to enter a numeric description
Import Python libs
Following the docs as written here  http://docs.saltstack.com/ref/states/writing.htmlreturn-data
Following the docs as written here  http://docs.saltstack.com/ref/states/writing.htmlreturn-data
Following the docs as written here  http://docs.saltstack.com/ref/states/writing.htmlreturn-data
If changes is True we place our dummy change dictionary into it.  Following the docs as written here  http://docs.saltstack.com/ref/states/writing.htmlreturn-data
since result is a boolean, if its random we just set it here,
for those we don't check the type:  those should be bool:  those should be int:
those should be integer:  those should be str:  those should be list:  those should be dict:
Check if tenant is already present
Check if role is already present
Check if service is already present
Import Python Libs
Import Salt Libs
hash is needed for set operations
import salt libs
Stand up a new web server.
Noop. The state system will call the mod_watch function instead.
make sure the entry doesn't exist
make sure the entry exists with only the specified  attribute values
make sure the entry exists, its olcRootDN attribute  has only the specified value, the olcRootDN attribute  doesn't exist, and all other attributes are ignored  the admin entry has its own password attribute
already a connection object
hack to get at the ldap3 module to access the ldap3.LDAPError  exception class.  https://github.com/saltstack/salt/issues/27578
collect all of the affected entries (only the key is  important in this dict; would have used an OrderedSet if  there was one)
update these after the op in case an exception  is raised
convert numbers to strings so that equality checks work  (LDAP stores numbers as strings)
Import python libs
Import salt libs
Import 3rd-party libs
NameError is a msgpack error from salt-ssh
msgpack error from salt-ssh
walk path only once and store the result  root: (dirs, files) structure, compatible for python2.6
Recurse skips root (we always do dirs, not root), so always check root:
Make a nice neat list of tuples exactly len(sources) long..
Make sure that leading zeros stripped by YAML loader are added back
Make sure the user exists in Windows  Salt default is 'root'  User not found, use the account salt is running under  If username not found, use System
Make sure that leading zeros stripped by YAML loader are added back
Don't create a file that is not already present
The specified user or group do not exist
If the source is a list then find which file exists
file being updated to verify using check_cmd  Reset ret
Since we generated a new tempfile and we are not returning here  lets change the original sfn to the new tempfile or else we will  get file not found
Remove trailing slash, if present and we're not working on "/" itself
Make sure that leading zeros stripped by YAML loader are added back
The specified user or group do not exist
issue 32707: skip this __salt__['file.check_perms'] call if children_only == True  Check permissions
walk path only once and store the result  root: (dirs, files) structure, compatible for python2.6
Make sure that leading zeros stripped by YAML loader are added back
The specified user or group do not exist
expand source into source_list
Use the most "negative" result code (out of True, None, False)
Only include comments about files that changed
Conflicts can occur if some kwargs are passed in here
we're searching for things that start with this *directory*.  use '/' since master only runs on POSIX
If we are instructed to keep symlinks, then process them.  Make this global so that emptydirs can use it if needed.
Check for maxdepth of the relative path  Since paths are all master, just use POSIX separator  Handle empty directories (include_empty==true) by removing the  the last piece if it is an empty string
verify the directory perms if they are set
get list of files in directory
if strptime_format is set, filter through the list to find names which parse and get their datetimes.
Files which don't match the pattern are not relevant files.
This is tightly coupled with the file_with_times data-structure above.
I'm adding 1 to keep_count below because it fixed an off-by one  issue in the tests. I don't understand why, and that bothers me.
<...snip...>
Perform the edit
Check the result
Changes happened, add them
Perform the edit
Check the result
Changes happened, add them
Add sources and source_hashes with template support  NOTE: FIX 'text' and any 'source' are mutually exclusive as 'text'        is re-assigned in the original code.
Try to create the file
Follow the original logic and re-assign 'text' if using source(s)...
Changes happened, add them
Changes happened, add them
Add sources and source_hashes with template support  NOTE: FIX 'text' and any 'source' are mutually exclusive as 'text'        is re-assigned in the original code.
Try to create the file
Follow the original logic and re-assign 'text' if using source(s)...
if header kwarg is unset of False, use regex search
Changes happened, add them
Changes happened, add them
The specified user or group do not exist
If the target is a dir, and overwrite_dir is False, copy into the dir
Don't create a file that is not already present
No reason to stop, return True
Monitoring state, no changes will be made so no test interface needed
Monitoring state, no changes will be made so no test interface needed
Still erroring after possible retry
The schedule.list gives us an item that is guaranteed to have an  'enabled' argument. Before comparing, add 'enabled' if it's not  available (assume True, like schedule.list does)
Import python libs
Import salt libs
Certain linux systems will ignore /etc/sysctl.conf, get the right  default configuration file.
otherwise, we don't have it set anywhere and need to set it
Import python libs
Import salt libs
This checks for setting permissions to nothing in the state,  when previous state runs have already set permissions to  nothing. We don't want to report a change in this case.
Import Python Libs
Import Salt Libs
Import python libs
Import salt libs
Define the module's virtual name
Import python libs
Import salt libs
Reset the certs found
Import salt libs
Salt imports
check if group exists
Basic usage (uses default pillar profile key 'grafana')
Pillar contains a list of rows  Pillar contains a single row
Send the command to target
Get errors and list of affeced minions
Did we got any respone from someone ?
Search for errors & status
Import Python libs
Import 3rd-party libs
Import salt libs
get value of first key
Using a credentials profile from pillars
Passing in a credentials profile
fallback
Python Libs
Salt Modules
We're actually looking for the full path to the cachefile here, so  prepend the winrepo_dir
Import python libs
add the user
Using a profile from pillars
Using a profile from pillars
Passing in a profile
override cross_zone_load_balancing:enabled
override UnHealthyHostCount:attributes:threshold
Import Python Libs
Import Salt Libs
load data from attributes_from_pillar and merge with attributes
Determine if any actual listener policies look like default policies,  so we can exclude them from deletion below (see note about this hack  above).
Import python libs
Import salt libs
Import python libs
need to convert boolean values to strings and make lowercase to  pass to gsettings
Import python libs
Import Salt libs
Define the module's virtual name
Connect as admin:sekrit
check if user exists
if the check does not return a boolean, return an error  this may be the case if there is a database connection error
if the check does not return a boolean, return an error  this may be the case if there is a database connection error
fallback
Deleting an autoscale group with running instances.  If instances exist, we must force the deletion of the asg.
Import Python libs
Import Salt libs
Import 3rd-party libs
merge with data from state
Import Python libs
Import Python libs
Import Salt libs
Define the state's virtual name
get keys
load confiration
handle bool and None value
we're good
apply change if needed
load configuration
delete property
we're good
apply change if needed
we're good
we're good
list of images to keep
retreive image_present state data for host  don't throw exceptions when not highstate run
retrieve images in use by vms
parse vmconfig
set hostname if needed
check if vm exists  update vm
process properties  skip special vmconfig_types
add property to changeset
process collections  skip create only collections
create set_ dict
add property to changeset
create remove_ array
remove property
process instances  skip create only instances
add or update instances
find instance with matching ids
ids have matched, disable add instance
handle changes
handle new properties  skip empty props like ips, options,..
create add_ array
add instance
remove instances
find instance with matching ids
keep instance if matched
create remove_ array
remove instance
we're good
delete vm  set archive to true if needed
we're good
we're good
Import Python Libs
Import Salt Libs
Import 3rd-party libs
Import salt libs
Salt imports
default to encrypted passwords  maybe encrypt if if not already and necessary
check if user exists
Import python libs
Import salt libs
volume already exists
Import python libs
Import 3rd-party libs
-- if new member list if different than the current
The group is not present, make it!
Test if gid is free
Import python libs
Check for test option
Configure the value
Check for test option
Check for test option
Import python libs
Import 3rd-party libs
Get only the path to the file without env referrences to check if exists
Import python libs
Import salt libs
Import 3rd-party libs
pwd incorrectly reports presence of home
Make name all Uppers since make.conf uses all Upper vars
Now finally return
Make name all Uppers since make.conf uses all Upper vars
Only load if telemetry is available.
Import Python libs
Define the state's virtual name
check if dataset exists
check name and type
check name and type
check params
check params
check params
retreive snapshots
create snapshot  check if we need need to consider hold
mark snapshot for hold as needed
Import python libs
Monitoring state, but changes may be made over HTTP
Import Python libs
Import salt libs
Import 3rd-party libs
We search through the dictionary getfacl returns for the owner of the  file if acl_name is empty.
We search through the dictionary getfacl returns for the owner of the  file if acl_name is empty.
Import Python Libs
Import Python libs
Import salt libs
Import Python Libs
if ther eisn't enough time remaining, we consider it a failure
Import Python Libs
Import Salt Libs
Import python libs
Import salt libs
Install the features
Set up logging
Retreive changes to made
error detected
setup return structure
get existing config if job is present
Import python libs
Import Salt libs
Import 3rd-party libs
Import python libs
Import salt libs
Import 3rd-party libs
Set up logger
Define the module's virtual name
Device exists
Decide whether to create or assemble  mdadm -E exits with 0 iff all devices given are part of an array
Attempt to create or assemble the array
Saving config
python std lib
salt modules
noth configured => configure with expected probes
noting expected => remove everything
new probes
old probes, to be removed
build expect probes config dictionary  using default values
must also be able to commit & make sure we have applied all necessary changes
Import Python libs
Import salt libs
Fall back to the repo name if humanname not provided
On a RedHat-based OS, 'enabled' is assumed to be true if  not explicitly set, so we don't need to update the repo  if it's desired to be enabled and the 'enabled' key is  missing from the repo definition
empty file before configure
This is another way to pass information back from the mod_repo  function.
Clear cache of available packages, if present, since changes to the  repositories may change the packages that are available.
Import Python Libs
Import Salt Libs
Import python libs
Import salt libs
Import python libs
Import salt libs
We won't be setting upstream because the act of checking out a new  branch will set upstream for us
--set-upstream does not assume the current branch, so we have to  tell it which branch we'll be using
Shouldn't be making any changes if the repo was up to date, but  report on them so we are alerted to potential problems with our  logic.
Single key
Two keys
check if git.latest should be applied
Just go with whatever the upstream currently is
Empty remote repo
Annotated tag
Non-annotated tag
git ls-remote did not find the rev, and because it's a  hex string <= 40 chars we're going to assume that the  desired rev is a SHA1
A specific rev is desired, but that rev doesn't exist on the  remote repo.
Local checkout doesn't have the remote_rev
Either the remote rev could not be found with git  ls-remote (in which case we won't know more until  fetching) or we're going to be checking out a new branch  and don't have to worry about fast-forwarding.
No local branch, no upstream tracking branch
Repo content would not be modified but the remote  URL would be modified, so we can't just say that  the repo is up-to-date, we need to inform the  user of the actions taken.
Now that we've fetched, check again whether or not  the update is a fast-forward.
We're cloning a fresh repo, there is no local branch or revision
Determine if supplied ref is a hash
The ref is a hash and it exists locally so skip to checkout
Check that remote is present and set to correct url
The fetch_url for the desired remote does not match the  specified URL (or the remote does not exist), so set the  remote URL.
Value matching 'baz'
Ensure entire multivar is unset
Ensure all variables in 'foo' section are unset, including multivars
Ensure that the key regex matches the full key name
Get matching keys/values
No changes need to be made
Get all keys matching the key expression, so we can accurately report  on changes made.
Single value
Multiple values
Get current value
Set/update config value
No reason to stop, return True
Import Python Libs
Define the module's virtual name
Function aliases
Import third party libs
Watch to rm etcd key
error detected
Import python libs
Import Python Libs
Import Salt Libs
Using a profile from pillars
Standard Libs
Get current subscriptions
Convert subscriptions into a data strucure we can compare against
We are using https and have auth creds - the password will be starred out,  so star out our password so we can still match it
Ensure the endpoint is set back to it's original value,  incase we starred out a password
Import python libs
Import salt libs
constraints, properties, resource defaults or resource op defaults  do not support specifying an id on 'show' command
item_id was provided,  return code 0 indicates, that resource already exists
Import python libs
Populate the 'changes' dict if anything changed
Check if rule exists
Import python libs
Because __opts__ is not available outside of functions
Because __opts__ is not available outside of functions
Import Python libs
Import Salt libs
Define the state's virtual name
ensure the pool is present
retrieve current properties
figure out if updates needed
update properties
also transform value so we match with the return
construct *vdev parameter for zpool.create
Import Python libs
Import 3rd-party libs
define the module's virtual name
When test=true return none
When test=true return none
Import python libs
If we've reached this far before returning, we have changes.
Import salt libs
Tested on OpenBSD 5.0
Import python libs
Import salt libs
Darwin
Windows - Needs its own branch as all settings need to be set at the same time
We need to update one of our proxy servers
Proxy settings aren't enabled
We need to update our bypass domains
Import python libs
Import third party libs
Set up logging
import python libs
import python libs
Import Python libraries
Import Salt libraries
'set' is a reserved word
import python libs
import Salt libs
import python libs
import python libs
Import python libs
Import third party libs
import python libs
pylint: disable=unused-argument
pylint: disable=unused-argument
can be any port  no need to specify ssl_key if cert and key  are in one single file
Get the Websocket connection to Salt
Get Salt's "real time" event stream.
Simple listener to print results of Salt's "real time" event stream.  Look at https://pypi.python.org/pypi/websocket-client/ for more examples.
Terminates websocket connection and Salt's "real time" event stream on the server.
Get the Websocket connection to Salt
Get Salt's "real time" event stream.
Simple listener to print results of Salt's "real time" event stream.  Look at https://pypi.python.org/pypi/websocket-client/ for more examples.
Terminates websocket connection and Salt's "real time" event stream on the server.
TBD: Add ability to run commands in this branch
TBD: Add logic to run salt commands here
TBD: Add ability to run commands in this branch
self.write_message(u'data: {0}\n\n'.format(json.dumps(event)))
TBD: Add logic to run salt commands here
if you have enabled websockets, add them!
Matches /all_events/[0-9A-Fa-f]{n}  Where n is the length of hexdigest  for the current hashing algorithm.  This algorithm is specified in the  salt master config file.
logger.debug('In presence')  encoding: utf-8
These represent a "real time" view into Salt's jobs.
This represents a "real time" view of minions connected to  Salt.
logger.debug(minion_info)
encoding: utf-8
can be any port  address to bind to (defaults to 0.0.0.0)  socket backlog  no need to specify ssl_key if cert and key  are in one single file
Import Python libs
pylint: disable=import-error  pylint: enable=import-error
instantiate the zmq IOLoop (specialized poller)
salt imports
Any is completed once one is done, we don't set for the rest
tag -> list of futures
request_obj -> list of (tag, future)
map of future -> timeout_callback
timeout the future  remove the timeout
if the request finished, no reason to allow event fetching, since we  can't send back to the client
add this tag and future to the callbacks
find the token (cookie or headers)
Find an acceptable content-type  Ignore any parameter, including q (quality) one
better return message?
do the common parts
timeout all the futures
because people are terrible and don't mean what they say
Use cgi.parse_header to correctly separate parameters from value
Allow X-Auth-Token in requests
Allow request headers
Allow X-Auth-Token in responses
Allow all methods
if any of the args are missing, its a bad request
Grab eauth config for the current backend for the current user
If we can't find the creds, then they aren't authorized
if you aren't authenticated, redirect to login
check clients before going, we want to throw 400 if one is bad
Make sure we have 'token' or 'username'/'password' in each low chunk.  Salt will verify the credentials are correct.
if we have nothing to wait for, don't wait
wait until someone is done
seed minions_remaining with the pub_data
if we have a min_wait, do that  we are completed when either all minions return or the job isn't running anywhere
its possible to get a return that wasn't in the minion_remaining list
fire a job off
only return the return data
if you aren't authenticated, redirect to login
if you aren't authenticated, redirect to login
if you aren't authenticated, redirect to login
if you have the tag, prefix
In Tornado >= v4.0.3, the headers come  back as an HTTPHeaders instance, which  is a dictionary. We must cast this as  a dictionary in order for msgpack to  serialize it.
Cors origin is either * or specific origin
Import Python libs
Eauth currently requires a running daemon and commands run through  this method require eauth so perform a quick check to raise a  more meaningful error.
encoding: utf-8
This pipe needs to represent the parent end of a pipe.  Clients need to ensure that the pipe assigned to ``self.pipe`` is  the ``parent end`` of a  `pipe <https://docs.python.org/2/library/multiprocessing.htmlexchanging-objects-between-processes>`_.
The token that we can use to make API calls.  There are times when we would like to kick off jobs,  examples include trying to obtain minions connected.
Options represent ``salt`` options defined in the configs.
Import Python libs
Import CherryPy without traceback so we can provide an intelligent log  message in the __virtual__ function
Everything looks good; return the module name
CherryPy wasn't imported; explain why
Missing port config
Import python libs
Import 3rd-party libs
Import Salt libs
These represent a "real time" view into Salt's jobs.
This represents a "real time" view of minions connected to Salt.
check if any connections were dropped
check if any new connections were made
Write the cookie file:
Read the cookie file:
Import Python libs
Import third-party libs  pylint: disable=import-error  pylint: enable=import-error
Import Salt libs
Import salt-api libs
Imports related to websocket
X-Auth-Token header trumps session cookie
Session is authenticated; inform caches
Always set response headers necessary for 'simple' CORS.
If this is a non-simple CORS preflight request swap out the handler.
Be conservative in what you send  Maps Content-Type to serialization functions; this is a tuple of tuples to  preserve order of preference.
Raises 406 if requested content-type is not supported
Transform the output from the handler into the requested output format
First call out to CherryPy's default processor
Be liberal in what you accept
Finally, make a Low State and put it in request
Release the session lock before executing any potentially  long-running Salt commands. This allows different threads to execute  Salt commands concurrently without blocking.
if the lowstate loaded isn't a list, lets notify the client
Make any 'arg' params a list if not already.  This is largely to fix a deficiency in the urlencoded format.
Sometimes Salt gives us a return and sometimes an iterator
Sending multiple positional args with urlencoded:
Calling runner functions:
the urlencoded_processor will wrap this in a list
Validate against the whitelist.
Mint token.
Grab eauth config for the current backend for the current user
Get sum of '*' perms, user-specific perms, and group-specific perms
Auth handled manually below
The eauth system does not currently support perms for the event  stream, so we're just checking if the token exists not if the token  allows access.
Release the session lock before starting the long-running response
Auth handled manually below
Look at https://pypi.python.org/pypi/websocket-client/ for more  examples.
Manually verify the token
Release the session lock before starting the long-running response
A handler is the server side end of the websocket connection. Each  request spawns a new instance of this handler
blocks until send is called on the parent end of this pipe.
Process to handle async push to a client.  Each GET request causes a process to be kicked off.
Don't do any lowdata processing on the POST data
Auth can be overridden in __init__().
Allow the Webhook URL to be overridden from the conf.
Add to global config
Add Salt and salt-api config options to the main CherryPy config dict
Import salt libs
Instantiate APIClient once for the whole app
Convert the response to JSON
Return the response
When started outside of salt-api __opts__ will not be injected
pylint: disable=C0103
Import python libs
Import salt libs
Import third party libs
Import Python libs
Import salt libs
Import python libs
Import salt libs
Function references are not picklable. Windows needs to pickle when  spawning processes. On Windows, these will need to be recalculated  in the spawned child process.
Import python libraries
Import salt libs
Find the channel where the message came from
Trim the ! from the front  cmdline = _text[1:].split(' ', 1)
Ensure the command is allowed
default to trying to run as a client module.
Fire event to event bus
Fire event to event bus
Import python libs
Import salt libs
Import third party libs
Import Salt libs
Import third party libs
This is here for older python installs, it is needed to setup an  encrypted tcp connection
Import Python libs
Error message displayed when an incorrect Token has been detected  Unicode Line separator character   \u2028
wait for a bit as to not burn through api calls
Ensure the user is allowed to run commands
Check for target. Otherwise assume *
Ensure the command is allowed
Default to trying to run as a client module.
Import python libs
Import salt libs
Import salt libs
Import Python Libs
pylint: disable=import-error
Default timeout as of docker-py 1.0.0
Define the module's virtual name
Import python libraries
Import salt libs
Import third-party libs
Import python libs
Import salt libs
Import third party libs
Define the module's virtual name
Import Python libs
Import Salt libs
Define the module's virtual name
Create a copy of the object that we will return.
Set the ID of the document to be the JID.
Add a timestamp field to the document
Check to see if the database exists.
Make a PUT request to create the database.
Call _generate_doc to get a dict object of the document we're going to  shove into the database.
Make sure the 'total_rows' is returned.. if not error out.
Return the rows.  Because this shows all the documents in the database, including the  design documents, verify the id is salt jid
Get the options..
Define a simple return object.
get_minions takes care of calling ensure_views for us.  For each minion we know about
Skip the minion if we didn't get any rows back. ( IE function that  they're looking for has a typo in it or some such ).
Set the respnse ..
Make sure the views are valid, which includes the minions..
Make the request for the view..
Verify that we got a response back.
Iterate over the rows to build up a list return it.
Get the options so we have the URL and DB..
Make a request to check if the design document exists.
If the document doesn't exist, or for some reason there are not views.
Determine if any views are missing from the design doc stored on the  server..  If we come across one, simply set the salt view and return out.  set_salt_view will set all the views, so we don't need to continue t  check.
Valid views, return true.
Create the new object that we will shove in as the design doc.
Import python libs
pylint: disable=import-error,no-name-in-module,redefined-builtin  pylint: enable=import-error,no-name-in-module,redefined-builtin
default to passive check type
checktype should be a string
Import python libs
Import Salt libs
Import third party libs
Define the module's virtual name
Import python libs
Import salt libs
Define the module's virtual name
Make a note of this minion for the external job cache
Import python libs
Import salt libs
Import third party libs
Define the module's virtual name
Ensure port is an int
https://github.com/saltstack/salt/issues/22171  Without this try:except: we get tons of duplicate entry errors  which result in job returns not being stored properly
Import Python libs
pylint: disable=import-error,no-name-in-module,redefined-builtin  pylint: enable=import-error,no-name-in-module,redefined-builtin
Import Salt Libs
Slack wants the body on POST to be urlencoded.
select the config source
browse the config for relevant options, store them in a dict
override some values with relevant profile options
override some values with relevant options from  keyword arguments passed via return_kwargs
c_cfg is a dictionary returned from config.option for  any options configured for this returner.
Using the default configuration key
Using ret_config to override the default configuration key
Look for the configuration item in the override location
if not configuration item found, fall back to the default location.
default place for the option in the config
Attribute not found, check for a default value
fallback (implicit else for all ifs)
Import python libs
Import salt libs
cache of the master mininon for this returner
Import python libs
Import salt libs
Import Python libs
Import Salt libs
Define the module's virtual name
Import Python libs
Import Salt libs
Import python libs
Import salt libs
Import third party libs
Define the module's virtual name
Ensure port is an int
https://github.com/saltstack/salt/issues/22171  Without this try:except: we get tons of duplicate entry errors  which result in job returns not being stored properly
Import python libs
Import salt libs
Import 3rd-party libs
Define the module's virtual name
Shut down and close socket
Strip the hostname from the carbon base if we are returning from virt  module since then we will get stable metric bases even if the VM is  migrate from host to host
Import python libs
Import Salt libs
Better safe than sorry here. Even though sqlite3 is included in python
Define the module's virtual name
Pop the jid off the list since it is not  needed and I am trying to get a perfect  pylint score :-)
Import Python libraries
Import Salt libraries
Define this module's virtual name
Import python libs
Import python libs
Import third party libs
Define the module's virtual name
Combine host and port to conform syntax of python memcache client
The following operations are neither efficient nor atomic.  If there is a way to make them so, this should be updated.
returns = {minion: return, minion: return, ...}
returns = {minion: return, minion: return, ...}
Import python libs
Import Salt libs
Import python libs
Import Salt libs
FIXME We'll need to handle this differently for Windows.  Import third party libs
Define the module's virtual name
Import python libs
Import Salt libs
Import third party libs
Define the module's virtual name
return data in the format {<minion>: { <unformatted full return data>}}
Set host to specified value or default to localhostname if no value provided
If eventtime in epoch not passed as optional argument use current system time in epoch
Fill in local hostname if not manually populated
Update time value on payload if need to use system time
send event to http event collector
Print debug info if flag set
Fill in local hostname if not manually populated
Print debug info if flag set
If eventtime in epoch not passed as optional argument use current system time in epoch
Update time value on payload if need to use system time
Import salt libs
Define the module's virtual name
some globals
try to load some faster json libraries. In order of fastest to slowest
Import python libs
Import Salt libs
Import third party libs
HTTP API header used to check the InfluxDB version
Define the module's virtual name
strip the 'return' key to avoid data duplication in the database
Import python libs
Import salt libs
Import 3rd-party libs
if a minion is returning a standalone job, get a jobid
Use atomic open here to avoid the file being read before it's  completely written to. Refs 1935
Use atomic open here to avoid the file being read before  it's completely written to. Refs 1935
rarely, the directory can be already concurrently created between  the os.path.exists and the os.makedirs lines above
Keep track of any empty t_path dirs that need to be removed later
Check if there are any stray/empty JID t_path dirs
No jid file means corrupted cache entry, scrub it  by removing the entire t_path directory
Remove the entire t_path from the original JID dir
Import python libs
Import Salt libs
Import third party libs
Import Python libs
Import 3rd-party libs  pylint: disable=import-error,no-name-in-module,redefined-builtin  pylint: enable=import-error,no-name-in-module,redefined-builtin
Import Salt Libs
Import Python libs
Import Salt Libs
Import python libs
import Salt libs
Import third party libs
Define the module's virtual name  currently only used iby _get_options
return data in the format {<minion>: { <unformatted full return data>}}
Import python libs
Import Salt libs
Define the module's virtual name
Get values from syslog module
parse for syslog options
Open syslog correctly based on options and tag
Send log of given level and facility
Close up to reset syslog to defaults
Import python libs
Import salt libs
Import third party libs
load is the published job  the list of minions that the job is targeted to (best effort match on the  master side)  return is the "return" from the minion data  out is the "out" from the minion data
Certain XMPP functionaility we're using doesn't work with versions under 1.3.1
PyLint wrongly reports an error when calling super, hence the above  disable call
Import Python Libs
Import Salt libs
Various constants
Import python libs
Import third party libs  pylint: disable=import-error  pylint: enable=import-error
Import salt libs
Get the signing algorithm
Get the ASN1 format of the certificate
Decode the certificate
The certificate has three parts:  - certificate  - signature algorithm  - signature  http://usefulfor.com/nothing/2009/06/10/x509-certificate-basics/
The signature is a BIT STRING (Type 3)  Decode that as well
Get the payload
First byte is the number of unused bits. This should be 0  http://msdn.microsoft.com/en-us/library/windows/desktop/bb540792(v=vs.85).aspx  Now get the signature itself
Import python libs
Import salt libs
invalid token, delete it!
fstr = '{0}.perms'.format(auth_back)     if fstr in self.loadauth.auth:         auth_data.append(getattr(self.loadauth.auth)())
Accept find_job so the CLI will function cleanly
Use current user if empty
Import python libs
Import python libs
Import salt libs
Import python libs
Import salt libs
Import third party libs
Only add binddn/bindpw to the connargs when they're set, as they're not  mandatory for initializing the LDAP object, but if they're provided  initially, a bind attempt will be done during the initialization to  validate them
search for the user's DN to be used for the actual authentication
the binddn can also be composited, e.g.    - {{ username }}@domain.com    - cn={{ username }},ou=users,dc=company,dc=tld  so make sure to render it first before using it
Only add binddn/bindpw to the connargs when they're set, as they're not  mandatory for initializing the LDAP object, but if they're provided  initially, a bind attempt will be done during the initialization to  validate them
Update connection dictionary with the user's password
TypeError here just means that one of the returned  entries didn't match the format we expected  from LDAP.
Import python libs
Can use an application ID  Or can use a directory ID  But not both
Import Python Libs
prefer C bindings over python when available
Import python libs
Import Salt Libs
Import 3rd-party libs
prefer C bindings over python when available  CSafeDumper causes test failures under python3
!reset instruction applies on document only.  It tells to reset previous decoded value for this present key.
Ensure obj is str, not py2 unicode or py3 bytes
If value was all zeros, node.value would have been reduced to  an empty string. Change it to '0'.
search implicit tag
python2's ConfigParser cannot parse a config from a string
Import Python libs
Import Salt Libs
Import 3rd-party libs
Attempt to import msgpack  There is a serialization issue on ARM and potentially other platforms  for some msgpack bindings, check for it
Fall back to msgpack_pure
Import python libs  import sys   Use if sys is commented out below
Import salt libs
Import third party libs  No need for zeromq in local mode
Attempt to import msgpack  There is a serialization issue on ARM and potentially other platforms  for some msgpack bindings, check for it
Fall back to msgpack_pure
msgpack only supports 'encoding' starting in 0.4.0.  Due to this, if we don't need it, don't pass it at all so  that under Python 2 we can still work with older versions  of msgpack.
msgpack only supports 'use_bin_type' starting in 0.4.0.  Due to this, if we don't need it, don't pass it at all so  that under Python 2 we can still work with older versions  of msgpack.
msgpack doesn't support datetime.datetime datatype  So here we have converted datetime.datetime to custom datatype  This is msgpack Extended types numbered 78
Should support OrderedDict serialization, so, let's  raise the exception
When using Python 3, write files in such a way  that the 'bytes' and 'str' types are distinguishable  by using "use_bin_type=True".
create a new one
For Python 2.5.  A no-op on 2.6 and above.
Change to salt source's directory prior to running any command  We're most likely being frozen and __file__ triggered this NameError  Let's work around that
The user can provide a different bootstrap-script version.  ATTENTION: A tag for that version MUST exist  If no bootstrap-script version was provided from the environment, let's  provide the one we define.
Store a reference wether if we're running under Python 3 and above
Add the esky bdist target if the module is available  may require additional modules depending on platform  bbfreeze chosen for its tight integration with distutils
Salt SSH Packaging Detection
pylint: disable=W0122  pylint: enable=W0122
In Windows, we're installing M2CryptoWin{32,64} which comes  compiled
Python 3 already has futures, installing it will only break  the current python installation whenever futures is imported
Custom Distutils/Setuptools Commands >
Write the version file
pylint: disable=E0602  pylint: enable=E0602
Write the syspaths file
Write the salt-ssh packaging file
pylint: disable=E0602  pylint: enable=E0602
Install M2Crypto first
Install PyCrypto
Download the required DLLs
Resume normal execution
Let's generate salt/_version.py to include in the sdist tarball
Let's the rest of the build command
This file was auto-generated by salt's setup on \
This file was auto-generated by salt's setup on \
Run build.run function  If our install attribute is present and set to True, we'll go  ahead and write our install time python modules.
Write the hardcoded salt version module salt/_version.py
Write the system paths file
Let's set the running_salt_install attribute so we can add  _version.py in the build command
Install M2Crypto first  Install PyCrypto
Download the required DLLs  Run install.run
input and outputs match 1-1
we have them as requirements in pkg/smartos/esky/requirements.txt  all these should be safe to force include
Overridden Methods >
!/usr/bin/python
http://stackoverflow.com/a/404750  determine if application is a script file or frozen exe
Import Sphinx libs
Use the salt func aliased name instead of the real name
make first list item
If we have a pair of annotation/content items, append the list  item and create a new list item
Non-semantic div for styling
Monkey-patch the Python domain remove the python module index
indices = [HTTPIndex]
If mocked function is used as a decorator, expose decorated function.  if args and callable(args[-1]):      functools.update_wrapper(ret, args[0])
Python stdlib
third-party libs for netapi modules
Define a fake version attribute for the following libs.
We're now able to import salt
Intersphinx Settings >  < Intersphinx Settings
Localization >  < Localization
Set a var if we're building docs for the live site or not
Use Google customized search or use Sphinx built-in JavaScript search
One entry per manual page. List of tuples  (source start file, name, description, authors, manual section).
Import Python libs
Import Salt Testing libs
Import 3rd-party libs  pylint: disable=import-error  pylint: enable=import-error
Not strictly speaking mandatory but just makes sense
This is mandatory so that the HTTP server isn't started  if you need to actually start (why would you?), simply  subscribe it back.
This is a required header when running HTTP/1.1
if we had some data passed as the request entity  let's make sure we have the content-length set
Get our application and run the request against it  XXX: perhaps not the best exception to raise?
Cleanup any previous returned response  between calls to this method
collapse the response into a bytestring
This backport uses bytearray instead of bytes, as bytes is the same  as str in Python 2.7.  s/\(b'[^']\+'\)/bytearray(\1)/g  plus manual fixes for implicit string concatenation.
A trailing IPv4 address is two parts
Ensure none of the internal objects accidentally  expose the right set of attributes to become "equal"
test that two addresses are supernet'ed properly
test same IP networks
if addresses are the same, sort by netmask
some generic IETF reserved addresses
this probably don't make much sense, but it's included for  compatibility with ipv4
V4 - check we're cached
V6 - make sure we're empty
-*- coding: utf-8 -*-
Make a local reference to the CherryPy app so we can mock attributes.
-*- coding: utf-8 -*-
Import Python libs
Import Salt Libs
Import Python libs
Import Salt Testing libs
Import salt libs
Import Python libs
Import Salt Testing libs
Import salt libs
We probably caught an error because files already exist. Ignore
Import Python libs
Import Salt Testing libs
Import salt libs
We need to setup the file roots
There's no use creating the empty-directory ourselves at this  point, the minions have already synced, it wouldn't get pushed to  them
There's no use creating the empty-directory ourselves at this  point, the minions have already synced, it wouldn't get pushed to  them
Import Python libs
Import Salt Libs
Import Salt Testing Libs
Import Python libs
Import Salt Libs
Import Python libs
Import Salt Libs
Import Salt Testing Libs
Travis is slow
Register the exit clean-up before making anything needing clean-up
Unused for now
Kill the process group since sending the term signal  would only terminate the shell, not the command  executed in the shell
As a last resort, kill the process group
process already terminated
This is effectively a place-holder - it gets set correctly after super()
This is effectively a place-holder - it gets set correctly after super()
NOTE: this simple update will not work for deep dictionaries
Import Python libs
Import Salt Testing libs
Import Salt libs
Import python libs
Import Salt Testing libs
Import Salt libs
Import python libs
Import Salt Testing libs
Import salt libs
Import python libs
Import Salt Testing libs
Import salt libs
Import Python libs
Import Salt Testing libs
Import salt libs
Import Salt Testing libs
Import Python libs
Import salt libs
Just in case a previous test was interrupted, remove the  directory and try adding it again.
Crete the FS_ROOT  Make sure we have a fresh root dir for this saltenv
Create the CACHE_ROOT
Import Salt Libs
Import Salt Testing Libs
Import Salt libs
this should not raises UnicodeEncodeError
display trace in error message for debugging on jenkins
Import python libs
Import 3rd-party libs
Import Salt libs
A sentinel to stop processing the queue  Just log everything, filtering will happen on the main process  logging handlers
Broken pipe
Import Python libs
Import Salt libs
Import python libs
Import salt libs
Avoid ${TMPDIR} and gettempdir() on MacOS as they yield a base path too long  for unix sockets: ``error: AF_UNIX path too long``  Gentoo Portage prefers ebuild tests are rooted in ${TMPDIR}
This tempdir path is defined on tests.integration.__init__
Import Python libs
Import python libs
Set up logging
DRY up the name we use
-*- coding: utf-8 -*-
Import python libs
Import salt libs
Import 3rd-party libs
Create our own IOLoop, we're in another process
bind the socket to localhost on the config provided port  become a server socket
Import Salt Testing libs
Update sys.path
Not required for raet tests
Import 3rd-party libs
Avoid ${TMPDIR} and gettempdir() on MacOS as they yield a base path too long  for unix sockets: ``error: AF_UNIX path too long``  Gentoo Portage prefers ebuild tests are rooted in ${TMPDIR}
Explicit and forced cleanup
These ports are hardcoded in the test configuration
Transplant configuration
Connection reset on windows
We're not on windows
Late import
Windows need the python executable to come first
We're not actually interested in processing the output, just consume it
There's no shell color support on windows...
Setup the multiprocessing logging queue listener
Set up PATH to mockbin
Wait for the daemons to all spin up
Let's not have false positives and wait one more seconds
Already synced!?
An errors has occurred
Synced!
This is broad but we'll see all kinds of issues right now  if we drop the proc out from under the socket while we're reading
Try to match stalled state functions
This is the supposed return format for state calls
If it's a tuple, turn it into a list
If it's a string, make it a one item list
If we've reached here, it's a bad type passed to keys
Import Python libs
Import Salt Testing libs
Import Salt libs
Import python libs
Import Salt Testing libs
Import salt libs
Clear existing attributes
Test no attributes
Test file not found
Clear existing attributes
Clear existing attributes
Write an attribute
Read the attribute
Test file not found
Test attribute not found
Clear existing attributes
Delete an attribute
Make sure it was actually deleted
Test file not found
Test attribute not found
Clear existing attributes
Test Clear
Test file not found
Import python libs
Import Salt Testing libs
Import salt libs
Failed to get the SHELL var, don't run
On some distros (notably Gentoo) os.getlogin() fails
Import Python libs
Import Salt Testing libs
Import salt libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Must be running on a mac
Import Python Libs
Import Salt Testing libs
Import salt libs
Import Python libs
Import Salt Testing libs
Import salt libs
Import Python Libs
Import Salt Libs
Import Salt Testing Libs
Module Variables
Data needed for cleanup
If sysctl testing file exists, delete it
delete temporary file
restore original sysctl file
remove sysctl.conf created by tests
Import python libs
Import Salt Testing libs
Import salt libs
Sleep longer, sometimes test systems get bogged down
Import python libs
Import Salt Testing libs
Import salt libs
Import Python libs
Import Salt Testing libs
Import salt libs
Import 3rd-party libs
Due to a segfault in lxc-destroy caused by invalid configs,  truncate the config.
Import python libs
Import Salt Testing libs
Import salt libs
Only run on linux for now until or if we can figure out a way to use  __grains__ inside of useradd.__virtual__
This uid is available, store it
Python Libs
Salt Libs
Salttesting libs
save added beacon
delete the beacon
save the results
Add beacon to disable
delete added beacon
assert beacon exists
assert beacons are disabled
disable added beacon
assert beacon ps is disabled
assert beacon exists
enable beacons on minion
assert beacons are enabled
enable added beacon
assert beacon ps is enabled
Import Python libs
Import Salt Testing libs
Import salt libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Let's install a bundle to use in tests
Delete any bundles that were installed
Import python libs
Import Salt Testing libs
Import salt libs
instead of using the 'clean' hosts file we're going to  use an empty one so we can prove the syntax of the entries  being added by the hosts module
Import python libs
Import Salt Testing libs
Import salt libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Create group name strings for tests
Now try to delete the added group
Delete ADD_GROUP
Delete DEL_GROUP if something failed
Delete CHANGE_GROUP
Import python libs
Import Salt Testing libs
Import salt libs
Correct Functionality
User does not exist
User does not exist
User does not exist
User does not exist
Correct Functionality
User does not exist
Correct Functionality
User does not exist
Correct Functionality
User does not exist
Import Salt Testing libs
Import salt libs
Import python libs
Import Salt Testing libs
Import salt libs
Import 3rd-party libs
name with space
'''''''  create  also with character_set only
Teardown, remove database
this is his name hash : user "2'標  and this is the same with a 'b' added
Import python libs
Import Salt Testing libs
Import salt libs
Import python libs
Import Salt Testing libs
Import salt libs
Import 3rd-party libs
Get all functions
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Create user strings for tests
Now try to delete the added user
Delete ADD_USER
Delete DEL_USER if something failed
Delete CHANGE_USER
Import Python libs
Import Salt Testing libs
Import salt libs
Import 3rd-party libs
Make /etc/mtab unreadable
Import python libs
Import Salt Testing libs
Import salt libs
Test invalid input
Test invalid input
Test list and get
Test passing set a bad disk
Normal Functionality
Pass set bad value for seconds
Normal Functionality
Test invalid input
Test invalid input
Import python libs
Import Salt Testing libs
Import salt libs
Import python libs
Import Salt Testing libs
Import salt libs
Missing Service
Raise an error
Service not found
Service not found
Import python libs
Import Salt Testing libs
Import salt libs
Import Python libs
Import Salt Testing libs
Import salt libs
Import python libs
Import Salt Testing libs
Import salt libs
Import python
Import Salt Testing libs
Import salt libs
These 2 services might return in different orders so test separately
Import Python libs
Import Salt Testing libs
Import Salt libs
Import 3rd-party libs
The AWS account ID is a 12-digit number.  http://docs.aws.amazon.com/general/latest/gr/acct-identifiers.html
Import Python libs
Import Salt Testing libs
Import salt libs
Import python libs
Import Salt Testing libs
Import salt libs
Import python libs
Import Salt Testing libs
Import salt libs
left behind... Don't fail because of this!
Import Python libs
Import Salt Testing libs
Import salt libs
Import python libs
Import Salt Testing libs
Import salt libs
This uid is available, store it
Import python libs
Import Salt Testing libs
Import salt libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Must be running on a mac
Remove the salttest cert, if left over.
check to ensure the cert was installed
uninstall cert
check to ensure the cert was uninstalled
Import python libs
Import Salt Testing libs
Import salt libs
Import python libs
Import Salt Testing libs
Import salt libs
Can't predict what will be returned, so can only test that the return  is the correct type, dict
Test reset_ignored
Test list_ignored and verify ignore
Test enable
Test disable in case it was already enabled
There's no way to know what the dictionary will contain, so all we can  check is that the return is a dictionary
Test update_available
Test update not available
Test update not available
Reset the catalog
Test reset the catalog
Import Pytohn libs
Import Salt Testing libs
Import salt libs
Import python libs
Import Salt Testing libs
Import salt libs
Import 3rd-party libs
set variable identifying the chroot you work in (used in the prompt below)
Re-append switching order
set variable identifying the chroot you work in (used in the prompt below)
Delete if exiting
Create the file
The first append
The second append
ret,     ['Cannot extend ID Z in "base:requisites.prereq_error_nolist".'     + ' It is not part of the high state.']
'A recursive requisite was found, SLS "requisites.use_recursion2"'     + ' ID "C" ID "A"'
'A recursive requisite was found, SLS "requisites.use_recursion"'     + ' ID "A" ID "A"'
Only run the state once and keep the return data
First, test the result of the state run when changes are expected to happen
Only run the state once and keep the return data
First, test the result of the state run when two changes are expected to happen
Finally, test the result of the state run when only one of the onchanges requisites changes.
Only run the state once and keep the return data
First, test the result of the state run of when changes are expected to happen
Only run the state once and keep the return data
First, test the result of the state run when a failure is expected to happen
Then, test the result of the state run when a failure is not expected to happen
Only run the state once and keep the return data
First, test the result of the state run when a failure is expected to happen
Then, test the result of the state run when a failure is not expected to happen
Only run the state once and keep the return data
First, test the result of the state run when a listener is expected to trigger
Then, test the result of the state run when a listener should not trigger
Only run the state once and keep the return data
First, test the result of the state run when a listener is expected to trigger
Then, test the result of the state run when a listener should not trigger
Only run the state once and keep the return data
Test the result of the state run when a listener is expected to trigger
Only run the state once and keep the return data
Both listeners are expected to trigger
Import Python libs
Import Salt Testing libs
Import salt libs
test True
test False
Import python libs
Import Salt Testing libs
Import salt libs
Let's create the testing virtualenv
Create a requirements file that depends on another one.
Import python libs
Import Salt Testing libs
Import salt libs  from salt.modules import linux_acl as acl
Import python libs
Import Salt Testing libs
Import salt libs
Import python libs
Import Salt Testing libs
Import salt libs
Import python libs
Import Salt Testing libs
Import salt libs
Import Python libs
Import Salt Testing libs
Import salt libs
Import Python Libs
Import Salt Testing libs
Import salt libs
Don't raise an exception if the directory exists
Navigate to the root of the repo to init, stage, and commit  Initialize a new git repository
Cleanup after yourself
Change to newly-created temp dir  Cleanup after yourself
Merge the second branch into the current branch  Merge should be a fast-forward
Using --abbrev-ref on HEAD will give us the current branch
Stage the new file so it shows up as a 'new' file
Import python libs
Import Salt Testing libs
Import salt libs
Correct Functionality
Test bad time zone
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Brew doesn't support local package installation - So, let's  Grab some small packages available online for brew
Now remove the installed package
Remove any installed packages
Import python libs
Import Salt Testing libs
Import salt libs
Test Package is installed
Test Package is not installed
Test if installed
Download the package
Test forget
Import Python libs
Import Salt Testing libs
Import Salt libs
Import 3rd-party libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Set volume back to what it was before
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
arg_str = "{0} --log-level=error".format(arg_str)
Sometimes tuples are returned???
Args converted in the form of key1='value1' ... keyN='valueN'
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Create the cloud instance name to be used throughout the tests
check if client_key and api_key are present
if test instance is still present, delete it
Import Python Libs
Import Salt Libs
Import Salt Testing Libs
Create the cloud instance name to be used throughout the tests
check if appropriate cloud provider and profile files are present
check if id, key, keyname, securitygroup, private_key, location,  and provider are present
create the instance
check if instance returned with salt installed
delete the instance
check if deletion was performed appropriately
if test instance is still present, delete it
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Create the cloud instance name to be used throughout the tests
check if api_key, ssh_key_file, and ssh_key_names are present
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Import Third-Party Libs
Create the cloud instance name to be used throughout the tests
if test instance is still present, delete it
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Create the cloud instance name to be used throughout the tests
check if personal access token, ssh_key_file, and ssh_key_names are present
if test instance is still present, delete it
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Import Third-Party Libs
Create the cloud instance name to be used throughout the tests
check if credentials and datacenter_id present
if test instance is still present, delete it
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Create the cloud instance name to be used throughout the tests
check if user, password, private_key, and keyname are present
if test instance is still present, delete it
Import Python Libs
Import Salt Libs
Import Salt Testing Libs
check if appropriate cloud provider and profile files are present  Create the cloud instance name to be used throughout the tests
check if project, service_account_email_address, service_account_private_key  and provider are present
create the instance
check if instance returned with salt installed
delete the instance  example response: ['gce-config:', '', '    gce:', '', 'cloud-test-dq4e6c:', 'True', '']
check if deletion was performed appropriately
create the instance
check if instance returned with salt installed
delete the instance  example response: ['gce-config:', '', '    gce:', '', 'cloud-test-dq4e6c:', 'True', '']
check if deletion was performed appropriately
salt-cloud -a show_instance myinstance
if test instance is still present, delete it
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Import Third-Party Libs
Create the cloud instance name to be used throughout the tests
check if personal access token, ssh_key_file, and ssh_key_names are present
if test instance is still present, delete it
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Setup logging  log_handler = logging.StreamHandler()  log_handler.setLevel(logging.INFO)  log.addHandler(log_handler)  log.setLevel(logging.INFO)
As described in the documentation of list_nodes (this may change with time)
check if appropriate cloud provider and profile files are present
check if instance with salt installed returned
destroy the instance
check if appropriate cloud provider and profile files are present
Machine is off
Machine is up again
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Create the cloud instance name to be used throughout the tests
check if personal access token, ssh_key_file, and ssh_key_names are present
Upload public key
List all keys
List key
Delete the public key if the above assertions fail
Delete public key
delete the instance
Final clean-up of created instance, in case something went wrong.  This was originally in a tearDown function, but that didn't make sense  To run this for each test when not all tests create instances.
Import Salt Testing libs
Import Salt libs
Import Salt Testing libs
Import Salt libs
Import Python libs
Import salttesting libs
Import Salt libs
Import Python libs
Import Salt testing libs
Import Salt libs
Create event bus connection
Import python libs
Import Salt Testing libs
Import salt libs
Remove existing logfile
Calculate the required timeout, since next will fail.  I needed this because after many attempts, I was unable to catch:    WARNING: Master hostname: salt not found. Retrying in 30 seconds
Let's remove our key from the master
We now fail when we're unable to properly set the syslog logger
Let's create an initial output file with some data
Let's create an initial output file with some data
Let's change umask
Data was appeneded to file
Let's remove the output file
Restore umask
Import python libs
Import Salt Testing libs
Import salt libs
Setup for scripts
shutdown for scripts
Call setup here to ensure config and script exist
Test suites is quite slow - extend the timeout
I take visual readability with aligned columns over strict PEP8  (bad-whitespace) Exactly one space required after comma  pylint: disable=C0326
Ensure that minions are shutdown
Import python libs
Import Salt Testing libs
Import salt libs
We're just after the first defined subnet from 'minion'
We now fail when we're unable to properly set the syslog logger
Import python libs
Import Salt Testing libs
Import salt libs
If there's a syslog device and the exit code was not 2,  'No such file or directory', raise the error
Import Python libs
Import salt testing libs
Remove the first option from the list  Only one left? Stop iterating
Remove the first option from the list  Only one left? Stop iterating
Import python libs
Import Salt Testing libs
Import salt libs
generate password
hash the password
set user password
set user password
Import Python Libs
Import Salt Testing libs
Import Salt Libs
the result of running self.cmd not in a shell
Import Python libs
Import Salt Testing libs
Import salt libs
Import python libs
Import Salt Testing libs
Import salt libs
Import python libs
Import Salt Testing libs
Import salt libs
Import python libs
Import Salt Testing libs
Import salt libs
Import 3rd-party libs
Jenkins throws an OSError from os.getcwd()??? Let's not worry  about it
If there's a syslog device and the exit code was not 2, 'No  such file or directory', raise the error
Import python libs
Import Salt Testing libs
Import salt libs
Import Python libs
Import Salt Testing libs
Import Salt libs
Import Python libs
Import Salt Testing libs
Import Salt libs  pylint: disable=import-error,no-name-in-module,redefined-builtin  pylint: enable=no-name-in-module,redefined-builtin
Invoke the loader
Make sure depends correctly allowed a function to load. If this  results in a KeyError, the decorator is broken.  Make sure depends correctly kept a function from loading
make sure it starts empty  get something, and make sure its a func
make sure we only loaded "test" functions
make sure the depends thing worked (double check of the depends testing,  since the loader does the calling magically
force a load all
the opts passed into modules is at least a subset of the whole opts
ensure it doesn't exist
force a load of our module
make sure we only loaded our custom module  which means that we did correctly refresh the file mapping
ensure it doesn't exist
ensure it doesn't exist
ensure it doesn't exist
ensure it doesn't exist
make sure it updates correctly
make sure that even if we remove the module, its still loaded until a clear
ensure it doesn't exist
ensure it doesn't exist
ensure it doesn't exist
update both the module and the lib
remove the lib, this means we should fail to load the module next time
bootstrap libs
update them all
Import Python libs
Import Salt Testing libs
Import salt libs
Check that custom grains are overwritten
Check that the grain is present  Check that the grains are merged
Import Python libs
Import Salt Testing libs
Import salt libs
We placed a test module under _modules.  The previous functions should also still exist.
A non existing function should, of course, not exist
There should be a new function for the test module, recho
Import Python libs
Import Salt Testing libs
Import Salt libs
Import Python libs
Import Salt Testing libs
Import salt libs
Import 3rd-party libs
if we couldn't find any, then we have no modules -- so something is broken
get the names of the globals you should have
Now, test each module!
Import Python libs
Import Salt Testing libs
Import salt libs
Import Python libs
Import Salt Testing libs
Import salt libs
Backend submitted as a string
Backend submitted as a list
Backend submitted as a string
Backend submitted as a list
Backend submitted as a string
Backend submitted as a list
Backend submitted as a string
Backend submitted as a list
Backend submitted as a string
Backend submitted as a list
Backend submitted as a string
Backend submitted as a list
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
First, check that we don't have the "bad" output that was displaying in  Issue 31330 where only the highstate outputter was listed
Now test that some expected good sample output is present in the return.
Import Python libs
Import Salt Testing libs
Import salt libs
Import Python libs
Import Salt Testing libs
Import salt libs
Import python libs
Import Salt Testing libs
Import salt libs
Import python libs
Import Salt Testing libs
Import salt libs
Import python libs
Import Salt Testing libs
Import salt libs
test first
save twice, no changes
test again, nothing is about to be changed
then add a record for IP address
test first
remove once, the key is gone
remove twice, nothing has changed
test again
Explicit no ending line break
Import python libs
Import Salt Testing libs
Import salt libs
Import python libs
Import Salt Testing libs
Import salt libs
Import python libs
Import Salt Testing libs
Import salt libs
Test packages with dot in pkg name  (https://github.com/saltstack/salt/issues/8614)
Test packages with epoch in version  (https://github.com/saltstack/salt/issues/31619)
Make sure that we have targets that match the os_family. If this  fails then the _PKG_TARGETS dict above needs to have an entry added,  with two packages that are not installed before these tests are run
If this assert fails, we need to find new targets, this test needs to  be able to test successful installation of packages, so this package  needs to not be installed before we run the states below
Don't perform this test on FreeBSD since version specification is not  supported.
Make sure that we have targets that match the os_family. If this  fails then the _PKG_TARGETS dict above needs to have an entry added,  with two packages that are not installed before these tests are run
If this assert fails, we need to find new targets, this test needs to  be able to test successful installation of packages, so this package  needs to not be installed before we run the states below
Make sure that we have targets that match the os_family. If this  fails then the _PKG_TARGETS dict above needs to have an entry added,  with two packages that are not installed before these tests are run
Don't perform this test on FreeBSD since version specification is not  supported.
Make sure that we have targets that match the os_family. If this  fails then the _PKG_TARGETS dict above needs to have an entry added,  with two packages that are not installed before these tests are run
If this assert fails, we need to find new targets, this test needs to  be able to test successful installation of packages, so these  packages need to not be installed before we run the states below
If this assert fails, we need to find a new target. This test  needs to be able to test successful installation of packages, so  the target needs to not be installed before we run the states  below
CentOS 5 has .i386 arch designation for 32-bit pkgs
If this assert fails, we need to find a new target. This test  needs to be able to test successful installation of the package, so  the target needs to not be installed before we run the states  below
Import Python libs
Import Salt Testing libs
Import salt libs
Import python libs
Import Salt Testing libs
Import salt libs
Import Python libs
Import Salt Testing libs
Import salt libs
Import python libs
Import Salt Testing libs
Import salt libs
10 second dns timeout
Import Python libs
Import Salt Testing libs
Import salt libs
Import python lins
Import Salt Testing libs
Import salt libs
Import Python libs
Import Salt Testing libs
Import salt libs
Import python libs
Import Salt Testing libs
Import salt libs
Import 3rd-party libs
left behind... Don't fail because of this!
This requirement refers to the name of the following  state, not its ID.
ensure, the number of lines didn't change, even after invoking 'file.replace' 3 times
ensure, the replacement succeeded
ensure, all runs of 'file.replace' reported success
create test file based on initial template
ensure, the resulting file contains the expected lines
ensure the initial file was properly backed up
ensure, all runs of 'file.replace' reported success
create test file based on initial template
ensure, the resulting file contains the expected lines
ensure the initial file was properly backed up
ensure, all runs of 'file.replace' reported success
create test file based on initial template
ensure, the resulting file contains the expected lines
ensure the initial file was properly backed up
ensure, all runs of 'file.replace' reported success
create test file based on initial template
ensure, the resulting file contains the expected lines
ensure the initial file was properly backed up
ensure, all 'file.replace' runs reported success
create test file based on initial template
get (m|a)time of file
define how far we predate the file
set (m|a)time of file 5 days into the past
get (m|a)time of file
ensure, the file content didn't change
ensure no backup file was created
ensure the file's mtime didn't change
ensure, all 'file.replace' runs reported success
create test file based on initial template
get (m|a)time of file
define how far we predate the file
set (m|a)time of file 5 days into the past
get (m|a)time of file
ensure, the file content didn't change
ensure no backup file was created
ensure the file's mtime didn't change
ensure, all 'file.replace' runs reported success
Parent directory exists but file does not and makedirs is False
Parent directory exists but file does not and makedirs is False
left behind... Don't fail because of this!
It should not append text again
This next time, it is already commented.
Get a path to the temporary file  Write some data to it
Cleanup the path if it already exists
Run the state
Cleanup the path if it already exists
Run the state
Import python libs
Import Salt Testing libs
Import salt libs
Import python libs
Import Salt Testing libs
Import salt libs
Let's populate the requirements file, just pep-8 for now
Let's run our state!!!
Always clean up the tests temp files
Now let's update the requirements file, which is now cached.
Let's run our state!!!
Always clean up the tests temp files
If we reached this point no assertion failed, so, cleanup!
Import python libs
Import Salt Testing libs
Import salt libs
MacOS users' primary group defaults to staff (20), not the name of  user
Import python libs
Import Salt Testing libs
Import salt libs
Verify that the return is a list, aka, an error
Import Python Libs
Import Salt Testing libs
Import Salt Libs
Import python libs
Import Salt Testing libs
Import salt libs
Import 3rd-party libs
We now create the missing virtualenv
The state should not have any issues running now
Permission denied
Since we don't have the virtualenv created, pip.installed will  thrown and error.
Let's create the testing virtualenv
Le't make sure we have pip 6.0 installed
Create the testing virtualenv
Import Python libs
Import Salt Testing libs
Import salt libs
Import 3rd-party libs
If the below assert fails then no states were run, and the SLS in  tests/integration/files/file/base/pkgrepo/managed.sls needs to be  corrected.
If the below assert fails then no states were run, and the SLS in  tests/integration/files/file/base/pkgrepo/absent.sls needs to be  corrected.
Import python libs
Import Salt Testing libs
Import salt libs
10 second dns timeout
Import Python libs
Import Salt Testing libs
Import Salt libs
Import 3rd-party libs
Import Salt Testing libs
Import Python Libs
Import Salt Libs
Import Salt Testing Libs
check a single grain
store a reference, for magic later!
once we've fired all the events, lets call it a day  wait so that we can ensure that the next future is ready to go  to make sure we don't explode if the next one is ready
Import python libs
Import salttesting libs
use singular form for arg and kwarg
Import python libs
Import salttesting libs
Import Salt Libs
Remove saltdev user
Import Python libs
Import Salt Testing libs
Import Salt libs
Remove all the volatile values before doing the compare.
Import Python libs
Import Salt libs
Import 3rd-party libs
Import python libs
Import Salt Libs
Import Salt Testing Libs
Import python libs
Import Salt Testing Libs
Import Salt Libs
Import 3rd-party libs
pylint: disable=unused-import
Import python libs
Import Salt Testing libs
Import salt libs
Import 3rd-party libs
Are there child processes still running?
Are there child processes still running?
make sure no one updated the counter  make sure the queue is still full
Import python libs
Import Salt Testing libs
Import Salt libs
Import python libs
Import Salt Testing Libs
Import Salt Libs
Import python libs
Import Salt Libs
Import Salt Testing Libs
Import python libs
Import Salt Testing libs
Import salt libs
GLOBALS
Try to create an instance with uppercase letters in  provider name. If it fails then a  FileserverConfigError will be raised, so no assert is  necessary.  Now try to instantiate an instance with all lowercase  letters. Again, no need for an assert here.
This is a valid provider, so this should  pass without raising an exception.
Dulwich is not supported for git_pillar nor  winrepo, so trying to use it should raise an  exception.
Set the provider name to a known invalid provider  and make sure it raises an exception.
Import python libs
Import Salt Testing libs
Import salt libs
Import python libs
Import Salt Testing libs
Import Salt libs
Import Python libs
Import Salt Testing libs
Import salt libs
Catch sys.stderr here since no logging is configured and  check_user WILL write to sys.stderr
No log message is triggered, only the DEBUG one which  tells us how many minion keys were accepted.
Too many open files
Import python libs
Import Salt Testing Libs
Import Salt Libs
Import python libs
Import Salt Testing libs
Import salt libs
Import python libs
Import Salt Testing libs
Import Salt libs
Import 3rd-party libs  pylint: disable=import-error
These should *not* be the same object!
Import Pytohn libs
Import Salt Testing libs
Import Salt libs
Import python libs
Import Salt Testing libs
Import salt libs
make sure that a get would get a regular old key error
Import python libs
Import Salt Testing libs
Import salt libs
The mock patch below will make sure that ALL calls to the which function  returns None
The mock patch below will make sure that ALL calls to the which function  return whatever is sent to it
Import Python libs
Import Salt Testing libs
Import salt libs
Import python libs
Import Salt Testing libs
Import salt libs
Import python libraries
Import Salt testing libraries
Import Salt libraries
Test manual de-serialize
Test cache de-serialize
First populate the cache
Then try to rehydate a func
Import python libs
Import Salt Testing libs
Import salt libs
First the assertion  Then wait for the terminal child to exit
Get current number of PTY's
We're unable to findout how many PTY's are open
While there's data to be read, the process is alive
term should be dead now
While there's data to be read, the process is alive
term should be dead now
While there's data to be read, the process is alive
Don't spin
term should be dead now
-*- coding: utf-8 -*-  Import Python libs
Import Salt Testing libs
Import salt libs
Import python libs
Import Salt Libs
Import Salt Testing Libs
Import Python libs
Import Salt Testing libs
Import salt libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
python 2.6 test
python libs
salt testing libs
salt libs
Import Python Libs
Import 3rd-party libs
Import Salt Libs
Import Salt Testing Libs
Import Salt Libs
Import python libs
Import Salt Libs
Import Salt Testing Libs
Import python libs
Import Salt Testing libs
Import salt libs
We *always* want *all* warnings thrown on this module
raise_warning should show warning until version info is >= (0, 17)
raise_warning should show warning until version info is >= (0, 17)
We *always* want *all* warnings thrown on this module
raise_warning({...}) should show warning until version info is >= (0, 17)
With no **kwargs, should not show warning until version info is >= (0, 17)
python libs
salt testing libs
salt libs
Import python libs
Import Salt Testing libs
Import Salt libs
Import Python libraries
Import 3rd-party libs
Test incorrect lengths
Make sure we raise an error if we don't pass in the requisite number of arguments
Unparseable without timelib installed
First test the valid JSON
Now pre-pend some garbage and re-test
Test to see if a ValueError is raised if no JSON is passed in
To to ensure safe exit if str passed doesn't evaluate to True
Try with yaml
Make sure we handle non-yaml junk data
LIGHT_YELLOW now == LIGHT_GRAY
pylint: disable=assignment-from-none
Test invalid version arg
Import python libs
Import Salt Testing libs
Import salt libs
Import 3rd-party libs
Import python libs
Import Salt Testing libs
Import salt libs
Import python libs
Import Salt Testing libs
Import salt libs
set variable identifying the chroot you work in (used in the prompt below)
set variable identifying the chroot you work in (used in the prompt below)
set variable identifying the chroot you work in (used in the prompt below)
set variable identifying the chroot you work in (used in the prompt below)
set variable identifying the chroot you work in (used in the prompt below)
Import Salt Testing libs
Import salt libs
Travis is slow
Wait a few seconds before tearing down the zmq context  Travis is slow
This is too fast and will be None but assures we're not blocking
Import Python libs
Import Salt Testing libs
Import salt libs
The keyring library uses `getcwd()`, let's make sure we in a good directory  before importing keyring
Import external deps
set the keyring for keyring lib
Import python libs
Import Salt Libs
Import Salt Testing Libs
Import Python libs
Import Salt Testing libs
Import Pytohn libs
Import Salt Testing libs
this spacing is like this on purpose to ensure it's stripped properly
Python libs
Salt libs
Salt testing libs
Python libs
Salt libs
Salt testing libs
Python libs
Salt libs
Salt testing libs
Third-party libs
Import Python libs
Import Salt Testing libs
Import Salt libs
Import 3rd-party libs
Import python libs
Import Salt Testing libs
Import 3rd party libs
bad quotes
not a string
Import python libs
Import Salt Testing libs
Import Salt libs
Make sure we cleanly return if the publisher isn't running
python libs
salt testing libs
salt libs
third-party libs
Import python libs
Import Salt Testing libs
Import Salt libs
Import Python libs
Import Salt Testing libs
Import Salt libs
Import pytohn libs
Import Salt Testing libs
Import Salt libs
Import 3rd-party libs
Import python libs
Import Salt Testing libs
Import Salt Libs
Import python libs
Import Salt Testing libs
Import Salt Libs
going all the way to an infinite loop is harsh on the  test machine
Import python libs
Import Salt Testing libs
Import Salt Libs
Import python libs
Import Salt Testing libs
Import Salt Libs
Import python libs
Import Salt Testing libs
Import Salt Libs
Import python libs
Import Salt Testing libs
Import Salt Libs
Import python libs
Import Salt Testing libs
Import Salt Libs
Import python libs
Import Salt Testing libs
Import salt libs
Mock getting the proper template files
Setup Matcher mock
Setup fileclient mock
Setup Matcher mock
Setup fileclient mock
Set this class-level attribute to change  the loader behavior
Import Python libs
Import Salt Testing libs
Import Salt libs
Import Python Libs
Import Salt Libs
close our pub_channel, to pass our FD checks
Import python libs
Import Salt Testing libs
Import Salt libs
we also require req server for auth
Import python libs
Import Salt Testing libs
Import python libs
Import Salt Testing libs
Import Salt libs
we also require req server for auth
Import python libs
Import Salt Testing libs
Import Salt libs
Now, python's logging logger class is ours.  Let's make sure we have at least one instance
Test for a format which includes digits in name formatting.
Remove the testing handler
Test for a format which does not include digits in name formatting.
Remove the testing handler
Only stream2 should contain the traceback
Both streams should contain the traceback
No streams should contain the traceback
Import python libs
Import Salt Testing Libs
Import Salt Libs
Import 3rd-party libs
The actuall representation of the minion options would make this HUGE!
Import python libs
Import Salt Testing libs
Import salt libs
Import Third-Party Libs
mock hostname should be more complex than the systems FQDN
os.path.join behavior  os.sep.join behavior
Should load from env variable, not the default configuration file.
Should load from env variable, not the default configuration file
Let's populate a minion configuration file with some basic  settings
Let's load the configuration
As proven by the assertion below, blah is True
Let's populate a master configuration file with some basic  settings
Let's load the configuration
As proven by the assertion below, blah is True
Check cloud.deploy.d path is the first element in the search_paths tuple
Check the second element in the search_paths tuple
Should load from env variable, not the default configuration file
Let's set the environment variable, yet, since the configuration  file path is not the default one, i.e., the user has passed an  alternative configuration file form the CLI parser, the  environment variable will be ignored.
Reset the environ
Our custom deploy scripts path was correctly added to the list
And it's even the first occurrence as it should
Import Python libs
Import Salt Testing libs
Import Salt libs
Import 3rd-party libs
pprint.pprint(out)
!pydsl
make all states in xxx run BEFORE states in this sls.
make all states in yyy run AFTER this sls.
Import Salt libs
Import Salt Testing libs
Import salt libs
Import 3rd-party libs
kill the thread
check default of empty load and enc clear
check that the load always gets passed
ensure no exceptions when we go to destroy the sreq, since __del__  swallows exceptions, we have to call destroy directly
Import Python Libs
Import Salt Testing libs
Import Salt libs
Import Python libs
Import Salt Testing libs
Import Salt libs
Import Python libs
Import Salt Testing libs
XXX install failure due to missing deps  XXX install failure due to missing field
Import python libs
Import Salt Testing libs
Import Salt libs
how many threads/coroutines to run at a time
set a global value
ensure we get the global value
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import python libs
Import Salt Testing libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Make sure this module runs on Windows system
Import python libs
Import Salt Testing libs
Import salt libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import python libs
Import Salt Testing libs
Import salt libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import python libs
Import Salt Libs
Import Salt Testing Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python Libs
Import Salt Testing Libs
wmi modules are platform specific...
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing libs
Import Salt libs
pylint: disable=import-error,no-name-in-module
the boto_lambda module relies on the connect_to_region() method  which was added in boto 2.8.0  https://github.com/boto/boto/commit/33ac26b416fbb48a60602542b4ce15dcc7029f12
compare individual items.
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python Libs
Import Salt Libs
Import Salt Testing Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Make sure this module runs on Windows system
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Import 3rd Party Libs
handle race condition
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Pytohn libs
Import Salt Testing libs
Import salt libs
Import python libs
Import Salt Testing libs
Import salt libs
Import Python libs
Import Salt Testing libs
Import Salt libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Import 3rd Party Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Libs
Import Salt Testing Libs
Import Python libs
Import Salt Testing libs
Import Mock libraries
Import Salt Execution module to test
Import Salt Utils
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import python libs
Import Salt Libs
Import Salt Testing Libs
Import third party libs
Validate true return
Validate string arguments
Validate list arguments
Validate tuple arguments
Validate dictionary arguments
Validate a simple list
Validate stop
Invalid GUID raises error
Empty return from prlctl raises error (name/snap_id mismatch?)
Validate a GUID passthrough
Validate listing all snapshots for the VM
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing libs
Import salt libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Import python libs
Import salt testing libs
Import Salt libs
Import Python libs
Import Salt Libs
Import Salt Testing Libs
import Python Libs
import Python Third Party Libs  pylint: disable=import-error
Import Salt Libs
Import Salt Testing Libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Test overall products length
Test translated fields
Test keys transition from the lowpkg.info
Import Python libs
Import Salt Testing libs
Import Salt libs
gracinet: not sure this is really useful, but other test modules have this as well
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing libs
Import Salt libs
if creation is failed, kubernetes return non json error message
if creation is failed, kubernetes return non json error message
Import python libs
Import Salt Libs
Import Salt Testing Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import python libs
Import Salt Testing libs
Import salt libs
Import third party libs
There should be no controller
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import salt libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Globals
Not using dict.get() here because we want to know if  _cmd_run_values doesn't account for all uses of cmd.run_all.
os.path.isdir() would return True on a non-stale worktree
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Make sure this module runs on Windows system
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import salt libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Libs
Import Salt Testing Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import the future
Import Python libs  We're not going to actually use OpenSSL, we just want to check that  it's installed.
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import salt module
Import Salt Testing libs
Given
Then
Given
Then
Import Python libs
Import Salt Testing libs
Import Salt libs
Import 3rd-party libs
Import Mock libraries
pylint: disable=import-error,no-name-in-module
the boto_elasticsearch_domain module relies on the connect_to_region() method  which was added in boto 2.8.0  https://github.com/boto/boto/commit/33ac26b416fbb48a60602542b4ce15dcc7029f12
Import Pytohn libs
Import Salt Testing libs
Import salt libs
Import 3rd-party libs
Import Python libs
Import Salt Libs
Import Salt Testing Libs
Globals
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing libs
Import salt libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Import Other Libs  pylint: disable=W0611  pylint: enable=W0611
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Libs
Import Salt Testing Libs
Global Variables
Import python libs
Import salt testing libs
will pass if executed along with other tests
if executed separately we need to export __salt__ dictionary ourselves
should throw an exception
should throw an exception
Import Python libs
Import Salt Testing libs
Import Salt libs
Import 3rd-party libs
Import Mock libraries
pylint: disable=import-error,no-name-in-module,unused-import
the boto_cloudtrail module relies on the connect_to_region() method  which was added in boto 2.8.0  https://github.com/boto/boto/commit/33ac26b416fbb48a60602542b4ce15dcc7029f12
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python modules
Import Salt Testing libs
Import Salt libs
Import Python libs
Import Salt Libs
Import Salt Testing Libs
Globals
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import python libraries
Import Salt Testing libs
Import salt libs
pyvenv using virtualenv options >
Import Python libs
Import Salt Libs
Import Salt Testing Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python Libs
Import Salt Testing Libs
wmi and pythoncom modules are platform specific...
Import Salt Libs
Globals
Import Python libs
Import Salt Testing libs
Import Mock libraries
Import Salt Execution module to test
Globals
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing libs
Import salt libs
Import Python libs
Import Salt Testing libs
Import Salt libs
Import 3rd-party libs
Import Mock libraries
pylint: disable=import-error,no-name-in-module,unused-import
the boto_s3_bucket module relies on the connect_to_region() method  which was added in boto 2.8.0  https://github.com/boto/boto/commit/33ac26b416fbb48a60602542b4ce15dcc7029f12
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing libs
Import Salt libs
Import 3rd-party libs  pylint: disable=import-error,no-name-in-module,unused-import
the boto_vpc module relies on the connect_to_region() method  which was added in boto 2.8.0  https://github.com/boto/boto/commit/33ac26b416fbb48a60602542b4ce15dcc7029f12
Import Python libs
Import Salt Testing libs
Import Python libs
Import Salt Testing libs
Import salt libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs  Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing libs
Import Salt libs
Import Python libs
Import Salt Libs
Import Salt Testing Libs
Globals
Import Python future libs  Import Python Libs  Import Salt Testing Libs  Import Salt Libs
The following used to make sure we are not  testing already existing data  Note strftime retunrns a str, so we need to make it unicode
we do not need to prefix this with u, as we are  using from __future__ import unicode_literals
returns a tuple with first item false, and second item a reason
returns a tuple with first item false, and second item a reason
time.sleep(15)  delays for 15 seconds
time.sleep(15)  delays for 15 seconds so you can run regedit & watch it happen
time.sleep(15)  delays for 15 seconds so you can run regedit and watch it happen
Import python libs
Import Salt Testing libs
Import Salt libs
Import 3rd-party libs
test None result with non existent grain and no default
test None result with os_family grain and no matching result
default is not present in dict1, check we only have merge in result
default is not present in dict1, and no merge, should get None
Test with just the base
Test the base with default
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Create, do not start
Create and start
Create and fail start
volume doesn't exist
volume exists, should not be stopped, and is started
volume exists, should be stopped, and is started
Gluster call fails
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import python libs
Import Salt Testing libs
Import Salt libs
First main option
Second main option
Another comment
Blank line should be above
First main option
Second main option
Another comment
Blank line should be above
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import python libs
Import salt libs
Import python libs
Import Salt Testing Libs
Import Salt Libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Libs
Import Salt Testing Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs  Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import third party libs
Globals
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python Libs
Import Salt Testing Libs
Import python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import salt libs
Globals
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Should properly negate bang-prefixed values
Should properly negate "not"-prefixed values
Should it really behave this way?
Test arguments that should appear after the --jump
Should quote arguments with spaces, like log-prefix often has
Should quote arguments with leading or trailing spaces
Should allow no-arg jump options
should build match-sets with single string
should handle negations for string match-sets
Should allow the --save jump option to CONNSECMARK                                      **{'save': ''}),                  '--jump CONNSECMARK --save ')
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import python libs
Globals
Import Python libs
Import Salt Testing libs
Import Salt libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import python libs
Import Salt Testing Libs
Import Salt Libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import python libs
Import Salt Testing Libs
Import Salt Libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Libs
Import Salt Testing Libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing libs
Import Salt libs
Import 3rd-party libs
Import Mock libraries
pylint: disable=import-error,no-name-in-module,unused-import
the boto_iot module relies on the connect_to_region() method  which was added in boto 2.8.0  https://github.com/boto/boto/commit/33ac26b416fbb48a60602542b4ce15dcc7029f12
Import Python libs
Import Salt Testing libs
Import Salt libs
Import 3rd-party libs
Import Mock libraries
pylint: disable=import-error,no-name-in-module
the boto_lambda module relies on the connect_to_region() method  which was added in boto 2.8.0  https://github.com/boto/boto/commit/33ac26b416fbb48a60602542b4ce15dcc7029f12
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing libs
Import Salt libs
Import 3rd-party libs
Import Mock libraries
pylint: disable=import-error,no-name-in-module
the boto_lambda module relies on the connect_to_region() method  which was added in boto 2.8.0  https://github.com/boto/boto/commit/33ac26b416fbb48a60602542b4ce15dcc7029f12
Import Pytohn libs
Import Salt Testing libs
Import salt libs
too easy to test (DRY)
Import Python libs
Import Salt Libs
Import Salt Testing Libs
Import Python libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs  Import Salt Libs  Globals
Import pytohn libs
Import Salt testing libs
Import Salt libs
Import 3rd-party libs
Import python libs
Import Salt libs
Import Salt Testing libs
wmi and pythoncom modules are platform specific...
This is imported late so mock can do its job
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Make sure this module runs on Windows system
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Libs
Import Salt Testing Libs
Import python libs
Import Salt Testing libs
Import Salt libs
when there are no identifiers,  we do not touch it
whenever we have an identifier, hourray even without comment  we can match and edit the crontab in place  without cluttering the crontab with new cmds
we can even change the other parameters as well  thx to the id
def test__render_tab(self):      pass
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import python Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing libs
Import Python libs
Import python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import python libs
Import salt libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing libs
Import salt libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import python libs
Import salt testing libs
Import salt libs
will pass if executed along with other tests
if executed separately we need to export __salt__ dictionary ourselves
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
get_size = False
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt libs
Import Salt testing libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Salt Testing Libs
Globals
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
import Python Libs
Import Salt Testing Libs
Import Salt libs
Import Salt Libs
note that the vpc_id does not need to be created in order to create  a security group within the vpc when using moto
note that the vpc_id does not need to be created in order to create  a security group within the vpc when using moto
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Make sure this module runs on Windows system
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import python libs
Import 3rd-party libs  pylint: disable=import-error,no-name-in-module,redefined-builtin  pylint: enable=import-error,no-name-in-module,redefined-builtin
Import Salt Testing libs
as we may have old packages, this test the two  behaviors (failure with old setuptools/distribute)
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Libs
Import Salt Testing Libs
import Python Libs
Import Salt Testing Libs
Import Salt Libs
Import Pytohn libs
Import Salt Testing libs
Import Salt Module
Import python libs
Import Salt Testing libs
Import Salt libs
upper- & lower-case
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Test _valid_composer=False throws exception
Test no directory specified throws exception
Test _valid_composer=False throws exception
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Attempt to import pyVim and pyVmomi libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Libs
Import Salt Testing Libs
Import Python libs
Import Salt Libs
Import Salt Testing Libs
Import Salt Libs
Import Salt Testing Libs
Import Salt Libs
Test when name begins with a hyphen
Test when name begins with an underscore
Test when name ends with a hyphen
Test when name ends with an underscore
Test when name is an empty string
Test when name is two letters long
Test when name is three letters long (valid)
Test when name is 48 letters long (valid)
Test when name is more than 48 letters long
Test when name contains an invalid character
Test when name contains non-ascii letters
Test when name contains spaces
Test when name contains letters and numbers
Test when name contains hyphens
Test when name contains underscores
Test when name start and end with numbers
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Global Variables
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Salt Testing Libs
Import python libs
Import Salt Testing libs
Import Salt libs
Import pytohn libs
Import Salt Testing libs
Import Salt libraries
overwrite the _send_pub method so we don't have to serialize MagicMock
make sure to return a JID, instead of a mock
Can we access test.ping?
Are we denied access to sys.doc?
Did we fire test.echo?
Request sys.doc  Did we fire it?
Unimplemented
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import python libs
Import Salt Testing libs
Import salt libs
Import 3rd-party libs
Test VCS installations with version info like >= 0.1  The pip version being used is already < 1.2
Reset the version attribute if existing
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing libs
Import Salt libs
Import 3rd-party libs
pylint: disable=import-error,no-name-in-module
Import 3rd-party libs
the boto_apigateway module relies on the connect_to_region() method  which was added in boto 2.8.0  https://github.com/boto/boto/commit/33ac26b416fbb48a60602542b4ce15dcc7029f12
should never get to the following calls
create api method for POST  create api method integration for POST  create api method response for POST/200  create api method integration response for POST
no api existed  create top level api
create api method for POST
create api method for POST  create api method integration for POST
create api method for POST  create api method integration for POST  create api method response for POST/200
Import Python libs
Import Salt Libs
Import Salt Testing Libs
Import Python libs
Import Salt Testing libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import python libs
Import Salt Testing libs
Late import so mock can do its job
Import python libs
Import Salt Testing libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python Libs
Import Salt Libs
Import Salt Testing Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
if the add fails, then it will only get called once.
Import Python libs
Import Salt Libs
Import Salt Testing Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Libs
Import Salt Testing Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
-*- coding: utf-8 -*-  Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Libs
Import Salt Testing Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Libs
Import Salt Testing Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import python libs
Import Salt Testing libs
Import Salt Libs
Globals
emulates the LDAP database.  each key is the DN of an entry and it  maps to a dict which maps attribute names to sets of values.
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Libs
Import Salt Testing Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing libs
Import Salt libs
Import 3rd-party libs
Import Mock libraries
pylint: disable=import-error,no-name-in-module,unused-import
Import 3rd-party libs
the boto_elasticsearch_domain module relies on the connect_to_region() method  which was added in boto 2.8.0  https://github.com/boto/boto/commit/33ac26b416fbb48a60602542b4ce15dcc7029f12
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing libs
Import Salt libs
Import 3rd-party libs
Import Mock libraries
pylint: disable=import-error,no-name-in-module,unused-import
Import 3rd-party libs
the boto_cloudtrail module relies on the connect_to_region() method  which was added in boto 2.8.0  https://github.com/boto/boto/commit/33ac26b416fbb48a60602542b4ce15dcc7029f12
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import python libs
@skipIf(syslog_ng.__virtual__() is False, 'Syslog-ng must be installed')
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Libs
Import Salt Testing Libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
import Python Libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing libs
Import Salt libs
Import 3rd-party libs
Import Mock libraries
pylint: disable=import-error,no-name-in-module,unused-import
Import 3rd-party libs
the boto_s3_bucket module relies on the connect_to_region() method  which was added in boto 2.8.0  https://github.com/boto/boto/commit/33ac26b416fbb48a60602542b4ce15dcc7029f12
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing libs
Import Salt libs
pylint: disable=import-error,unused-import
Import 3rd-party libs
connections keep getting cached from prior tests, can't find the  correct context object to clear it. So randomize the cache key, to prevent any  cache hits
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import python libs
Import Salt Testing libs
Import salt libs
Import 3rd-party libs
rvm.install is not run anymore while checking rvm.is_installed
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing libs
Import Python libs
Import Salt Libs
Import Salt Testing Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
-*- coding: utf-8 -*-  Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Libs
Import Salt Testing Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
-*- coding: utf-8 -*-  Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Libs
Import Salt Testing Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing libs
Import Salt libs
Import 3rd-party libs
Import Mock libraries
pylint: disable=import-error,no-name-in-module,unused-import
Import 3rd-party libs
the boto_iot module relies on the connect_to_region() method  which was added in boto 2.8.0  https://github.com/boto/boto/commit/33ac26b416fbb48a60602542b4ce15dcc7029f12
Import Python libs
Import Salt Testing libs
Import Salt libs
Import 3rd-party libs
Import Mock libraries
pylint: disable=import-error,no-name-in-module
Import 3rd-party libs
the boto_lambda module relies on the connect_to_region() method  which was added in boto 2.8.0  https://github.com/boto/boto/commit/33ac26b416fbb48a60602542b4ce15dcc7029f12
Import Python libs
Import Salt Testing libs
Import Salt libs
Import 3rd-party libs
Import Mock libraries
pylint: disable=import-error,no-name-in-module
Import 3rd-party libs
the boto_cognitoidentity module relies on the connect_to_region() method  which was added in boto 2.8.0  https://github.com/boto/boto/commit/33ac26b416fbb48a60602542b4ce15dcc7029f12
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Libs
Import Salt Testing Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
volume_present should never try to add a conflicting  volume
volume_present should not have tried to remove a volume  that didn't exist
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Libs
Import Salt Testing Libs
Import python libs
Import Salt Testing libs
old behavior, do not remove with identifier set and  even if command match !
old behavior, remove if no identifier and command match
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt testing libs
Import Salt libs
Import 3rd-party libs
will pass if executed along with other tests
if executed separately we need to export __salt__ dictionary ourselves
given
when
given
when
given
when
given
when
when
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python Libs
Import Salt Testing Libs
Import Salt Libs
Globals
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt testing libs
Import Salt libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Libs
Import Salt Testing Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Libs
Import Salt Testing Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import python libs
Import Salt Testing libs
Import Salt libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Libs
Import Salt Testing Libs
Import python libs
Import Salt Testing libs
Import third party libs
make sure the newline
pillar.get should return the pillar_value
make sure no errors are returned
This breaks if the start of the week is in a previous month.
Add some files which do not match fake_strptime_format
if we generate less than the number of files expected,  then the oldest file will also be retained  (correctly, since it's the first in it's category)
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import python libs
Import Salt Testing libs
Import salt libs
once again with the same value
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python libs
Import Salt Testing Libs
Import Salt Libs
Import Python Libs
Import Salt Testing Libs
Import 3rd-party libs  pylint: disable=import-error
Let's create a fake AsyncHTTPTestCase so we can properly skip the test case
create a few futures
create an any future, make sure it isn't immediately done
finish one, lets see who finishes
make sure it returned the one that finished
Import Python libs
Import Salt Testing Libs
Import Salt libs
Let's create a fake AsyncHTTPTestCase so we can properly skip the test case
Request not supported content-type
send as JSON
Case 1. dictionary type of lowstate
Case 2. string type of arg
Case 3. Combine Case 1 and Case 2.
send as json
Test in form encoded
Test in JSON
Test in YAML
Import Python libs
Import 3rd-party libs
Import python libs
Import Salt Testing libs
Import salt libs
Import Python libs
Import Salt Testing libs
Import salt libs
Import Python libs
Import Salt Testing libs
Import Salt libs
Create temp job cache dir without files in it.
Make sure there are no files in the directory before continuing
Call clean_old_jobs function, patching the keep_jobs value with a  very small value to force the call to clean the job.
Assert that the JID dir was removed
Create temp job cache dir without files in it.
Make sure there are no files in the directory
Call clean_old_jobs function
Get the name of the JID directory that was created to test against
Assert the JID directory is still present to be cleaned after keep_jobs interval
Create temp job cache dir and jid file
Make sure there is a jid file in a new job cache director
Even though we created a valid jid file in the _make_tmp_jid_dirs call to get  into the correct loop, we need to mock the 'os.path.isfile' check to force the  "corrupted file" check in the clean_old_jobs call.
Assert that the JID dir was removed
Create temp job cache dir and jid file
Make sure there is a jid directory
Call clean_old_jobs function, patching the keep_jobs value with a  very small value to force the call to clean the job.
Assert that the JID dir was removed
First, create the /tmp/salt_test_job_cache/jobs/ directory to hold jid dirs
Then create a JID temp file in "/tmp/salt_test_job_cache/"
Import Python libs
Import Salt Testing libs
Import python libs
Import Salt Testing libs
Import Python libs
Import 3rd party libs
Import salt libs
ensure that sls & yaml have the same base
ensure that sls is ordered, while yaml not
ensure that sls & yaml have the same base
ensure that sls is ordered, while yaml not
prove that yaml does not handle well with OrderedDict  while sls is jinja friendly.
BLAAM! yaml was unable to serialize OrderedDict,  but it's not the purpose of the current test.
BLAAM! yml_src is not valid !
ensure that repr and str are yaml friendly
ensure that repr and str are already quoted
configparser appends empty lines
Import python libs
Import Salt Testing libs
Turn on expensive tests execution
When no tests are specifically enumerated on the command line, setup  a default run: +unit -cloud_provider
Transplant configuration
Get current limits
Get required limits
Check minimum required limits
We're either not running any integration test suites, or we're  only running unit tests by passing --unit or by passing only  `unit.<whatever>` to --name.  We don't need the tests daemon  running
Return an empty status if no tests have been enabled
We are not explicitly running the unit tests and none of the  names passed to --name is a unit test.
MacOS needs more open filehandles for running unit test suite
We executed ALL unittests, we can skip running unittests by name  below
Bif - skip
Import Python Libs
Import salt libs
Import third party libs
If given a root_dir, keep the tmp files there as well
sys.stdout for no newline
pylint: disable=C0103
Import python libs
Import 3rd-party libs
This is a Jenkins triggered Pull Request  We need some more data about the Pull Request available to the  environment
Sleep a random number of seconds
delete_vm(opts)
DO NOT print the state return here!
No regex matching
Not a number!?
No output!?  Anything else, raise the exception
Grab packages and log file (or just log file if build failed)
Download unittest reports  Download coverage report
Import python libs
Import python libs
Import third party libs
Import Python Libs
open device  Arguments here are:    device    snaplen (maximum number of bytes to capture _per_packet_)    promiscious mode (1 for true)    timeout (in milliseconds)
TCP protocol
Parse IP header  take first 20 characters for the ip header
now unpack them:)
track new connections  track closing connections
track new connections  track closing connections  packet does not match requirements
passed parameters
reference timer for printing in intervals
the ports we want to monitor
get the established connections to 4505 and 4506  these would only show up in tcpdump if data is transferred  but then with different flags (PSH, etc.)
reset the so far collected stats
Import Python libs
Import Salt libs
Import 3rd-party libs
Create log file in the artifact dir so it is sent back to master if the  job fails
Install build deps
Make the sdist
Do the thing
Import python libs
Import salt libs
Import 3rd party libs
Create the Salt __opts__ variable
Populate grains if it hasn't been done already
file_roots and pillar_roots should be set in the minion config
ensure we have a minion id
Populate template variables
Set maximum number of items that will be written to the history file
Import system libs
Import salt libs
Figure out what the reqs/sec has been up to this point and then adjust up or down
Re-force dtype to ints in case of empty list  Make sure the array is 2D with 2 columns.  This is needed when dealing with an empty list
No need to bother with assignments if one of the dimensions  of the cost matrix is zero-length.
Look for the starred columns
We need to swap the columns because we originally  did a transpose on the input cost matrix.
Erase all prime markings
Silenced by default to reduce verbosity. Turn on at runtime for  performance profiling.
Don't get num_samples from an ensembles length!
special notation for singleton tuples
create new with correct sparse
convert dtype
force copy
store whether originally we wanted numeric dtype
not a data type (e.g. a column named dtype in a pandas DataFrame)
if input is object, convert to float.
no dtype conversion required
dtype conversion required. Let's select the first element of the  list of accepted types.
To ensure that array flags are maintained
FIXME NotFittedError_ --> NotFittedError in 0.19
update the docstring of the descriptor
Import error caused by circular imports.
uniform class weights
Get class weights for the subsample, covering all classes in  case some labels that were present in the original data are  missing from the sample.
Make missing classes' weight zero
x may be of the form dev-1ea1592
1 / (1 + exp(-x)) = (1 + tanh(x / 2)) / 2  This way of computing the logistic is both fast and stable.
little danse to see if np.copy has an 'order' keyword argument  Copy, but keep the order  Before an 'order' argument was introduced, numpy wouldn't muck with  the ordering
Compat where astype accepted no copy argument
Don't raise the numpy deprecation warnings that appear in  1.9, but avoid Python bug due to simplefilter('ignore')
Numpy < 1.8.0 don't handle empty arrays in reduceat
numpy.argpartition was introduced in v 1.8.0
Prior to 1.7.0, np.frombuffer wouldn't work for empty first arg.
Backport of numpy function in1d 1.8.1 to support numpy 1.6.2  Ravel both arrays, behavior for the first array could be different
Otherwise use sorting
Backport fix for scikit-learn/scikit-learn2986 / scipy/scipy4142
Find index of median prediction for each sample
Copyright (c) 2011, 2012  Authors: Pietro Berkes,           Andreas Muller           Mathieu Blondel           Olivier Grisel           Arnaud Joly           Denis Engemann           Giorgio Patrini           Thierry Guillemot  License: BSD 3 clause
Python 2
Python 3+
WindowsError only exist on Windows
Conveniently import all assertions in one place.
assert_raises_regexp is deprecated in Python 3.4 in favor of  assert_raises_regex but lets keep the backward compat in scikit-learn with  the old name for now
Verify some things
Checks the message of all warnings belong to warning_class  substring will match, the entire message with typo won't
To remove when we support numpy 1.7  XXX: once we may depend on python >= 2.6, this can be replaced by the
warnings module context manager.  very important to avoid uncontrolled state propagation
Filter out numpy-specific warnings in numpy >= 1.9
very important to avoid uncontrolled state propagation
concatenate exception names
transpose all variables
Lazy import to avoid mutually recursive imports
Lazy import to avoid mutually recursive imports
get rid of abstract base classes
possibly get rid of meta estimators
drop duplicates, sort for reproducibility  itemgetter is used to ensure the sort does not extend to the 2nd item of  the tuple
this fails if no $DISPLAY specified
This can fail under windows,   but will succeed when called by atexit
Pandas Dataframes and Series  Cython typed memoryviews internally used in pandas do not support  readonly buffers.
This is often substantially faster than X[indices]
Reverse the order here from the original matlab code because  there was an error on return when arnorm==0
Use a plane rotation to eliminate the damping parameter.  This alters the diagonal (rhobar) of the lower-bidiagonal matrix.
Use a plane rotation to eliminate the subdiagonal element (beta)  of the lower-bidiagonal matrix, giving an upper-bidiagonal matrix.
Update x and w.
Test for convergence.  First, estimate the condition of the matrix  Abar,  and the norms of  rbar  and  Abar'rbar.
Distinguish between     r1norm = ||b - Ax|| and     r2norm = rnorm in current code            = sqrt(r1norm^2 + damp^2*||x||^2).     Estimate r1norm from     r1norm = sqrt(r2norm^2 - damp^2*||x||^2).  Although there is cancellation, it might be accurate enough.
Allow for tolerances set by the user.
Check that all estimator yield informative messages when  trained on empty datasets
SpectralEmbedding is non-deterministic,  see issue 4236  cross-decomposition's "transform" returns X and Y
Test that all estimators check their input for NaN's and infs
FIXME!  in particular GaussianProcess!
Test that estimators can be pickled, and once pickled  give the same answer as before.
test if NotFittedError is raised
this is clustering on the features  let's not test that here.
be tolerant of noisy datasets (not actually speed)
Due to the jl lemma and often very few samples, the number  of components of the random matrix projection will be probably  greater than the number of features.  So we impose a smaller number (avoid "auto" mode)
SelectKBest has a default of k=10  which is more feature than we have in most case.
We need to make sure that we have non negative data, for things  like NMF
catch deprecation warnings
fit_transform method should work on non fitted estimator
check for consistent n_samples
raises error on malformed input for transform  If it's not an array, it does not have a 'T' property
Those transformers yield non-deterministic output when executed on  a 32bit Python. The same transformers are stable on 64bit Python.  FIXME: try to isolate a minimalistic reproduction case only depending  scipy and/or maybe generate a test dataset that does not  cause such unstable behaviors.
The precise message can change depending on whether X or y is  validated first. Let us test the type of exception only:
some estimators can't do features less than 0
some estimators only take multioutputs
catch deprecation warnings
pickle and unpickle!
fit  with lists
fit another time with ``fit_predict`` and compare results  there is no way to make Spectral clustering deterministic :(
MiniBatchKMeans
raises error on malformed input  raises error on malformed input for decision_function
some want non-negative input
Common test for Regressors as well as Classifiers
These only work on 2d, so this test makes no sense
catch deprecation warnings  fit
We need to make sure that we have non negative data, for things  like NMF
catch deprecation warnings  separate estimators to control random seeds
checks whether regressors have decision_function or predict_proba
FIXME CCA, PLS is not robust to rank 1 effects
doesn't have function  has function. Should raise deprecation warning
the sparse version has a parameter that doesn't do anything
NaiveBayes classifiers have a somewhat different interface.  FIXME SOON!
This is a very small dataset, default n_iter are likely to prevent  convergence
Let the model compute the class frequencies
Count each label occurrence to reweight manually
some want non-negative input  catch deprecation warnings
Make a physical copy of the original estimator parameters before fitting.
Fit the model
Compare the state of the model parameters with the original parameters
test sparsify with dense inputs
pickle and unpickle with sparse coef_
catch deprecation warnings  separate estimators to control random seeds
this comes from getattr. Gets rid of deprecation decorator.
init is not a python function.  true for mixins
they can need a non-default argument
deprecated parameter, not in get_params
Estimators in mono_output_task_error raise ValueError if y is of 1-D  Convert into a 2-D y for those estimators.
HuberRegressor depends on scipy.optimize.fmin_l_bfgs_b  which doesn't return a n_iter for old versions of SciPy.
These return a n_iter per component.
Get the unique set of labels
Check that we don't mix string type with number type
Known to fail in numpy 1.3 for array of arrays
Invalid inputs
check float and contains non-integer float values  [.1, .2, 3] or [[.1, .2, 3]] or [[1., .2]] and not [1., 2., 3.]
This is the first call to partial_fit
An explicit zero was found, combine its weight with the weight  of the implicit zeros
If an there is an implicit zero and it is not in classes and  class_prior, make an entry for it
have shape an length but don't support indexing.  ugly hack to make iloc work.
Pandas data frames also are array-like: we want to make sure that  input validation in cross-validation does not try to call that  method.
Newer NumPy has a ravel that needs less copying.
important to access flags instead of calling np.isfortran,  this catches corner cases.
Maltyped or malformed data.
Generating normal random vectors with shape: (A.shape[1], size)
Deal with "auto" mode
Sample the range of A using by linear projection of Q  Extract an orthonormal basis
Checks if the number of iterations is explicitely specified
this implementation is a bit faster with smaller shape[1]
project M to the (k + p) dimensional space using the basis vectors
compute the SVD on the thin matrix: (k + p) wide
In case of transpose u_based_decision=false  to actually flip based on u and not v.
transpose back the results according to the input convention
Use the max to normalize, as with the log this is what accumulates  the less errors
unlike svd case, eigh can lead to negative eigenvalues
old = stats until now  new = the current increment  updated = the aggregated stats
line search failed: try different one.
Outer loop: our Newton iteration  Compute a search direction pk by applying the CG method to   del2 f(xk) p = - fgrad f(xk) starting from 0.
Inner loop: solve the Newton update by conjugate gradient, to  avoid inverting the Hessian
if undirected and csc storage, then transposing in-place  is quicker than later converting to csr.
Oldish versions of scipy don't have that
Author: Hamzeh Alsalhi <ha258@cornell.edu>  License: BSD 3 clause
In most cases a scalar will have been made an array
Use samples as indices for a if a is array-like
use uniform distribution if no class_probability is given
If 0 is not present in the classes insert it with a probability 0.0
XXX we should be testing the public API here
Square
Rectangular variant
Square
Rectangular variant
n == 2, m == 0 matrix
Check that dtype conversion works
Changing dtype forces a copy even if copy=False
Check that copy can be skipped if requested dtype match
Check that copy can be forced, and is the case by default:
test custom sampling without replacement algorithm
n_population < n_sample
n_population == n_samples
n_population >= n_samples
n_population < 0 or n_samples < 0
This test is heavily inspired from test_random.py of python-core.  For the entire allowable range of 0 <= k <= N, validate that  the sample is of the correct length and contains only unique items
test edge case n_population == n_samples == 0
This test is heavily inspired from test_random.py of python-core.  For the entire allowable range of 0 <= k <= N, validate that  sample generates all possible permutations
a large number of trials prevents false negatives without slowing normal  case
Counting the number of combinations is not as good as counting the  the number of permutations. However, it works with sampling algorithm  that does not provide a random permutation of the subset of integer.
doesn't error on actual estimator
check that a ValueError/AttributeError is raised when calling predict  on an unfitted estimator
check that CorrectNotFittedError inherit from either ValueError  or AttributeError
Confirm that input validation code does not return np.matrix
force_all_finite
nan check
no change if allowed
got converted
doesn't copy if it was already good
other input formats  convert lists to arrays  raise on too deep lists
convert weird stuff to arrays
empty list is considered 2D by default:
If considered a 1D collection when ensure_2d=False, then the minimum  number of samples will break:
Invalid edge case when checking the default minimum sample of a scalar
But this works if the input data is forced to look like a 2 array with  one sample and one feature:
The same message is raised if the data has 2 dimensions even if this is  not mandatory
Only the feature check is enabled whenever the number of dimensions is 2  even if allow_nd is enabled:
check error for bad inputs
check that asymmetric arrays are properly symmetrized  Check for warnings and errors
Check is ValueError raised when non estimator instance passed
Despite ensembles having __len__ they must raise TypeError  XXX: We should have a test with a string, but what is correct behaviour?
We compare path length and not costs (-> set distances to 0 or 1)
Non-reachable nodes have distance 0 in graph_py
Check the check_random_state utility function behavior
First a function...
... then a class.
Border case not worth mentioning in doctests
Non-regression test that shows null-space computation is better with  initialization of eigsh from [-1,1] instead of [0,1]
Test if eigsh is working correctly  New initialization [-1,1] (as in original ARPACK)  Was [0,1] before, with which this test could fail
Eigenvalues of s.p.d. matrix should be nonnegative, w[0] is smallest
fun with read-only data in dataframes  this happens in joblib memmapping
check that gen_even_slices contains all samples
check that passing negative n_chunks raises an error
Sparsify the array a little bit
Sparsify the array a little bit
default params for incr_mean_variance
Test _incremental_mean_and_var with whole data
Test that it raises an Error for non-csc matrices.
total effect of samples is preserved
When the user specifies class weights, compute_class_weights should just  return them.
Test with user-defined weights
Test with `None` weights
Not "auto" for subsample
Not a list or preset for multi-output
Incorrect length list for multi-output
next sample
random sample
and also confusable as sequences of sequences
empty second dimension
3d
Empty iterable
Multilabel indicator
Smoke test for all supported format
We don't support those format at the moment
Mix with binary or multiclass and multilabel
Only mark explicitly defined sparse examples as valid sparse  multilabel-indicators
Densify sparse examples before testing
with uniform weights, results should be identical to stats.mode
set this up so that each row should have a weighted mode of 6,  with a score that is easily reproduced
Try to add some smallish numbers in logspace
Check that extmath.randomized_svd is consistent with linalg.svd
generate a matrix X of approximate effective rank `rank` and no noise  component (very structured signal):
compute the singular values of X using the slow exact method
ensure that the singular values of both methods are equal up to the  real rank of the matrix
check the singular vectors too (while not checking the sign)
check the sparse matrix representation
compute the singular values of X using the fast approximate method
Check that extmath.randomized_svd can handle noisy matrices
generate a matrix X wity structure approximate rank `rank` and an  important noisy component
compute the singular values of X using the slow exact method
compute the singular values of X using the fast approximate  method without the iterated power method
the approximation does not tolerate the noise:
compute the singular values of X using the fast approximate  method with iterated power method
the iterated power method is helping getting rid of the noise:
Check that extmath.randomized_svd can handle noisy matrices
let us try again without 'low_rank component': just regularly but slowly  decreasing singular values: the rank of the data matrix is infinite
the approximation does not tolerate the noise:
compute the singular values of X using the fast approximate method  with iterated power method
the iterated power method is still managing to get most of the  structure at the requested rank
Check that transposing the design matrix has limited impact
in this case 'auto' is equivalent to transpose
Check that svd_flip works in both situations, and reconstructs input.
Without transpose
With transpose
check single axis
Check correctness and robustness of logistic sigmoid implementation
Check fast dot blas wrapper function
ndim == 0
ndim == 1
ndim > 2
min(shape) == 1
test for matrix mismatch error
Test cov-like use case + dtypes.
col < row
Test square matrix * rectangular use case.
Two-pass algorithm, stable.  We use it as a benchmark. It is not an online algorithm  https://en.wikipedia.org/wiki/Algorithms_for_calculating_varianceTwo-pass_algorithm
Older versions of numpy have different precision  In some old version, np.var is not stable
Naive one pass var: >tol (=1063)
Assign this twice so that the test logic is consistent
Check that the nose implementation of assert_less gives the  same thing as the scikit's
Check that the nose implementation of assert_less gives the  same thing as the scikit's
Linear Discriminant Analysis doesn't have random state: smoke test
multiple exceptions in a tuple
This check that ignore_warning decorateur and context manager are working  as expected
Check the decorator
Check the context manager
This class is inspired from numpy 1.7 with an alteration to check  the reset warning filters after calls to assert_warns.  This assert_warns behavior is specific to scikit-learn because  and clears all previous filters.
Test that assert_warns is not impacted by externally set  filters and is reset internally.  This is because `clean_warning_registry()` is called internally by  assert_warns and clears all previous filters.
Test that the warning registry is empty after assert_warns
Should raise an AssertionError
FIXME: we should probably reset __new__ for full generality
accepted values of parameter WHICH in _SEUPD
accepted values of parameter WHICH in _NAUPD
Note: slices producing 0-size arrays do not necessarily change  data pointer  so we use and allocate size+1
ARPACK overwrites its initial resid,  make a copy
ARPACK will use a random initial vector.
sigma not used
set solver mode and parameters
Use _aligned_zeros to work around a f2py bug in Numpy 1.9.1
initialization
Use _aligned_zeros to work around a f2py bug in Numpy 1.9.1
Use _aligned_zeros to work around a f2py bug in Numpy 1.9.1
initialization
Build complex eigenvalues from real and imaginary parts
Arrange the eigenvectors: complex eigenvectors are stored as  real,imaginary in consecutive columns
we got less or equal as many eigenvalues we wanted
when tol=0, ARPACK uses machine tolerance as calculated  by LAPACK's _LAMCH function.  We should match this
when tol=0, ARPACK uses machine tolerance as calculated  by LAPACK's _LAMCH function.  We should match this
standard eigenvalue problem
general eigenvalue problem
standard eigenvalue problem
general eigenvalue problem
sigma is not None: shift-invert mode
unrecognized mode
Get a low rank approximation of the implicitly defined gramian matrix.  This is not a stable way to approach the problem.
In 'LM' mode try to be clever about small eigenvalues.  Otherwise in 'SM' mode do not try to be clever.
Gramian matrices have real non-negative eigenvalues.
Authors: Manoj Kumar           Thomas Unterthiner           Giorgio Patrini  License: BSD 3 clause
The following swapping makes life easier since m is assumed to be the  smaller integer below.
Modify indptr first
Prevent modifying X in place
normalize by P(x) = P(f_1, ..., f_n)
Combine mean of old and new data, taking into consideration  (weighted) number of observations
If the ratio of data variance between dimensions is too small, it  will cause numerical errors. To address this, we artificially  boost the variance by epsilon, a small fraction of the standard  deviation of the largest dimension.
Put epsilon back in each time
Update if only no priors is provided  Empirical prior, with sample_weight taken into account
empirical prior, with sample_weight taken into account
label_binarize() returns arrays with dtype=np.int64.  We convert it to np.float64 to support sample_weight consistently
Count raw events from data before updating the class log prior  and feature log probas
XXX: OPTIM: we could introduce a public finalization method to  be called by the user explicitly just once after several consecutive  calls to partial_fit and prior any call to predict[_[log_]proba]  to avoid computing the smooth log probas at each call to partial fit
LabelBinarizer().fit_transform() returns arrays with dtype=np.int64.  We convert it to np.float64 to support sample_weight consistently;  this means we also don't have to cast X to floating point
XXX The following is a stopgap measure; we need to set the dimensions  of class_log_prior_ and feature_log_prob_ correctly.
Compute  neg_prob · (1 - X).T  as  ∑neg_prob - X · neg_prob
skip index generation if totally dense
Among non zero components the probability of the sign is 50%/50%
build the CSR structure by concatenating the rows
Generate a projection matrix of size [n_components, n_features]
Check contract
Convert data
submodules with build utilities
add cython extension module for isotonic regression
the following packages depend on cblas, so they have to be build  after the above.
add the test directory
Make sure that DeprecationWarning within this package always gets printed
This variable is injected in the __builtins__ by the build  process. It used to enable importing subpackages of sklearn when  the binaries are not built
We are not importing the rest of the scikit during the build  process, as it may not be compiled yet
Non-modules:
Ensure that this module is still importable under Python3 to avoid  crashing code-inspecting tools like nose.
Setting a new item creates a new link which goes at the end of the linked  list, and the inherited dictionary is updated with the new key/value pair.
Deleting an existing item uses self.__map to find the link which is  then removed by updating the links in the predecessor and successor nodes.
Useful for very coarse version differentiation.
Jython always uses 32 bits.
This is a bit ugly, but it avoids running this again.
This is about 2x faster than the implementation above on 3.2+
XXX: This conflicts with the debug flag used in children class
FIXME: Too much logic duplicated
XXX: We actually need a debug flag to disable this  silent failure.
By default we want a pickle protocol that only changes with  the major python version and not the minor one  Initialise the hash obj
builtin  type  classobj  function
forces order of items in Set to ensure consistent hash
delayed import of numpy, to avoid tight coupling
The object will be pickled by the pickler hashed at the end.
Python 2 compat
Customizable pure Python pickler in Python 2  customizable C-optimized pickler under Python 3.3+
We need the class definition to derive from it not the multiprocessing.Pool  factory function
Some system have a ramdisk mounted by default, we can use it instead of /tmp  as the default folder to dump big arrays to share with subprocesses
Folder and file permissions to chmod temporary files generated by the  memmaping pool. Only the owner of the Python process can access the  temporary files and folder.
a is already a real memmap instance.
Recursive exploration of the base ancestry
Do not zero the original data when unpickling
Simple, contiguous memmap
For non-contiguous data, memmap the total enclosing buffer and then  extract the non-contiguous view with the stride-tricks API
offset that comes from the striding differences between a and m
offset from the backing memmap
The backing memmap buffer is necessarily contiguous hence C if not  Fortran
If the array is a contiguous view, no need to pass the strides
Compute the total number of items to map from which the strided  view will be extracted.
m is a real mmap backed memmap instance, reduce a preserving striding  information
This memmap instance is actually backed by a regular in-memory  buffer: this can happen when using binary operators on numpy.memmap  instances
a is already backed by a memmap file, let's reuse it directly
check that the folder exists (lazily create the pool temp folder  if required)
Warm up the data to avoid concurrent disk access in  multiple children processes
The worker process will use joblib.load to memmap the data
Make the dispatch registry an instance level attribute instead of  a reference to the class dictionary under Python 2
Under Python 3 initialize the dispatch table with a copy of the  default registry
Python 2 pickler dispatching is not explicitly customizable.  Let us use a closure to workaround this limitation.
writes to a message oriented win32 pipe are atomic
Register smart numpy.ndarray reducers that detects memmap backed  arrays and that is alse able to dump to memmap large in-memory  arrays over the max_nbytes threshold
Communication from child process to the parent process always  pickles in-memory numpy.ndarray without dumping them as memmap  to avoid confusing the caller and make it tricky to collect the  temporary folder
An in-memory store to avoid looking at the disk-based function  source code to check if a function definition has changed
__getstate__ and __setstate__ are required because of __slots__
Should be a light as possible (for speed)
Argument "warn" is for compatibility with MemorizedFunc.clear
Public interface
Pydoc does a poor job on other objects
Private interface
Here, we go through some effort to be robust to dynamically  changing code and collision. We cannot inspect.getsource  because it is not reliable when using IPython's magic "%run".
No metadata available here.
Private `object` interface
Public interface
Partial application, to be able to specify extra keyword  arguments in decorators
Private `object` interface
We need to remove 'joblib' from the end of cachedir
Compressed pickle header format: _ZFILE_PREFIX followed by _MAX_LEN  bytes which contains the length of the zlib compressed data as an  hexadecimal string. For example: 'ZF0x139              '
Pickling needs file-handles at the beginning of the file
Store the length of the data
Load the array from the disk
Count the number of npy files that we have created:  By default we want a pickle protocol that only changes with  the major python version and not the minor one
delayed import of numpy, to avoid tight coupling
When compressing, as we are not writing directly to the  disk, it is more efficient to use standard pickling  Pickling doesn't work with memmaped arrays
This converts the array in a container
XXX: We should have a logging mechanism
Be careful to register our new method.
By default, if compress is enabled, we want to be using 3 by  default
People keep inverting arguments, and the resulting error is  incomprehensible
Environment variables to protect against bad situations when nesting
In seconds, should be big enough to hide multiprocessing dispatching  overhead.  This settings was found by running benchmarks/bench_auto_batching.py  with various parameters on various platforms.
Should not be too high to avoid stragglers: long jobs running alone  on a single worker while other workers have no work to process any more.
We capture the KeyboardInterrupt and reraise it as  something different, as multiprocessing does not  interrupt processing for a KeyboardInterrupt
Try to pickle the input function, to catch the problems early when  using with multiprocessing:
Don't delay the application, to avoid keeping the input  arguments in memory
Update the smoothed streaming estimate of the duration of a batch  from dispatch to completion  First record of duration for this batch size after the last  reset.  Update the exponentially weighted average of the duration of  batch for the current effective size.
`backend=None` was supported in 0.8.2 with this effect
Make it possible to pass a custom multiprocessing context as  backend to change the start method to forkserver or spawn or  preload modules on the forkserver helper process.
Not starting the pool in the __init__ is a design decision, to be  able to close it ASAP, and not burden the user with closing it  unless they choose to use the context manager API with a with block.
This lock is used coordinate the main thread of this process with  the async callback thread of our the pool.
multiprocessing is not available or disabled, fallback  to sequential mode
The list of exceptions that we will capture
Sequential mode: do not use a pool instance to avoid any  useless dispatching overhead
Daemonic processes cannot have children
Prevent posix fork inside in non-main posix threads
Set an environment variable to avoid infinite loops
We are using multiprocessing, we also want to capture  KeyboardInterrupts
If job.get() catches an exception, it closes the queue:
Batching is never beneficial with the threading backend
No batch size adjustment
Reset estimation of the smoothed mean batch duration: this  estimate is updated in the multiprocessing apply_async  CallBack as long as the batch_size is constant. Therefore  we need to reset the estimate whenever we re-tune the batch  size.
Fixed batch size strategy
No more tasks available in the iterator: tell caller to stop.
Wait for an async callback to dispatch new jobs  We need to be careful: the job list can be filling up as  we empty it and Python list are not thread-safe by default hence  the use of the lock
Stop dispatching any new job in the async callback thread
Convert this to a JoblibException
A flag used to abort the dispatching of jobs in case an  exception is found
prevent further dispatch via multiprocessing callback thread
The main thread will consume the first pre_dispatch items and  the remaining items will later be lazily dispatched by async  callbacks upon task completions.
Only set self._iterating to True if at least a batch  was dispatched. In particular this covers the edge  case of Parallel used with an exhausted iterator.
The iterable was consumed all at once by the above for loop.  No need to wait for async callbacks to trigger to  consumption.
on some platform st_blocks is not available (e.g., Windows)  approximate by rounding to next multiple of 512  We need to convert to int to avoid having longs on some systems (we  don't want longs to avoid problems we SQLite)
if a rmtree operation fails in rm_subdirs, wait for this much time (in secs),  then retry once. if it still fails, raise the exception
allow the rmtree to fail once, wait and re-try.  if the error is raised again, fail
Available in Python 3
Copied from python3 tokenize
This behaviour mimics the Python interpreter
This behaviour mimics the Python interpreter
If the error is at the console, don't build any context, since it would  otherwise produce 5 blank lines printed out (there is no file at the  console)
Initialize a list of names on the current line, which the  tokenizer below will populate.
prune names list of duplicates, but keep the right order
some locals
Drop topmost frames if requested
Obtain possible configuration from the environment, assuming 1 (on)  by default, upon 0 set to None. Should instructively fail if some non  0/1 value is set.
2nd stage: validate that locking is available on the system and             issue a warning if not
3rd stage: backward compat for the assert_spawning helper  Python 3.4+
The next line set the .args correctly. This is needed to  make the exception loadable with pickle
Updating module locals so that the exceptions pickle right. AFAIK this  works only at module-creation time
All the lines after the function definition:
In Python 3, quote is elsewhere
Happens in doctests, eg
inspect.formatargspec can not deal with the same  number of arguments in python 2 and 3
Catch a common mistake  Special case for functools.partial objects
First argument is 'self', it has been removed by Python  we need to add it back:  XXX: Maybe I need an inspect.isbuiltin to detect C-level methods, such  as on ndarrays.
Missing argument
XXX: Not using logging framework
Once '__signature__' will be added to 'C'-level  callables, this check won't be necessary
In this case we skip the first parameter of the underlying  function (usually `self` or `cls`).
Was this function wrapped by a decorator?
An object with __call__  We also check that the 'obj' is not an instance of  _WrapperDescriptor or _MethodWrapper to avoid  infinite recursion (and even potential segfault)
For classes and objects we skip the first parameter of their  __call__, __new__, or __init__ methods
Raise a nicer error message for builtins
Add annotation and default value
Keyword arguments mapped by 'functools.partial'  (Parameter._partial_kwarg is True) are mapped  in 'BoundArguments.kwargs', along with VAR_KEYWORD &  KEYWORD_ONLY
We're done here. Other arguments  will be mapped in 'BoundArguments.kwargs'
*args
plain argument
**kwargs
plain keyword argument
Non-keyword-only parameters w/o defaults.
... w/ defaults.
*args
Keyword-only parameters.
**kwargs
Support for binding arguments to 'functools.partial' objects.  See 'functools.partial' case in 'signature()' implementation  for details.  Simulating 'functools.partial' behavior
We have an '*args'-like argument, let's fill it with  all positional arguments we have left and move on to  the next phase
Memorize that we have a '**kwargs'-like parameter
We have no value for this parameter.  It's fine though,  if it has a default value, or it is an '*args'-like  parameter, left alone by the processing of positional  arguments.
Process our '**kwargs'-like parameter
OK, we have an '*args'-like parameter, so we won't need  a '*' to separate keyword-only arguments
x_old equals one of our samples
gelss need to pad y_subpopulation to be of the max dim of X_subpopulation
Initialization of the values of the parameters
Convergence loop of the bayesian ridge regression
Launch the convergence loop
Initialization of the values of the parameters
Prune the weights with a precision over a threshold
add other directories
inverse Lipschitz constant for log loss
inverse Lipschitz constant for squared loss
Ridge default max_iter is None
As in SGD, the alpha is scaled by n_samples.
if loss == 'multinomial', y should be label encoded.
initialization
assume fit_intercept is False
check_finite=False is an optimization available only in scipy >=0.12
new scipy, don't need to initialize because check_finite=False
old scipy, we need the garbage upper triangle to be non-Inf
atom already selected or inner product too small
new scipy, don't need to initialize because check_finite=False
old scipy, we need the garbage upper triangle to be non-Inf
selected same atom twice, or inner product too small
default for n_nonzero_coefs is 0.1 * n_features  but at least one.
or subsequent target will be affected
default for n_nonzero_coefs is 0.1 * n_features  but at least one.
Author: Gael Varoquaux, Alexandre Gramfort  License: BSD 3 clause
We are generating 1 - weights, and not weights
Center X and y to avoid fit the intercept
Scale alpha by alpha_max  Sort alphas in assending order  Get rid of the alphas that are too small  We also want to keep the first one: it should be close to the OLS  solution
Calculate the values where |y - X'w -c / sigma| > epsilon  The values above this threshold are outliers.
Calculate the linear loss due to the outliers.  This is equal to (2 * M * |y - X'w -c / sigma| - M**2) * sigma
n_sq_outliers includes the weight give to the outliers while  num_outliers is just the number of outliers.
Calculate the quadratic loss due to the non-outliers.-  This is equal to |(y - X'w - c)**2 / sigma**2| * sigma
Gradient due to the squared loss.
Gradient due to the penalty.
Gradient due to sigma.
Gradient due to the intercept.
Sigma or the scale factor should be non-negative.  Setting it to be zero might cause undefined bounds hence we set it  to a value close to zero.
Logistic loss is the negative of the log of the logistic function.
Case where we fit the intercept.
Logistic loss is the negative of the log of the logistic function.
Case where we fit the intercept.
Calculate the double derivative with respect to intercept  In the case of sparse matrices this returns a matrix object.
For the fit intercept case.
`loss` is unused. Refactoring to avoid computing it does not  significantly speed up the computation and decreases readability
np.unique(y) gives labels in sorted order.
If class_weights is a dict (provided by the user), the weights  are assigned to the original labels. If it is "balanced", then  the class_weights are assigned after masking the labels with a OvR.
'auto' is deprecated and will be removed in 0.19
SAG multinomial solver needs LabelEncoder, not LabelBinarizer
To deal with object dtypes, we need to convert into an array of floats.
Hack so that we iterate only once for the multinomial case.
init cross-validation generator
OvR in case of binary problems is as good as fitting  the higher label
We need this hack to iterate only once over labels, in the case of  multi_class = multinomial, without changing the value of the labels.
'auto' is deprecated and will be removed in 0.19
hack to iterate only once for multinomial case.
X can be touched inplace thanks to the above line
Workaround to find alpha_max for sparse matrices.  since we should not destroy the sparsity of such matrices.
return self for chaining fit and predict calls
No Gram variant of multi-task exists right now.  Fall back to default enet_multitask
Do the ordering and type casting here, as if it is done in the path,  X is copied and a reference is kept here
Doing this so that it becomes coherent with multioutput.
This makes sure that there is no duplication in memory.  Dealing right with copy_X is important in the following:  Multiple functions touch X and subsamples of X and can induce a  lot of duplication of memory
Making sure alphas is properly ordered.  We want n_alphas to be the number of alphas used for each l1_ratio.
We are not computing in parallel, we can modify X  inplace in the folds
init cross-validation generator
Compute path for all folds and compute MSE to get the best alpha
Remove duplicate alphas in case alphas is provided.
X and y must be of type float64
return self for chaining fit and predict calls
iteration count for learning rate schedule  must not be int (e.g. if ``learning_rate=='optimal'``)
raises ValueError if not registered
uniform sample weights
user-provided array
XXX should have random_state_!  numpy mtrand expects a C long which is a signed 32 bit integer under  Windows
Allocate datastructures from input arguments
labels can be encoded as float, int, or string literals  np.unique sorts in asc order; largest class id is positive class
Clear iteration count for multiple call to fit.
the above might assign zero to all classes, which doesn't  normalize neatly; work around this to produce uniform  probabilities
normalize
Allocate datastructures from input arguments
Clear iteration count for multiple call to fit.
numpy mtrand expects a C long which is a signed 32 bit integer under  Windows
According to the lsqr documentation, alpha = damp^2.
w = inv(X^t X + alpha*Id) * X.T y
dual_coef = inv(X X^t + alpha*Id) y
Unlike other solvers, we need to support sample_weight directly  because K might be a pre-computed kernel.
Only one penalty, we can solve multi-target problems in one time.
Note: we must use overwrite_a=False in order to be able to        use the fall-back solution below in case a LinAlgError        is raised
K is expensive to compute and store in memory so change it back in  case it was user-given.
One penalty per target. We need to solve each target separately.
cholesky if it's a dense array and cg in any other case
SAG supports sample_weight directly. For other solvers,  we implement sample_weight via a simple rescaling.
use SVD solver if matrix is singular
use SVD solver if matrix is singular
precompute max_squared_sum for all targets
When y was passed as a 1d-array, we flatten the coefficients.
we don't (yet) support multi-label classification in Ridge
modify the sample weights with the corresponding class weight
compute diagonal of the matrix: dot(Q, dot(diag(v_prime), Q^T))
handle case where y is 2-d
handle case where y is 2-d
FIXME non-uniform sample weights not yet supported
assert n_samples >= n_features
The scorer want an object that will make the predictions but  they are already computed efficiently by _RidgeGCV. This  identity_estimator will just return them
modify the sample weights with the corresponding class weight
assume linear model by default
MAD (median absolute deviation)
number of data samples
choose random sample set
check if random sample set is valid
fit model for current random sample set
check if estimated model is valid
residuals of all data for current random sample model
classify data into inliers and outliers
extract inlier data set
score of inlier data set
same number of inliers but worse score -> skip current random  sample
save current random sample as best sample
break if sufficient number of inliers or score is reached
estimate final model using all inliers
seed should never be 0 in SequentialDataset
transform variance to std in-place
transform variance to norm in-place
OvR normalization, like LibLinear's predict_probability
Sample weight can be implemented via a simple rescaling.
copy was done in fit if necessary
recompute Gram
precompute if n_samples > n_features
make sure that the 'precompute' array is contiguous.
Xy is 1d, make sure it is contiguous.
Make sure that Xy is always F contiguous even if X or y are not  contiguous: the goal is to make it fast to extract the data for a  specific target.
also test verbose output
no more than max_pred variables can go into the active set
no more than max_pred variables can go into the active set
Test that the ``return_path=False`` option with Gram and Xy remains  correct
consistency test that checks that LARS Lasso is handling rank  deficient input data (with n_features < rank) in the same way  as coordinate descent Lasso
Test that LassoLars and Lasso using coordinate descent give the  same results.
The path should be of length 6 + 1 in a Lars going down to 6  non-zero coefs
Assure that estimators receiving multidimensional y do the right thing
test error on unknown IC
When using automated memory mapping on large input, the  fold data is in read-only mode  This is a non-regression test for:  https://github.com/scikit-learn/scikit-learn/issues/4597  The following should not fail despite copy=False
not normalized data
Test LinearRegression on a simple dataset.  a simple dataset
test it also for degenerate input
It would not work with under-determined systems
LinearRegression with explicit sample_weight
make sure the "OK" sample weights actually work
Test multiple-outcome linear regressions
XXX: if normalize=True, should we expect a weighted standard deviation?       Currently not weighted, but calculated with respect to weighted mean
Sparse cases
test set
Ensure the unconstrained fit has a negative coefficient
On same data, constrained fit has non-negative coefficients
We use a large number of samples and of informative features so that  the l1_ratio selected is more toward ridge than lasso
Well-conditioned settings, we should have selected our  smallest penalty  Non-sparse ground truth: we should have selected an elastic-net  that is closer to ridge than to lasso
We are in well-conditioned settings with low noise: we should  have a good test-set performance
Ensure the unconstrained fit has a negative coefficient
On same data, constrained fit has non-negative coefficients
Y_test = np.c_[y_test, y_test]
This dataset is not trivial enough for the model to converge in one pass.
Check that n_iter_ is invariant to multiple calls to fit  when warm_start=False, all else being equal.
Fit the same model again, using a warm start: the optimizer just performs  a single pass before checking that it has already converged
Train a model to converge on a lightly regularized problem
Fitting a new model on a more regularized version of the same problem.  Fitting with high regularization is easier it should converge faster  in general.
Fit the solution to the original, less regularized version of the  problem but from the solution of the highly regularized variant of  the problem as a better starting point. This should also converge  faster than the original model that starts from zero.
Raise error when selection is not in cyclic or random.
With no input checking, providing X in C order should result in false  computation
test that the feature score of the best features
Check lasso stability path  Load diabetes data and add noisy features
Check randomized lasso
Check randomized sparse logistic regression
Check randomized sparse logistic regression on sparse data
center here because sparse matrices are usually not centered  labels should not be centered
sparse data has a fixed decay of .01
Test that explicit warm restart...
... and implicit warm restart are equivalent.
Input format tests.
Check whether expected ValueError on bad alpha, i.e. 0  since alpha is used to compute the optimal learning rate
assert_almost_equal(clf.coef_[0], clf.coef_[1], decimal=7)
Check whether expected ValueError on bad l1_ratio
Check whether expected ValueError on bad learning_rate
Check whether expected ValueError on bad eta0
Check whether expected ValueError on bad alpha
Check whether expected ValueError on bad penalty
Check whether expected ValueError on bad loss
Test parameter validity check
Test parameter validity check
Checks coef_init not allowed as model argument (only fit)  Provided coef_ does not match dataset.
Checks coef_init shape for the warm starts  Provided coef_ does not match dataset.
Checks intercept_ shape for the warm starts  Provided intercept_ does not match dataset.
Checks intercept_ shape for the warm starts in binary case
simple linear function without noise
Target must have at least two labels
Multi-class average test case
Checks coef_init and intercept_init shape for multi-class  problems  Provided coef_ does not match dataset
Provided coef_ does match dataset
Provided intercept_ does not match dataset
Provided intercept_ does match dataset.
log loss multiclass probability estimates
the following sample produces decision_function values < -1,  which would cause naive normalization to fail (see comment  in SGDClassifier.predict_proba)
Test L1 regularization
test sparsify with dense inputs
pickle and unpickle with sparse coef_
we give a small weights to class 1
now the hyperplane should rotate clock-wise and  the prediction on this point should shift
should be similar up to some epsilon due to learning rate schedule
ValueError due to not existing class label.
ValueError due to wrong class_weight argument type.
Make sure that in the balanced case it does not change anything  to use "balanced"
build an very very imbalanced dataset out of iris data
we give a small weights to class 1
now the hyperplane should rotate clock-wise and  the prediction on this point should shift
Test if ValueError is raised if sample_weight has wrong shape  provided sample_weight too long
classes was not specified
check that coef_ haven't been re-allocated
check that coef_ haven't been re-allocated
Partial_fit should work after initial fit in the multiclass case.  Non-regression test for 2496; fit would previously produce a  Fortran-ordered coef_ that subsequent partial_fit couldn't handle.
Test multiple calls of fit w/ different shaped inputs.
Non-regression test: try fitting with a different label set.
Check whether expected ValueError on bad penalty
Check whether expected ValueError on bad loss
simple linear function without noise
simple linear function without noise
simple linear function without noise
simple linear function with noise
simple linear function without noise
simple linear function with noise
simple linear function without noise
simple linear function with noise
ground_truth linear model that generate y from X and to which the  models should converge if the regularizer would be set to 0.0
check that coef_ haven't been re-allocated
Test if l1 ratio extremes match L1 and L2 penalty settings.
Generate some weird data with hugely unscaled features
Use MinMaxScaler to scale the data without introducing a numerical  instability (computing the standard deviation naively is not possible  on this data)
smoke test: model is stable on scaled data
Test gradient of different loss functions  cases is a list of (p, y, expected)
Test BayesianRidge on diabetes
Test with more samples than features  Test that scores are increasing at each iteration
Test with more features than samples  Test that scores are increasing at each iteration
Check that the model could approximately learn the identity function
Test BayesianRegression ARD classifier
Check that the model could approximately learn the identity function
Generate data with outliers by replacing 10% of the samples with noise.
SciPy performs the tol check after doing the coef updates, so  these would be almost same but not equal.
No n_iter_ in old SciPy (<=0.9)  And as said above, the first iteration seems to be run anyway.
The huber model should also fit poorly on the outliers.
With more samples than features
Currently the only solvers to support sample_weight.
Currently the only solvers to support sample_weight.
Sample weight can be implemented via a simple rescaling  for the square loss.
Ridge with explicit sample_weight
we need more samples than features
Test error is raised when number of targets and penalties do not match.
test that can work with both dense or sparse matrices
check that efficient and brute-force LOO give same results
check that efficient and SVD efficient LOO give same results
check best alpha
check that we get same best alpha with a scorer
check that we get same best alpha with sample weights
simulate several responses
simulate several responses
test dense matrix  test sparse matrix  test that the outputs are the same
we give a small weights to class 1
now the hyperplane should rotate clock-wise and  the prediction on this point should shift
check if class_weight = 'balanced' can handle negative labels.
we give a small weights to class 1
Test _RidgeCV's store_cv_values attribute.
with len(y.shape) == 1
with len(y.shape) == 2
Check using GridSearchCV directly
make sure the "OK" sample weights actually work
Test that self.n_iter_ is correct.
Simple sanity check on a 2 classes dataset  Make sure it predicts the correct result on simple datasets.
Test logistic regression with the iris dataset
Test multinomial LR on a binary problem.
Test sparsify and densify members.
Test that an exception is raised on inconsistent input
Wrong dimensions for training data
Wrong dimensions for test data
Test that we can write to coef_ and intercept_
Test proper NaN handling.  Regression test for Issue 252: fit used to go into an infinite loop.
same result for same random state  different results for different random states
Second check that our intercept implementation is good
First check that _logistic_grad_hess is consistent  with _logistic_loss_and_grad
Now check our hessian along the second direction of the grad
In the fit_intercept=False case, the feature vector of ones is  penalized. This should be taken care of.
Check gradient.
Test that OvR and multinomial are correct using the iris dataset.
The cv indices from stratified kfold (where stratification is done based  on the fine-grained iris classes, i.e, before the classes 0 and 1 are  conflated) is used for both clf and clf1
Train clf on the original dataset where classes 0 and 1 are separated
Conflate classes 0 and 1 and train clf1 on this modified dataset
Ensure that what OvR learns for class2 is same regardless of whether  classes 0 and 1 are separated or not
helper for returning a dictionary instead of an array
Multinomial case: remove 90% of class 0
Binary case: remove 90% of class 0 and 100% of class 2
'auto' is deprecated and will be removed in 0.19
Some basic attributes of Logistic Regression
Compare solutions between lbfgs and the other solvers
extract first column of hessian matrix
Dummy data such that the decision function becomes zero.
Predicted probabilites using the soft-max function should give a  smaller loss than those using the logistic function.
Test that the maximum number of iteration is reached
old scipy doesn't have maxiter
Test that self.n_iter_ has the correct format.
multinomial case
A 1-iteration second fit on same data should give almost same result  with warm starting, and quite different result without warm starting.  Warm starting does not work with liblinear solver.
old scipy doesn't have maxiter
Check if median is solution of the Fermat-Weber location problem  Check when maximum iteration is exceeded a warning is emitted
Check for exact the same results as Least Squares
Check that Theil-Sen can be verbose
Classifier can be retrained on different labels and features.
we give a small weights to class 1
now the hyperplane should rotate clock-wise and  the prediction on this point should shift
partial_fit with class_weight='balanced' not supported
Already balanced, so "balanced" weights should have no effect
should be similar up to some epsilon due to learning rate schedule
Check that the sparse_coef property works
Test ElasticNet for various values of alpha and l1_ratio with sparse X  training samples  X[1, 0] = 0
test samples
generate a ground truth model
generate training ground truth labels
check the convergence is the same as the dense version
check that the coefs are sparse
check that the coefs are sparse
XXX: There is a bug when precompute is not None!
this is used for sag regression
sparse data has a fixed decay of .01
sparse data has a fixed decay of .01
idx = k
sum the squares of the second sample because that's the largest
simple linear function without noise
simple linear function with noise
test if the multinomial loss and gradient computations are consistent
comparison
Generate coordinates of line
Estimate parameters of corrupted data
Ground truth / reference inlier mask
When residual_threshold=0.0 there are no inliers and a  ValueError with a message should be raised
3-D target values
Estimate parameters of corrupted data
Ground truth / reference inlier mask
XXX: Remove in 0.20
one-dimensional
Estimate parameters of corrupted data
Ground truth / reference inlier mask
e = 0%, min_samples = X
e = 0%, min_samples = 10
sanity check
check that mask is correct
check that if base_estimator.fit doesn't support  sample_weight, raises error
holds the sign of covariance
force copy. setting the array to be fortran-ordered  speeds up the calculation of the (partial) Gram matrix  and allows to easily swap columns
interpolation factor 0 <= ss < 1  In the first iteration, all alphas are zero, the formula  below would make ss a NaN
Update the cholesky decomposition for the Gram matrix
least squares solution
This happens because sign_active[:n_active] = 0
is this really needed ?
equiangular direction of variables in the active set  correlation between each unactive variables and  eqiangular vector
if huge number of features, this takes 50% of time, I  think could be avoided if we just update it using an  orthogonal (QR) decomposition of X
update the sign, important for LAR
mimic the effect of incrementing n_iter on the array references
update correlations
See if any coefficient has changed sign
handle the case when idx is not length of 1
handle the case when idx is not length of 1
resize coefs in case of early stop
precompute if n_samples > n_features
init cross-validation generator
Unique also sorts  Take at most max_n_alphas values
Select the alpha that minimizes left-out error
Store our parameters
Now compute the full model  it will call a lasso internally when self if LassoLarsCV  as self.method == 'lasso'
impedance matching for the above Lars.fit (should not be documented)
get the number of degrees of freedom equal to:  Xc = X[:, mask]  Trace(Xc * inv(Xc.T, Xc) * Xc.T) ie the number of non-zero coefs
probabilities of the positive class
Y[i, j] gives the probability that sample i has the label j.  In the multi-label case, these are not disjoint.
Only one estimator, but we still want to return probabilities  for two classes.
Then, probabilities should be normalized to 1.
FIXME: there are more elaborate methods than generating the codebook  randomly.
Set the number of local seeding trials if none is given  This is what Arthur/Vassilvitskii tried, but did not report  specific results for other than mentioning in the conclusion  that it helped.
Pick first center randomly
Initialize list of closest distances and calculate current potential
Pick the remaining n_clusters-1 points  Choose center candidates by sampling with probability proportional  to the squared distance to the closest existing center
Compute distances to center candidates
Decide which candidate is the best  Compute potential when including center candidate
Store result if it is the best local trial so far
Permanently add best center candidate found in local tries
subtract of mean of x for more accurate distance computations  The copy was already done above
precompute squared norms of data points
elkan doesn't make sense for a single cluster, full will produce  the right result.
init
Allocate memory to store the distances for each sample to its  closer center for reallocation in case of ties
iterations  labels assignment is also called the E-step of EM
computation of the means is also called the M-step of EM
rerun E-step in case of non-convergence so that predicted labels  match cluster centers
distances will be changed in-place
Currently, this just skips a copy of the data if it is not in  np.array or CSR format already.  XXX This skips _check_test_data, which may change the dtype;  we should refactor the input validation.
Perform label assignment to nearest centers
reset counts of reassigned centers, but don't reset them too small  to avoid instant reassignment. This is a pretty dirty hack as it  also modifies the learning rates.
implementation for the sparse CSR representation completely written in  cython
dense variant in mostly numpy (not as memory efficient though)  find points from minibatch that are assigned to this center
inplace remove previous count scaling
inplace sum with new points members of this cluster
update the count statistics for this center
inplace rescale to compute mean of all points (old and new)
update the squared diff if necessary
Normalize inertia to be able to compare values when  batch_size changes
Early stopping based on absolute tolerance on squared change of  centers position (using EWA smoothing)
update the convergence context to maintain state across successive calls:
using tol-based early stopping needs the allocation of a  dedicated before which can be expensive for high dim data:  hence we allocate it outside of the main loop
no need for the center buffer if tol-based early stopping is  disabled
Initialize the centers using only a fraction of the data as we  expect n_samples to be very large when using MiniBatchKMeans
Compute the label assignment on the init dataset
Empty context to be used inplace by the convergence check routine
Perform the iterative optimization until the final convergence  criterion  Sample a minibatch from the full dataset
Monitor convergence and do early stopping if necessary
this is the first call partial_fit on this object:  initialize the cluster centers
separate function for each seed's iterative loop  For each seed, climb gradient until convergence or max_iter
Find mean of points within bandwidth
If converged or at max_iter, adds the cluster
nothing near seeds
Bin points
Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>  License: BSD 3 clause
Place preference on the diagonal of S
Intermediate results
Execute parallel affinity propagation updates
tmp = A + S; compute responsibilities
tmp = Rnew
Damping
tmp = Rp; compute availabilities
Damping
Check for convergence
Reduce labels to a sorted, gapless, list
Author: Gael Varoquaux gael.varoquaux@normalesup.org          Brian Cheung          Wei LI <kuantkid@gmail.com>  License: BSD 3 clause
Normalize the rows of the eigenvectors.  Samples should lie on the unit  hypersphere centered at the origin.  This transforms the samples in the  embedding space to the space of partition matrices.
If there is an exception we try to randomize and rerun SVD again  do this max_svd_restarts times.
Initialize first column of rotation matrix with a row of the  eigenvectors
otherwise calculate rotation and continue
Authors: Manoj Kumar <manojkumarsivaraj334@gmail.com>           Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>           Joel Nothman <joel.nothman@gmail.com>  License: BSD 3 clause
Keep centroids and squared norm as views. In this way  if we change init_centroids and init_sq_norm_, it is  sufficient,
We need to find the closest subcluster among all the  subclusters so that we can insert our new subcluster.
If the subcluster has a child, we need a recursive strategy.
If it is determined that the child need not be split, we  can just update the closest_subcluster
things not too good. we need to redistribute the subclusters in  our child node, and add a new subcluster in the parent  subcluster to accommodate the new child.
good to go!
not close to any other subclusters, and we still  have space, so add.
We do not have enough space nor is it closer to an  other subcluster. We need to split.
To enable getting back subclusters.
Cannot vectorize. Enough to convince to use cython.
Perform just the final global clustering step.
Called by partial_fit, before fitting.
Should raise an error if one does not fit before predicting.
To use in predict to avoid recalculation.
The global clustering step that clusters the subclusters of  the leaves. It assumes the centroids of the subclusters as  samples and finds the final centroids.
Affinity Propagation algorithm  Compute similarities  Compute Affinity Propagation
Test also with no copy
Test AffinityPropagation.predict
Test exception in AffinityPropagation.predict  Not fitted.
Predict not supported when affinity="precomputed".
Mock object for testing get_submatrix.
Overridden to reproduce old get_submatrix test.
Test Kluger methods on a checkerboard dataset.
cannot take log of sparse matrix
adding any constant to a log-scaled matrix should make it  bistochastic
perform label assignment using the dense array input
perform label assignment using the sparse CSR input
Check that dense and sparse minibatch update give the same results
extract a small minibatch
step 1: compute the dense minibatch update
compute the new inertia on the same batch to check that it decreased
check that the incremental difference computation is matching the  final observed value
step 2: compute the sparse minibatch update
compute the new inertia on the same batch to check that it decreased
check that the incremental difference computation is matching the  final observed value
step 3: check that sparse and dense updates lead to the same results
check that the number of clusters centers and distinct labels match  the expectation
check that the labels assignment are perfect (up to a permutation)
check error on dataset being too small
Reorder the labels so that the first instance is in cluster 0,  the second in cluster 1, ...
check that a warning is raised if the precompute_distances flag is not  supported
two regression tests on bad n_init argument  previous bug: n_init <= 0 threw non-informative TypeError (3858)
Check that a warning is raised, as the number clusters is larger  than the init_size
increase n_init to make random init stable enough
increase n_init to make random init stable enough
do the same with batch-size > X.shape[0] (regression test)  there should not be too many exact zero cluster centers
there should not be too many exact zero cluster centers
Give a perfect initialization, but a large reassignment_ratio,  as a result all the centers should be reassigned and the model  should not longer be good
Small test to check that giving the wrong number of centers  raises a meaningful error
Now check that the fit actually works
use the partial_fit API for online learning
compute the labeling on the complete dataset
Check if copy_x=False returns nearly equal X after de-centering.
check if my_X is centered
centers must not been collapsed
sanity check: predict centroid labels
sanity check: re-predict labeling for training set samples
re-predict labels for training set using fit_predict
sanity check: predict centroid labels
sanity check: re-predict labeling for training set samples
sanity check: re-predict labeling for training set samples
sanity check: predict centroid labels
check that models trained on sparse input also works for dense input at  predict time
sanity check: re-predict labeling for training set samples
sanity check: predict centroid labels
check that models trained on sparse input also works for dense input at  predict time
check that the labels assignment are perfect (up to a permutation)
check warning when centers are passed
to many clusters desired
Test that same global labels are obtained after calling partial_fit  with None
Test the predict method predicts the nearest centroid.
Test that n_clusters = Agglomerative Clustering gives  the same results.
Test that the wrong global clustering step raises an Error.
Test that a small number of clusters raises a warning.
Test that sparse and dense data give same results
Test that nodes have at max branching_factor number of subclusters
Raises error when branching_factor is set to one.
Test that the leaf subclusters have a threshold lesser than radius
Test estimate_bandwidth
Test MeanShift algorithm
Test MeanShift.predict
Non-regression: before fit, there should be not fitted attributes.
With a bin size of 0.01 and min_bin_freq of 1, 6 bins should be found  we bail and use the whole data here.
Authors: Vincent Michel, 2010, Gael Varoquaux 2012,           Matteo Visconti di Oleggio Castello 2014  License: BSD 3 clause
Smoke test FeatureAgglomeration
test hierarchical clustering on a precomputed distances matrix
test hierarchical clustering on a precomputed distances matrix
Test that using ward with another metric than euclidean raises an  exception
Check that fitting with no samples raises a ValueError
Test scikit linkage with full connectivity (i.e. unstructured) vs scipy
Test error management in _hc_cut
test on five random datasets
test that return_distance when set true, gives same  output on both structured and unstructured clustering.
get children
check if we got the same clusters
check if the distances are the same
check that the labels are the same
check that the distances are correct
check that the labels are the same
check that the distances are correct
Test that the full tree is computed if n_clusters is small
When n_clusters is less, the full tree should be built  that is the number of merges should be n_samples - 1
Test n_components returned by linkage, average and ward tree
Connectivity matrix having five components.
We don't care too much that it's good, just that it *worked*.  There does have to be some lower limit on the performance though.
raise error on unknown affinity
Tests the DBSCAN algorithm with a feature vector array.  Parameters chosen specifically for this task.  Different eps to other test, because distance is not normalised.  Compute DBSCAN  parameters chosen for task
number of clusters, ignoring noise if present
Tests the DBSCAN algorithm with a callable metric.  Parameters chosen specifically for this task.  Different eps to other test, because distance is not normalised.  metric is the function reference, not the string key.  Compute DBSCAN  parameters chosen for task
number of clusters, ignoring noise if present
Tests the DBSCAN algorithm with balltree for neighbor calculation.
number of clusters, ignoring noise if present
DBSCAN.fit should accept a list of lists.
ensure sample_weight is validated
sample_weight should work with precomputed distance matrix
sample_weight should work with estimator
Only the sample in the middle of the dense area is core. Its two  neighbors are edge samples. Remaining samples are noise.
It's no longer possible to extract core samples with eps=1:  everything is noise.
Calculate neighborhood for all samples. This leaves the original point  in, which needs to be considered later (i.e. point i is in the  neighborhood of point i. While True, its useless information)
This has worst case O(n^2) memory complexity
Initially, all samples are noise.
A list of all core samples found.
fix for scipy sparse indexing issue
no core samples
initialize with [-1,1] as in ARPACK
Make the connectivity matrix symmetric:
Convert connectivity matrix to LIL
Compute the number of nodes
prepare the main fields
update the moments
List comprehension is faster than a for loop
Separate leaves in children (empty lists up to now)  sort children to get consistent output with unstructured version
2 is scaling factor to compare w/ unstructured version
for the linkage function of hierarchy to work on precomputed  data, provide as first argument an ndarray of the shape returned  by pdist: it is a flat array containing the upper triangular of  the distance matrix.
Translate to something understood by scipy
Put the diagonal to zero
FIXME We compute all the distances, while we could have only computed  the "interesting" distances
create inertia heap and connection matrix
prepare the main fields
recursive merge loop  identify the merge
store distances
Keep track of the number of elements per cluster
Separate leaves in children (empty lists up to now)
return numpy array for efficient caching
Matching names to tree-building strategies
Early stopping is likely to give a speed up only for  a large number of clusters. The actual threshold  implemented here is heuristic
Author: Gael Varoquaux <gael.varoquaux@normalesup.org>  License: BSD 3 clause  Copyright: INRIA
The base class needs this for the score method
Covariance does not make sense for a single feature
The base class needs this for the score method
List of (alpha, scores, covs)
No need to see the convergence warnings on this grid:  they will always be points that will not converge  during the cross-validation  Compute the cross-validated loss on the current grid
Author: Virgile Fritsch <virgile.fritsch@inria.fr>  License: BSD 3 clause
Check early stopping
minimum breakdown value
avoid division truncation
set covariance  set precision
compute empirical covariance of the test set  compute log likelihood
compute mahalanobis distances
Check that the costs always decrease (doesn't hold if alpha == 0)  Check that the 2 approaches give similar results
Small subset of rows to test the rank-deficient case  Need to choose samples such that none of the variances are zero
Smoke test with specified alphas
Medium data set
Large data set
1D data set
Check that the code does not break with X.shape = (3, 1)  (i.e. n_support = n_samples)
test centered case
Tests ShrunkCovariance module on a simple dataset.  compare shrunk covariance obtained from data and from MLE estimate
same test with shrinkage not provided
same test with shrinkage = 0 (<==> empirical_covariance)
test shrinkage coeff on a simple data set (without saving precision)
Tests LedoitWolf module on a simple dataset.  test shrinkage coeff on a simple data set
test shrinkage coeff on a simple data set (without saving precision)
test shrinkage coeff on a simple data set (without saving precision)
Compare our blocked implementation to the naive implementation
check that the result is consistent with not splitting data into blocks.
test shrinkage coeff on a simple data set (without saving precision)
test shrinkage coeff on a simple data set (without saving precision)
avoid division truncation
optionally center data
number of blocks to split the covariance matrix into
formula from Chen et al.'s **implementation**
Not calling the parent object to fit, to avoid computing the  covariance matrix (and potentially the precision)
Calculate Spearman rho estimate and set return accordingly.
Use a 95% CI, i.e., +/-1.96 S.E.  https://en.wikipedia.org/wiki/Fisher_transformation
upper bound on the cost function
single y, constant prediction
Determine increasing if auto-determination requested
Store _X_ and _y_ to maintain backward compat during the deprecation  period of X_ and y_
Handle the left and right bounds on X
The ability to turn off trim_duplicates is only used to it make  easier to unit test that removing duplicates in y does not have  any impact the resulting interpolation function (besides  prediction speed).
Transform y by running the isotonic regression algorithm and  transform X accordingly.
It is necessary to store the non-redundant part of the training set  on the model to make it possible to support model persistence via  the pickle module as the object built by scipy.interp1d is not  picklable directly.
Build the interpolation function
copy __dict__  remove interpolation method
XXX Workaround that will be removed when list of list format is  dropped
To account for pos_label == 0 in the dense case
pick out the known labels from y
preserve label ordering
picks out all indices obtaining the maximum per row
For corner case where last row has a max of 0
Automatically increment on new class
sort classes and reorder columns
Compute the most frequent value in array only
Since two different arrays can be provided in fit(X) and  transform(X), the imputation data will be computed in transform()  when the imputation is done per sample (i.e., when axis=1).
Count the zeros
Mean
Mask the missing elements
Ignore the error, columns with a np.nan statistics_  are not an error at this point. These columns will  be removed in transform
astype necessary for bug in numpy.hsplit before v1.9
Median
Most frequent
Most frequent  scipy.stats.mstats.mode cannot be used because it will no work  properly if the first element is masked and if its frequency  is equal to the frequency of the most frequent valid element  See https://github.com/scipy/scipy/issues/2636
To be able access the elements by columns
Since two different arrays can be provided in fit(X) and  transform(X), the imputation data need to be recomputed  when the imputation is done per sample
Checking one attribute is enough, becase they are all set together  in partial_fit
Reset internal state before fitting
Cast input to array, as we need to check ndim. Prior to 0.17, that was  done inside the scaler object fit_transform.  If copy is required, it will be done inside the scaler object.
Checking one attribute is enough, becase they are all set together  in partial_fit
Reset internal state before fitting
Checking one attribute is enough, becase they are all set together  in partial_fit
Reset internal state before fitting
First pass  Next passes
Cast input to array, as we need to check ndim. Prior to 0.17, that was  done inside the scaler object fit_transform.  If copy is required, it will be done inside the scaler object.
allocate output data
No features selected.
All features selected.
Sparse matrix, axis = 0
Verify the shapes of the imputed matrix for different strategies.
np.median([]) raises a TypeError for numpy >= 1.10.1
np.mean([]) raises a RuntimeWarning for numpy >= 1.10.1
Test imputation using the mean and median strategies, when  missing_values != 0.
Create the columns
Shuffle them the same way
Mean doesn't support columns containing NaNs, median does
Test median imputation with sparse boundary cases
Test imputation within a pipeline + gridsearch.
Test for pickling imputers.
Test imputation with copy
Test scaling of dataset along single axis
check inverse transform
1-d inputs
This does not raise a warning as the number of samples is too low  to trigger the problem in recent numpy
Test scaling of 2d array along first axis
Check that X has been copied
check inverse transform
Check that the data hasn't been modified
Check that X has not been copied
Check that X has not been copied
Test if partial_fit run over many batches of size 1 and 50  gives the same results as fit
Test mean at the end of the process
Test std after 1 step
Test std until the end of partial fits, and
Test if partial_fit run over many batches of size 1 and 50  gives the same results as fit
Test mean at the end of the process
Test std until the end of partial fits, and
with_mean=False is required with sparse input
chunk = sparse.csr_matrix(data_chunks)
Check that sparsity is not destroyed
Check some postconditions after applying partial_fit and transform
(i+1) because the Scaler has been already fitted
raises on invalid range
Check min max scaler on toy data with zero variance features
function interface
Test scaling of dataset along single axis
check inverse transform
Check that X has not been modified (copy)
test that scaler converts integer input to floating  for both sparse and dense matrices
Check that X has not been modified (copy)
Check that StandardScaler.fit does not change input
check scaling and fit with direct calls on sparse data
check transform and inverse_transform after a fit on a dense array
Check if non finite inputs raise ValueError
test csc has same outcome
raises value error on axis != 0
Check that X has not been copied
null scale
function interface
Check warning when scaling integer data
Test scaling of dataset along single axis
check inverse transform
function interface
Test if partial_fit run over many batches of size 1 and 50  gives the same results as fit
Test mean at the end of the process
Test std after 1 step
Test std until the end of partial fits, and
set the row number 3 to zero
set the row number 3 to zero without pruning (can happen in real life)
build the pruned variant using the regular constructor
check inputs that support the no-copy optim
set the row number 3 to zero
set the row number 3 to zero without pruning (can happen in real life)
build the pruned variant using the regular constructor
check inputs that support the no-copy optim
set the row number 3 to zero
set the row number 3 to zero without pruning (can happen in real life)
build the pruned variant using the regular constructor
check inputs that support the no-copy optim
Cannot use threshold < 0 for sparse
center fit time matrix
check outcome
test negative input to fit
test negative input to transform
Edge case: all non-categorical
Edge case: all categorical
Test that one hot encoder raises error for unknown features  present during transform.
Raise error if handle_unknown is neither ignore or error.
Scalers that have a partial_fit method
with a different shape, this may break the scaler unless the internal  state is reset
Test that the numpy.log example still works.
Test that rounding is correct
Test that rounding is correct
Test that rounding is correct
Check that invalid arguments yield ValueError
Fail on y_type
Sequence of seq type should raise ValueError
Fail on the number of classes
Fail on the dimension of 'binary'
Test fit_transform
Check that invalid arguments yield ValueError
Fail on unseen labels
fit_transform()
fit().transform()
ensure fit is no-op as iterable is not consumed
fit().transform()
fit_transform()
Wrong shape
check label_binarize
check inverse
we want all classifiers that don't expose a random_state  to be deterministic (and we don't want to expose this one).
Compute the arithmetic mean of the predictions of the calibrated  classifiers
Normalize the probabilities
XXX : for some reason all probas can be 0
Deal with cases where the predicted probability minimally exceeds 1.0
extra_compile_args=['-O0 -fno-inline'],
FIXME Remove l1/l2 support in 1.0
FIXME Remove l1/l2 support in 1.0
get 1vs1 weights for all n*(n-1) classifiers.  this is somewhat messy.  shape of dual_coef_ is nSV * (n_classes -1)  see docs for details
dual coef for class1 SVs:  dual coef for class2 SVs:  build weight for class1 vs class2
The order of these must match the integer values in LibSVM.  XXX These are actually the same in the dense case. Need to factor  this out.
Used by cross_val_score.
XXX this is ugly.  Regression models should not have a class_weight_ attribute.
Precondition: X is a csr_matrix of dtype np.float64.
NOTE: _validate_for_predict contains check for is_fitted  hence must be placed before any other attributes are used.
In binary case, we need to flip the sign of coef, intercept and  decision function.
coef_ being a read-only property, it's better to mark the value as  immutable to avoid hiding potential bugs for the unsuspecting user.  sparse matrix do not have global flags  regular dense array
binary classifier
Regarding rnd.randint(..) in the above signature:  seed for srand in range [0..INT_MAX); due to limitations in Numpy  on 32-bit platforms, we can't get to the UINT_MAX limit that  srand supports
many class dataset:
test that the result with sorted and unsorted indices in csr is the same  we use a subset of digits as iris, blobs or make_classification didn't  show the problem
make sure dense and sparse SVM give the same result
make sure we scramble the indices
make sure unsorted indices give same result
multi class:
Test that it gives proper exception on deficient input  impossible value of C
impossible value of nu
Similar to test_SVC
check decision_function
sparsify the coefficients on both models and check that they still  produce the same results
Test class weights
Test weights on individual samples
Test that sparse liblinear honours intercept_scaling param
many class dataset:
Test that the "dense_fit" is called even though we use sparse input  meaning that everything works fine.
also load the iris dataset
If random_seed >= 0, the libsvm rng is seeded (by calling `srand`), hence  we should get deterministic results (assuming that there is no other  thread calling this wrapper calling `srand` concurrently).
Gram matrix for test data but compute KT[i,j]  for support vectors j only.
Bad kernel
Test OneClassSVM
Test OneClassSVM decision function
Generate train data
fit the model
Test decision_function  Sanity check, test that decision_function implemented in python  returns the same as the one in libsvm  multi class:
kernel binary:
with five classes:
linear kernel
rbf kernel
Test class weights  we give a small weights to class 1  so all predicted values belong to class 2
Test that it gives proper exception on deficient input  impossible value of C
impossible value of nu
error for precomputed kernelsx
sample_weight bad dimensions
predict with sparse input when trained with dense
Incorrect loss value - test if explicit error message is raised
FIXME remove in 1.0
Test basic routines using LinearSVC
by default should have intercept
the same with l1 penalty
l2 penalty with dual formulation
l2 penalty, l1 loss
test also decision function
similar prediction for ovr and crammer-singer:
classifiers shouldn't be the same
Test Crammer-Singer formulation in the binary case
when intercept_scaling is low the intercept value is highly "penalized"  by regularization
when intercept_scaling is sufficiently high, the intercept value  is not affected by regularization
when intercept_scaling is sufficiently high, the intercept value  doesn't depend on intercept_scaling value
binary-class case
stdout: redirect
actual call
Test that SVR(kernel="linear") has coef_ with the right sign.  Non-regression test for 2933.
wrap dictionary in a singleton list to support either dict  or list of dicts
Product function that can handle iterables (np.product can't).
This is used to make discrete sampling without replacement memory  efficient.  XXX: could memoize information used here
Reverse so most frequent cycling parameter comes first
Try the next grid
check if all distributions are given as lists  in this case we want to sample without replacement
look up sampled parameter settings in parameter grid
Out is a list of triplet: score, estimator, n_test_samples
Find the best parameters by comparing on the mean validation score:  note that `sorted` is deterministic in the way it breaks ties
Author: Mathieu Blondel <mathieu@mblondel.org>          Arnaud Joly <a.joly@ulg.ac.be>          Maheshakya Wijewardena <maheshakya.10@cse.mrt.ac.lk>  License: BSD 3 clause
Checking in case of constant strategy if the constant  provided by the user is in y.
numpy random_state expects Python int and not long as size argument  under Windows
Get same type even for self.n_outputs_ == 1  Compute probability only once
numpy random_state expects Python int and not long as size argument  under Windows
Get same type even for self.n_outputs_ == 1
flag to indicate exit status of fit() method: converged (True) or  n_iter reached (False)
EM algorithms  reset self.converged_ to False
Expectation step
Need to make sure that there are responsibilities to output  Output zeros because it was just a quick initialization
Gaussian mixture parameters estimators (used by the M-Step)
Attributes computation
Check all the parameters values of the derived class
if we enable warm_start, we will have a unique initialisation
ignore underflow
simple 3 cluster dataset
simple 3 cluster dataset
the same for spherical covariances
This function tests the deprecated old GMM class
Create a training set by sampling from the predefined distribution.
This function tests the deprecated old GMM class
This function tests the deprecated old GMM class  Test the aic and bic criteria
we build a dataset with 2 2d component. The components are unbalanced  (respective weights 0.9 and 0.1)
This is a non-regression test for issue 2640. The following call used  to trigger:  numpy.linalg.linalg.LinAlgError: 2-th leading minor not positive definite
Check positive definiteness for all covariance types
This function tests the deprecated old GMM class  Create sample data
This function tests the deprecated old GMM class  Create sample data
test bad parameters
Check good weights matrix
Check good means matrix
Define not positive-definite precisions
Check precisions with bad shapes
Check not positive precisions
Check the correct init of precisions_init
compare the precision matrix compute from the  EmpiricalCovariance.covariance fitted on X*sqrt(resp)  with _sufficient_sk_full, n_components=1
use equation Nk * Sk / N = S_tied
test against 'full' case
computing spherical covariance equals to the variance of one-dimension  data after flattening, n_components=1
test aginst with _naive_lmvnpdf_diag
full covariances
diag covariances
tied
test whether responsibilities are normalized
Check a warning message arrive if we don't do fit
recover the ground truth
needs more data to pass the test with rtol=1e-7
the accuracy depends on the number of data and randomness, rng
Assert the warm_start give the same result for the same number of iter
Assert that by using warm_start we can converge to a good solution
Check if the score increase
We check that each step of the EM without regularization improve  monotonically the training set likelihood
Do one training iteration at a time so we can make sure that the  training log likelihood increases after each iteration.
We train the GaussianMixture on degenerate data by defining two clusters  of a 0 covariance.
Free memory and developers cognitive load:
initialization step
EM algorithms  reset self.converged_ to False
Expectation step
Check for convergence.
Maximization step
Need to make sure that there is a z value to output  Output zeros because it was just a quick initialization
Weight labels by their number of occurrences
Distribute the most frequent labels first
Total weight of each fold
Mapping from label index to fold index
Distribute samples by adding the largest weight to the lightest fold
don't want to use the same seed in each label's shuffle
We make a copy of labels to avoid side-effects during iteration
random partition
pass through: skip indexing
Check for sparse predictions
Adjust length of sample weights
Adjust length of sample weights
cannot compute the kernel values with custom function
Initialization
Elimination  Remaining features
Rank the remaining features
Get ranks
for sparse case ranks is matrix
Eliminate the worse features
Compute step score on the previous selection iteration  because 'estimator' must use features  that have not been eliminated yet
Set final attributes
Compute step score when only n_features_to_select features left
Re-execute an elimination with best_k over the whole set
Fixing a normalization error, n is equal to get_n_splits(X, y) - 1  here, the scores are normalized by get_n_splits(X, y)
Author: Nikolay Mayorov <n59_ru@hotmail.com>  License: 3-clause BSD
Here we rely on NearestNeighbors to select the fastest algorithm.
Algorithm is selected explicitly to allow passing an array as radius  later (not all algorithms support this).
Ignore points with unique labels.
Selection  Fails in Python 3.x when threshold is str;  result is array of True
Check 1d list and other dtype:
Check wrong shape raises error
Check wrong shape raises error
Check 1d list and other dtype:
Check wrong shape raises error
Check wrong shape raises error
Feature 0 is highly informative for class 1;  feature 1 is the same everywhere;  feature 2 is a bit informative for class 2.
== doesn't work on scipy.sparse matrices
Check that chi2 works with a COO matrix  (as returned by CountVectorizer, DictVectorizer)  if we got here without an exception, we're safe
Check if the supports are equal
sparse model
All the noisy variable were filtered out
make sure that cross-validation is stratified
Test when floor(step * n_features) <= 0
Test when step is between (0,1) and floor(step * n_features) > 0
Test when step is an integer
test that is gives the same result as with float
Test whether the F test yields meaningful results  on a simple simulated classification problem
Test whether the F test yields meaningful results  on a simple simulated regression problem
Test whether f_regression returns the same value  for any numeric data_type
Test whether the F test yields meaningful results  on a simple simulated classification problem
Test whether the relative univariate feature selection  gets the correct items in a simple classification problem  with the percentile heuristic
Check other columns are empty
Test whether the relative univariate feature selection  gets the correct items in a simple classification problem  with the k best heuristic
Test whether k="all" correctly returns all features.
Test whether k=0 correctly returns no features.
Test whether the relative univariate feature selection  gets the correct items in a simple classification problem  with the fdr, fwe and fpr heuristics
Test whether the relative univariate feature selection  gets the correct items in a simple regression problem  with the percentile heuristic
Check inverse_transform respects dtype
Test whether the relative univariate feature selection  selects all features when '100%' is asked.
Test whether the relative univariate feature selection  gets the correct items in a simple regression problem  with the k best heuristic
Test whether the relative univariate feature selection  gets the correct items in a simple regression problem  with the fpr, fdr or fwe heuristics
Test that fdr heuristic actually has low FDR.
As per Benjamini-Hochberg, the expected false discovery rate  should be lower than alpha:  FDR = E(FP / (TP + FP)) <= alpha
Make sure that the empirical false discovery rate increases  with alpha:
Test whether the relative univariate feature selection  gets the correct items in a simple regression problem  with the fwe heuristic
Test whether k-best and percentiles work with tied pvalues from chi2.  chi2 will return the same p-values for the following features, but it  will return different scores.
Test for stable sorting in k-best with tied scores.
Assert that SelectKBest and SelectPercentile can handle NaNs.  First feature has zero variance to confuse f_classif (ANOVA) and  make it return a NaN.
In discrete case computations are straightforward and can be done  by hand on given vectors.
Mean of the distribution, irrelevant for mutual information.
Setup covariance matrix with correlation coeff. equal 0.5.
True theoretical mutual information.
Theory and computed values won't be very close, assert that the  first figures after decimal point match.
Assert the same tolerance.
Test that adding unique label doesn't change MI.
Here X[:, 0] is the most informative feature, and X[:, 1] is weakly  informative.
Test VarianceThreshold with custom variance.
Check with sample weights
Check that the model is rewritten if prefit=False and a fitted model is  passed
Check that prefit=True and calling fit raises a ValueError
Calculate the threshold from the estimator directly.
Set a higher threshold to filter out more features.
XXX where should this function be called? fit? scoring functions  themselves?
flatten matrix to vector in sparse case
Reuse f_obs for chi-squared statistics
compute the correlation
Request a stable sort. Mergesort takes more memory (~40MB per  megafeature on x86-64).
Now perform some acrobatics to set the right named parameter in  the selector
check if X has negative values. Doesn't play well with np.log.  zeroth component  1/cosh = sech  cosh(0) = 1.0
In the literature, this is `exp(E[log(theta)])`
diff on `component_` (only calculate it when `cal_diff` is True)
The next one is a copy, since the inner loop overwrites it.
Iterate between `doc_topic_d` and `norm_phi` until convergence
The optimal phi_{dwk} is proportional to  exp(E[log(theta_{dk})]) * exp(E[log(beta_{dw})]).
Contribution of document d to the expected sufficient  statistics for the M step.
In the literature, this is called `lambda`
In the literature, this is `exp(E[log(beta)])`
Run e-step in parallel
merge result
This step finishes computing the sufficient statistics for the  M-step.
E-step
update `component_` related variables
initialize parameters or check
normalize doc_topic_distr
compute E[log p(theta | alpha) - log q(theta | gamma)]
Compensate for the subsampling of the population of documents
E[log p(beta | eta) - log q (beta | lambda)]
some constant terms
handle corner cases first
This is the first partial_fit
Update stats - they are 0 if this is the fisrt step
NNDSVD initialization
and their norms
choose update
The following multiplication with a boolean array is more than twice  as fast as indexing into grad.
max(0.001, tol) to force alternating minimizations of W and H
stopping condition  as discussed in paper
L2 regularization corresponds to increase of the diagonal of HHt  adds l2_reg only on the diagonal  L1 regularization corresponds to decrease of each element of XHt
The following seems to be required on 64-bit Windows w/ Python 3.5.
so W and Ht are both in C order in memory
X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)
X_new = X * V = U * S * V^T * V = U * S
Handle n_components==None
Center data
flip eigenvectors' sign to enforce deterministic output
Get variance explained by singular values
Compute noise covariance using Probabilistic PCA model  The sigma2 maximum likelihood (cf. eq. 12.46)
Center data
sign flipping is done inside
Center data
Authors: Pierre Lafaye de Micheaux, Stefan van der Walt, Gael Varoquaux,           Bertrand Thirion, Alexandre Gramfort, Denis A. Engemann  License: BSD 3 clause
u (resp. s) contains the eigenvectors (resp. square roots of  the eigenvalues) of W * W.T
j is the index of the extracted component
builtin max, abs are faster than numpy counter parts.
Some standard non-linear functions.  XXX: these should be optimized, as they can be a bottleneck.
make interface compatible with other decompositions  a copy is required only for non whitened data
Centering the columns (ie the variables)
Whitening and preprocessing by PCA
see (13.6) p.267 Here X1 is white and data  in X has been projected onto a subspace by PCA
overwriting cov is safe
Not passing in verbose=max(0, verbose-1) because Lars.fit already  corrects the verbosity level.
Not passing in verbose=max(0, verbose-1) because Lars.fit already  corrects the verbosity level.
This ensure that dimensionality of code is always 2,  consistant with the case n_jobs > 1
Enter parallel code block
Avoid integer division problems
Fortran-order dict, as we are going to access its row vectors
If max_iter is 0, number of iterations returned should be zero
Cost function
assert(dE >= -tol * errors[-1])  A line return
Avoid integer division problems
If n_iter is zero, we need to return zero.
Update dictionary  XXX: Can the residuals be of any use?
Maybe we need a stopping criteria based on the amount of  modification in the dictionary
XXX : kwargs is not documented
If sparse and not csr or csc, convert to csr
svds doesn't abide by scipy.linalg.svd/randomized_svd  conventions, so reverse its outputs.
center kernel
compute eigenvectors
sort eigenvectors in descending order
remove eigenvectors with a zero eigenvalue
handle corner cases first
All elements are equal, but some elements are more equal than others.
We need a lot of components for the reconstruction to be "almost  equal" in all positions. XXX Test means or sums instead?
Assert the 1st component is equal
Assert that 20 components has higher explained variance than 10
Assert that all the values are greater than 0
Assert that total explained variance is less than 1
Compare sparse vs. dense
histogram kernel produces singular matrix inside linalg.solve  XXX use a least-squares approximation?
non-regression test: previously, gamma would be 0 by default,  forcing all eigenvalues to 0 under the poly kernel
transform new data
inverse transform
X_fit_ needs to retain the old, unmodified copy of X
transform new data
n_components=None (default) => remove_zero_eig is True
Test the linear separability of the first 2D KPCA transform
2D nested circles are not linearly separable
Project the circles data into the first 2 components of a RBF Kernel  PCA model.  Note that the gamma value is data dependent. If this test breaks  and the gamma value has to be updated, the Kernel PCA example will  have to be updated too.
The data is perfectly linearly separable in that space
PCA on dense arrays
Test get_covariance and get_precision
test explained_variance_ratio_ == 1 with all components
PCA on dense arrays
Loop excluding the extremes, invalid inputs for arpack
Test get_covariance and get_precision
PCA on dense arrays
Loop excluding the 0, invalid for randomized
Test get_covariance and get_precision
test if we avoid numpy warnings for computing over empty arrays
Check that PCA output has unit-variance
the component-wise variance is thus highly varying:
test fit_transform
in that case the output components still have varying variances  we always center, so no test for non-centering.
Ignore warnings from switching to more power iterations in randomized_svd  Check that PCA output has unit-variance
compare to empirical variances
Same with correlated data
Test that the projection of data can be inverted
Test that randomized PCA is inversible on dense data
Test that it gives different scores if whiten=True
case: n_components >= .8 * min(X.shape) => 'full'
Smoke test for the case of more components than features.
Test that sparse matrices are accepted as input
Test that the function is called in the same way, either directly  or through the NMF class
Y is defined by : Y = UV + noise
Test that CD gives similar results
Test that SparsePCA won't return NaN when there is 0 feature in all  samples.
Test that CD gives similar results
function as fun arg
Check that the mixing model described in the docstring holds:
test for issue 697
Test the FastICA algorithm on very simple data.
Mixing matrix
Check that the mixing model described in the docstring holds:
Check that we have estimated the original sources
reversibility test in non-reduction case
Incremental PCA on dense arrays.
Get the reconstruction of the generated data X  Note that Xt has the same "components" as X, just separated  This is what we want to ensure is recreated correctly
Normalize
Make sure that the first element of Yt is ~1, this means  the reconstruction worked as expected
Test that the projection of data can be inverted.
same check that we can find the original data from the transformed  signal (since the data is almost of rank n_components)
Test that n_components is >=1 and <= n_features.
Test that fit and partial_fit get equivalent results.
Test that IncrementalPCA and PCA are approximate (to a sign flip).
Test that IncrementalPCA and PCA are approximate (to a sign flip).
test verbosity
Ignore warnings from switching to more power iterations in randomized_svd  Test FactorAnalysis ability to recover the data covariance structure
Some random settings for the generative model  latent variable of dim 3, 20 of it  using gamma to model different noise variance  per component
generate observations  wlog, mean is 0
Sample Covariance
default prior parameter should be `1 / topics`  and verbose params should not affect result
Test LDA batch learning_offset (`fit` method with 'batch' learning)
Find top 3 words in each LDA component
Find top 3 words in each LDA component
Find top 3 words in each LDA component
test `_check_params` method
test pass dense matrix with sparse negative input.
Test the relationship between LDA score and perplexity
normalize 'votes' into real [0,1] probabilities
Mask mapping each class to its members.  Number of clusters in each class.
callable metric is only valid for brute force and ball_tree
For cross-validation routines to split data correctly
Include an extra neighbor to account for the sample itself being  returned, which is removed later
argpartition doesn't guarantee sorted order, so we sort again
If the query data is the same as the indexed data, we would like  to ignore the first nearest neighbor of every sample, i.e  the sample itself.
Corner case: When the number of duplicates are more  than the number of neighbors, the first NN will not  be the sample, but a duplicate.  In that case mask the first duplicate.
kneighbors does the None handling.
construct CSR matrix representation of the k-NN graph
See https://github.com/numpy/numpy/issues/5456  if you want to understand why this is initialized this way.
If the query data is the same as the indexed data, we would like  to ignore the first nearest neighbor of every sample, i.e  the sample itself.
run the choose algorithm code so that exceptions will happen here  we're using clone() in the GenerativeBayes classifier,  so we can't do this kind of logic in __init__
XXX: perhaps non-copying operation better
needed since _fit_X[np.array([])] doesn't work if _fit_X sparse
Called once on fitting, output is independent of hashes
Number of candidates considered including duplicates  XXX: not sure whether this is being calculated correctly wrt       duplicates from different iterations through a single tree
Creates a g(p,x) for each tree
Calculate hashes of shape (n_samples, n_estimators, [hash_size])
descend phase
Checks whether desired number of neighbors are returned.  It is guaranteed to return the requested number of neighbors  if `min_hash_match` is set to 0. Returned distances should be  in ascending order.
Test unfitted estimator
Desired number of neighbors should be returned.
Test unfitted estimator
Select a random point in the dataset as the query
At least one neighbor should be returned when the radius is the  mean distance from the query to the points of the dataset.
All distances to points in the results of the radius query should  be less than mean_dist
Multiple points
dists and inds should not be 1D arrays or arrays of variable lengths  hence the use of the object dtype.
Radius-based queries do not sort the result points and the order  depends on the method, the random_state and the dataset order. Therefore  we need to sort the results ourselves before performing any comparison.
Distances to exact neighbors are less than or equal to approximate  counterparts as the approximate radius query might have missed some  closer neighbors.
Build an exact nearest neighbors model as reference model to ensure  consistency between exact and approximate methods
Build a LSHForest model with hyperparameter values that always guarantee  exact results on this toy dataset.
define a query aligned with the first axis
Compute the exact cosine distances of the query to the four points of  the dataset
The first point is almost aligned with the query (very small angle),  the cosine distance should therefore be almost null:
The second point form an angle of 45 degrees to the query vector
The third point is orthogonal from the query vector hence at a distance  exactly one:
The last point is almost colinear but with opposite sign to the query  therefore it has a cosine 'distance' very close to the maximum possible  value of 2.
If we query with a radius of one, all the samples except the last sample  should be included in the results. This means that the third sample  is lying on the boundary of the radius query:
Checks whether returned neighbors are from closest to farthest.
Returned neighbors should be from closest to farthest, that is  increasing distance values.
Checks whether `fit` method sets all attribute values correctly.
Checks whether inserting array is consistent with fitted data.  `partial_fit` method should set all attribute values correctly.
Test unfitted estimator
Insert wrong dimension
size of _input_array = samples + 1 after insertion  size of original_indices_[1] = samples + 1  size of trees_[1] = samples + 1
For zero candidates
For candidates less than n_neighbors
Smoke tests for graph methods.
load and shuffle iris dataset
load and shuffle digits
Filter deprecation warnings.
Dist could be multidimensional, flatten it so all values  can be looped
Test unsupervised neighbors methods
test the types of valid input into NearestNeighbors
As a feature matrix (n_samples by n_features)
Check X=None in prediction
Must raise a ValueError if the matrix is not of correct shape
Test unsupervised radius-based query
Test prediction with y_str
Test k-NN classifier on multioutput data
Multioutput prediction
Test k-NN classifier on multioutput data
Multioutput prediction
Check proba
Test k-neighbors in multi-output regression with uniform weight
Test kneighbors_graph to build the k-Nearest Neighbor graph.
n_neighbors = 1
Test kneighbors_graph to build the k-Nearest Neighbor graph  for sparse input.
Test radius_neighbors_graph to build the Nearest Neighbor graph.
Test radius_neighbors_graph to build the Nearest Neighbor graph  for sparse input.
Test bad argument values: these should all raise ValueErrors
Test computing the neighbors for various metrics  create a symmetric matrix
KD tree doesn't support all metrics
Find a reasonable radius.
Test kneighbors et.al when query is None
don't check indices here: if there are any duplicate distances,  the indices may not match.  Distances should not have this problem.
Test if BallTree with callable metric is picklable
simultaneous sort rows using function
simultaneous sort rows using numpy
make boolean arrays: ones and zeros
Check if both callable metric and predefined metric initialized  DistanceMetric object is picklable
don't check indices here: if there are any duplicate distances,  the indices may not match.  Distances should not have this problem.
simultaneous sort rows using function
simultaneous sort rows using numpy
draw a tophat sample
check that samples are in the right range
5 standard deviations is safe for 100 samples, but there's a  very small chance this test could fail.
Smoke test for various metrics and algorithms
toy sample
Check classification on a toy dataset, including sparse versions.
Same test, but with a sparse matrix to fit and test.
Fit with sparse, test with non-sparse
Fit with non-sparse, test with sparse
Fit and predict with non-CSR sparse matrices
classification
repeat input validation for grid search (which calls set_params)
remove the color dimension if useless
divide by the amount of overlap  XXX: is this the most efficient way? memory-wise yes, cpu wise?
handle stop words
normalize white spaces
normalize white spaces
triggers a parameter validation
Alias transform to fit_transform for convenience
Add a new value when a new vocabulary item is seen
Ignore out-of-vocabulary items for fixed_vocab=True
disable defaultdict behaviour
We intentionally don't call the transform method to make  fit_transform overridable without unwanted side effects in  TfidfVectorizer.
use the same matrix-building strategy as fit_transform
We need CSR format for fast row manipulations.
We need to convert X to a matrix, so that the indexing  returns 2D objects
perform idf smoothing if required
log+1 instead of log makes sure terms with zero idf don't get  suppressed entirely.
preserve float family dtype
convert counts or binary occurrences to floats
*= doesn't work
X is already a transformed view of raw_documents so  we set copy to False
Test delayed input validation in fit (useful for grid search).
Assert that no zeros are materialized in the output.
Negative elements are the diagonal: the elements of the original  image. Positive elements are the values of the gradient, they  should all be equal on grad_x and grad_y
Checking that the function works whatever the type of mask is
Newer versions of scipy have face in misc
Newer versions of scipy have face in misc
Newer versions of scipy have face in misc
make a collection of faces
CSR matrices can't be compared for equality
check that the memory layout does not impact the resulting vocabulary
check some classical latin accentuated symbols
mix letters accentuated and not
check some classical latin accentuated symbols
mix letters accentuated and not
decode_error default to strict, so this should fail  First, encode (as bytes) a unicode string.
Then let the Analyzer try to decode it as ascii. It should fail,  because we have given it an incorrect encoding.
fit on stopwords only
check normalization
check normalization
the lack of smoothing make IDF fragile in the presence of feature with  only zeros
raw documents as an iterator
build a vectorizer v1 with the same vocabulary as the one fitted by v1
compare that the two vectorizer give the same output on the test sample
stop word from the fixed list
stop word found automatically by the vectorizer DF thresholding  words that are high frequent across the complete corpus are likely  to be not informative (either real stop words of extraction  artifacts)
test tf-idf with new data
test tf alone
test idf transform with unlearned idf vector
test idf transform with incompatible n_features
L1-normalized term frequencies sum to one
test the direct tfidf vectorizer  (equivalent to term count vectorizer + tfidf transformer)
test the direct tfidf vectorizer with new data
test transform on unfitted vectorizer with empty vocabulary
ascii preprocessor?
error on bad strip_accents param
error with bad analyzer type
Check that the rows are normalized
ngrams generate more non zeros
makes the feature values bounded
Check that the rows are normalized
test for Value error on unfitted/empty vocabulary
test bounded number of extracted features
The most common feature is "the", with frequency 7.
check the ability to change the dtype
check the ability to change the dtype
raw documents
label junk food as -1, the others as +1
split the dataset for model development and final evaluation
find the best parameters for both the feature extraction and the  classifier
Check that the best model found by grid search is 100% correct on the  held out evaluation set.
on this toy dataset bigram representation which is used in the last of  the grid_search is considered the best estimator since they all converge  to 100% accuracy models
raw documents
label junk food as -1, the others as +1
split the dataset for model development and final evaluation
find the best parameters for both the feature extraction and the  classifier
Check that the best model found by grid search is 100% correct on the  held out evaluation set.
raw documents
label junk food as -1, the others as +1
No collisions on such a small dataset
When norm is None and non_negative, the tokens are counted up to  collisions
np.nan can appear when using pandas to load text fields from a csv file  with missing values.
Non-regression test: TfidfVectorizer used to ignore its "binary" param.
Process everything as sparse regardless of setting
XXX we could change values to an array.array as well, but it  would require (heuristic) conversion of dtype to typecode...
COO matrix is not subscriptable
XXX remove in 0.19 when r2_score default for multioutput changes
shallow copy of steps
check if first estimator expects pairwise input
if we have a weight for this transformer, multiply output
Determine output settings
reshape is necessary to preserve the data contiguity against vs  [:, np.newaxis] that does not.
Check parameters
Set min_weight_leaf from min_weight_fraction_leaf
Allow presort to be 'auto', which means True if the dataset is dense,  otherwise it will be False.
Use BestFirst if max_leaf_nodes given; use DepthFirst otherwise
Classification
Regression
Initialize saturation & value; calculate chroma & value shift
Generate the node content string
Should labels be shown?
Write node ID
Clean up any trailing newlines
Add node with description
color cropped nodes grey
Add edge to parent
The depth of each node for plotting with 'leaf' option  The colors to render each node with
Now recurse the tree and add node & edge attributes
Check correctness of export_graphviz
Test multi-output with weighted samples
Test regression output with plot_options
Test classifier with degraded learning set
Check for errors of export_graphviz
Check feature_names error
Check class_names error
also load the boston dataset  and randomly permute it
Check classification on a weighted toy dataset.
Check on a XOR problem
Check the array representation.  Check resize
Check when y is pure.
Check variable importances.
Check if variable importance before fit raises ValueError.
use values of max_features that are invalid
Test that it gives proper exception on deficient input.  predict before fit
Wrong dimensions
Test with arrays that are non-contiguous.
predict before fitting
predict on vector with different dims
wrong sample shape
apply before fitting
test both DepthFirstTreeBuilder and BestFirstTreeBuilder  by setting max_leaf_nodes
test for integer parameter  count samples on nodes, -1 means it is a leaf
test for float parameter  count samples on nodes, -1 means it is a leaf
Test if leaves contain more than leaf_count training examples
test both DepthFirstTreeBuilder and BestFirstTreeBuilder  by setting max_leaf_nodes
drop inner nodes
Check on dense input
Check on sparse input
Test that n_classes_ and classes_ have proper shape.  Classification, single output
Check class rebalancing.
Check that it works no matter the memory layout
Nothing
Contiguous
csr matrix
csc_matrix
Check sample weighting.  Test that zero-weighted samples are not taken into account
Test that low weighted samples are not taken into account at low depth
Test that sample weighting is the same as having duplicates
Check sample weighting raises errors.
Test if class_weight raises errors and warnings when expected.
Invalid preset string
Not a list or preset for multi-output
Incorrect length list for multi-output
Sanity check: we cannot request more memory than the size of the address  space. Currently raises OverflowError.
Non-regression test: MemoryError used to be dropped by Cython  because of missing "except *".
Gain testing time
Check the default (depth first search)
Due to numerical instability of MSE and too strict test, we limit the  maximal depth
n_samples set n_feature to ease construction of a simultaneous  construction of a csr and csc matrix
Ensure that X_sparse_test owns its data, indices and indptr array
Ensure that we have explicit zeros
Perform the comparison
Private function to keep pretty printing in nose yielded tests
Assert that leaves index are correct
Ensure only one leave node per sample
Ensure max depth is consistent with sum of indicator
Currently we don't support sparse y
Authors: Clay Woolam <clay@woolam.org>  Licence: BSD
kernel parameters
clamping factor
actual graph construction (implementations should override this)
label construction  construct a categorical distribution for classification only
initialize distributions
clamp
set the transduction item
this one has different base parameters
iterate over the collected file path to load the jpeg files as numpy  arrays
Checks if jpeg reading worked. Refer to issue 3594 for more  details.
average the color channels to compute a gray levels  representation
wrap the loader in a memoizing function that will return memmaped data  arrays for optimal memory usage
load and memoize the pairs as np arrays
pack the results as a Bunch instance
wrap the loader in a memoizing function that will return memmaped data  arrays for optimal memory usage
load and memoize the pairs as np arrays
pack the results as a Bunch instance
selected abnormal samples:
select all samples with positive logged_in attribute:
The zlib compression format use by joblib is not compatible when  switching from Python 2 to Python 3, let us use a separate folder  under Python 3:
Backward compat for Python 2 users
Samples in X are ordered with sample_id,  whereas in y, they are ordered with sample_id_bis.
save category names in a list, with same order than y
reorder categories in lexicographic order
Python 2
Python 3
Numpy recarray wants Python 2 str but not unicode
Numpy recarray wants Python 3 str but not bytes...
x coordinates of the grid cells  y coordinates of the grid cells
Define parameters for the data files.  These should not be changed  unless the data model changes.  They will be saved in the npz file  with the downloaded data.
Python 2
Python 3+
normalize dataset name
check if this data set has been already downloaded
load dataset matlab file
flatten column names
if target or data names are indices, transform then into names
1) there is only one array => it is considered data  2) there are multiple arrays
Download is not complete as the .tar.gz file is removed after  download.
Use an object array to shuffle: avoids memory copy
we shuffle but use a fixed seed for the memoization
the data is stored as int16 for compactness  but normalize needs floats
XXX remove closing when Python 2.7+/3.1+ required
convert from array.array, give data the right dtype
Python 2
Python 3
Grab the module-level docstring to use as a description of the  dataset
convert to array for fancy indexing
last column is target value
Initialize X and y
Initially draw informative features from the standard normal
Create each cluster; a variant of make_blobs
Fill useless features
Randomly replace labels
Randomly shift and scale
Randomly permute samples
Randomly permute features
pick a nonzero number of labels per document by rejection sampling
pick a non-zero document length by rejection sampling
generate a document of length n_words  if sample does not belong to any class, generate noise word
Randomly generate a well conditioned input set
Randomly generate a low rank, fat tail input set
Generate a ground truth model with only n_informative features being non  zeros (the other features are not correlated to y and should be ignored  by a sparsifying regularizers such as L1 or elastic net)
Add noise
Randomly permute samples and features
Index of the singular values
generate dictionary
encode signal
Permute the lines: we don't want to have asymmetries in the final  SPD matrix
Form the diagonal vector into a row matrix
Build multivariate normal distribution
Sort by distance from origin
Label by quantile
Python 2
Python 3+
Grab the module-level docstring to use as a description of the  dataset
Columns are not in the same order compared to the previous  URL resource on lib.stat.cmu.edu
avg rooms = total rooms / households
avg bed rooms = total bed rooms / households
avg occupancy = population / households
target in units of 100,000
get_data_home will point to a pre-existing folder
clear_data_home will delete both the content and the folder it-self
if the folder is missing it will be created again
Create very separate clusters; check that vertices are unique and  correspond to classes
Cluster by sign, viewed as strings to allow uniquing
Also test return_distributions and return_indicator with True
Test that y ~= np.dot(X, c) + bias + N(0, 1.0).
Test that y ~= np.dot(X, c) + bias + N(0, 1.0)
Check that the number of filenames is consistent with data/target
This test is slow.
create temporary dir
remove temporary dir
create fake data set in cache
transposing the data array
test sparsity
test shuffling and subset
The first 23149 samples are the training samples
test some precise values
test X's shape
test can change X's values
test y
test loading from file descriptor
test X'shape
21 features in file
because we "close" it manually and write to it,  we need to remove it manually.
because we "close" it manually and write to it,  we need to remove it manually.
in python 3 integers are valid file opening arguments (taken as unix  file descriptors)
slicing a csr_matrix can unsort its .indices, so test that we sort  those correctly
make sure y's shape is: (n_samples, n_labels)  when it is sparse
allow a rounding error at the last decimal place
allow a rounding error at the last decimal place
XXX we have to update this to support Python 3.x
The data is croped around the center as a rectangular bounding box  around the face. Colors are converted to gray levels:
the target is array of person integer ids
names of the persons can be found using the target_names array
It is possible to ask for the original data without any croping or color  conversion and not limit on the number of picture per person
The data is croped around the center as a rectangular bounding box  around the face. Colors are converted to gray levels:
the target is whether the person is the same or not
names of the persons can be found using the target_names array
It is possible to ask for the original data without any croping or color  conversion
the ids and class names are the same as previously
Strip trailing space to avoid nightmare in doctests
fetch the constructor or the original constructor before  deprecation wrapping if any  No explicit constructor to introspect
Simple optimisation to gain speed (inspect is slow)
non-optimized default implementation; override when a better  method is possible for a given clustering algorithm
non-optimized default implementation; override when a better  method is possible for a given clustering algorithm  fit method of arity 1 (unsupervised transformation)  fit method of arity 2 (supervised transformation)
We can't have more than one value on y_type => The set is no more needed
No metrics support "multiclass-multioutput" format
intersect y_pred, y_true with labels, eliminate items not in labels  also eliminate weights of eliminated items
If there is no label, it results in a Nan instead, we set  the jaccard to 1: lim_{x->0} x/x = 1  Note with py2.6 and np 1.3: we can't check safely for nan.
remove infs
build appropriate warning  E.g. "Precision and F-score are ill-defined and being set to 0.0 in  labels with no predicted samples"
Only negative labels
calculate weighted counts
labels are now from 0 to len(labels) - 1 -> use bincount
Pathological case
Retain only selected labels
Clipping
This happens in cases when elements in y_pred have type "str".
If y_pred is of single dimension, assume y_true to be binary  and then check.
Renormalize
Handles binary class case  this code assumes that positive and negative labels  are encoded as +1 and -1 respectively
The hinge_loss doesn't penalize good enough predictions.
the exact version is faster for k == 2: use it by default globally in  this module instead of the float approximate variant
Compute the ARI using the contingency data
log(a / b) should be calculated as log(a) - log(b) for  possible loss of precision
B contains 2 of the 3 biclusters in A, so score should be 2/3
Assert 1 < n_labels < n_samples
Check that adjusted scores are almost zero on random labels
Test for regression where contingency cell exceeds 2**16  leading to overflow in np.outer, resulting in EMI > 1
For sample i, store the mean distance of the cluster to which  it belongs in intra_clust_dists[i]
For sample i, store the mean distance of the second closest  cluster in inter_clust_dists[i]
Find inter_clust_dist for all samples belonging to the same  label.
Leave out current sample.
Now iterate over all other labels, finding the mean  cluster distance that is closest to every sample.
pass None as weights to np.average: uniform mean
pass None as weights to np.average: uniform mean
return scores individually
passing to np.average() None as weights results is uniform mean
For multi-output multi-class estimator
Standard regression scores
Standard Classification Scores
Score functions that need decision values
Score function for probabilistic classification
Clustering scores
reorder the data points according to the x axis and using y to  break ties
Reductions such as .sum used internally in np.trapz do not return a  scalar by default for numpy.memmap instances contrary to  regular numpy.ndarray instances.
make y_true a boolean vector
sort scores and corresponding truth values
y_score typically has many tied values. Here we extract  the indices associated with the distinct values. We also  concatenate a value for the end of the curve.  We need to use isclose to avoid spurious repeated thresholds  stemming from floating point roundoff errors.
stop when full recall attained  and reverse the outputs so recall is decreasing
Add an extra threshold position if necessary
doctest: +ELLIPSIS
If all labels are relevant or unrelevant, the score is also  equal to 1. The label ranking has no meaning.
if the scores are ordered, it's possible to count the number of  incorrectly ordered paires in linear time by cumulatively counting  how many false labels of a given score have a score higher than the  accumulated true labels with lower score.
When there is no positive or no negative labels, those values should  be consider as correct, i.e. the ranking doesn't matter.
swap average_weight <-> score_weight
Average the results
Ensure that distances between vectors and themselves are set to 0.0.  This may not be the case due to floating point rounding errors.
Allocate output arrays
Update indices and minimum values using chunk
1.0 - cosine_similarity(X, Y) without copy
Helper functions - distance  If updating this dictionary, update the doc in both distance_metrics()  and also in pairwise_distances()!
Special case to avoid picklability checks in delayed
Make symmetric  NB: out += out.T will produce incorrect results
Calculate diagonal  NB: nonzero diagonals are allowed for both metrics and kernels
import GPKernel locally to prevent circular imports
Test that a value error is raised if the metric is unknown
Test always returns float dtype
Test converts list to array-like
Not all metrics support sparse input  ValueError may be triggered by bad callable
paired_distances should allow callable metric where metric(x, x) != 0  Knowing that the callable is a strict metric would allow the diagonal to  be left uncalculated and set to 0.
Callable version of pairwise.rbf_kernel.
callable function, X=Y
Test that a value error is raised when the lengths of X and Y should not  differ
Check pairwise minimum distances computation for any metric
Non-euclidean Scipy distance (callable)
Compare with naive implementation
Check the pairwise Euclidean distances computation
Check the paired Euclidean distances computation
Check the paired manhattan distances computation
test negative input
different n_features in X and Y
sparse matrices
the diagonal elements of a linear kernel are their squared norm
the diagonal elements of a rbf kernel are 1
the diagonal elements of a laplacian kernel are 1
off-diagonal elements are < 1 but > 0:
Turns a numpy matrix (any n-dimensional array) into tuples.  Tuplify each sub-array in the input.  Single dimension input, just return tuple of contents.
both float32
mismatched A
mismatched B
Those metrics don't support binary inputs
Those metrics don't support multiclass inputs
Metric undefined with "binary" or "multiclass" input
Metrics with an "average" argument
Threshold-based metrics with an "average" argument
Metrics with a "pos_label" argument
pos_label support deprecated; to be removed in 0.18:
Metrics with a "normalize" option
Threshold-based metrics with "multilabel-indicator" format support
Classification metrics with  "multilabel-indicator" format
Regression metrics with "multioutput-continuous" format support
Symmetric with respect to their input arguments y_true and y_pred  metric(y_true, y_pred) == metric(y_pred, y_true).
Asymmetric with respect to their input arguments y_true and y_pred  metric(y_true, y_pred) != metric(y_pred, y_true).
No Sample weight support
not work for confusion_matrix, as its output is a  matrix instead of a number. Testing of  confusion_matrix with sample_weight is in  test_classification.py
Test the symmetry of score and loss functions
We shouldn't forget any metrics
Symmetric metric
Mix format support
NB: We do not test for y1_row, y2_row as these may be  interpreted as multilabel or multioutput data.
Ensure that classification metrics with string labels
Ugly, but handle case with a pos_label and label
Ugly, but handle case with a pos_label and label
Non-regression test: scores should work with a single sample.  This is important for leave-one-out cross validation.  Score functions tested are those that formerly called np.squeeze,  which turns an array of size 1 into a 0-d array (!).
assert that no exception is thrown
Those metrics are not always defined with one sample  or in multiclass classification
Generate some data
To make sure at least one empty label is present
XXX cruel hack to work with partial functions
Check representation invariance
Test in the binary case
Test in the multilabel case
for both random_state 0 and 1, y_true and y_pred has at least one  unlabelled entry
To make sure at least one empty label is present
No averaging
Micro measure
Macro measure
Weighted measure
Test _average_binary_score for weight.sum() == 0
check that the score is invariant under scaling of the weights by a  common factor
Check that if sample_weight.shape[0] != y_true.shape[0], it raised an  error
GC closes the mmap file descriptors
Test that all scorers have a working repr
check that cross_val_score definitely calls the scorer  and doesn't make any assumptions about the estimator apart from having a  fit.
Sanity check on the make_scorer factory function.
test fbeta score that takes an argument
test that custom scorer can be pickled
smoke test the repr:
Test that the scorer work with multilabel-indicator format  for multilabel and multi-output multi-class classifier
get sensible estimators for each metric
Non-regression test for 6147: some score functions would  return singleton memmap when computed on memmap data instead of scalar  float values.
import some data to play with
restrict to a binary classification task
add noisy features to make the problem harder and avoid perfect results
run classifier, get class probabilities and label predictions
only interested in probabilities of the positive case  XXX: do we really want a special API for the binary case?
Test Area under Receiver Operating Characteristic (ROC) curve
Test whether the returned threshold matches up with tpr  make small toy dataset
compare tpr and tpr_correct to see if the thresholds' order was correct
Test to ensure that we don't return spurious repeating thresholds.  Duplicated thresholds can arise due to machine precision issues.
This random forest classifier can only return probabilities  significant to two decimal places
How well can the classifier predict whether a digit is less than 5?  This task contributes floating point roundoff errors to the probabilities
Check for repeating values in the thresholds
roc_curve not applicable for multi-class problems
roc_curve for confidence scores
roc_curve for hard decisions
Incompatible shapes
Too few x values
x is not in order
y_true contains three different class values
Use {-1, 1} for labels; make sure original labels aren't modified
Contains non-binary labels
Test that average_precision_score and roc_auc_score are invariant by  the scaling or shifting of probabilities
No relevant labels
Only relevant labels
Degenerate case: only one label
Check tie handling in score  Basic check with only ties and increasing label space
Check that Label ranking average precision works for various  Basic check with increasing label space size and decreasing score
The best rank correspond to 1. Rank higher than 1 are worse.  The best inverse ranking correspond to n_labels.
Rank need to be corrected to take into account ties  ex: rank 1 ex aequo means that both label are rank 2.
Let's count the number of relevant label with better rank  (smaller rank).
Weight by the rank of the actual label
Score with ties
import some data to play with
restrict to a binary classification task
add noisy features to make the problem harder and avoid perfect results
run classifier, get class probabilities and label predictions
only interested in probabilities of the positive case  XXX: do we really want a special API for the binary case?
Dense label indicator matrix format
Test Precision Recall and F1 Score for binary classification task
ensure the above were meaningful tests:
Test that average_precision_score function returns an error when trying  to compute average_precision_score for multiclass task.
y_true contains three different class values
Bad beta
Bad pos_label
Bad average option
Test confusion matrix - binary classification case
Add spurious labels and ignore them.
corrcoef of same vectors must be 1
corrcoef, when the two vectors are opposites of each other, should be -1
For the zero vector case, the corrcoef cannot be calculated and should  result in a RuntimeWarning
But will output 0
And also for any other vector with 0 variance
But will output 0
Check that sample weight is able to selectively exclude  Now the first half of the vector elements are alone given a weight of 1  and hence the mcc will not be a perfect 0 as in the previous case
Test Precision Recall and F1 Score for multiclass classification task
averaging tests
compute scores with default labels introspection
Test confusion matrix - multi-class case
compute confusion matrix with default labels introspection
Test confusion matrix - multi-class case with subset of labels
compute confusion matrix with only first two labels considered
compute confusion matrix with explicit label ordering for only subset  of labels
Test performance report
Test performance report with added digits in floating point values
Dense label indicator matrix format
sp_hamming only works with 1-D arrays
Dense label indicator matrix format
Ensure warning if f1_score et al.'s average is implicit for multiclass
check binary passes without warning
check that we got all the shapes and axes right  by doubling the length of y_true and y_pred
mean_absolute_error and mean_squared_error are equal because  it is a binary problem.
evecs /= np.linalg.norm(evecs, axis=0)   doesn't work with numpy 1.6
1) within (univariate) scaling by with classes std-dev  avoid division by zero in normalization
2) Within variance scaling  SVD of centered (within)scaled data
Scaling of within covariance is: V' 1/S
OvR normalization, like LibLinear's predict_probability
handle special case of two classes
compute the likelihood of the underlying gaussian models  up to a multiplicative constant.  compute posterior probabilities
XXX : can do better to avoid precision overflows
Objective: C (Kullback-Leibler divergence of P and Q)
Objective: C (Kullback-Leibler divergence of P and Q)
Degrees of freedom of the Student's t-distribution. The suggestion  degrees_of_freedom = n_components - 1 comes from  "Learning a Parametric Embedding by Preserving Local Structure"  Laurens van der Maaten, 2009.  the number of nearest neighbors to find
Use the precomputed distances to find  the k nearest neighbors and their distances
Find the nearest neighbors for every point  LvdM uses 3 * perplexity as the number of neighbors  And we add one to not count the data point itself  In the event that we have very small  of points  set the neighbors to n - 1
Initialize embedding randomly
Don't always calculate the cost since that calculation  can be nearly as expensive as the gradient
Early exaggeration
Save the final number of iterations
Final optimization
Randomly choose initial configuration
overrides the parameter p
Compute distance and monotonic regression
similarities with 0 are considered as missing values
Compute stress
speed up row-wise access to boolean connection mask
sparse graph, find all the connected components
dense graph, find all connected components start from node 0
Whether to drop the first eigenvector
lobpcg used with eigen_solver='amg' has bugs for low number of nodes  for details see the source code in scipy:  https://github.com/scipy/scipy/blob/v0.11.0/scipy/sparse/linalg/eigen  /lobpcg/lobpcg.pyL237  or matlab:  http://www.mathworks.com/matlabcentral/fileexchange/48-lobpcg-m
currently only symmetric affinity_matrix supported
Not symmetric similarity matrix:
Not squared similarity matrix:
init not None and not correct format:
Isomap should preserve distances when all neighbors are used
grid of equidistant points in 2D, n_components = n_dim
distances from each point to all others
Same setup as in test_isomap_simple_grid, with an added dimension
grid of equidistant points in 2D, n_components = n_dim
add noise in a third dimension
compute input kernel
compute output kernel
make sure error agrees
Create S-curve dataset
Compute isomap embedding
Re-embed a noisy version of the points
Make sure the rms error on re-embedding is comparable to noise_scale
Test utility routines
note: ARPACK is numerically unstable, so this test will fail for        some random seeds.  We choose 2 because the tests pass.
re-embed a noisy version of X using the transform method
Test the error raised when parameter passed to lle is invalid
Test stopping conditions of gradient descent.
Test that the highest P_ij are the same when few neighbors are used
Test gradient of Kullback-Leibler divergence.
Test trustworthiness score.
Affine transformation
Early exaggeration factor must be >= 1.
Number of gradient descent iterations must be at least 200.
Precomputed distance matrices must be square matrices.
'init' must be 'pca' or 'random'.
Verbose options write to stdout.
t-SNE should allow metrics that cannot be squared (issue 3526).
When Barnes-Hut's angle=0 this corresponds to the exact method.
Introduce a point into a quad tree where a similar point already exists.  Test will hang if it doesn't complete.
check the case where points are arbitrarily close on both axes  close to machine epsilon - x axis
check the case where points are arbitrarily close on both axes  close to machine epsilon - y axis
Make sure translating between 1D and N-D indices are preserved
Connect all elements within the group at least once via an  arbitrary path that spans the group.
We should retrieve the same component mask by starting by both ends  of the group
connection
Test spectral embedding with amg solver
Test that SpectralClustering fails with an unknown eigensolver
Test that SpectralClustering fails with an unknown affinity type
Verify using manual computation with dense eigh
this might raise a LinalgError if G is singular and has trace  zero
build Hessian estimator
find the eigenvectors and eigenvalues of each local covariance  matrix. We want V[i] to be a [n_neighbors x n_neighbors] matrix,  where the columns are eigenvectors
choose the most efficient way to find the eigenvectors
find regularized weights: this is like normal LLE.  because we've already computed the SVD of each covariance matrix,  it's faster to use this rather than np.linalg.solve
calculate eta: the median of the ratio of small to large eigenvalues  across the points.  This is used to determine s_i, below
Now calculate M.  This is the [N x N] matrix whose null space is the desired embedding
select bottom s_i eigenvectors and calculate alpha
compute Householder matrix which satisfies   Hi*Vi.T*ones(n_neighbors) = alpha_i*ones(s)  using prescription from paper
compute n_components largest eigenvalues of Xi * Xi^T
wrap dictionary in a singleton list to support either dict  or list of dicts
Product function that can handle iterables (np.product can't).
This is used to make discrete sampling without replacement memory  efficient.  XXX: could memoize information used here
Reverse so most frequent cycling parameter comes first
Try the next grid
check if all distributions are given as lists  in this case we want to sample without replacement
look up sampled parameter settings in parameter grid
Out is a list of triplet: score, estimator, n_test_samples
Find the best parameters by comparing on the mean validation score:  note that `sorted` is deterministic in the way it breaks ties
We need this for the build_repr to work properly in py2.7  see 6304
Weight labels by their number of occurrences
Distribute the most frequent labels first
Total weight of each fold
Mapping from label index to fold index
Distribute samples by adding the largest weight to the lightest fold
We make a copy of labels to avoid side-effects during iteration
random partition
int values are checked during split based on the input
int values are checked during split based on the input
cannot compute the kernel values with custom function
training score becomes worse (2 -> 1), test error better (0 -> 1)
Smoke test
test with multioutput y
test with multioutput y
test with X and y as list
test with 3d X and
Check if ValueError (when labels is None) propagates to cross_val_score  and cross_val_predict  And also check if labels is correctly passed to the cv object
Error raised for non-square X
test error is raised when the precomputed kernel is not array-like  or sparse
Default score of the Ridge regression estimator
test with custom scoring object
set random y
Naive loop (should be same as cross_val_predict):
3 fold cv is used --> atleast 3 samples per class  Smoke test
test with multioutput y
test with multioutput y
test with X and y as list
The mockup does not have partial_fit()
Naive loop (should be same as cross_val_predict):
Special case: empty grid (useful to get default estimator settings)
Smoke test the score etc:
Test exception handling on scoring
smoketest grid search
check that best params are equal  check that we can call score and that it gives the correct result
giving no scoring function raises an error
ensure the test is sane
Check if ValueError (when labels is None) propagates to GridSearchCV  And also check if labels is correctly passed to the cv object
Should not raise an error
Test search over a "grid" with only one point.  Non-regression test: grid_scores_ wouldn't be set by GridSearchCV.
Test that grid search will capture errors on data with different  length
Test that grid search works with both dense and sparse matrices
Test that grid search works when the input features are given in the  form of a precomputed kernel matrix
compute the training kernel matrix corresponding to the linear kernel
compute the test kernel matrix
test error is raised when the precomputed kernel is not array-like  or sparse
Regression test for bug in refitting  Simulates re-fitting a broken estimator; this used to break with  sparse SVMs.
Pass X as list in GridSearchCV
Pass y as list in GridSearchCV
check cross_val_score doesn't destroy pandas dataframe
Now without a score, and without y
Make a dataset with a lot of noise to get various kind of prediction  errors across CV folds and parameter settings
Check the consistency with the best_score_ and best_params_ attributes
Test that a fit search can be pickled
Ensure that grid scores were set to zero as required for those fits  that are expected to fail.
refit=False because we want to test the behaviour of the grid search part
FailingClassifier issues a ValueError so this is what we look for.
Test if get_n_splits works correctly
Test if the repr works without any errors
Use python sets to get more informative assertion failure messages
Train and test split should not overlap
Check that the union of train an test split cover all the indices
Check that a all the samples appear at least once in a test fold
Check that the accumulated test samples cover the whole dataset
Check that errors are raised if there is not enough samples
Check that a warning is raised if the least populated class has too few  members.
Check that despite the warning the folds are still computed even  though all the classes are not necessarily represented at on each  side of the split at each split
Check that errors are raised if all n_labels for individual  classes are less than n_folds.
When shuffle is not  a bool:
Check all indices are returned in the test folds
Check all indices are returned in the test folds even when equal-sized  folds are not possible
Check if get_n_splits returns the number of folds
Manually check that KFold preserves the data ordering on toy datasets
Check if get_n_splits returns the number of folds
Check that KFold returns folds with balanced sizes (only when  stratification is possible)  Repeat with shuffling turned off and on
Check the indices are shuffled properly
Assert that there is no complete overlap
Set all test indices in successive iterations of kf2 to 1
Check that all indices are returned in the different test folds
Check that error is raised if there is a class with only one sample
Train size or test size too small
Test the StratifiedShuffleSplit, indices are drawn with a  equal chance
See https://github.com/scikit-learn/scikit-learn/issues/6121 for  the original bug report
Make sure the repr works
Test that the length is correct
Second test: train and test add up to all the data
Third test: train and test are disjoint
n_splits = no of 2 (p) label combinations of the unique labels = 3C2 = 3  n_splits = no of unique labels (C(uniq_lbls, 1) = n_unique_labels)
Use numpy.testing.assert_equal which recursively compares  lists of lists
Check if split works correctly
Check if get_n_splits works correctly
Parameters of the test
Check that folds have approximately the same size
Check that each label appears only in 1 fold
Get the test fold indices from the test set indices of each fold
Check that folds have approximately the same size
Test if nested cross validation works with different combinations of cv
Adjust length of sample weights
Ensure the estimator has implemented the passed decision function
Concatenate the predictions
Check for sparse predictions
Adjust length of sample weights
pass through: skip indexing
Make a list since we will be iterating multiple times over the folds
Because the lengths of folds can be significantly different, it is  not guaranteed that we use all of the available training data when we  use the first 'n_max_training_samples' samples.
split train and test
Naive-Bayes
Check that brier score has improved after calibration
Check invariance against relabeling [0, 1] -> [1, 2]
Check invariance against relabeling [0, 1] -> [-1, 1]
check that calibration can also deal with regressors that have  a decision_function
Check failure cases:  only "isotonic" and "sigmoid" should be accepted as methods
base-estimators should provide either decision_function or  predict_proba (most regressors, for instance, should fail)
LinearSVC does not currently support sample weights but they  can still be used for the calibration step (with a warning)
As the weights are used for the calibration, they should still yield  a different predictions
test multi-class setting with classifier that implements  only decision function
Use categorical labels to check that CalibratedClassifierCV supports  them correctly
Naive-Bayes
check that _SigmoidCalibration().fit only accepts 1d array or 2d column  arrays
probabilities outside [0, 1] should not be accepted when normalize  is set to False
Data is 6 random integer points in a 100 dimensional space classified to  three classes.
Test whether label mismatch between target y and classes raises  an Error  FIXME Remove this test once the more general partial_fit tests are merged
Sample weights all being 1 should not change results
Check that duplicate entries and correspondingly increased sample  weights yield the same result
Fit for the first time the GNB  Partial fit a second time with an incoherent X
Check the ability to predict the learning set.
Verify that np.log(clf.predict_proba(X)) gives the same results as  clf.predict_log_proba(X)
Partial fit on the whole data at once should be the same as fit too
Test input checks for the fit method  check shape consistency for number of samples at fit time
check shape consistency for number of input features at predict time
check shape consistency
classes is required for first call to partial fit
check consistency of consecutive classes values
check consistency of input shape for partial_fit
check consistency of input shape for predict
The 100s below distinguish Bernoulli from multinomial.  FIXME: write a test to show this.
check shape consistency for number of samples at fit time
coef_ and intercept_ should have shapes as in other linear models.  Non-regression test for issue 2127.
Multinomial NB
Bernoulli NB
Gaussian NB
Fit Bernoulli NB w/ alpha = 1.0
Check manual estimate matches
Classes are China (0), Japan (1)
Fit BernoulliBN w/ alpha = 1.0
Check the class prior is correct
Testing data point is:  Chinese Chinese Chinese Tokyo Japan
Check the predictive probabilities are correct
A few test classes
Check that clone raises an error on buggy estimators.
Regression test for cloning estimators with empty arrays
Regression test for cloning estimators with default parameter as np.nan
Smoke test the str of the base estimator
deprecated attribute should not show up as params
test both ClassifierMixin and RegressorMixin
delegation before fit raises an exception
smoke test delegation
training score becomes worse (2 -> 1), test error better (0 -> 1)
The mockup does not have partial_fit()
Check that params are set  Smoke test the repr:
Test with two objects
Check that we can't use the same stage name twice
Check that params are set  Smoke test the repr:
Check that params are not set when naming them wrong
Test clone
Check that apart from estimators, the parameters are the same
Remove estimators that where copied
Test pipeline raises set params error message for nested models.
expected error message
nested model check
check shapes of various prediction functions
test that the fit_predict method is implemented on a pipeline  test that the fit_predict on pipeline yields same results as applying  transform and clustering steps separately
first compute the transform and clustering step separately
check if it does the expected thing
test setting parameters
Test whether pipeline works with a transformer at the end.  Also test pipeline.transform and pipeline.inverse_transform
Test whether pipeline works with a transformer missing fit_transform
test fit_transform:
test that n_jobs work for FeatureUnion
fit_transform should behave the same
transformers should stay fit after fit_transform
Special case: empty grid (useful to get default estimator settings)
Smoke test the score etc:
Test exception handling on scoring
smoketest grid search
check that best params are equal  check that we can call score and that it gives the correct result
giving no scoring function raises an error
Test search over a "grid" with only one point.  Non-regression test: grid_scores_ wouldn't be set by GridSearchCV.
Test that grid search will capture errors on data with different  length
Test that grid search works with both dense and sparse matrices
Test that grid search works when the input features are given in the  form of a precomputed kernel matrix
compute the training kernel matrix corresponding to the linear kernel
compute the test kernel matrix
test error is raised when the precomputed kernel is not array-like  or sparse
Regression test for bug in refitting  Simulates re-fitting a broken estimator; this used to break with  sparse SVMs.
Pass X as list in GridSearchCV
Pass y as list in GridSearchCV
check cross_val_score doesn't destroy pandas dataframe
Now without a score, and without y
Make a dataset with a lot of noise to get various kind of prediction  errors across CV folds and parameter settings
Check the consistency with the best_score_ and best_params_ attributes
Test that a fit search can be pickled
Ensure that grid scores were set to zero as required for those fits  that are expected to fail.
refit=False because we want to test the behaviour of the grid search part
FailingClassifier issues a ValueError so this is what we look for.
Authors: Andreas Mueller <amueller@ais.uni-bonn.de>           Gael Varoquaux gael.varoquaux@normalesup.org  License: BSD 3 clause
Test that estimators are default-constructible, cloneable  and have working repr.
Meta sanity-check to make sure that the estimator introspection runs  properly
some can just not be sensibly default constructed
Tested in test_transformer_n_iter below
Multitask models related to ENet cannot handle  if y is mono-output.
The ProjectedGradientNMF class is deprecated
Dependent on external solvers and hence accessing the iter  param is non-trivial.
The ProjectedGradientNMF class is deprecated
The ProjectedGradientNMF class is deprecated
Check basic properties of random matrix generation
Check some statical properties of Gaussian random matrix  Check that the random matrix follow the proper distribution.  Let's say that each element of a_{ij} of A is taken from    a_ij ~ N(0.0, 1 / n_components).
Check some statical properties of sparse random matrix
tests on random projection transformer
remove 0 distances to avoid division by 0
remove 0 distances to avoid division by 0
check that the automatically tuned values for the density respect the  contract for eps: pairwise distances are preserved according to the  Johnson-Lindenstrauss lemma
when using sparse input, the projected data can be forced to be a  dense numpy array
the output can be left to a sparse matrix instead  output for dense input will stay dense:
output for sparse output will be sparse:
the number of components is adjusted from the shape of the training  set
once the RP is 'fitted' the projection is always the same
fit transform with same random seed will lead to the same results
Try to transform with an input X of size different from fitted.
Test multi target regression raises
no exception should be raised if the base estimator supports weights
train the multi_target_forest and also get the predictions.
train the forest with each column and assert that predictions are equal
test to check meta of meta estimators
train the forest with each column and assert that predictions are equal
compute exact kernel  abbreviations for easier formula
reduce to n_samples_x x n_samples_y by summing over features
approximate kernel mapping
test error is raised on negative input
test error on invalid sample_steps
test that the sample interval is set correctly
test that the sample_interval is initialized correctly
test that the sample_interval is changed in the fit method
test that the sample_interval is set correctly
compute exact kernel  abbreviations for easier formula
approximate kernel mapping
test error is raised on negative input
test that RBFSampler approximates kernel on random data  compute exact kernel
approximate kernel mapping
some basic tests
With n_components = n_samples this is exact
test that nystroem works with singular kernel matrix
Non-regression: Nystroem should pass other parameters beside gamma.
Test Nystroem on a callable.
Test either above import has failed for some reason  "import *" is discouraged outside of the module level, hence we  rely on setting up the variable above
We know that we can have division by zero
We know that we can have division by zero
Correctness oracle
Correctness oracle
Correctness oracle
Correctness oracle
test with 2d array
Correctness oracle
when strategy = 'mean'
import reload  Python 3+ import for reload. Builtin in Python2
Degenerate data with only one feature (still should be separable)
One element class
Assert that it works with 1D data
Primarily test for commit 2f34950 -- "reuse" of priors  LDA shouldn't be able to separate those
Test that priors passed as a list are correctly handled (run to see if  failure)
Test if the coefficients of the solvers are approximately the same.
NOTE: clf_lda_eigen.explained_variance_ratio_ is not of n_components  length. Make it the same length as clf_lda_svd.explained_variance_ratio_  before comparison.
arrange four classes with their means in a kite-shaped pattern  the longer distance should be transformed to the first component, and  the shorter distance to the second component.
Fit LDA and transform the means
the transformed within-class covariance should be the identity matrix
the means of classes 0 and 3 should lie on the first component
the means of classes 1 and 2 should lie on the second component
should be able to separate the data perfectly
QDA classification.  This checks that QDA implements fit and predict and returns  correct values for a simple toy dataset.
Assure that it works with 1D data
QDA shouldn't be able to separate those
Classes should have at least 2 elements
The default is to not set the covariances_ attribute
Test the actual attribute:
the default is reg_param=0. and will cause issues  when there is a constant variable
adding a little regularization fixes the problem
Case n_samples_in_a_class < n_features
ensure that we trigger DeprecationWarning even if the sklearn.lda  was loaded previously by another test.
ensure that we trigger DeprecationWarning even if the sklearn.qda  was loaded previously by another test.
make features correlated
Test that check_classification_target return correct type. 5782
we are doing something sensible
Test predict_proba
predict assigns a label if the probability that the  sample has the label is greater than 0.5.
Test that ovr works with classes that are always present or absent.  Note: tests is the case where _ConstantPredictor is utilised
Build an indicator matrix where two features are always on.  As list of lists, it would be: [[int(i >= 5), 2, 3] for i in range(10)]
y has a constantly absent label
test input as label indicator matrix
test input as label indicator matrix
decision function only estimator. Fails in current implementation.
Estimator with predict_proba disabled, depending on parameters.
predict assigns a label if the probability that the  sample has the label is greater than 0.5.
decision function only estimator. Fails in current implementation.
predict assigns a label if the probability that the  sample has the label is greater than 0.5.
Not fitted exception!  lambda is needed because we don't want coef_ to be evaluated right away
Doesn't have coef_ exception!
Compute the votes
Extract votes and verify
For each sample and each class, there only 3 possible vote levels  because they are only 3 distinct class pairs thus 3 distinct  binary classifiers.  Therefore, sorting predictions based on votes would yield  mostly tied predictions:
Classifiers are in order 0-1, 0-2, 1-2  Use decision_function to compute the votes and the normalized  sum_of_confidences, which is used to disambiguate when there is a tie in  votes.
For the first point, there is one vote per class  For the rest, there is no tie and the prediction is the argmax  For the tie, the prediction is the class with the highest score
test that ties can not only be won by the first two labels
Test that the OvO doesn't mess up the encoding of string labels
Check that we got increasing=True and no warnings
Check that we got increasing=True and no warnings
Check that we got increasing=False and no warnings
Check that we got increasing=False and no warnings
Check that we got increasing=False and CI interval warning
check we don't crash when all x are equal:
Set y and x for decreasing
Check that relationship decreases
Set y and x for decreasing
Check that relationship increases
Set y and x
Create model and fit
Check that an exception is thrown
Set y and x
Create model and fit
Set y and x
Create model and fit
Predict from  training and test x and check that we have two NaNs.
Set y and x
Create model and fit
Make sure that we throw an error for bad out_of_bounds value
Set y and x
Create model and fit
Make sure that we throw an error for bad out_of_bounds value in transform
Create model and fit
Get deterministic RNG with seed
Create regression and samples
Get some random weights and zero out
This will hang in failure case.
we also want to test that everything still works when some weights are 0
Build interpolation function with ALL input data, not just the  non-redundant subset. The following 2 lines are taken from the  .fit() method, without removing unnecessary points
fit with just the necessary data
https://github.com/scikit-learn/scikit-learn/issues/6628
avoid StratifiedKFold's Warning about least populated class in y
Use python sets to get more informative assertion failure messages
Train and test split should not overlap
Check that the union of train an test split cover all the indices
Check that a all the samples appear at least once in a test fold
Check that the accumulated test samples cover the whole dataset
Check that errors are raised if there is not enough samples
Check that a warning is raised if the least populated class has too few  members.
Check that despite the warning the folds are still computed even  though all the classes are not necessarily represented at on each  side of the split at each split
Check that errors are raised if all n_labels for individual  classes are less than n_folds.
When n is not integer:
When n_folds is not integer:
Check all indices are returned in the test folds
Check all indices are returned in the test folds even when equal-sized  folds are not possible
Manually check that KFold preserves the data ordering on toy datasets
Check the indices are shuffled properly, and that all indices are  returned in the different test folds
Parameters of the test
Check that folds have approximately the same size
Check that each label appears only in 1 fold
Check that folds have approximately the same size
Check that each label appears only in 1 fold
Check that no label is on both sides of the split
Should fail if there are more folds than labels
Check that error is raised if there is a class with only one sample
Check that error is raised if the test set size is smaller than n_classes  Check that error is raised if the train set size is smaller than  n_classes
Train size or test size too small
Test the StratifiedShuffleSplit, indices are drawn with a  equal chance
See https://github.com/scikit-learn/scikit-learn/issues/6121 for  the original bug report
Make sure the repr works
Test that the length is correct
Second test: train and test add up to all the data
Third test: train and test are disjoint
Smoke test
test with multioutput y
test with multioutput y
test with X and y as list
test with 3d X and
Error raised for non-square X
test error is raised when the precomputed kernel is not array-like  or sparse
X mock dataframe
test with custom scoring object
set random y
Check that iterating twice on the ShuffleSplit gives the same  sequence of train-test when the random_state is given
Naive loop (should be same as cross_val_predict):
Smoke test
test with multioutput y
test with multioutput y
test with X and y as list
Make a list since we will be iterating multiple times over the folds
Because the lengths of folds can be significantly different, it is  not guaranteed that we use all of the available training data when we  use the first 'n_max_training_samples' samples.
in-place tricks shouldn't have modified X
BernoulliRBM should work on small sparse matrices.
Sparse vs. dense should not affect the output. Also test sparse input  validation.
Test numerical stability (2785): would previously generate infinities  and crash with an exception.
Test that larger alpha yields weights closer to zero"""
Initialize parameters
Compute the number of layers
Pre-allocate gradient matrices
analytically compute the gradients
Test lbfgs on classification.  It should achieve a score higher than 0.95 for the binary and multi-class  versions of the digits dataset.
Test partial_fit on classification.  `partial_fit` should yield the same results as 'fit'for binary and  multi-class classification.
Test partial_fit on regression.  `partial_fit` should yield the same results as 'fit' for regression.
catch convergence warning
Test partial_fit error handling."""
no classes passed
l-bfgs doesn't support partial_fit
Test that invalid parameters raise value error"""
Test that predict_proba works as expected for binary class."""
Test that predict_proba works as expected for multi class."""
Iterate over the hidden layers
For the hidden layers
For the last layer
Forward propagate
Backward propagate
The calculation of delta[last] here works with following  combinations of output activation and loss function:  sigmoid and binary cross entropy, softmax and categorical cross  entropy, and identity with squared loss
Compute gradient for the last layer
set all attributes, allocate weights etc for first call  Initialize parameters
Compute the number of layers
Initialize coefficient and intercept layers
Use the initialization method recommended by  Glorot et al.
this was caught earlier, just to make sure
Make sure self.hidden_layer_sizes is a list
Validate input parameters.
Ensure y is 2D
First time training the model
Initialize lists
Run the Stochastic optimization algorithm
Run the LBFGS algorithm
Store meta information for the parameters
Save sizes and indices of coefficients for faster unpacking
Save sizes and indices of intercepts for faster unpacking
Run LBFGS
update weights
update no_improvement_count based on training loss or  validation score according to early_stopping
for learning rate that needs to be updated at iteration end
restore best weights
compute validation score, use that for stopping
update best parameters  use validation_scores_, not loss_curve_  let's hope no-one overloads .score with mse
Make sure self.hidden_layer_sizes is a list
Initialize layers
forward propagate
Run input checks
Force data to 2D numpy.array
Check shapes of DOE & observations
Run input checks
Check input shapes
Run input checks
Normalize input
Initialize output
Get pairwise componentwise L1-distances to the input training set  Get regression function and correlation
Scaled predictor
Predictor
Universal Kriging
Ordinary Kriging
Mean Squared Error might be slightly negative depending on  machine precision: force to zero!
Use built-in autocorrelation parameters
Initialize output
Retrieve data
Cholesky decomposition of R
Universal Kriging
Ordinary Kriging
The determinant of R is equal to the squared product of the diagonal  elements of its Cholesky decomposition C
Initialize output
Force optimizer to fmin_cobyla if the model is meant to be isotropic
Use specified starting point as first guess
Backup of the given attributes
This will iterate over fmin_cobyla optimizer
Restore the given attributes
Check regression weights if given (Ordinary Kriging)  Force to column vector
Check correlation parameters
Force verbose type to bool
Force normalize type to bool
Check optimizer
Force random_start type to int
Choose hyperparameters based on maximizing the log-marginal  likelihood (potentially starting from several initial values)
First optimize starting from theta specified in kernel
Precompute quantities required for predictions which are independent  of actual query points
Line 6 (compute np.diag(v.T.dot(v)) via einsum)
Compute log-marginal-likelihood Z and also store some temporaries  which can be reused for computing Z's gradient
Compute gradient based on Algorithm 5.1 of GPML  XXX: Get rid of the np.diag() in the next line
Line 9: (use einsum to compute np.diag(C.T.dot(C))))
Line 12: (R.T.ravel().dot(C.ravel()) = np.trace(R.dot(C)))
theta for compound kernel
Simple optimisation to gain speed (inspect is slow)
vector-valued parameter
convert from upper-triangular matrix to square matrix
Hyperparameter l kept fixed
convert from upper-triangular matrix to square matrix
Hyperparameter l kept fixed
gradient with respect to length_scale
gradient with respect to length_scale
gradient with respect to p
We have to fall back to slow way of computing diagonal
Check that values returned in theta are consistent with  hyperparameter values (being their logarithms)
Check that values of theta are modified correctly
Check addition
Check multiplication
modifiable attributes must not be identical
Test auto-kernel  For WhiteKernel: k(X) != k(X,X). This is assumed by  pairwise_kernels
Test cross-kernel
Test get_params()
Test set_params()
Repeat test_1d and test_2d for several built-in correlation  models specified as strings.
test the MSE estimate to be sane.  non-regression test for ignoring off-diagonals of feature covariance,  testing with nugget that renders covariance useless, only  using the mean function, with low effective rank of data
Checks that optimizer improved marginal likelihood
XXX: quite hacky, works only for current kernels
Fit non-normalizing GP on normalized y  Fit normalizing GP on unnormalized y
Compare predicted mean, std-devs and covariances
Test for fixed kernel that first dimension of 2d GP equals the output  of 1d GP and that second dimension is twice as large
Standard deviation and covariance do not depend on output
Test hyperparameter optimization
Checks that optimizer improved marginal likelihood
Normalize target value  demean y
Choose hyperparameters based on maximizing the log-marginal  likelihood (potentially starting from several initial values)
First optimize starting from theta specified in kernel
Precompute quantities required for predictions which are independent  of actual query points
Support multi-dimensional output of self.y_train_
Compute "0.5 * trace(tmp.dot(K_gradient))" without  constructing the full matrix tmp.dot(K_gradient) since only  its diagonal is required
if x[1] != 1 we should have lapack  how do we do that now?
this one turned up on FreeBSD
Orthogonality of weights  ~~~~~~~~~~~~~~~~~~~~~~~~
Orthogonality of latent scores  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
"Non regression test" on canonical PLS    The results were checked against the R-package plspm
x_weights_sign_flip holds columns of 1 or -1, depending on sign flip  between R and python
x_weights = X.dot(x_rotation)  Hence R/python sign flip should be the same in x_weight and x_rotation  This test that R / python give the same result up to column  sign indeterminacy
2) Regression PLS (PLS2): "Non regression test"  ===============================================  The results were checked against the R-packages plspm, misOmics and pls
x_loadings[:, i] = Xi.dot(x_weights[:, i]) \forall i
Orthogonality of weights  ~~~~~~~~~~~~~~~~~~~~~~~~
Orthogonality of latent scores  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Ensure 1d Y is correctly interpreted
Compare 1d to column vector
causes X[:, -1].std() to be zero
Scaling should be idempotent
Author: Edouard Duchesnay <edouard.duchesnay@cea.fr>  License: BSD 3 clause
check_finite=False is an optimization available only in scipy >=0.12
Inner loop of the Wold algo.  1.1 Update u: the X weights  We use slower pinv2 (same as np.linalg.pinv) for stability  reasons
Mode A regress each X column on y_score  If y_score only has zeros x_weights will only have zeros. In  this case add an epsilon to converge to a more acceptable  solution
Mode A regress each Y column on x_score  2.2 Normalize y_weights
Normalize
feature has low resolution use unique vals
create axis based on percentiles and grid resolution
convert feature_names to list  if not feature_names use fx indices as name
explicit loop so "i" is bound for exception below
compute PD functions
create contour levels for two-way plots
prevent x-axis ticks from overlapping
classification
regression
compute leaf for each sample in ``X``.
mask all which are not in sample mask.
update each leaf (= perform line search)
update predictions (both in-bag and out-of-bag)
update predictions
we only need to fit one tree for binary clf.
create one-hot label encoding
we only need to fit one tree for binary clf.
print the header line
plot verbose info each time i % verbose_mod == 0
adjust verbose frequency (powers of 10)
no inplace multiplication!
add tree to ensemble
if is_classification  is regression
do oob?
self.n_estimators is the number of additional est to fit
if do oob resize arrays or create new if not available
if not warmstart - clear the estimator state
init state
fit initial model - FIXME make sample_weight optional
init predictions
Allow presort to be 'auto', which means True if the dataset is dense,  otherwise it will be False.
Set min_weight_leaf from min_weight_fraction_leaf
perform boosting iterations
subsampling  OOB score before adding this stage
fit next stage of trees
we don't need _make_estimator
for use in inner loop, not raveling the output in single-class case,  not doing input validation.
no yield from in Python2.X
Default implementation
n_classes will be equal to 1 in the binary classification or the  regression case.
no yield from in Python2.X
Retrieve settings
Build estimators
Draw features
Draw samples, using sample weights, and then fit
Draw samples, using a mask, and then fit
Resort to voting
Convert data
Remap output
Check parameters
if max_samples is float:
Free allocated memory, if any
Parallel loop
Advance random state to state after training  the first n_estimators
Default implementation
Check data
Parallel loop
Reduce
Check data
Parallel loop
Reduce
Check data
Parallel loop
Reduce
Check data
Parallel loop
Reduce
Validate or convert input data  Pre-sort indices to avoid that each individual tree of the  ensemble sorts the indices.
Remap output
reshape is necessary to preserve the data contiguity against vs  [:, np.newaxis] that does not.
Check parameters
Free allocated memory, if any
We draw from the random state to get the random state we  would have got if we hadn't used a warm_start.
Collect newly grown trees
Decapsulate classes_ attributes
Default implementation
Check data
Assign chunk of trees to jobs
Parallel loop
Reduce
Check data
Assign chunk of trees to jobs
Parallel loop
Reduce
ensure_2d=False because there are actually unit test checking we fail  for 1d.  Pre-sort indices to avoid that each individual tree of the  ensemble sorts the indices.
Set parameters
Don't instantiate estimators now! Parameters of base_estimator might  still change. Eg., when grid-searching with the nested object syntax.  This needs to be filled by the derived classes.
Compute the number of jobs
Partition estimators between jobs
also load the boston dataset  and randomly permute it
also load the iris dataset  and randomly permute it
Check classification on a toy dataset.
test fit before feature importance
deviance requires ``n_classes >= 2``.
Test GradientBoostingClassifier on synthetic dataset used by  Hastie et al. in ESLII Example 12.7.
Friedman1
Friedman2
Friedman3
Predict probabilities.
check if probabilities are in [0, 1].
derive predictions from probabilities
Test input checks (shape and type of X and y).
X has wrong shape
test if max_features is valid.
Test to make sure random state is set properly.
Test if max features is set properly for floats and str.
test if prediction for last stage equals ``predict``
test if prediction for last stage equals ``predict``
test if prediction for last stage equals ``predict_proba``
test that staged_functions make defensive copies
regressor has no staged_predict_proba
Check model serialization.
Check if we can fit even though all targets are equal.
classifier should raise exception
Check if quantile loss with alpha=0.5 equals lad.
Test with non-integer class labels.
Test with float class labels.
Test with float class labels.
This will raise a DataConversionWarning that we want to  "always" raise, elsewhere the warnings gets ignored in the  later tests, and the tests that check for this warning fail
Test if oob improvement has correct shape.
one for 1-10 and then 9 for 20-100
100 lines for n_estimators==100
Test if warm start equals fit.
Test if warm start equals fit - set n_estimators.
last 10 trees have different depth
Test if fit clears state.
Test if warm start with equal n_estimators does nothing
the last 10 are not zeros
Test if monitor return value works.
Test greedy trees with max_depth + 1 leafs.
Test greedy trees with max_depth + 1 leafs.
Test if ZeroEstimator works for classification.
Test precedence of max_leaf_nodes over max_depth.
Predict probabilities.
derive predictions from probabilities
Check BaseEnsemble methods.
also load the iris dataset  and randomly permute it
also load the boston dataset  and randomly permute it
Trained on sparse format
Check regression for various parameter settings on sparse input.
Trained on sparse format
Trained on dense format
Test that bootstrapping samples generate non-perfect base estimators.
without bootstrap, all trees are perfect on the training set
with bootstrap, trees are no longer perfect on the training set
Test that bootstrapping features may generate duplicate features.
Predict probabilities.
Normal case
Degenerate case, where some classes are missing
Check that oob prediction is a good estimation of the generalization  error.
Test with few estimators
Check that oob prediction is a good estimation of the generalization  error.
Test with few estimators
Check singleton ensembles.
Test that it gives proper exception on deficient input.
Test support of decision_function
Check parallel classification.
Classification
predict_proba
decision_function
Check parallel regression.
Check that bagging ensembles can be grid-searched.  Transform iris into a binary classification task
Grid search with scoring based on decision_function
Check base_estimator and its default values.
Classification
Regression
Test if fitting incrementally with warm start gives a forest of the  right size and the same results as a normal fit.
Test that nothing happens when fitting without increasing n_estimators
modify X to nonsense values, this should not change anything
warm started classifier with 5+5 estimators should be equivalent to  one classifier with 10 estimators
Check using oob_score and warm_start simultaneously fails
Load the iris dataset and randomly permute it
Common random state
Toy sample
Load the iris dataset and randomly permute it
Load the boston dataset and randomly permute it
_samme_proba calls estimator.predict_proba.  Make a mock object so I can control what gets returned.
Make sure that the correct elements come out as smallest --  `_samme_proba` should preserve the ordering in each example.
Check classification on a toy dataset.
Check consistency on dataset iris.
Somewhat hacky regression test: prior to  ae7adc880d624615a34bafdb1d75ef67051b8200,  predict_proba returned SAMME.R values for SAMME.
Check consistency on dataset boston house prices.
Check staged predictions.
AdaBoost classification
AdaBoost regression
Check pickability.
Check variable importances.
Test that it gives proper exception on deficient input.
Test different base estimators.
XXX doesn't work with y_class because RF doesn't support classes_  Shouldn't AdaBoost run a LabelBinarizer?
Flatten y to a 1d array
Trained on sparse format
Trained on dense format
predict
decision_function
predict_log_proba
predict_proba
score
staged_decision_function
staged_predict
staged_predict_proba
staged_score
Verify sparsity of data is maintained during training
Trained on sparse format
Trained on dense format
predict
staged_predict
Check binomial deviance loss.  Check against alternative definitions in ESLII.
Check log odds estimator.
Smoke test for init estimators with sample weights.
skip multiclass
check if predictions match
one-hot encoding
also load the iris dataset  and randomly permute it
also load the boston dataset  and randomly permute it
also make a hastie_10_2 dataset
also test apply
Check consistency on dataset iris.
Check consistency on dataset boston house prices.
Regression models should not have a classes_ attribute.
XXX: Remove this test in 0.19 after transform support to estimators  is removed.
Check with parallel
Weight of each B of size k
For all B of size k  For all values B=b
Compute true importances
Estimate importances with totally randomized trees
Check correctness
csc matrix
non-contiguous targets in classification
csc matrix
No bootstrap
Check that base trees can be grid-searched.
Test that n_classes_ and classes_ have proper shape.
Classification, single output
Classification, multi-output
Create the RTE with sparse=False
Assert that type is ndarray, not scipy.sparse.csr.csr_matrix
Assert that dense and sparse hashers have same array.
Ignore warnings from switching to more power iterations in randomized_svd  test random forest hashing on circles dataset  make sure that it is linearly separable.  even after projected to two SVD dimensions  Note: Not all random_states produce perfect results.
test fit and transform:
Single variable with 4 values
Test precedence of max_leaf_nodes over max_depth.
Test if leaves contain more than leaf_count training examples
test boundary value
drop inner nodes
drop inner nodes
Test if leaves contain at least min_weight_fraction_leaf of the  training set
test both DepthFirstTreeBuilder and BestFirstTreeBuilder  by setting max_leaf_nodes
drop inner nodes
Nothing
Contiguous
csr matrix
csc_matrix
coo_matrix
Check class_weights resemble sample_weights behavior.
Test if class_weight raises errors and warnings when expected.
Invalid preset string
Warning warm_start with preset
Not a list or preset for multi-output
Incorrect length list for multi-output
Test if fit clears state and grows a new forest when warm_start==False.
Test if warm start with equal n_estimators does nothing and returns the  same forest and raises a warning.
If we had fit the trees again we would have got a different forest as we  changed the random state.
Test that the warm start computes oob score when asked.  Use 15 estimators to avoid 'some inputs do not have OOB scores' warning.
Test that oob_score is computed even if we don't need to train  additional trees.
load the iris dataset  and randomly permute it
also load the boston dataset  and randomly permute it
Trained on sparse format
Trained on dense format
Generate train/test data
fit the model
predict scores (the lower, the more normal)
check that there is at most 6 errors (false positive or false negative)
toy sample (the last two samples are outliers)
Test LOF
assert detect outliers:
also load the boston dataset
also load the iris dataset
Test partial dependence for classifier
only 4 grid points instead of 5 because only 4 unique X[:,0] vals
now with our own grid
Test partial dependence for multi-class classifier
Test partial dependence for regressor
Test input validation of partial dependence.
first argument must be an instance of BaseGradientBoosting
Gradient boosting estimator must be fit
wrong ndim for grid
Test partial dependence plot function.
check with str features and array feature names
Test partial dependence plot function input checks.
not fitted yet
first argument must be an instance of BaseGradientBoosting
must be larger than -1
too large feature value
str feature but no feature_names
not valid features value
Test partial dependence plot function on multi-class input.
now with symbol labels
label not in gbrt.classes_
label not provided
here above max_features has no links with self.max_features
ensure_2d=False because there are actually unit test checking we fail  for 1d.  Pre-sort indices to avoid that each individual tree of the  ensemble sorts the indices.
ensure that max_sample is in [1, n_samples]:
code structure from ForestClassifier/predict_proba  Check data
Take the opposite of the scores as bigger is better (here less  abnormal) and add 0.5 (this value plays a special role as described  in the original paper) to give a sense to scores = 0:
Check parameters
Initialize weights to 1 / n_samples
Normalize existing weights
Check that the sample weights sum is positive
Check parameters
Clear any previous fit results
Boosting step
Early termination
Stop if error is zero
Stop if the sum of sample weights has become non-positive
Normalize
Displace zero probabilities so the log is defined.  Also fix negative elements which may occur with  negative sample weights.
Check that algorithm is supported
Fit
Instances incorrectly classified
Error fraction
Stop if classification is perfect
Boost weight using multi-class AdaBoost SAMME.R alg
Only boost the weights if it will fit again  Only boost positive weights
Instances incorrectly classified
Error fraction
Stop if classification is perfect
Boost weight using multi-class AdaBoost SAMME alg
Only boost the weights if I will fit again  Only boost positive weights
The weights are all 1. for SAMME.R
The weights are all 1. for SAMME.R
The weights are all 1. for SAMME.R
The weights are all 1. for SAMME.R
Fit
Fit on the bootstrapped sample and obtain a prediction  for all samples in the training set
Calculate the average loss
Stop if fit is perfect
Discard current estimator only if it isn't the only one
Boost weight using AdaBoost.R2 alg
Evaluate predictions of all estimators
Sort the predictions
Find index of median prediction for each sample
Return median predictions
! /usr/bin/env python  Copyright (C) 2007-2009 Cournapeau David <cournape@gmail.com>                2010 Fabian Pedregosa <fabian.pedregosa@inria.fr>  License: 3-clause BSD
We can actually import a restricted version of sklearn that  does not need the compiled code
Avoid non-useful msg:  "Ignoring attempt to set 'name' (from ... "
For these actions, NumPy is not required, nor Cythonization  They are required to succeed without Numpy for example when  pip is used to install Scikit-learn when Numpy is not yet present in  the system.
Generate Cython sources, unless building from source release
WindowsError is not defined on unix systems
changed target file, recompute hash
Update the hashes dict with the new hash
Save hashes once per module. This prevents cythonizing prev.  files again when debugging broken code in a single file
Check whether this commit is part of a pull request or not  The documentation should be always built when executed from one of the  main branches
This PR does not seem to have any documentation related file changed.
Create a temporary folder for the data fetcher
define grammar of a greeting
If this works, then _ustr(obj) has the same behaviour as str(obj), so  it won't break any existing code.
this line is related to debugging the asXML bug
collapse out indents if formatting is not desired
not a bound method, get info directly from __call__ method
no need for special handling if attribute doesnt exist
no need for special handling if attribute doesnt exist
no need for special handling if attribute doesnt exist
argument cache for optimizing repeated calls when backtracking through recursive expressions
Preserve the defining literal.
remove white space from quote chars - wont work anyway
strip off quotes
replace escaped characters
replace escaped quotes
only got here if no expression matched, raise exception for match that made it the furthest
suppress whitespace-stripping in contained parse expressions, but re-enable it on the Combine itself
flatten t tokens
last resort, just use MatchFirst
try to avoid LR with this extra test
it's easy to get these comment structures wrong - they're very common, so may as well make them available
Python 2 compat
Python 3
u'zh': u'http://zh.wikipedia.org/wiki/Wikipedia',
change the User Agent to avoid being blocked by Wikipedia  downloading a couple of articles should not be considered abusive
The training data folder must be passed as first argument
Split the dataset in training and test set:
Print the classification report
Plot the confusion matrix
the training data folder must be passed as first argument
split the dataset in training and test set:
Print the classification report
Print and plot the confusion matrix
The training data folder must be passed as first argument
Split the dataset in training and test set:
TASK: Build a vectorizer that splits strings into sequence of 1 to 3  characters instead of word tokens
TASK: Build a vectorizer / classifier pipeline using the previous analyzer  the pipeline instance should stored in a variable named clf
TASK: Fit the pipeline on the training set
TASK: Predict the outcome on the testing set in a variable named y_predicted
Print the classification report
Plot the confusion matrix
the training data folder must be passed as first argument
split the dataset in training and test set:
TASK: Build a vectorizer / classifier pipeline that filters out tokens  that are too rare or too frequent
TASK: print the cross-validated scores for the each parameters set  explored by the grid search
TASK: Predict the outcome on the testing set and store it in a variable  named y_predicted
Print the classification report
Print and plot the confusion matrix
Try to override the matplotlib configuration as early as possible
Add any paths that contain templates here, relative to this directory.
generate autosummary even if no references
The suffix of source filenames.
The encoding of source files.
Generate the plots for the gallery
The master toctree document.
General information about the project.
The version info for the project you're documenting, acts as replacement for  |version| and |release|, also used in various other places throughout the  built documents.  The short X.Y version.  The full version, including alpha/beta/rc tags.
There are two options for replacing |today|: either, you set today to some  non-false value, then it is used:  Else, today_fmt is used as the format for a strftime call.
List of documents that shouldn't be included in the build.
List of directories, relative to source directory, that shouldn't be  searched for source files.
The reST default role (used for this markup: `text`) to use for all  documents.
If true, '()' will be appended to :func: etc. cross-reference text.
If true, the current module name will be prepended to all description  unit titles (such as .. function::).
If true, sectionauthor and moduleauthor directives will be shown in the  output. They are ignored by default.
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
The theme to use for HTML and HTML Help pages.  Major themes that come with  Sphinx are currently 'default' and 'sphinxdoc'.
Theme options are theme-specific and customize the look and feel of a theme  further.  For a list of options available for each theme, see the  documentation.
Add any paths that contain custom themes here, relative to this directory.
The name for this set of Sphinx documents.  If None, it defaults to  "<project> v<release> documentation".
A shorter title for the navigation bar.  Default is the same as html_title.
The name of an image file (relative to this directory) to place at the top  of the sidebar.
The name of an image file (within the static path) to use as favicon of the  docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32  pixels large.
Add any paths that contain custom static files (such as style sheets) here,  relative to this directory. They are copied after the builtin static files,  so a file named "default.css" will overwrite the builtin "default.css".
If not '', a 'Last updated on:' timestamp is inserted at every page bottom,  using the given strftime format.
If true, SmartyPants will be used to convert quotes and dashes to  typographically correct entities.
Custom sidebar templates, maps document names to template names.
Additional templates that should be rendered to pages, maps page names to  template names.
If false, no module index is generated.
If false, no index is generated.
If true, the index is split into individual pages for each letter.
If true, links to the reST sources are added to the pages.
If true, an OpenSearch description file will be output, and all pages will  contain a <link> tag referring to it.  The value of this option must be the  base URL from which the finished HTML is served.
If nonempty, this is the file name suffix for HTML files (e.g. ".xhtml").
Output file base name for HTML help builder.
The paper size ('letter' or 'a4').
The font size ('10pt', '11pt' or '12pt').
Grouping the document tree into LaTeX files. List of tuples  (source start file, target name, title, author, documentclass  [howto/manual]).
The name of an image file (relative to this directory) to place at the top of  the title page.
For "manual" documents, if this is true, then toplevel headings are parts,  not chapters.
Documents to append as an appendix to all manuals.
If false, no module index is generated.
to hide/show the prompt in code examples:
The following is used by sphinx.ext.linkcode to provide links to github
Python 2 built-in
make sure that the Agg backend is set before importing any  matplotlib
this script can be imported by nosetest to find tests to run: we should not  impose the matplotlib requirement in that case.
value is another dictionary
Make sure searchindex uses UTF-8 encoding
parse objects
download and initialize the search index
Decode bytes under Python 3
we don't have it cached  cache it for the future
failed to resolve
replace '\' with '/' so it on the web
for some reason, the relative link goes one directory too high up
heading
modules for which we embed links into example code
resize the image
get the last working module name
This is a.b, not e.g. a().b
need to get a in a().b
Join import path to relative path
The following is a list containing all the figure names
The __doc__ is often printed in the example, we  don't with to echo it
create something to replace the thumbnail
Don't embed hyperlinks when a latex builder is used.
Add resolvers for the packages for which we want to show links
patterns for replacement
ensure greediness
embed links after build is finished
HACK: Stop nosetests running setup() above
string conversion routines
GAEL: Toctree commented out below because it creates  hundreds of sphinx warnings  out += ['.. autosummary::', '   :toctree:', '']
Extra mangling domains
local import to avoid testing dependency
Try Python 2 first, otherwise load from Python 3
Python 2 only
range of number of samples (observation) to embed
range of admissible distortions
range of number of samples (observation) to embed
Need an internet connection hence not enabled by default
select only non-identical samples pairs
cross_val_predict returns an array of the same size as `y` where each entry  is a prediction obtained by cross validation:
Note: the following is identical to X[rows[:, np.newaxis], cols].sum() but  much faster in scipy <= 0.16
Author: Matt Terry <matt.terry@gmail.com>  License: BSD 3 clause
Extract the subject & body
Use FeatureUnion to combine the features from subject and body
Pipeline for pulling features from the post's subject line
Pipeline for pulling ad hoc features from post's body
weight components in FeatureUnion
Use a SVC classifier on the combined features
Generate sample data
Add noise to targets
Visualize training and prediction time
Visualize learning curves
Make sure that it X is 2D
Generate toy data.
Fit the huber regressor over a series of epsilon values.
Display results
import some data to play with
avoid this ugly slicing by using a two-dim dataset
shuffle
standardize
Plot the three one-against-all classifiers
The two Lasso implementations on Dense data
The two Lasso implementations on Sparse data
Load the diabetes dataset
Use only one feature
Split the data into training/testing sets
Split the targets into training/testing sets
Create linear regression object
Train the model using the training sets
Plot outputs
Fit the ARD Regression
we create 50 separable points
fit the model
plot the line, the points, and the nearest vectors to the plane
normalize data as done by Lars to allow for comparison
Display results
Display results
Generating simulated data with Gaussian weights
Fit the Bayesian Ridge Regression and an OLS for comparison
Simulate regression data with a correlated design  The Donoho-Tanner phase transition is around n_samples=25: below we  will completely fail to recover in the well-conditioned case
The coefficients of our model
Plot stability selection path, using a high eps for early stopping  of the path, to save computation time
Plot only the 100 first coefficients
run the classifier
classify small against large digits
print the training scores
Plot the three one-against-all classifiers
X is the 10x10 Hilbert matrix
distort the clean signal
generate some sparse data to play with
add noise
Split data in train set and test set
Lasso
ElasticNet
Fit line using all data
Robustly fit linear model with RANSAC algorithm
Predict data of estimated models
Compare estimated coefficients
generate points used to plot
create matrix versions of these arrays
import some data to play with
we create an instance of Neighbours Classifier and fit the data.
Put the result into a color plot
Generate data
Select the optimal percentage of features with grid search
Attempt to remove the temporary cachedir, but don't worry if it fails
Plot the ground truth
Incorrect number of clusters
Different variance
Unevenly sized blobs
Number of run (with randomly generated dataset) for each strategy so as  to be able to compute an estimate of the standard deviation
k-means models can do several random inits so as to be able to trade  CPU time for convergence robustness
Datasets generation parameters
Generate data (swiss roll dataset)  Make it thinner
Define the structure A of the data. Here a 10 nearest neighbors
Generate data  Newer versions of scipy have face in misc
Resize it to 10% of the original size to speed up the processing
Define the structure A of the data. Pixels connected to their neighbors.
Compute clustering
4 circles
We use a mask that limits to the foreground: the problem that we are  interested in here is not separating the objects from the background,  but separating them one from the other.
Convert the image into a graph with the value of the gradient on the  edges.
Take a decreasing function of the gradient: we take it weakly  dependent from the gradient the segmentation is close to a voronoi
Force the solver to be arpack, since amg is numerically  unstable on this example
2 circles
Load the Summer Palace photo
Convert to floats instead of the default 8 bits integer coding. Dividing by  255 is important so that plt.imshow behaves works well on float data (need to  be in the range [0-1]
Load Image and transform to a 2D numpy array.
Generate sample data
Compute Affinity Propagation
Plot result
in this case the seeding of the centers is deterministic, hence we run the  kmeans algorithm only once with n_init=1
Obtain labels for each point in mesh. Use last trained model.
Generate sample data
Compute DBSCAN
Number of clusters in labels, ignoring noise if present.
Plot result
Generate waveform data
Make the noise sparse
load the raccoon face as a numpy array  Newer versions of scipy have face in misc
Resize it to 10% of the original size to speed up the processing
Convert the image into a graph with the value of the gradient on the  edges.
Take a decreasing function of the gradient: an exponential  The smaller beta is, the more independent the segmentation is of the  actual image. For beta=1, the segmentation is close to a voronoi
Apply spectral clustering (this step goes much faster if you have pyamg  installed)
Create a graph capturing local connectivity. Larger number of neighbors  will give more homogeneous clusters to the cost of computation  time. A very large number of neighbors gives more evenly distributed  cluster sizes, but may not impose the local manifold structure of  the data
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward  make connectivity symmetric
Newer versions of scipy have face in misc
create an array from labels and values
original face
compressed face
equal bins face
Generate sample data
The following bandwidth can be automatically detected using
Plot result
2D embedding of the digits dataset
Generating the sample data from make_blobs  This particular setting has one distinct cluster and 3 clusters placed close  together.
Create a subplot with 1 row and 2 columns
Initialize the clusterer with n_clusters value and a random generator  seed of 10 for reproducibility.
The silhouette_score gives the average value for all the samples.  This gives a perspective into the density and separation of the formed  clusters
Compute the silhouette scores for each sample
Aggregate the silhouette scores for samples belonging to  cluster i, and sort them
Label the silhouette plots with their cluster numbers at the middle
The vertical line for average silhouette score of all the values
Labeling the clusters  Draw white circles at cluster centers
Generate sample data
Initialise the different array to all False
Use all colors that matplotlib provides by default.
Compute clustering with Birch with and without the final clustering step  and plot.
Plot result
Example settings
Generate the data
Estimate the covariance
Plot the results
fit a Minimum Covariance Determinant (MCD) robust estimator to data
compare estimators learnt from the full data set with true parameters
Display results
Color samples
spanning a range of possible shrinkage coefficient values
under the ground-truth model, which we would not have access to in real  settings
GridSearch for an optimal shrinkage coefficient
Ledoit-Wolf optimal shrinkage coefficient estimate
OAS coefficient estimate
example settings
computation
fit a Minimum Covariance Determinant (MCD) robust estimator to data  compare raw robust estimates with the true location and covariance
compare estimators learned from the full data set with true  parameters
simulation covariance matrix (AR(1) process)
Circle out the test data
plot error lines showing +/- std. errors of the scores
alpha=0.2 controls the translucency of the fill color
Scale data
Classify using k-NN
get the separating hyperplane
add non-discriminative features
Standard scientific Python imports
Import datasets, classifiers and performance metrics
The digits dataset
To apply a classifier on this data, we need to flatten the image, to  turn the data in a (samples, feature) matrix:
Create a classifier: a support vector classifier
We learn the digits on the first half of the digits
Now predict the value of the digit on the second half:
Put the result into a color plot
figure number
fit the model
plot the line, the points, and the nearest vectors to the plane
fit the model
get the separating hyperplane
plot the parallels to the separating hyperplane that pass through the  support vectors
plot the line, the points, and the nearest vectors to the plane
import some data to play with
avoid this ugly slicing by using a two-dim dataset
title for the plots
Plot the decision boundary. For that, we will assign a color to each  point in the mesh [x_min, x_max]x[y_min, y_max].
Put the result into a color plot
set up dataset
l1 data (only 5 informative features)
set up the plot for each regressor
To get nice curve, we need a large number of iterations to  reduce the variance
plot the decision function
fit the model
Generate sample data
Add noise to targets
figure number
fit the model
get the separating hyperplane
plot the parallels to the separating hyperplane that pass through the  support vectors
Put the result into a color plot
fit the model and get the separating hyperplane
get the separating hyperplane using weighted classes
import some data to play with
avoid this ugly slicing by using a two-dim dataset
we create an instance of SVM and fit out data.
Put the result into a color plot
fit the model
plot the decision function for each datapoint on the grid
evaluate decision function in a grid
plot the scores of the grid  grid_scores_ contains parameter settings and scores  We extract just the scores
plot the line, the points, and the nearest vectors to the plane
Plot the cross-validation score as a function of percentile of features
Compute cross-validation score using 1 CPU
Break up the dataset into non-overlapping training (75%) and testing  (25%) sets.  Only take the first fold.
Try GMMs using different types of covariances.
Since we have class labels for the training data, we can  initialize the GMM parameters in a supervised manner.
Train the other parameters using the EM algorithm.
Plot the test data with crosses
generate random sample, two components
generate spherical data centered on (20, 20)
generate zero centered stretched Gaussian data
concatenate the two datasets into the final training set
fit a Gaussian Mixture Model with two components
as the DP will not use every component it has access to  unless it needs it, we shouldn't plot the redundant  components.
Plot an ellipse to show the Gaussian component
Number of samples per component
Number of samples per component
Fit a Gaussian mixture with EM
Plot an ellipse to show the Gaussian component
as the DP will not use every component it has access to  unless it needs it, we shouldn't plot the redundant  components.
Plot an ellipse to show the Gaussian component
Number of samples per component
Generate random sample following a sine curve
import some data to play with
ANOVA SVM-C  1) anova filter, take 3 best ranked features  2) svm
Load the digits dataset
Plot pixel ranking
The iris dataset
Some noisy data not correlated
Add the noisy data to the informative features
Compare to the weights of an SVM
Loading a dataset
Some noisy data not correlated
Add noisy data to the informative features for make the task harder
Build a classification task using 3 informative features
Create the RFE object and compute a cross-validated score.  The "accuracy" scoring is proportional to the number of correct  classifications
Load the boston dataset.
We use the base estimator LassoCV since the L1 norm promotes sparsity of features.
Set a minimum threshold of 0.25
Reset the threshold till the number of features equals two.  Note that the attribute can be set directly instead of repeatedly  fitting the metatransformer.
Generate sample data
Old versions of scipy have face in the top level package
Convert from uint8 representation with values between 0 and 255 to  a floating point representation with values between 0 and 1.
Load faces data
global centering
local centering
List of the different estimators, whether to center and transpose the  problem, and whether the transformer uses the clustering API.
Adding homoscedastic noise
Adding heteroscedastic noise
Generate sample data
Compute ICA
We can `prove` that the ICA model applies by reverting the unmixing.
For comparison, compute PCA
Generate a signal
Percentage of variance explained for each components
Author: Jake Vanderplas <jakevdp@cs.washington.edu>
histogram 1
Plot all available kernels
import some data to play with
avoid this ugly slicing by using a two-dim dataset
Create color maps
we create an instance of Neighbours Classifier and fit the data.
Put the result into a color plot
Generate sample data
Add noise to targets
Fit regression model
Parameters of the study
Initialize the range of `n_samples`
Generate some structured data
Metrics to collect for the plots
pick one query at random to study query time variability in LSHForest
import some data to play with
avoid this ugly slicing by using a two-dim dataset
Create color maps
Put the result into a color plot
if basemap is available, we'll use it.  otherwise, we'll improvise later...
Get matrices/arrays of species IDs and locations
Plot map of South America with distributions of each species
evaluate only on the land: -9999 indicates ocean
plot contours of the density
Initialize size of the database, iterations and required neighbors.
Set `n_estimators` values
load the data
project the 64-dimensional data to a lower dimension
use grid search cross-validation to optimize the bandwidth
use the best estimator to compute the kernel density estimate
sample 44 new points from the data
turn data into a 4x11 grid
Parameters
Load data
We only take the two corresponding features
Train
Plot the decision boundary
The tree structure can be traversed to compute various properties such  as the depth of each node and whether or not it is a leaf.
If we have a test node
For a group of samples, we have the following common node.
Import the necessary modules and libraries
Fit regression model
Predict
compute the entropies of transduced label distributions
select five digit examples that the classifier is most uncertain about
keep track of indices that we get labels for
labeling 5 points, remote from labeled set
generate ring with inner box
Learn with LabelSpreading
shuffle everything around
Learn with LabelSpreading
calculate uncertainty values for each transduced distribution
pick the top 10 most uncertain labels
plot
step size in the mesh
title for the plots
Plot the decision boundary. For that, we will assign a color to each  point in the mesh [x_min, x_max]x[y_min, y_max].
Put the result into a color plot
Plot also the training points
import some data to play with
Use same random seed for multiple calls to make_multilabel_classification to  ensure same distributions
Display progress logs on stdout
introspect the images arrays to find the shapes (for plotting)
for machine learning we use the 2 data directly (as relative pixel  positions info is ignored by this model)
the label to predict is the id of the person
split into a training and testing set
Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled  dataset): unsupervised feature extraction / dimensionality reduction
Hack to detect whether we are running by the sphinx builder
benchmark throughput
initialize random generator
quotes_historical_yahoo_ochl was named quotes_historical_yahoo before matplotlib 1.4
Choose a time period reasonably calm (not too long ago so that we get  high-tech firms, and before the 2008 crash)
The daily variations of the quotes are what carry most information
Learn a graphical structure from the correlations
standardize the time series: using correlations rather than covariance  is more efficient for structure recovery
We use a dense eigen_solver to achieve reproducibility (arpack is  initiated with random vectors that we don't control). In addition, we  use a large number of neighbors to capture the large-scale structure.
Plot the nodes using the coordinates of our embedding
Add a label to each node. The challenge here is that we want to  position the labels to avoid overlap with other labels
Reconstruction with L2 (Ridge) penalization
Reconstruction with L1 (Lasso) penalization  the best value of alpha was determined using cross validation  with LassoCV
Backward compat for Python 2
Whether or not a model has been fitted
update decision surface if already fitted.
Hack to detect whether we are running by the sphinx builder
Main  Create the vectorizer and limit the number of features to a reasonable  maximum
Iterator over parsed Reuters SGML files.
test data statistics
We will feed the classifier with mini-batches of 1000 documents; this means  we have at most 1000 docs in memory at any time.  The smaller the document  batch, the bigger the relative overhead of the partial fit methods.
Create the data_stream that parses Reuters SGML files and iterates on  documents as a stream.
Main loop : iterate on mini-batches of examples
update estimator with examples in the current mini-batch
Plot fitting times
if basemap is available, we'll use it.  otherwise, we'll improvise later...
choose points associated with the desired species
Load the compressed data
Set up the data grid
The grid in x,y coordinates
We'll make use of the fact that coverages[6] has measurements at all  land points.  This will help us decide between land and water.
Fit, predict, and plot for each species.
Standardize features
Predict species distribution using the training data
We'll predict only for the land points.
plot contours of the prediction
stop after 5M links to make it possible to work in RAM
Load the faces datasets
Test on a subset of people
Plot the completed faces
Estimate the score on the entire dataset, with no missing values
Standard scientific Python imports
Import datasets, classifiers and performance metrics
The digits dataset
To apply an classifier on this data, we need to flatten the image, to  turn the data in a (samples, feature) matrix:
We learn the digits on the first half of the digits
Now predict the value of the digit on the second half:
Create a classifier: a support vector classifier
plot the results:  second y axis for timeings
vertical line for dataset dimensionality = 64
visualize the decision surface, projected down to the first  two principal components of the dataset
predict and plot  Plot the decision boundary. For that, we will assign a color to each  point in the mesh [x_min, x_max]x[y_min, y_max].
Put the result into a color plot
Plot also the training points
Plot the PCA spectrum
iterate over datasets  preprocess dataset, split into training and test part
Put the result into a color plot
Load Data
Models we will use
Hyper-parameters. These were set by cross-validation,  using a GridSearchCV. Here we are not performing cross-validation to  save time.  More components tend to give better prediction performance, but larger  fitting time
Training RBM-Logistic Pipeline
Training Logistic regression
rescale the data, use the traditional train/test split
mlp = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=400, alpha=1e-4,                      algorithm='sgd', verbose=10, tol=1e-4, random_state=1)
Next line to silence pyflakes.
Variables for manifold learning.
Create our sphere.
Plot our dataset.
compatibility matplotlib < 1.0
Perform Locally Linear Embedding Manifold learning
don't show points that are too close
Random 2D projection using a random unitary matrix
Spectral embedding of the digits dataset
t-SNE embedding of the digits dataset
Center the data
Rotate the data
Next line to silence pyflakes. This import is needed.
This import is needed to modify the way figure behaves
Create classifiers
Train uncalibrated random forest classifier on whole train and validation  data and evaluate on test data
split train, test for calibration
Gaussian Naive-Bayes with no calibration
Gaussian Naive-Bayes with isotonic calibration
Gaussian Naive-Bayes with sigmoid calibration
Create dataset of classification task with many redundant and few  informative features
Calibrated with isotonic calibration
Calibrated with sigmoid calibration
Logistic regression with no calibration as baseline
Plot calibration curve for Gaussian Naive Bayes
Plot calibration curve for Linear SVC
Evaluate the models using crossvalidation
Display progress logs on stdout
define a pipeline combining a text feature extractor with a simple  classifier
uncommenting more parameters will give better exploring power but will  increase processing time in a combinatorial way
find the best parameters for both the feature extraction and the  classifier
Generate sample data
Split train and test data
Estimate the coef_ on full data with optimal regularization parameter
import some data to play with
setup plot details
Binarize the output
Add noisy features
Split into training and test
Run classifier
get some data
build a classifier
run randomized search
run grid search
import some data to play with
Split the data into a training set and a test set
Run classifier, using a model that is too regularized (C too low) to see  the impact on the results
Compute confusion matrix
import some data to play with
Add noisy features
Run classifier with cross-validation and plot ROC curves
Loading the Digits dataset
To apply an classifier on this data, we need to flatten the image, to  turn the data in a (samples, feature) matrix:
Split the dataset in two equal parts
Import some data to play with
Binarize the output
Add noisy features to make the problem harder
shuffle and split training and test sets
Learn to predict each class against the other
First aggregate all false positive rates
Then interpolate all ROC curves at this points
Finally average it and compute AUC
Cross validation with 100 iterations to get smoother mean test and train  score curves, each time with 20% data randomly selected as a validation set.
This dataset is way to high-dimensional. Better do PCA:
Maybe some original features where good, too?
Use combined features to transform dataset:
A few constants
Observations
medium term irregularity
medium term irregularities
import some data to play with
Plot the predicted probabilities. For that, we will assign a color to  each point in the mesh [x_min, m_max]x[y_min, y_max].
Generate sample data
Specify Gaussian Process
Specify Gaussian Processes with fixed and optimized hyperparameters
First the noiseless case
Observations
Mesh the input space for evaluations of the real function, the prediction and  its MSE
Instanciate a Gaussian Process model
Fit to data using Maximum Likelihood Estimation of the parameters
Make the prediction on the meshed x-axis (ask for MSE as well)
now the noisy case
Instanciate a Gaussian Process model
Fit to data using Maximum Likelihood Estimation of the parameters
Make the prediction on the meshed x-axis (ask for MSE as well)
2 latents vars:
Transform data  ~~~~~~~~~~~~~~
each Yj = 1*X1 + 2*X2 + noize
compare pls2.coef_ with B
note that the number of components exceeds 1 (the dimension of y)
Create and fit an AdaBoosted decision tree
importing necessary libraries
Fit regression model
Predict
predict class probabilities for all classifiers
get class probabilities for the first sample in the dataset
Number of cores to use to perform parallel fitting of the forest model
Load the faces dataset
Build a forest and compute the pixel importances
Plot pixel importances
compute test set deviance
A learning rate of 1. may not be optimal for both SAMME and SAMME.R
Boosting might terminate early, but the following arrays are always  n_estimators long. We crop them to the actual number of trees here:
prevent overlapping y-axis labels
First the noiseless case
Observations
Mesh the input space for evaluations of the real function, the prediction and  its MSE
Make the prediction on the meshed x-axis
Make the prediction on the meshed x-axis
Make the prediction on the meshed x-axis
Loading some example data
Estimate best n_estimator using cross-validation
Compute best n_estimator for test data
negative cumulative sum of oob improvements
min loss according to OOB
min loss according to test (normalize such that first loss is 0)
min loss according to cv (normalize such that first loss is 0)
Predict on new data
It is important to train the ensemble of trees on a different subset  of the training data than the linear regression model to avoid  overfitting, in particular if the total number of leaves is  similar to the number of training samples
Unsupervised transformation based on totally random trees
The gradient boosted model by itself
The random forest model by itself
Parameters
Load data
We only take the two corresponding features
Shuffle
Standardize
Train
Add a title at the top of each column
Build a classification task using 3 informative features
Build a forest and compute the feature importances
Print the feature ranking
make a synthetic dataset
use RandomTreesEmbedding to transform data
Visualize result using PCA
Learn a Naive Bayes classifier on the transformed data
Learn an ExtraTreesClassifier for comparison
scatter plot of original and reduced data
transform grid using RandomTreesEmbedding
transform grid using ExtraTreesClassifier
split 80/20 train-test
Needed on Windows because plot_partial_dependence uses multiprocessing
map labels from {-1, 1} to {0, 1}
compute test set deviance
clf.loss_ assumes that y_test[i] in {0, 1}
fit the model
Change this for exploring the bias-variance decomposition of other  estimators. This should work well for estimators with high variance (e.g.,  decision trees or KNN), but poorly for estimators with low variance (e.g.,  linear models).
Generate data
Loop over estimators to compare  Compute predictions
Bias^2 + Variance + Noise decomposition of the mean squared error
Generate a binary classification dataset.
Map a classifier name to a list of (<n_estimators>, <error rate>) pairs.
Range of `n_estimators` values to explore.
Record the OOB error for each `n_estimators=i` setting.
Generate the "OOB error rate" vs. "n_estimators" plot.
Uncomment the following line to use a larger set (11k+ documents)
Display progress logs on stdout
Perform an IDF normalization on the output of HashingVectorizer
Vectorizer results are normalized, which makes KMeans behave as  spherical k-means for better results. Since LSA/SVD results are  not normalized, we have to redo the normalization.
Show confusion matrix
Display progress logs on stdout
Load some categories from the training set
split a training set and a test set
mapping from integer feature name to original token string
keep selected feature names
Train Liblinear model
Train SGD model
Train SGD with Elastic Net penalty
Train NearestCentroid without threshold
parameters
compute the same step_size than in LR-sag
Split training and testing. Switch train and test subset compared to  LYRL2004 split, to have a larger training dataset.
to store the results
start time  stop time
start time  stop time
Memoize the data extraction and memory map the resulting  train / test splits in readonly mode
Nomenclature in the function follows Lee & Seung
plot the actual surface  dummy point plot to stick the legend to since surface plot do not  support legends (yet?)
start time  stop time
start time  stop time
Option parser
Generate dataset
Set transformer input
Set GaussianRandomProjection input
Set SparseRandomProjection input
Perform benchmark
Shuffle data
add noise
Keep the last samples as held out query vectors: note since we used  shuffle=True we have ensured that index and query vectors are  samples from the same distribution (a mixture of 100 gaussians in this  case)
Initialize index sizes
Plot precision
Plot speed up
If this is enabled, tests are much slower and will crash with the large data
Determine when to switch to batch computation for matrix norms,  in case the reconstructed (dense) matrix is too large
The following datasets can be dowloaded manually from:  CIFAR 10: http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz  SVHN: http://ufldl.stanford.edu/housenumbers/train_32x32.mat
There is a different convention for l here
s = sp.linalg.norm(A, ord=2)   slow
If we're not plotting, dump the timing to stdout
plot the actual surface
dummy point plot to stick the legend to since surface plot do not  support legends (yet?)
RandomizedPCA error is always worse (approx 100x) than other PCA  tests
limit dataset to 5000 people (don't care who they are!)
Memoize the data extraction and memory map the resulting  train / test splits in readonly mode
Normalize features
start time  stop time
Delayed import of matplotlib.pyplot
plot the actual surface  dummy point plot to stick the legend to since surface plot do not  support legends (yet?)
Author: Mathieu Blondel <mathieu@mblondel.org>  License: BSD 3 clause
we remove data with label 4  normal data are then those of class 1
normal data are those with attribute 2  abnormal those with attribute 4
start time  stop time
Option parser
List sampling algorithm  We assume that sampling algorithm has the following signature:    sample(n_population, n_sample)
Set Python core input
Set custom automatic method selection
Set custom tracking based method
Set custom reservoir based method
Set custom reservoir based method
Numpy permutation based
Remove unspecified algorithm
Perform benchmark